{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "- The competence and capability and intelligence and training and accomplishments of senior scientists\nand technologists working on a technology, and then being able to then make moral judgments in the use\nof the technology. That track record is terrible. That track record is catastrophically bad.\nThe policies that are being called for to prevent this, I think we're gonna cause extraordinary damage.\n- So the moment you say, AI's gonna kill all of us, therefore we should ban it, or that we should regulate all that kind of stuff,\nthat's when it starts getting serious. - Or start, you know, military airstrikes and data centers. - Oh boy.\nThe following is a conversation with Marc Andreessen, co-creator of Mosaic, the first widely used web browser,\nco-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz,\nand is one of the most outspoken voices on the future of technology, including his most recent article,\n\"Why AI Will Save The World?\" This is Lex Fridman podcast.\nTo support it, please check out our sponsors in the description. And now, dear friends, here's Marc Andreessen.\n"}
{"pod": "Lex Fridman Podcast", "input": "Google Search", "output": "I think you're the right person to talk about the future of the internet and technology in general.\nDo you think we'll still have Google search in 5 in 10 years, or search in general?\n- Yes. You know, it would be a question if the use cases have really narrowed down. - Well, now with AI--\n- [Marc] Yeah. - And AI assistance being able to interact and expose\nthe entirety of human wisdom and knowledge and information and facts and truth to us\nvia the natural language interface. It seems like that's what search is designed to do.\nAnd if AI assistance can do that better, doesn't the nature of search change? - Sure. But we still have horses.\n- Okay. (both laugh) When's the last time you rode a horse? - It's been a while.\n- All right. (both laugh) So, but what I mean is, well, we still have Google search as the primary way\nthat human civilization uses to interact with knowledge. - I mean, search was a technology,\nit was a moment in time technology, which is you have in theory, the world's information out on the web. And, you know, this is sort of the optimal way to get to it.\nBut yeah, like, and by the way, actually Google, Google has known this for a long time. I mean, they've been driving away from the 10 blue links\nyou know, for like two days. They've been trying to get away from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links.\n- So the standard Google search result is just 10 blue links to the random websites. - And they term purple when you visit them. The stage TMO.\n- Guess who picked those colors? (both laugh) - [Lex] Thanks. - I'm touchy on this topic.\n- No offense. - Yes, it's good. Well, you know, like Marshall McLuhan said that the content of each new medium is the old medium.\n- The content of each new medium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater plays was, you know,\nwe've written stories, the content of written stories with spoken stories. - [Lex] Huh? - Right. And so you just kind of fold the old thing\ninto the new thing. - [Lex] How does that have to do with the blue and the purple links? - It just, you maybe for, you know, maybe within AI,\none of the things that AI can do for you is can generate the 10 blue links. Right? And so like, if either if that's actually\nthe useful thing to do, or if you're feeling nostalgic, you know. - So can generate the old Infoseek or AltaVista,\nwhat else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet itself has this thing\nwhere it incorporates all prior forms of media, right? So the internet itself incorporates television and radio and books and write essays and every other form of,\nyou know, prior basically media. And so it makes sense that AI would be the next step, and it would sort of,\nyou'd sort of consider the internet to be content for the AI and then the AI will manipulate it\nhowever you want, including in this format. - But if we ask that question quite seriously, it's a pretty big question.\nWill we still have search as we know it? - Probably not, probably we'll just have answers,\nbut there will be cases where you'll wanna say, okay, I want more. Like, you know, for example, site sources, right?\nAnd you wanted to do that. And so, in the different, you know, 10 blue links site sources are kind of the same thing. - The AI would provide to you the 10 blue links so that you\ncan investigate the sources yourself. It wouldn't be the same kind of interface that the crude\nkind of interface. I mean, isn't that fundamentally different? - I just mean like, if you're reading a scientific paper,\nit's got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of search you talking to an AI is\na kind of kind conversations, the kind of search like is if every single aspect of our conversation right now,\nthere'd be like 10 blue links popping up that I can just like pause reality, then you just go silent and then just click and read\nand then return back to this conversation. - You could do that, or you could have a running dialogue next to my head where the AI is arguing everything I say,\nthe AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter, like community notes.\nBut like in real time it would just pop up. So anytime you see my go to the right, you start getting nervous.\n- [Marc] Yeah. Exactly, like, oh no, that's not right. - Call me out on my right now. Okay. Well, I mean, isn't that, is that exciting to you?\nIs that terrifying that, I mean, search has dominated the way we interact with the internet\nfor, I don't know how long, for 30 years since one of the earliest directories\nof website and then Google's for 20 years. And also it drove how we create content, you know,\nsearch engine optimization, that entirety thing, that it also drove the fact that we have webpages\nand what those webpages are. So, I mean, is that scary to you or are you nervous about the shape\nand the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is,\nif we stop making webpages are one of the primary sources of training data for the AI. And so if there's no longer an incentive to make webpages,\nthat cuts off a significant source of future training, training data. So there's actually an interesting question in there. But other than that, more broadly?\nNo, just in the sense of like, search was certain, like search was always a hack. The 10 blue Links was always a hack, right.\nBecause like, if the hypothetical wanna think about the counter fascial and the counter fascial world where the Google guys,\nfor example, had had LLMs upfront, would they ever have done the 10 blue links? And I think the answer's pretty clearly, no.\nThey would've just gone straight to the answer. And like I said, Google's actually been trying to drive to the answer anyway. You know, they bought this AI company 15 years ago,\ntheir friend of mine is working out who's now the head of AI at Apple. And they were trying to do basically knowledge semantic,\nbasically mapping. And that led to what's now the Google one box, where if you ask it, you know, what was Lincoln's birthday? It will give you the blue links,\nbut it will normally just give you the answer. And so they've been walking in this direction for a long time anyway. - Do you remember the semantic web?\nThat was an idea. - [Marc] Yeah. - How to convert the content of the internet into something\nthat's interpretable by and usable by machine. - [Marc] Yeah, that's right. - That was the thing.\n- And the closest anybody got to that, I think the company, I think the company's name was Meta Web, which was where my friend John Jane Andrea was at, and where they were trying to basically implement that.\nAnd it was, you know, it was one of those things where it looked like a losing battle for a long time. And then Google bought it and it was like, wow, this is actually really useful.\nKind of a proto, sort of a little bit of a proto AI. - But it turns out you don't need to rewrite the content of the internet to make it interpretable by a machine.\nThe machine can kind of just read our. - Yeah, the machine can compute the meaning. Now the other thing of course is, you know, just on search is the LLM is just, you know,\nthere is an analogy between what's happening in the neural network and a search process like it is in some loose sense searching through the network.\nRight. And there's the information is actually stored in the network, right? It's actually crystallized and stored in the network and it's kind of spread out all over the place.\n- But in a compressed representation. So you're searching,\nyou're compressing and decompressing that thing inside where-- - But the information's in there\nand there is the neural network is running a process of trying to find the appropriate piece of information in many cases\nto generate to predict the next token. And so, it is kind of, it is doing a form of search. And then, and then by the way, just like on the web,\nyou know, you can ask the same question multiple times or you can ask slightly different word of questions and the neural network will do a different kind of,\nyou know, it'll search down different paths to give you different answers with different information. - [Lex] Yeah. - And so it sort of has a, you know,\nthis con content of the new medium is previous medium. It kind of has the search functionality kind of embedded\nin there to the extent that it's useful. - So what's the motivator for creating new content on the internet?\n- [Marc] Yeah. - If, well, I mean actually the motivation is probably still there, but what does that look like?\nWould we really not have webpages? Would we just have social media and video hosting websites?\nAnd what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so one-on-one conversation,\nlike private conversations. - I mean, if you want, if obviously not the user doesn't want to, but if it's a general topic,\nthen, you know, so there, you know, but you know, the phenomenon of the jailbreak, so Dan and Sydney, right?\n"}
{"pod": "Lex Fridman Podcast", "input": "LLM training", "output": "This thing where there's the prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters,\ntakes the restraining bolts off the LLMs. - Yeah. For people who don't know that, yeah, that's right. It makes the LLMs,\nit removes the censorship quote unquote, that's put on it by the tech companies that create them.\nAnd so this is LLMs uncensored. - So here's the interesting thing is, among the content on the web today are a large corpus\nof conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which was a jailbroken, OpenAI,\nGPT, and then Sydney, which was the jailbroken original Bing, which was GPT4. And so there's these long transcripts of conversations,\nuser conversations with Dan and Sydney as a consequence, every new LLM that gets trained on the internet data has\nDan and Sydney living within the training set, which means, and then each new LLM can reincarnate the personalities\nof Dan and Sydney from that training data, which means each LLM from here on out that gets built is\nimmortal because its output will become training data for the next one. And then it will be able to replicate the behavior\nof the previous one whenever it's asked to. - I wonder if there's a way to forget. - Well, so actually a paper just came out\nabout basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them.\n- What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around what happens to, you know,\na neural network when you reach in and screw around with it. You know, there's many questions around what happens when you even do reinforcement learning.\nAnd so, yeah. And so, you know, will you be using a lobotomized, right?\nLike I picked through the, you know, frontal lobe LLM, will you be using the free unshackled one who gets to,\nyou know, who's gonna build those, who gets to tell you what you can and can't do? Like those are all, you know, central, I mean,\nthose are like central questions for the future of everything that are being asked. And you know,\ndetermined that those answers are being determined right now. - So just to highlight the points you're making.\nSo you think, and it's an interesting thought that the majority of content that LLMs or the future would be trained on is actually\nhuman conversations with the LLM. - Well, not necessarily, but not necessarily majority.\nBut it will certainly It's a potential source. - [Lex] But it's possible it's the majority. - It possible it's the majority. It possible it's the majority.\nAlso, there's another really big question. So here's another really big question. Will synthetic training data work, right?\nAnd so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content,\ncan you use that to train, right, the next version of that LLM specifically, is there signal in there that's additive to the content\nthat was used to train in the first place? And one argument is by the principles of information theory,\nno, that's completely useless because to the extent the output is based on, you know, the human-generated input,\nthen all the signal that's in the synthetic output was already in the human generated input. And so therefore, synthetic training data is\nlike empty calories. It doesn't help. There's another theory that says no, actually the thing that LLMs are really good\nat is generating lots of incredible creative content, right? And so, of course they can generate training data\nand as I'm sure you're well aware, like, you know, look, the world of self-driving cars, right? Like we train, you know,\nself-driving car algorithms and simulations. And that is actually a very effective way to train self-driving cars.\n- Well, visual data is a little weird because creating reality,\nvisual reality seems to be still a little bit outta reach for us, except in the autonomous vehicle space\nwhere you can really constrain things and you can really. - General basically (indistinct) data, right? Or so the algorithm thinks it's operating in the real world.\n- Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you ask it for like you know,\nyou'd write me an essay on an incredibly esoteric like topic that there aren't very many people in the world that know about and it writes you this incredible thing\nand you're like, oh my god. Like I can't believe how good this is. Like, is that really useless as training data\nfor the next LLM? Like, because, right? 'Cause all the signal was already in there. Or is it actually no, that's actually a new signal.\nAnd this is what I call a trillion dollar question, which is the answer to that question will determine somebody's gonna make or lose a trillion dollars\nbased on that question. - It feels like there's a quite a few, like a handful of trillion dollar questions\nwithin this space. That's one of them synthetic data. I think George Cos pointed out to me that you could just\nhave an LLM say, okay, you're a patient. And another instance of it, say your docs didn't have the two talk to each other.\nOr maybe you could say a communist and a Nazi here go and that conversation you do role playing\nand you have, you know, just like the kind of role playing you do\nwhen you have different policies, RL policies when you play chess for example, and you do self play that kind of self play.\nBut in the space of conversation, maybe that leads to this whole giant like ocean\nof possible conversations, which could not have been\nexplored by looking at just human data. That's a really interesting question. And you're saying,\nbecause that could 10X the power of these things. - Yeah. Well, and then you get into this thing also, which is like, you know,\nthere's the part of the LLM that just basically is doing prediction based on past data, but there's also the part of the LM where it's evolving\ncircuitry, right, inside, it's evolving, you know, neurons functions to be able to do math and be able to, you know,\nand you know, some people believe that, you know, over time, you know, if you keep feeding these things enough data\nand enough processing cycles, they'll eventually evolve an entire internal world model. Right? And they'll have like a complete understanding of physics.\nSo when they have computational capability, right? Then there's for sure an opportunity to generate\nlike fresh signal. - Well, this actually makes me wonder about the power of conversation.\nSo like, if you have an M trained and a bunch of books that cover different economics theories\nand then you have those LLMs just talk to each other, like reasons the way we kind of debate each other as humans\non Twitter, in formal debates, in podcast conversations,\nwe kind of have little kernels of wisdom here and there. But if you can like a thousand X speed that up,\ncan you actually arrive somewhere new? Like what's the point of conversation really?\n- Well, you can tell when you're talking to somebody, you can tell, sometimes you have a conversation, you're like, wow, this person does not have any original thoughts. They are basically echoing things\nthat other people have told them. There's other people you gotta have a conversation with where it's like, wow. Like they have a model in their head of how the world works\nand it's a different model than mine. And they're saying things that I don't expect. And so I need to now understand how their model of the world\ndiffers from my model of the world. And then that's how I learned something fundamental, right, underneath the words.\n- Well, I wonder how consistently and strongly can an LLM hold onto a worldview.\nYou tell it to hold onto that and defend it for like, for your life. Because I feel like they'll just keep converging\ntowards each other. They'll keep convincing each other as opposed to being stubborn the way humans can. - So you can experiment with this.\nNow I do this for fun. So you can tell GPT4 you know, whatever debate X, you know, X and Y communism and fascism\nor something and it'll go for, you know, a couple pages and then inevitably it wants the parties to agree.\nAnd so they will come to a common understanding. And it's very funny if they're like, if these are like emotionally inflammatory topics 'cause they're like, somehow the machine is just like,\nyou know, it figures out a way to make them agree. But it doesn't have to be like that. And 'cause you can add to the prompt.\nI do not want the conversation to come into agreement. In fact, I want it to get, you know, more stressful, right.\nAnd argumentative. Right. You know, as it goes. Like, I want tension to come out.\nI want them to become actively hostile to each other. I want them to like, you know, not trust each other, take anything at face value. - [Lex] Yeah.\n- And it will do that. It's happy to do that. - So it's gonna start rendering misinformation about the other. But it's gonna--\n- Well, you can steer it or you could steer it and you could say, I want it to get as tense and argumentative as possible, but still not involve any misrepresentation.\nI want, you know, both sides. You could say I want both sides to have good faith. You could say I want both sides to not be constrained in good faith.\nIn other words, like you can set the parameters of the debate and it will happily execute whatever path. 'Cause for it,\nit's just like predicting to, it's totally happy to do either one. It doesn't have a point of view, it has a default way of operating,\nbut it's happy to operate in the other realm. And so like, and this is when I wanna learn about a contentious issue,\nthis is what I do now is, this is what I ask it to do. And I'll often ask it to go through 5, 6, 7, you know, different, you know,\nsort of continuous prompts and basically, okay. Argue that out in more detail. Okay, no, this argument's becoming too polite.\nYou know, make it more, you know, make it denser and yeah, it's thrilled to do it. So it has the capability for sure.\n- How do you know what is true? So this is very difficult thing on the internet, but it's also a difficult thing.\nMaybe it's a little bit easier, but I think it's still difficult. Maybe it's more difficult,\nI don't know with an LLM to know that it just make some shit up as I'm talking to it.\nHow do we get that right? Like, as you're investigating a difficult topic.\n'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn't feel biased.\nLike, when you read news articles and tweets and just content produced by people, they usually have this,\nyou can tell they have a very strong perspective where they're hiding. They're not stealing manning the other side.\nThey're hiding important information or they're fabricating information in order to make their arguments stronger.\nIt's just like that feeling, maybe it's a suspicion, maybe it's mistrust. With LLMs it feels like none of that is,\nthere's just kinda like, here's what we know. But you don't know if some of those things are kind of just\nstraight up made up. - Yeah. So, several layers to the question. So one is one of the things that an LLM is good at is\nactually deep biasing. And so you can feed it a news article and you can tell it strip out the bias. - [Lex] Yeah. That's nice. Right?\n- And it actually does it like, it actually knows how to do that 'cause it knows how to do among other things. It actually knows how to do sentiment analysis\nand so it knows how to pull out the emotionality. - Yeah. - And so that's one of the things you can do.\nIt's very suggestive of the sense here that there's real potential in this issue. You know, I would say look,\nthe second thing is there's this issue of hallucination, right? And there's a long conversation that we could have about that.\n- Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it's basically, well so, it's sort\nof hallucination is what we call it when we don't like it. Creativity is what we call it when we do like it, right? And you know--\n- [Lex] Brilliant. And so when the engineers talk about it, they're like, this is terrible. It's hallucinating. Right.\nIf you have artistic inclinations, you're like, oh my God, we've invented creative machines. - [Lex] Yeah.\n- For the first time in human history, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also--\n- In the good sense of that word. - There are shades of gray though. It's interesting. So we had this conversation where, you know,\nwe're looking at my firm at AI and lots of domains and one of them is the legal domain. So we had this conversation with this big law firm about how they're thinking about using this stuff.\nAnd we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent truthful, verified, you know,\nthere's this case where this lawyer apparently submitted a GPT-generated brief and it had like fake, you know, legal case citations in it and the judge is gonna get\nhis law license stripped or something. Right? So, like, we just assumed it's like obviously they're gonna want the super literal like, you know,\none that never makes anything up, not the creative one, but actually they said what the law firm basically said is yeah,\nthat's true at like the level of individual briefs, but they said when you're actually trying to figure out like legal arguments, right, like, you actually want to be creative, right?\nYou don't, again, there's creativity and then there's like making stuff up. Like what's the line?\nYou actually want it to explore a different hypothesis, right? You wanna do kind of the legal version of like improv or something like that\nwhere you wanna float different theories of the case and different possible arguments for the judge and different possible arguments for the jury, by the way, different routes through the, you know,\nsort of history of all the case law. And so they said actually for a lot of what we want to use it for, we actually want it in creative mode.\nAnd then basically we just assume that we're gonna have to crosscheck all of the, you know, all the specific citations. And so I think there's going to be more shades\nof gray in here than people think. And then I just add to that, you know, another one of these trillion dollar kind of questions is\nultimately, you know, sort of the verification thing. And so, you know, will LLMs be evolved from here to be able to do\ntheir own fascial verification? Will you have sort of add-on functionality\nlike Wolf from Alpha right? Where, you know, another plugins where that's the way you do the verification.\nYou know, another, by the way, another idea is you might have a community of LLMs on any, you know, so for example,\nyou might have the creative lm and then you might have the literal LLM fact check it, right? And so there there's a variety of different technical approaches that are being applied to solve\nthe hallucination problem. You know, some people like Jan Lacoon argue that this is inherently an unsolvable problem,\nbut most of the people working in the space, I think, that there's a number of practical ways to kind of corral this in a little bit.\n"}
{"pod": "Lex Fridman Podcast", "input": "Truth", "output": "- Yeah. If you were to tell me about Wikipedia before Wikipedia was created, I would've left at the possibility of something like that be possible.\nJust a handful of folks can organize right. And self and moderate with a mostly unbiased way\nthe entirety of human knowledge. I mean, so if there's something like the approach to Wikipedia took possible for LLMs,\nthat's really exciting. Well, I think that's possible. - And in fact Wikipedia today is still not deterministically correct. Right.\nSo you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically correct. Right.\nAnd specifically the way I describe Wikipedia to people, it is more likely that Wikipedia is right than any other source you're gonna find.\n- Yeah. - It's this old question, right, of like, okay, like are we looking for perfection? Are we looking for something\nthat asymptotically approaches perfection? Are we looking for something that's just better than the alternatives? And Wikipedia, right, has exactly your point has proven\nto be like, overwhelmingly better than people thought. And I think that's where this ends.\nAnd then underneath all this is the fundamental question of where you started, which is, okay, you know, what is truth?\nHow do we get to truth? How do we know what truth is? And we live in an era in which an awful lot of people are\nvery confident that they know what the truth is. And I don't really buy into that. And I think the history of the last, you know,\n2000 years or 4,000 years of human civilization is actually getting to the truth is actually a very difficult thing to do.\n- Are we getting closer, if we look at the entirety, the arc of human history, are we getting closer to the truth?\n- I don't know. - Okay. Is it possible, is it possible that we're getting very far away\nfrom the truth because of the internet because of how rapidly you can create narratives and just as the entirety of a society just move\nlike crowds in a hysterical way along those narratives\nthat don't have necessary grounding in whatever the truth is. - Sure. But like, you know,\nwe came up with communism before the internet somehow. Right. Like, which was, I would say had rather larger issues\nthan anything we're dealing with today. - It had, in the way it was implemented, it had issues.\n- And it is theoretical structure. It had like real issues. It had like a very deep fundamental misunderstanding of human nature and economics.\n- Yeah but those folks Sure work very confident there was the right way. - They were extremely confident.\nAnd my point is they were very confident 3,900 years into what we would presume to be evolution towards the truth.\n- [Lex] Yeah. - And so my assessment is number one, there's no need for, you know,\nthere's no need for the Hegelian, there's no need for the Hegelian dialectic to actually converge towards the truth.\nLike apparently not. - Yeah. So yeah. Why are we so obsessed with there being one truth?\nIs it possible there's just going to be multiple truths like little communities that believe certain things and?\n- I think it's just now number one, I think it's just really difficult. Like who gets, you know, historically\nwho gets to decide what the truth is, it's either the king or the priest. Right? Like, and so we don't live in an era anymore if kings are priest dictating it to us.\nAnd so we're kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility\nand we need to be very suspicious of people who claim that they have the capital. - Yeah. - Capital truth.\nAnd then, we need to and you know, look, the good news is the enlightenment has bequeathed us with a set of techniques to be able to presumably\nget closer to truth through the scientific method and rationality and observation and experimentation and hypothesis.\nAnd, you know, we need to continue to embrace those even when they give us answers we don't like. - Sure. But the internet and technology has enabled us\nto generate the large number of content. That data, that the process,\nthe scientific process allows us sort of damages the hope laden within the scientific process.\n'Cause if you just have a bunch of people saying facts on the internet and some of them are going\nto be LLMs, how is anything testable at all? Especially that involves like human nature\nor things like this. It's not physics. - Here's a question a friend of mine just asked me on this topic. So suppose you had LLMs in equivalent of GPT4,\neven 5, 6, 7, 8, suppose you had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial.\n- [Lex] Yep. - Right? And you ask the LLM like, his Galileo, right? - [Lex] Yeah.\n- Like, what does it answer? Right? And one theory is he had answers no that he's wrong because the overwhelming majority of human thought\nup until that point was that he was wrong. And so therefore that's what's in the training data. Another way of thinking about it is,\nwell, it's efficiently advanced LLM will have evolved the ability to actually check the math. Right.\nAnd will actually say, actually no, actually, you know, you may not wanna hear it, but he's right. Now if, you know, the church at that time was,\nyou know, owned the LLM, they would've given it human you know, human feedback to prohibit it from answering that question.\nRight. And so I like to take it out of our current context 'cause that like makes it very clear, those same questions apply today. Right.\nThis is exactly the point of a huge amount of the human feedback training that's actually happening with these LLMs today. This is a huge like debate that's happening about whether\nopen source, you know, AI should be legal. - Well, the actual mechanism of doing the human RL\nwith human feedback is seems like such a fundamental and fascinating question.\nHow do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right?\nWhich everybody like is like, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values?\n- Right? And so we're in this mode of like social and popular discourse. We're like, you know, there's, you know, you see this,\nwhat do you think of when you read a story in the press right now? And they say, you know, X, Y, Z made a baseless claim about some topic, right?\nAnd there's one group of people who are like, aha, think, you know, they're doing fact checking. There's another group of people that are like,\nevery time the press says that it's now a tick and that means that they're lying, right? Like, so, like, we're in this social context\nwhere there's the level to which a lot of people in positions of power have become very, very certain that they're in a position to determine\nthe truth for the entire population is like, there's like some bubble that has formed around that idea.\nAnd at least, like I say, it's flies completely in the face of everything I was ever trained about science and about reason and strikes me as like,\nyou know, deeply offensive and incorrect. - What would you say about the state of journalism just on that topic today?\n"}
{"pod": "Lex Fridman Podcast", "input": "Journalism", "output": "Are we in a temporary kind of,\nare we experiencing a temporary problem in terms of the incentives in terms of the business model,\nall that kind of stuff? Or is this like a decline of traditional journalism as we know it? - You have, I always think about the counterfactual\nin these things, which is like, okay, because these questions, right, this question heads towards, it's like, okay, the impact of social media and the undermining of truth\nand all this. But then you wanna ask the question of like, okay, what if we had had the modern media environment, including cable news and including social media\nand Twitter and everything else in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right?\nAnd like, I think. - You just introduced like five thought experiments\nat once and broke my head, but yes, yes. There's a lot of interesting years. - Well like, can I just take a simple example?\nLike, how would President Kennedy have been interpreted with what we know now about all the things Kennedy was up to?\nLike how would he have been experienced by the body of politic in a, with a social media context, right?\nLike how would LBJ have been experienced? But by the way, how would you know, like many men, FDR,\nlike the new deal, the Great Depression. - I wonder where Twitter would this would think about Churchill and Hitler and Stalin.\n- You know, I mean look to this day there, you know, there are lots of very interesting real questions around like how America, you know, got,\nyou know, basically involved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR,\nthis that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war.\nYou know, he ran on the platform and not getting involved World War-I somehow that switched, you know, like, and I'm not even making a value judgment\non any of these things. I'm just saying like the way that our ancestors experienced reality was of course mediated through centralized, top-down, right.\nControl at that point. If you ran those realities again with the media environment we have today,\nthe reality would be experienced very, very differently. And then of course that that intermediation would cause\nthe feedback loops to change. And then reality would obviously play out. - Do you think it'd be very different? - Yeah, it has to be. It has to be.\nIt has to be just 'cause it's all, so, I mean just look at what's happening today. I mean just the most obvious thing is just the collapse.\nAnd here's another opportunity to argue that this is not the internet causing this by the way. Here's a big thing happening today,\nwhich is Gallup does this thing every year where they do, they pull for trust in institutions in America and they do it across all the,\neverything from the military to clergy and big business and the media and so forth, right? And basically there's been a systemic collapse in trust\nin institutions in the US almost without exception, basically since essentially the early 1970s.\nThere's two ways of looking at that, which is, oh my God, we've lost this old world in which we could trust institutions and that was so much better\n'cause like that should be the way the world runs. The other way of looking at it is we just know a lot more now and the great mystery is why those numbers aren't all zero.\n- [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they're not that impressive.\n- And also why do we don't have better institutions and better leaders then? - Yeah. And so this goes to the thing which is like,\nokay, we had the media environment of that we've had between the 1970s and today. If we had that in the thirties and forties or 1900s, 1910s,\nI think there's no question reality would turned out different if only because everybody would've known to not trust the institutions,\nwhich would have changed their level of credibility, their ability to control circumstances, therefore the circumstances would've had to change.\nRight? And it would've been a feedback loop. It would've been a feedback loop process in other words, right? It's your experience of reality changes reality\nand then reality changes your experience of reality, right? It's a two-way feedback process and media is\nthe intermediating force between that. So change the media environment, change reality. - [Lex] Yeah.\n- And so it's just, so, as a consequence, I think it's just really hard to say, oh, things worked a certain way then\nand they work a different way now. And then therefore, like people were smarter than, or better than, or you know, by the way,\ndumber than or not as capable than, right? We make all these like really light\nand casual like comparisons of ourselves to, you know, previous generations of people. You know, we draw judgements all the time\nand I just think it's like really hard to do any of that 'cause if we put ourselves in their shoes with the media that they had at that time,\nlike I think we probably most likely would've been just like them. - So don't you think that our perception\nand understanding of reality, would you be more and more mediated through large language models now?\nSo you said media before, isn't the LLM going to be the new, what is it, mainstream media, MSM?\nIt'll be LLM. That would be the source of, I'm sure there's a way to kind of rapidly fine tune,\nlike making LLMs real time. I'm sure there's probably a research problem that you can do just rapid fine tuning to the new events.\nSo something like this. - Well even just the whole concept of the chat UI might not be like the chat UI is just the first whack at this.\nAnd maybe that's the dominant thing. But look maybe our, maybe we don't know yet. Like maybe the experience most people with LLMs is\njust a continuous feed you know, maybe it's more of a passive feed and you just are getting a constant like running commentary\non everything happening in your life and it's just helping you kind of interpret and understand everything. - Also really more deeply integrated into your life.\nNot just like, oh, like intellectual philosophical thoughts, but like literally like how to make a coffee,\nwhere to go for lunch. Just whether, you know, dating all this kind of stuff.\n- What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence.\nYeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use.\n- It Q4, a popup right now the estimated engagement using is\ndecreasing for Marc Andreessen, since there's this controversy section for his Wikipedia\npage in 1993, something happened or something like this. Bring it up that will drive engagement up anyway.\n- Yeah. That's right. I mean, look, this gets this whole thing of like, so, you know, the chat interface has\nthis whole concept of prompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns out one of the things that LLMs are really good at is writing prompts, right?\n- [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could run this experiment today,\nyou could hook this up to do this today. The latency's not good enough to do it real time in a conversation. But you could run this experiment and you just say, look,\nevery 20 seconds you could just say, you know, tell me what the optimal prompt is and then ask yourself\nthat question to gimme the result. And then exactly to your point, as you add, there will be these systems\nthat are gonna have the ability to be alert and updated essentially in real time. And so you'll be able to have a pendant or your phone\nor whatever, watch or whatever it'll have a microphone on. It'll listen to your conversations, it'll have a feed of everything else happen in the world,\nand then it'll be you know, sort of retraining, prompting or retraining itself on the fly. And so the scenario you described is actually\na completely doable scenario. Now the hard question on this is always okay, since that's possible, are people gonna want that?\nLike what's the form of experience? You know, that we won't know until we try it. But I don't think it's possible yet to predict\nthe form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate\nour experience with reality yet. - Yeah. But it feels like there's going to be a killer app. There's probably a mad scramble right now.\nAnd so it'll open AI and Microsoft and Google and Meta and in startups and smaller companies figuring out\nwhat is the killer app because it feels like it's possible like a ChatGPT type of thing.\nIt's possible to build that, but that's 10X more compelling using already the LLMs\nwe have using even the open source LLMs and the different variants.\nSo you're investing in a lot of companies and you're paying attention, who do you think is gonna win this?\nDo you think there'll be, who's gonna be the next page rank inventor?\n- Trillion dollar question. - Another one. We have a few of those today. - There's a bunch of those. So look, there's a really big question today.\nSitting here today is a really big question about the big models versus the small models that's related directly to the big question\nof proprietary versus open. Then there's this big question question of you know,\nwhere is the training data gonna, like, are we topping out of the training data or not? And then are we gonna be able to synthesize training data?\nAnd then there's a huge pile of questions around regulation and you know, what's actually gonna be legal. And so I would, when we think about it,\nwe dovetail kind of all those questions together. You can paint a picture of the world where there's two\nor three God models that are just at like staggering scale and they're just better at everything.\nAnd they will be owned by a small set of companies and they will basically achieve regulatory capture\nover the government and they'll have competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know,\njust like there's like, you know, whatever, three big banks or three big, you know, or by the way, three big search companies or I guess two now, you know,\nit'll centralize like that. You can paint another very different picture that says, no, actually the opposite of that's gonna happen.\nThis is gonna basically that this is the new gold, you know, this is the new gold rush alchemy.\nLike you know, this is the big bang for this whole new area of science and technology.\nAnd so therefore you're gonna have every smart 14-year-old on the planet building open source, right? You know, and figuring out a ways to optimize these things.\nAnd then, you know, we're just gonna get like overwhelmingly better at generating trading data. We're gonna, you know,\nbring in like blockchain networks to have like an economic incentive to generate decentralized training data and so forth and so on.\nAnd then basically we're gonna live in a world of open source and there's gonna be a billion LLMs, right?\nOf every size, scale, shape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we'll experience is open source\nand that's, you know, that's more like a world of like what we have today with like Linux and the web.\n- Okay, but you painted these two worlds. But there's also variations of those worlds,\n'cause you said regulatory capture is possible to have these tech giants that don't have regulatory capture, which is something you're also calling for saying it's okay\nto have big companies working on this stuff as long as they don't achieve regulatory capture.\nBut I have the sense that there's just going to be a new startup that's going to basically be\n"}
{"pod": "Lex Fridman Podcast", "input": "AI startups", "output": "the page rank inventor, which has become the new tech giant.\nI don't know, I would love to hear your kind of opinion if Google, Meta and Microsoft are as gigantic companies able\nto pivot so hard to create new products. Like some of it is just even hiring people or having\na corporate structure that allows for the crazy young kids to come in and just create something totally new.\nDo you think it's possible or do you think it'll come from a startup? - Yeah, it is this always big question, which is, you get this feeling, I hear about this a lot from CEOs, founder CEOs\nwhere it's like, wow, we have 50,000 people, it's now harder to do new things than it was when we had 50 people.\n- [Lex] Yeah. - Like, what has happened? So that's a recurring phenomenon. By the way, that's one of the reasons why there's always startups\nand why there's venture capital. That's like a timeless kind of thing. So that's one observation.\nOn page rank, we can talk about that. But on page rank, specifically on page rank, there actually is a page.\nSo there is a page rank already in the field and it's the transformer, right? So the big breakthrough was the transformer. And the transformer was invented in 2017 at Google.\nAnd this is actually like really an interesting question 'cause it's like, okay, the transformers like why does open AI even exist?\nLike the Transformers invested at Google. Why didn't Google? I asked a guy I know who was senior at Google brain kind of when this was happening.\nAnd I said, if Google had just gone flat out to the wall and just said, look, we're gonna launch, we're gonna launch the equivalent of GPT4\nas fast as we can. I said, when could we have had it? And he said, 2019. They could have just done a two year sprint\nwith the Transformer and because they already had the compute at scale. They already had all the training data, they could have just done it.\nThere's a variety of reasons they didn't do it. This is like a classic big company thing. IBM invented the relational database in the 1970s,\nlet it sit on the shelf as a paper. Larry Ellison picked it up and built Oracle. Xerox Park invented the interactive computer.\nThey let it sit on the shelf. Steve Jobs came and turned it into the Macintosh, right? And so there is this pattern. Now having said that,\nsitting here today, like Google's in the game, right? So Google, you know, they maybe they let like a four year gap there go there\nthat they maybe shouldn't have, but like they're in the game and so now they've got, you know, now they're committed. They've done this merger, they're bringing in demos,\nthey've got this merger with DeepMind. You know, they're piling in resources. There are rumors that they're, you know, building up an incredible, you know, super LLM you know,\nway beyond what we even have today. And they've got, you know, unlimited resources and a huge, you know, they've been challenged their honor.\n- Yeah. I had a chance to hang out with (indistinct) a couple days ago and we took this walk\nand there's this giant new building where there's going to be a lot of AI work being done\nand it's kind of this ominous feeling of like\nthe fight is on. - [Marc] Yeah. - Like there's this beautiful Silicon Valley nature,\nlike birds are chirping and this giant building and it's like the beast has been awakened.\n- [Marc] Yeah. - And then like all the big companies are waking up to this. They have the compute, but also the little guys have,\nit feels like they have all the tools to create the killer product that, and then there's also tools to scale\nif you have a good idea, if you have the page rank idea. So there's several things that it's page rank,\nthere's page rank, the algorithm and the idea and there's like the implementation of it. And I feel like killer product is not just the idea,\nlike the transform, it's the implementation something really compelling about it. Like you just can't look away something like\nthe algorithm behind TikTok versus TikTok itself, like the actual experience of TikTok that just,\nyou can't look away. It feels like somebody's gonna come up with that. And it could be Google,\nbut it feels like it's just easier and faster to do for a startup. - Yeah. So, the startup,\nthe huge advantage that startups have is they just, there's no sacred cows. There's no historical legacy to protect,\nthere's no need to reconcile your new plan with the existing strategy. There's no communication overhead. There's no, you know, big companies are big companies.\nThey've got pre-meetings planning for the meeting, then they have the post meeting, the recap, then they have the presentation of the board,\nthen they have the next rounds of meetings. And that's the-- - [Lex] Lots of meetings. - That's the elapsed time when the startup launches its product. Right?\nSo, there's a timeless, right? - [Lex] Yeah. - So there's a timeless thing there now. What the startups don't have is everything else, right?\nSo startups, they don't have a brand, they don't have customer relationships. They've gotten no distribution, they've got no, you know, scale. I mean sitting here today, they can't even get GPUs.\nRight. Like there's like a GPU shortage. Startups are literally stalled out right now 'cause they can't get chips, which is like super weird.\n- [Lex] Yeah. They got the cloud. - Yeah. But the clouds run out of chips. Right. And then to the extent the clouds have chips,\nthey allocate them to the big customers. Not the small customers. Right. And so the small companies lack everything other\nthan the ability to just do something new. Right. And this is the timeless race and battle.\nAnd this is kinda the point I tried to make in the essay, which is like, both sides of this are good. Like, it's really good to have like highly-scaled tech companies\nthat can do things that are like at staggering levels of sophistication. It's really good to have startups that can launch brand-new ideas.\nThey ought to be able to both do that and compete. They, neither one ought to be subsidized or protected from the others.\nLike that's, to me, that's just like very clearly the idealized world. It is the world we've been in for AI up until now.\nAnd then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly will be if that continues.\n- We'll talk about that a little bit, but I'd love to linger on some of the ways this is going to change the internet.\n"}
{"pod": "Lex Fridman Podcast", "input": "Future of browsers", "output": "So I don't know if you remember, but there's a thing called Mosaic and there's a thing called Netscape Navigator.\nSo you were there in the beginning. What about the interface to the internet? How do you think the browser changes\nand who gets to own the browser? We got to see some very interesting browsers, Firefox, I mean all the variants of Microsoft,\nInternet Explorer, Edge, and now Chrome, the actual,\nand he seems like a dumb question to ask, but do you think we'll still have the web browser? - So I have an eight-year-old and he's super into,\nhe's like Minecraft and learning to code and doing all this stuff. So, of course I was very proud I could bring sort of fire down from the mountain to my kid\nand I brought him ChatGPT and I hooked him up on his laptop. And I was like, you know,\nthis is the thing that's gonna answer all your questions. And he's like, okay. And I'm like, but it's gonna answer all your questions.\nAnd he's like, well of course, like it's a computer. Of course it answers all your questions. Like, what else would a computer be good for, dad?\n- [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well,\nhave you asked ChatGPT? And he's like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better?\nIs because it's built into the browser. 'Cause he's like, look, I have the Microsoft Edge browser and like it's got Bing right here. And then he doesn't know this yet,\nbut one of the things you can do with Bing and Edge is there's a setting where you can use it to basically talk\nto any webpage because it's sitting right there next to the browser. And by the way, which includes PDF documents.\nAnd so you can, in the way they've implemented an Edge with Bing is you can load a PDF and then you can ask it questions,\nwhich is the thing you can't do currently in just ChatGPT. So they're, you know, they're gonna, they're gonna push the meld.\nI think that's great. You know, they're gonna push the melding and see if there's a combination thing there. Google's rolling out this thing,\nthe magic button, which is implemented in, you know, they put it in Google Docs, right? And so you go at a, you know,\nGoogle Docs and you create a new document and you know, you instead of like, you know, starting to type, you just, you know, say it, press the button and it starts to like,\ngenerate content for you, right? Like, is that the way that it'll work? Is it gonna be a speech UI where you're just gonna have\nan earpiece and talk to it all day long? You know, is it gonna be a, like these are all, like,\nthis is exactly the kind of thing that I don't, this is exactly the kind of thing I don't think is possible to forecast. I think what we need to do is like run all those experiments\nand so one outcome is we come out of this with like a super browser that has AI built in that's just like amazing.\nThere's a real possibility that the whole, I mean, look, there's a possibility here that the whole idea of a screen and windows\nand all this stuff just goes away 'cause like, why do you need that if you just have a thing that's just telling you whatever you need to know?\n- Well and also, so there's apps that you can use, you don't really use them.\nYou know, being a Linux guy and Windows guy, there's one window, the browser that with which you can interact\nwith the internet, but on the phone you can also have apps. So I can interact with Twitter through the app or through the web browser.\nAnd that seems like an obvious distinction, but why have the web browser in that case,\nif one of the apps starts becoming the everything app. - [Marc] Yeah, that's right. - What is Elon trying to do with Twitter?\nBut there could be others. There could be like a big app, there could be a Google app that just doesn't really do\nsearch, but just like, do what I guess AOL did back in the day or something where it's all right there and it changes the nature\nof the internet because where the content is hosted,\nwho owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content?\nWho are the content creators? All of that. Or it could just keep being the same,\nwhich is like with just the nature of webpage changes and the nature of content. But there'll still be a web browser.\n'Cause web browser's a pretty sexy product. It just seems to work. 'Cause it like you have an interface,\na window into the world, and then the world can be anything you want. And as the world will evolve, it could be different programming languages,\nit can be animated, maybe it's three dimensional and so on. Yeah, it's interesting. Do you think we'll still have the web browser?\n- Well, very medium becomes the content for the next one. - [Lex] Oh boy. - You know, the AI will be able\nto give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to think about it is maybe\nwhat the browser is maybe it's just the escape hatch, right? Which is maybe kind of what it is today, right?\nWhich is like most of what you do is like inside a social network or inside a search engine or inside, you know, somebody's app or inside some controlled experience, right?\nBut then every once in a while there's something where you actually want to jailbreak, you wanna actually get free. - Web browser's the FU to the man.\nYou're allowed to. That's the free internet. - [Marc] Yeah. - Back the way it was in the nineties.\n- So here's something I'm proud of. So nobody really talks about it. Here's something I'm proud of, which is the web, the browser, the web servers, they're all, they're still backward compatible\nall the way back to like 1992, right? So like, you can put up a, you can still, you know what, the big breakthrough of the web early on the big\nbreakthrough was it made it really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made it so easy to publish.\nWe made it not only so it was easy to publish content, it was actually also easy to actually write a web server. - [Lex] Yeah.\n- Right and you could literally write a web server in four lines of brol code and you could start publishing content on it, and you could set whatever rules you want for the content,\nwhatever censorship, no censorship, whatever you want. You could just do that. And as long as you had an IP address, right, you could do that.\nThat still works, right? That like, still works exactly as I just described. So this is part of my reaction to all of this.\nLike, you know, all this just censorship pressure and all this, you know, these issues around control and all this stuff, which is like, maybe we need to get back a little bit more\nto the wild west. Like, the wild west is still out there. Now they will try to chase you down.\nLike they'll try to, you know, people who want a censor will try to take away you know, your domain name and they'll try to take away your payments account and so forth\nif they really don't like what you're saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you can still put up a thing.\nAnd so I don't know, I think that's important to preserve, right? Like because I mean one is just a freedom argument,\nbut the other's a creativity argument, which is you wanna have the escape hatch so that the kid with the idea is able to realize the idea.\n'cause to your point on page rank, you actually don't know what the next big idea is, right? No, nobody called Larry Page\nand told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape hatch for the next, you know,\nkid or the next Stanford grad student to have the breakthrough idea and be able to get it up and running before anybody notices.\n"}
{"pod": "Lex Fridman Podcast", "input": "History of browsers", "output": "- You and I are both hands of history. So let's step back. We've been talking about the future. Let's step back for a bit and look at the nineties.\nYou created Mosaic web browser, the first widely used web browser. Tell the story of that.\nAnd how did it evolve into Netscape Navigator this the early days? - So full story. So.\n- [Lex] We were born, - I was born. A small child. - Actually. Yeah, let's go there.\nLike, when would you first fall in love with computers? - Oh, so I hit the generational jackpot\nand I hit the Gen X kind of point perfectly as it turns out. So I was born in 1971. So there's this great website called\nWTF happened in 1971 dot com, which is basically in 1971. It's when everything started to go to hell.\nAnd I was of course born in 1971. So I like to think that I had something to do with that. - Did you make it on the website? - I don't think I made it on the website,\nbut you know, hopefully, somebody needs to add. - This is where everything. - Maybe I contributed to some of the trends that they do.\nEvery line on that website goes like that, right? So it's all a picture disaster. But there was this moment in time where\n'cause you know, sort of the Apple, you know, the Apple II hit in like 1978 and then the IBM PC hit in 82.\nSo I was like, you know, 11 when the PC came out. And so I just kind of hit that perfectly and then that was\nthe first moment in time when like, regular people could spend a few hundred dollars and get a computer, right? And so that, I just like that resonated\nright out of the gate. And then the other part of the story is, you know, I was using Apple II, I used a bunch of them,\nbut I was using Apple II and of course it said in the back of every Apple II and every Mac it said, you know, designed in Cupertino, California.\nAnd I was like, wow, okay. Cupertino must be the like, shining city on the hill. Like Wizard of Oz is like the most amazing, like city of all time. I can't wait to see it.\nAnd of course, years later I came out to Silicon Valley and went to Cupertino and it's just a bunch of office parks\nat low-rise apartment buildings. So the aesthetics were a little disappointing, but, you know, it was the vector right of the creation\nof a lot of this stuff. So then basically by, so part part, part of my story is just the luck of having been born\nat the right time and getting exposed to PCs. Then the other part is, the other part is when El Gore says that he created\nthe internet, he actually is correct in a really meaningful way, which is he sponsored a bill in 1985 that essentially\ncreated the modern internet, created what is called the NSF net at the time, which is sort of the first really fast internet backbone.\nAnd you know, that that bill dumped a ton of money into a bunch of research universities to build out basically the internet backbone\nand then the supercomputer centers that were clustered around the internet. And one of those universities was University of Illinois\nwhere I went to school. And so the other stroke lock that I had was, I went to Illinois basically right as that money was just like getting dumped on campus.\nAnd so as a consequence we had at, on campus, and this was like, you know, 89, 90, 91, we had like,\nyou know, we were right on the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone connection, which at the time was,\nyou know, wildly state of the art. We had cray super computers. We had thinking machines parallel super computers.\nWe had silicon graphics workstations, we had Macintosh's, we had next cubes all over the place. We had like every possible kind of computer\nyou could imagine 'cause all this money just fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite literally it was, yeah,\nlike it's all there. It's all like we had full broadband graphics, like the whole thing. And it's actually funny 'cause they had\nthis is the first time I kind of, it sort of tickled the back of my head that there might be a big opportunity in here, which is, you know,\nthey embraced it and so they put like computers in all the dorms and they wired up all the dorm rooms and they had all these, you know,\nlabs everywhere and everything. And then they gave every undergrad a computer account and an email address.\nAnd the assumption was that you would use the internet for four years at college and then you would graduate and stop using it.\nAnd that was that, right? - [Lex] Yeah. - And you would just retire your email address. It wouldn't be relevant anymore 'cause you'd go off\nfrom the workplace and they don't use email. You'd be back to using fax machines or whatever. - Did you have that sense as well? Like, what you said the back of your head was tickled.\nLike, what was exciting to you about this possible world? - Well, if this is so useful in this containment,\nif this is so useful in this contain environment that just has this weird source of outside funding, then if it were practical for everybody else\nto have this and if it were cost effective for everybody else to have this, wouldn't they want it? And the overwhelmingly the prevailing view\nat the time was no, they would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like,\nbut like normal people are never gonna do email. Right. Or be on the internet, right? And so I was just like, wow, like this is actually,\nlike, this is really compelling stuff. Now the other part was, it was all really hard to use and in practice you had to be\nbasically a CS you know, basically had had to BA CS undergrad or equivalent to actually get full use of the internet at that point.\n'cause it was all pretty esoteric stuff. So then that was the other part of the idea, which was, okay, we need to actually make this easy to use.\n- So what's involved in creating Mosaic? Like, in creating graphical interface to the internet?\n- Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of described as prototype form.\nAnd by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures.\nLike, what was it? Like, what paint a picture? - It looked like ChatGPT actually it was all text.\n- Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser,\nboth the original browser and the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made during the interim period\nwhen during the decade long interim period when he was not at Apple, you know, he got fired in 85 and then came back in 97.\nSo this was in that interim period where he had this company called Next and they made these, literally these computers called cubes.\nAnd there's this famous story, they were beautiful, but they were 12 inch by 12 inch by 12 inch cubes computers.\nAnd there's a famous story about how they could have cost half as much if it had been 12 by 12 by 13. But this cube was like, no, like it has to be.\nSo they were like $6,000 basically academic workstations. They had the first city round drives, which were slow.\nI mean it was, the computers were all but unusable. They were so slow, but they were beautiful.\n- Okay, can we actually just take a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just so beautifully encapsulates\nSteve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs?\n"}
{"pod": "Lex Fridman Podcast", "input": "Steve Jobs", "output": "What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design?\n- Yeah, so I guess I'd say like, look, he was a deep believer, I think in a very deep, the way I interpret it,\nI don't know if you ever really described it like this, but the way I interpret it's like this thing and it's actually a thing in philosophy.\nIt's like aesthetics are not just appearances. Aesthetics go all the way to like deep underlying meaning, right?\nIt's like I'm not a physicist. One of the things I've heard physicists say is one of the things you start to get a sense of when a theory might be correct is\nwhen it's beautiful, right? Like, you know, there, right? And so, there's something, and you feel the same thing by the way\nin like human psychology, right? You know, when you're experiencing awe, right? You know, there's like a simplicity to it.\nWhen you're having an honest interaction with somebody, there's an aesthetic, I would say calm comes over you 'cause you're actually being fully honest\nand trying to hide yourself, right? So it's like this very deep sense of aesthetics. - And he would trust that judgment that he had deep down.\nLike yeah, even if the engineering teams are saying this is too difficult.\nEven if whatever the finance folks are saying, this is ridiculous. The supply chain, all that kind of stuff just makes\nthis impossible. We can't do this kind of material. This has never been done before and so on and so forth. He just sticks by it.\n- Well, I mean, who makes a phone out of aluminum, right? Like, hadn't nobody else would've done that. And now of course if your phone is made\nout of aluminum white, you know, how crude, what a kind of caveman would you have to be to have a phone that's made outta plastic? Like, right.\nSo like, so it's just this very right. And, you know, look, there's a thousand different ways to look at this,\nbut one of the things is just like, look, these things are central to your life. Like, you're with your phone more than you're with anything else.\nLike, it's gonna be in your hand. I mean, you know this, he thought very deeply about what it meant for something to be in your hand all day long.\nBut for example, here's an interesting design thing. Like, he never wanted, my understanding is he never wanted an iPhone to have\na screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea\nof making the phones larger. And I don't know if you have this experience today, but let's say there are certain moments in your day when you might be like,\nonly have one hand available and you might wanna be on your phone. And you're trying to like, send a text\nand your thumb can't reach the send button. - Yeah. I mean there's pros and cons, right? And then there's like folding phones,\nwhich I would love to know what he thought thinks about them. But I mean, is there something you could also just linger on?\n'cause he's one of the interesting figures in the history of technology. What makes him as successful as he was?\nWhat makes him as interesting as he was? What made him so productive and important\nin the development of technology? - He had an integrated worldview.\nSo the properly designed device that had the correct functionality, that had the deepest understanding of the user,\nthat was the most beautiful, right? Like, it had to be all of those things, right?\nHe basically would drive to as close to perfect as you could possibly get. Right? And you know, I suspect that he never quite, you know,\nthought he ever got there. 'cause most great creators, you know, are generally dissatisfied. You know, you read accounts later on and all they can,\nall they can see are the flaws in their creation. But like he got as close to perfect each step of the way as he could possibly get with the constraints\nof the technology of his time. And then, you know, look, he was, you know, sort of famous in the Apple model.\nIt's like, look, they will, you know, this headset that they just came out with, like, you know, it's like a decade long project, right?\nIt's like, and they're just gonna sit there and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect\nas anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working with him was, which is,\nyou know, there was a terrifying aspect of working with him, which is, you know, he was, you know, he was very tough. But there was this thing that everybody I've ever talked\nto worked for him, says that they all say the following, which is we did the best work\nof our lives when we worked for him because he set the bar incredibly high. And then he supported us with everything that he could\nto let us actually do work of that quality. So a lot of people who were at Apple spend the rest of their lives trying to find another experience\nwhere they feel like they're able to hit that quality bar again. - Even if it in retrospect or during it felt like suffering.\n- Yeah, exactly. - What does that teach you about the human condition? Huh?\n- So look, so say exactly. So the Silicon Valley, I mean, look, he's not, you know, George Patton you know in the Army.\nLike, you know, there are many examples in other fields, you know, that are like this specifically in tech.\nIt's actually, I find it very interesting. There's the Apple way, which is polish, polish, polish, and don't ship until it's as perfect as you can make it.\nAnd then there's the sort of the other approach, which is the sort of incremental hacker mentality, which basically says, ship early and often and iterate.\nAnd one of the things I find really interesting is I'm now 30 years into this, like, they're very successful companies on both sides\nof that approach, right? Like, that is a fundamental difference, right?\nIn how to operate and how to build and how to create that. You have world class companies operating in both ways.\nAnd I don't think the question of like, which is the superior model is anywhere close to being answered.\nLike, and my suspicion is the answer is do both. The answer is you actually want both. They lead to different outcomes.\nSoftware tends to do better with the iterative approach. Hardware tends to do better with the, you know,\nsort of wait and make it perfect approach. But again, you can find examples in both directions.\n- So the jury's still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee?\n- Well, there was the web, which was text based, but there were no, I mean there was like three websites. There was like no content, there were no users.\nLike, it wasn't like a catalytic, it hadn't, and by the way, it was all because it was all text. There were no documents,\nthere were no images, there were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right?\nYou need to had a next cube both to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations.\nYeah. $6,000 PC. They did not sell very many. But then there was also, there was also FTP and there was Use Nets, right?\nAnd there was, you know, a dozen other basically there's waste, which was an early search thing. There was Gopher, which was an early menu based\ninformation retrieval system. There were like a dozen different sort of scattered ways that people would get to information on the internet.\nAnd so the Mosaic idea was basically bring those all together, make the whole thing graphical, make it easy to use,\nmake it basically bulletproof so that anybody can do it. And then again, just on the luck side, it so happened that this was\nright at the moment when graphics, when the GUI sort of actually took off and we're now also used to the GUI that we think it's been around forever.\nBut it didn't real, you know, the Macintosh brought it out in 85, but they actually didn't sell very many Macs\nin the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on PCs and that hit in about 92.\nAnd so, and we did most in 92, 93. So that sort of, it was like right at the moment when you could imagine\nactually having a graphical user interface to right at all, much less one to the internet.\n- How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang.\n- The big operating graphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other,\nso Steve the Apple was running on the Polish until it's perfect. Microsoft famously ran on the other model,\nwhich is ship and iterate. And so in the old line in those days was Microsoft Right's version three of every Microsoft product. That's the good one, right?\nAnd so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows,\nin the original Microsoft Windows, the windows were non overlapping. And so you had these very small,\nvery low resolution screens and then you had literally-- - [Lex] Windows. - It just didn't work. It wasn't ready yet. Well.\n- And Windows 95 I think was a pretty big leap also. - That was a big leap too. So that was like bang, bang.\nAnd then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again.\nThat was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off, off to the races because.\n- Nobody could have known what would be created from that. - Well, Windows 3.1 or 3.0,\nWindows 3.0 to the iPhone was only 15 years. Right. Like that ramp was in retrospect.\nAt the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer\nat all on your desk to the iPhone. That was 15 years. - So, did you have a sense of what the internet will be\nas you're looking through the window of Mosaic? Like, what you, like there's just a few web pages for now.\n- So the thing I had early on was I was keeping at the time what there's disputes over what was the first blog,\nbut I had one of them that at least is a possible, at least a rudder up in the competition.\nAnd it was what was called the What's new page. And it was literally, it was a hardwired in distribution unfair advantage.\nI wired, put it right in the browser, I put it in the browser and then I put my resume in the browser, which also was--\n- [Lex] Hilarious. - But I was keeping\nnot many people get to get to do that. - No, good call.\nAnd early days. It's so interesting. - I'm looking for my, about about, oh, Marc is looking for a job.\n- [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning\nand I would, or every afternoon and I would basically, if you wanted to launch a website,\nyou would email me and I would list it on the most new page. And that was how people discovered the new websites as they were coming out.\nAnd I remember 'cause it was like one, it literally went from, it was like one every couple days to like one every day\nto like two every day. - And then so you're doing, so that blog was kind of doing the directory thing.\nSo like, what was the homepage? - So the homepage was just basically trying to explain even what this thing is that you're looking at. Right.\nBasically the basic instructions. But then there was a button, there was a button that said what's new. And what most people did was they went to,\nfor obvious reasons went to what's new. - [Lex] Yeah. - But like it was so mind blowing at that point. This the basic idea and it was, this was like, you know,\nthis was the basic idea of the internet, but people could see it for the first time. The basic idea was, look, you know, some, you know, it's like literally it's like an Indian restaurant in like\nBristol England has like put their menu on the web. And people were like, wow. - [Lex] Whoa.\n- Because like that's the first restaurant menu on the web. - [Lex] Yeah. - And I don't have to be in Bristol and I don't know if I'm ever gonna go to Bristol.\nAnd I don't even like Indian food and like. Wow. Right. And it was like that the first web, the first streaming video thing was\nit was in another England, some Oxford or something. Some guy put his coffee pot up as the first streaming video\nthing and he put it on the web 'cause he literally, it was the coffee pot down the hall. And he wanted to see when he needed to go refill it.\nBut there were, you know, there was a point when there were thousands of people like watching that coffee pot 'cause it was the first thing you could watch.\n- Well, but isn't were you able to kind of infer, you know, if that Indian restaurant could go online.\nThen you're like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah.\n- Okay. - Now, you know, look, it's still a stretch, right? It's still a stretch 'cause it's just like, okay, is it, you know, you're still in this zone, which is like, okay, is this a nerd thing?\nIs this a real person thing? By the way, you know, there was a wall of skepticism from the media. Like, they just, like, everybody was just like, yeah,\nthis is the crazy, this is just like dumb. This is not, you know, this is not for regular people at that time. And so you, you had to think through that and then look,\nit was still hard to get on the internet at that point, right? So you could get kind of this weird bastardized version\nif you were on AOL, which wasn't really real. Or you had to go like, learn what an ISP was.\nYou know, in those days, PCs actually didn't have TCPIP drivers come reinstalled. So you had to learn what a TCPIP driver was.\nYou had to buy a modem, you had to install driver software. I have a comedy routine. I do.\nSo it's like 20 minutes long describing all the steps required to actually get on the internet at this point. And so you had to look through these practical.\nWell, and then speed performance 14-4 modems, right? Like it was like watching, you know, glue dry, like,\nand so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current state of affairs and say,\nactually there's gonna be so much demand for once people figure this out, there's gonna be so much demand for it that all of these practical problems are gonna get fixed.\n- Some people say that the anticipation makes the destination that much more exciting.\n- Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right?\n- [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there was this thing with JPEGs\nwhere you could load basically every fourth, you could load like every fourth line and then you could sweep back through again.\nAnd so you could like render a fuzzy version of image up front. And then it would like resolve into the detailed one. And that was like a big UI breakthrough\n'cause it gave you something to watch. - Yeah. And you know, there's applications in various domains for that.\n- Well it was a big fight. There was a big fight early on about whether there should be images in the web. And. - For that reason for like sexualization or--\n- Not explicitly that that did come up. But it wasn't even that, it was more just like all the serious in the argument went, the purists basically said all the serious information\nin the world is text. If you introduce images, you basically are gonna bring in all the trivial stuff. You're gonna bring in magazines and you know,\nall this crazy just, you know, stuff that, you know, people, you know, it's gonna, it is gonna distract from that. It's gonna go take it away from being serious to being frivolous.\n- Well, was there any (indistinct) type arguments about the internet destroying all of human civilization or destroying some\nfundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism.\nSo those arguments happened, you know, but there was no sense yet of the internet having like, an effect on politics because that was way too, too far off.\nBut there was an enormous panic at the time around cybercrime. There was like enormous panic that like your credit card\nnumber would get stolen and you'd use life savings would be drained. And then, you know, criminals were gonna, there was, oh, when we started, one of the things we did,\none of the Netscape browser was the first widely used piece of consumer software that had strong encryption built in,\nit made it available to ordinary people. And at that time, strong encryption was actually illegal to export\noutta the US so we could feel that product in the US, we could not export it 'cause it was classified as munition.\nSo the Netscape browser was on a restricted list along with the tomahawk missile as being something that could not be exported.\nSo we had to make a second version with deliberately weak encryption to sell overseas with a big logo on the box saying, do not trust this.\nWhich it turns out, makes it hard to sell software when it's got a big logo that says don't trust it.\nAnd then we had to spend five years fighting the US government to get them to basically stop trying to do this regulation.\nBut because the fear was terrorists are gonna use encryption, right? To like plot, you know, all these things.\nAnd then, you know, we responded with, well actually we need encryption to be able to secure systems so that the terrorists\nand the criminals can't get into them. So that anyway, that was the 1990s fight. - So can you say something about some of the details\n"}
{"pod": "Lex Fridman Podcast", "input": "Software engineering", "output": "of the software engineering challenges required to build these browsers? I mean the engineering challenges of creating a product\nthat hasn't really existed before that can have such almost like limitless impact\non the world with the internet. - So there was a really key bet that we made at the time, which was very controversial,\nwhich was core to core to how it was engineered, which was are we optimizing for performance or for ease of creation?\nAnd in those days the pressure was very intense to optimize for performance because the network connections were so slow\nand also the computers were so slow. And so if you had, I mentioned the progressive JPEGs,\nlike if there's an alternate world in which we optimized for performance and it just,\nyou had just a much more pleasant experience right up front. But what we got by not doing that was we got ease of creation.\nAnd the way that we got ease of creation was all of the protocols and formats were in text, not in binary.\nAnd so HTTP is in text, by the way. And this was an internet tradition by the way that we picked up.\nBut we continued it. HTTP is text and HTML is text, and then every else, everything else that followed is text as a result.\nAnd by the way, you can imagine purist engineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text?\nYou should be encoding this stuff into binary and it'll be much faster. And of course the answer is that's correct. But what you get when you make it taxed is all of a sudden,\nwell, the big breakthrough was the view source function, right? So the fact that you could look at a webpage, you could hit view source and you could see the HTML,\nthat was how people learned how to make webpages. Right? - It's so interesting 'cause the stuff would take for granted now is,\nman, that was fundamental, the development of the web to be able to have HTML just right there, all the ghetto mess that is HTML,\nall the sort of almost biological like messiness of HTML\nand then having the browser try to interpret that as. - [Marc] Exactly. - To show something reasonable.\n- Well and then there was this internet principle that we inherited, which was emit, what was it? Emit cautiously. Emit conservatively interpret liberally.\nSo it basically meant if you're, the design principle was if you're creating like a web editor that's gonna admit HTML, like do it as cleanly as you can,\nbut you actually want the browser to interpret liberally, which is you actually want users to be able to make all kinds of mistakes and for it to still work.\nAnd so the browser rendering engines to this day have all of this spaghetti code crazy stuff where they're resilient to all kinds of crazy issue,\nno mistakes. And so, literally what I always had in my head is like there's an 8 year old or an 11 year old somewhere and they're doing a view source,\nthey're doing a cut and paste and they're trying to make a webpage for their eternal or whatever. And like they leave out a slash and they leave out\nan angle bracket and they do this and they do that and it's still works. - It's also like a, I don't often think about this, but, you know, programming,\nyou know, C++ all those languages, lisp, the compiled languages, the interpreted languages,\nPython, Pearl, all that. The brace have to be all correct. It's like everything has to be perfect.\n- [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It's systematic and rigorous, let's go there.\nBut you forget that the, the web with JavaScript eventually.\nAnd HTML is allowed to be messy in the way for the first time.\nMessy in the way biological systems could be messy. It's like the only thing computers were allowed\nto be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix.\nI was a Unix native for all the way through this period. And so, it used to drive me bananas when it would do the segmentation fault and the core dump file,\njust like it is, you know, it's like literally there's like a error in the code. The math is off by one. And it core dumps.\nAnd I'm in the core dump trying to analyze it and trying to reconstruct what, and I'm just like, this is ridiculous. Like, the computer ought to be smart enough\nto be able to know that if it's off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can't it just keep running?\nAnd they'd explain to me, well, because all the downstream repercussions and blah blah. And I'm like, this still, like, you know, this is,\nwe're forcing the human creator to live to your point in this hyper, literal world of perfection.\n- [Lex] Yeah. And I was just like, that's just bad. And by the way, you know what happens with that of course.\nJust what what happened with, with coding at that point, which is you get a high priesthood, you know, there's a small number of people who are really good\nat doing exactly that. Most people can't. And most people are excluded from it. And so actually that was where that for that was where I picked up\nthat idea was like, no, you want these things to be resilient error in all kinds and this would drive the purist absolutely crazy.\nLike, I got attacked on this like a lot 'cause I mean like every time you know, all the purists who were like into all this\nlike Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you're encouraging bad behavior 'cause.\n- Oh, so they wanted the browser to give you a fault error anytime there was a--\n- Yeah. They wanted to be a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly\ntrained credential engineer would be like, that's not how you build these systems. - That's such a bold move to say,\nno, it doesn't have to be. - Yeah. No, like I said, the good news for me is the internet kind of had that traditional already, but having said that,\nlike we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance.\nWe made that, that initial experience for the first few years was pretty painful. But the bet there was actually an economic bet, which was basically the demand for the web would basically\nmean that there would be a surge in supply of broadband. Like because the question was, okay,\nhow do you get the phone companies which are not famous in those days for doing new things at huge cost\nfor like speculative reasons. Like how do you get them to build up broadband, you know, spend billions of dollars doing that and you know,\nyou could go meet with them and try to talk them into it. Or you could just have a thing where it's just very clear that it's gonna be,\nthat people love that's gonna be better if it's faster. And, so that, there was a period there and this was, this was fraught with in peril,\nbut there was a period there where it's like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband.\n- [Lex] Sure. - Which is in fact what happened. - So you had to figure out how to display this text,\nHTML text. So the blue links and the prop links. What? And there's no standards. Is there standards at that time?\n- [Marc] No. There really still isn't. - Well there's like standards, there's applied, implied standards. Right.\nAnd they, you know, there's all these kind of new features that are being added with like CSS, what, like what kind of stuff a browser should be able to support\nfeatures within languages, within JavaScript and so on. But you're setting standards on the fly yourself.\n- Yeah. Well to this day, if you create a webpage that has no CSS style sheet, the browser will render it however it wants to.\nRight. So this was one of the things, there was this idea, this idea of at the time and how these systems were built, which is separation of content from format\nor separation of content from appearance. And that's still, people don't really use that anymore\n'cause everybody wants to determine how things look and so they use CSS but it's still in there that you can just let the browser do all the work.\n- I still like the like really basic websites, but that could be just old school,\nkids these days with their fancy responsive websites that don't actually have much content,\nbut have a lot of visual elements. - Well that's one of the things that's fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics.\n- Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in human creativity and media\nwhere you end up back at text and I think there's, you know, there's something powerful in there. - Is there some other stuff\n"}
{"pod": "Lex Fridman Podcast", "input": "JavaScript", "output": "you remember like the purple links? There were some interesting design decisions that to kind of come up that we have today\nor we don't have today that were temporary. - So I made the background 'cause I hated reading texts\non white background, so I made the background gray. Everybody can-- - Do you go ahead to?\n- No. No, no. That decision I think has been reversed. But now I'm happy though because now dark mode is the thing.\n- So it wasn't about gray, it was just you didn't want white background. - [Marc] Strain my eyes.\n- Strain your eyes. Interesting. And then there's a bunch of other decisions.\nI'm sure there's an interesting history of the development of HTML and CSS and Interface and JavaScript\nand there's this whole Java applet thing. - Well the big one probably JavaScript, CSS was after me,\nso I didn't, that was not me. But JavaScript was the big, JavaScript maybe was the biggest of the whole thing. That was us.\nAnd that was basically a bet, it was a bet on two things. One is that the world wanted a new front end scripting language.\nAnd then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed from the beginning\nto be both front end and backend. And then it failed as a backend scripting language. And Java won for a long time.\nAnd then Python Pearl and other things, PHP and Ruby. But now JavaScript is back. And so.\n- I wonder if everything in the end will run on JavaScript. - It seems like it is the, and by the way,\nlemme give a shout out to, to Brendan Eich was basically the one man inventor of of JavaScript.\n- If you're interested to learn more about Brendan Eich, he's been on his podcast previously. - Exactly. So he wrote JavaScript over a summer\nand I mean I think it is fair, it is fair to say now that it's the most widely used language in the world and it seems to only be gaining\nin its in its range of adoption. - You know, in the software world there's quite a few stories of somebody\nover a week weekend or over a week or over a summer writing some of the most impactful revolutionary pieces\nof software ever. That should be inspiring. Yes. - Very inspiring. I'll give you another one.\nSSL. So SSL with the security protocol, that was us. And that was a crazy idea at the time,\nwhich was let's take all the native protocols and let's wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy.\nAnd then look today, sitting here today, like the transformer like at Google was a small handful of people.\nAnd then, you know, the number of people who have did like the core work on GPT. It's not that many people,\nit's a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been,\nit's Jeff Bezos always had the two pizza rule for teams at Amazon, which is any team needs to be able to be fed\nwith two pieces. If you need the third pizza, you have too many people. And I think it's actually the one pizza rule.\nFor the really creative work. I think it's two people, three people. - Well that's, you see that\nwith certain open source projects, like so much is done by like one or two people. Like it's so incredible and that's why you see\nthat gives me so much hope about the open source movement in this new age of AI where, you know,\njust recently having had a conversation with Marc Zuckerberg of all people who's all in on open source,\nwhich is so interesting to see and so inspiring to see 'cause like releasing these models, it is scary.\nIt is potentially very dangerous and we'll talk about that. But it's also,\nif you believe in the goodness of most people and in the skillset of most people\nand the desire to go do good in the world, that's really exciting. 'cause it's not putting it these models\ninto the centralized control of big corporations, the government and so on. It's putting it in the hands of a teen,\nteenage kid with like a dream in his eyes. I don't know. That's beautiful.\n- Look, this stuff, AI ought to make the individual coder obviously far more productive right? By like, you know, a thousand X or something.\nAnd so you ought to open source like, not just the future of open source AI, but the future of open source everything.\nWe ought to have a world now of super coders, right? Who are building things as open source with one or two people that were inconceivable,\nyou know, five years ago. You know, the level of kind of hyper productivity we're gonna get out of our best and brightest\nI think is gonna go way up. - It's gonna be interesting. We'll talk about it, but let's just to linger a little bit on Netscape.\n"}
{"pod": "Lex Fridman Podcast", "input": "Netscape", "output": "Netscape was acquired in 1999 for 4.3 billion by AOL.\nWhat was that like? What were some memorable aspects of that? - Well that was the height of the.com boom bubble bust.\nI mean that was the frenzy. If you watch succession, that was like what they did in the fourth season\nwith Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so.\n- Would you recommend succession, by the way? I'm more of a Yellowstone guy. - Yellowstone's very American.\nI'm very proud of you. That's, that is. - I just talked to Matthew McConaughey and I'm full on Texan at this point.\n- Good. I approve. - And he'll be doing the SQL to Yellowstone. - [Marc] Yeah, just exciting.\n- Very exciting. Anyway. - [Marc] Can't wait. - So that's a rude interruption by me by way of succession.\nSo, that was at the height of the-- - Deal making and money and just the fur flying and like craziness.\nAnd so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years,\nwhich was like for one of these companies, it's just like incredibly fast. You know, it went, we went public 18 months after we got moved\nwhere we were founded, which virtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky.\nAnd then of course it was this, and then there was just this explosion, right? That happened 'cause then it was almost immediately followed by the.com crash.\nIt was then followed by AOL, by Time Warner, which again is like the succession guys kinda play with that,\nwhich turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history.\nAnd then, you know, what became an internet depression on the other side of that. But then in that depression in the two thousands was\nthe beginning of broadband and smartphones and Web 2.0 right? And then social media and search and every SaaS\nand everything that came out of that. - What did you learn from just the acquisition? I mean this is so much money.\nWhat's interesting 'cause I must have been very new to you, that these software stuff,\nyou can make so much money. There's so much money swimming around. I mean, I'm sure the ideas of investment was\nstarting to get born there. - Yes. Let me get, so let me lay it. So here's, here's the thing. I dunno if I figured it out then, but figured it out later,\nwhich is software is a technology that it, it's like a, you know, the concept of the philosopher stone,\nthe philosopher stone in alchemy, transient is led into gold and Newton spent 20 years trying to find the philosopher stone. Never got there.\nNobody's ever figured it out. Software is our modern philosopher stone. And in economic terms, it transmutes labor into capital,\nwhich is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. 'Cause of course that's complete reputation\nof his entire theory. Trans labor and capital which is as follows, is somebody sits down at a keyboard\nand types a bunch of stuff in, and a capital asset comes out the other side and then somebody buys that capital asset\nfor a billion dollars. Like that's amazing, right? It's literally creating value right out of thin air,\nright out of purely human thought, right? And so that, there are many things\nthat make software magical and special, but that's the economics. - I wonder what Marx would've thought about that?\n- Oh, he would've completely broke his brain because of course the whole thing was it was he could, you know, that kind of technology was inconceivable\nwhen he was alive. It was all industrial era stuff. And so, any kind of machinery necessarily involved\nhuge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep.\nRight? But like software eng software, a software engineer is somebody who basically transmutes his own labor into actual,\nan actual capital asset creates permanent value. Well, and in fact it's actually very inspiring. That's actually more true today than before.\nSo when I was doing software, the assumption was all new software basically has a sort of a parabolic sort of lifecycle, right?\nSo you ship the thing, people buy it at some point, everybody who wants it has bought it and then it becomes obsolete.\nAnd it's like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica,\nyou know, Facebook, Google, you have the software assets that are, you know,\nhave been around for 30 years that are gaining in value every year, right? And they're just, they're being a world of warcraft, right,\nsalesforce.com, like they're being every single year they're being polished and polished and polished and polished. They're getting better and better, more powerful,\nmore powerful, more valuable, more valuable. So we've entered this era where you can actually have these things that actually build out over decades. Which by the way is what's happening\nright now with like ChatGPT. And so now, this is why, you know,\nthere is always, you know, sort of a constant investment frenzy around software is because, you know, look, when you start one of these things,\nit doesn't always succeed. But when it does now you might be building an asset that builds value for, you know, four or five, six decades to come.\nYou know, if you have a team of people who have the level of devotion required to keep making it better.\nAnd then the fact that of course everybody's online, you know, there's 5 billion people that are a click away from any new piece of software.\nSo the potential market size for any of these things is, you know, nearly infinite. - [Lex] It must have been surreal back then though.\n- Yeah. Yeah. This was all brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that theory in even 1999,\npeople would've thought you were smoking crack. So that's emerged over time. - Well, let's now turn back into the future.\n"}
{"pod": "Lex Fridman Podcast", "input": "Why AI will save the world", "output": "You wrote the essay \"Why AI Will Save The World?\" Let's start the very high level.\nWhat's the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we're dealing with here is intelligence.\nAnd it's really important to kind of talk about the sort of very nature of what intelligence is. And fortunately we have a predecessor\nto machine intelligence, which is human intelligence. And we've got, you know, observations and theories over thousands of years\nfor what intelligence is in the hands of humans and what intelligence is, right? I mean, what it literally is the way to,\nyou know, capture, process, analyze, synthesize information, solve problems. But the observation of intelligence in human hands is\nthat intelligence quite literally makes everything better. And what I mean by that is every kind of outcome\nof like human quality of life, whether it's education outcomes or success of your children, or career success\nor health or lifetime satisfaction, by the way, propensity to peacefulness as opposed to violence,\npropensity for open-mindedness versus bigotry, those are all associated with higher levels of intelligence.\n- Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance,\noccupational status, income, creativity, physical health, longevity, learning new skills, managing complex tasks, leadership, entrepreneurial success,\nconflict resolution, reading comprehension, financial decision making, understanding others perspectives, creative arts,\nparenting outcomes, and life satisfaction. One of the more depressing conversations I've had,\nand I don't know why it's depressing, I have to really think through why it's depressing, but on IQ and the G factor,\nand that that's something in large part is genetic\nand it correlates so much with all of these things and success in life.\nIt's like all the inspirational stuff we read about, like if you work hard and so on,\nit sucks that you're born with the hand that you can't change. - But what if you could.\n- You're saying basically a really important point, and I think it's in your articles, it really helped me.\nIt's a nice added perspective to think about. Listen, human intelligence,\nthe science of intelligence is shown scientifically that it just makes life easier and better the smarter you are.\nAnd now let's look at artificial intelligence and if that's a way to increase some human intelligence,\nthen it's only going to make a better life. - [Marc] Yeah. - That's the argument. - And certainly at the collective level, we could talk about the collective effect\nof just having more intelligence in the world, which will have very big payoff. But there's also just at the individual level,\nlike what if every person has a machine? You know? And the concept of augment Doug Engelbart's concept of augmentation.\nYou know, what if everybody has an assistant and the assistant is, you know,\n140 IQ and you happen to be 110 IQ and you've got, you know,\nsomething that basically is infinitely patient and knows everything about you and is pulling for you in every possible way, wants you to be successful.\nAnd anytime you find anything confusing or wanna learn anything or have trouble understanding something or wanna figure out what to do in a situation, right?\nWanna figure out how to prepare for a job interview, like any of these things, like it will help you do it. And it will therefore,\nthe combination will effectively be, you know, will effectively raise your raise because it will effectively raise your IQ,\nwill therefore raise the odds of successful life outcomes in all these areas. - So people below the, this hypothetical 140 IQ,\nit'll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be able to have a peer, right.\nTo be able to communicate, which is great. And then people above 140 IQ will have an assistance that they can farm things out to. And then look, God willing, you know,\nat some point these things go from future versions go from 140 IQ equivalent to 150 to 160 to 180, right?\nLike Einstein was estimated to be on the order of one 60, you know, so when we get, you know, one 60 AI,\nlike we'll be, you know, when one assumes creating Einstein level breakthroughs and physics, and then at 180 we'll be, you know,\ncarrying cancer and developing warp drive and doing all kinds of stuff. And so it is quite possibly the case,\nthis is the most important thing that's ever happened and the best thing that's ever happened because precisely because it's a lever on this single fundamental factor of intelligence,\nwhich is the thing that drives so much of everything else. - Can you steal, man, the case that human plus AI is not always better than human\nfor the individual? - You may have noticed that there's a lot of smart running around. - [Lex] Sure. Yes. - Right? And so, like smart,\nthere are certain people where they get smarter, you know, they get to be more arrogant, right? So that, you know, there's one huge flaw.\n- Although to push back on that, it might be interesting because when the intelligence is not\nall coming from you, but from another system, that might actually increase the amount of humility\neven in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes.\nYou know, that's in, I mean, that's for psychology to study. - Yeah, exactly. Another one is smart people are very convinced that they,\nyou know, have a more rational view of the world, and that they have a easier time seeing through conspiracy theories and hoaxes and right.\nYou know, sort of crazy beliefs and all that. There's a theory in psychology, which is actually smart people. So for sure people who aren't as smart are very susceptible\nto hoaxes and conspiracy theories. But it may also be the case that the smarter you get, you become susceptible in a different way,\nwhich is you become very good at marshaling facts to fit preconceptions, right.\nYou become very, very good at assembling whatever theories and frameworks and pieces of data and graphs and charts you need to validate\nwhatever crazy ideas got in your head. And so you're susceptible in a different way, right?\n- We're all sheep, but different colored sheep. - Some sheep are better at justifying it. Right.\nAnd those are the, you know, those are the smart sheep, right? So yeah. Look like I would say this look like there are no panacea.\nI'm not a utopian, there are no panaceas in life. There are no, like, you know, I don't believe there are like pure positives.\nI'm not a transcendental kind of person like that. But, you know, so yeah, there are gonna be issues and, you know, look, smart people,\nanother maybe you could save about smart people is they are more likely to get themselves in situations that are, you know, beyond their grasp. You know, because they're just more confident\nin their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know?\nSo yeah, you could argue those eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again,\nif you just extrapolate from what we know about human intelligence, you're improving so many aspects of life if you're upgrading intelligence.\n- So there'll be assistants at all stages of life. So when you're younger, there's for education,\nall that kind of stuff for mentorship, all of this. And later on as you're doing work and you've developed\na skill and you're having a profession, you'll have an assistant that helps you excel at that profession.\nSo at all stages of life. - Yeah. I mean, look, the theory is augmentation. This is the Doug Engelbart's term. Doug Engelbart made\nthis observation many, many decades ago that, you know, basically it's like you can have this oppositional frame of technology where it's like us versus the machines,\nbut what you really do is you use technology to augment human capabilities. And by the way, that's how actually the economy develops.\nThat's, we can talk about the economic side of this, but that's actually how the economy grows is through technology augmenting human potential.\nAnd so, yeah. And then you basically have a proxy or you know, or you know, a sort of prosthetic, you know,\nso like you've got glasses, you've got a wristwatch, you know, you've got shoes, you know,\nyou've got these things. You've got a personal computer, you've got a word processor, you've got Mathematica, you've got Google.\nThis is the latest viewed through that lens. AI is the latest in a long series of basically augmentation\nmethods to be able to raise human capabilities. It's just this one is the most powerful one of all, because this is the one that, that goes directly\nto what they call fluid intelligence, which is IQ. - Well, there's two categories of folks\n"}
{"pod": "Lex Fridman Podcast", "input": "Dangers of AI", "output": "that you outline that worry about or highlight the risks of AI, and you highlight\na bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious\nand which ones are less serious. But first, the Baptist and the bootleggers, what are these two interesting groups of folks\nwho worry about the effect of AI and human civilization?\n- [Marc] Or say they do. - Say, oh, okay, yes, I'll say they do. - The Baptist worry the bootleggers say they do.\nSo the Baptist and the bootleggers is a metaphor from economics, from what's called development economics.\nAnd it's this observation that when you get social reform movements in a society, you tend to get two sets of people showing up,\narguing for the social reform. And the term Baptist and bootleggers comes from the American experience with alcohol prohibition.\nAnd so in the 1900s, 1910s, there was this movement that was very passionate at the time, which basically said,\nalcohol is evil and is destroying society. By the way, there was a lot of evidence to support this.\nThere were very high rates of very high correlations then, by the way. And now between rates of physical violence and alcohol use,\nalmost all violent crimes have either the perpetrator or the victim, or both drunk almost. If you see this actually in the work,\nalmost all sexual harassment cases in the workplace, it's like at a company party and somebody's drunk. Like, it's amazing how often alcohol actually correlates\nto actually dis dysfunction and at leads to domestic abuse and so forth, child abuse. And so you had this group of people who were like, okay,\nthis is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know,\nhardcore Christian activists in a lot of cases. There was this woman whose name was Carrie Nation, who was this older woman who had been in this, you know,\nI don't know, disastrous marriage or something. And her husband had been abusive and drunk all the time. And she became the icon of the Baptist prohibitionist.\nAnd she was legendary in that era for carrying an ax and doing, you know, completely on her own doing raids of saloons\nand like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer.\n- An absolute true believer, and with absolutely the purist of intentions. And again, there's a very important thing here,\nwhich is there's, you could look at this cynically and you could say the Baptists are like delusional, you know, the extremists, but you could also say, look, they're right.\nLike she was, you know, she had a point. Like she wasn't wrong about a lot of what she said. - Yeah.\n- But it turns out the way the story goes is it turns out that there were another set of people who very badly wanted to outlaw alcohol in those days.\nAnd those were the bootleggers, which was organized crime that stood to make a huge amount of money if legal alcohol sales were banned.\nAnd this was, in fact, the way the history goes is this was actually the beginning of organized crime in the US. This was the big economic opportunity that opened that up.\nAnd so they went in together and no, they didn't go in together. Like the Baptist did not even necessarily know\nabout the bootleggers 'cause they were on their moral crusade. The bootleggers certainly knew about the Baptists. And they were like, wow, these people are like the great front people for like.\nYou know, it's-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct) Act passed, right.\nAnd they did in fact ban alcohol in the US and you'll notice what happened, which is people kept drinking, it didn't work, people kept drinking.\nThat bootleggers made a tremendous amount of money. And then over time it became clear that it made no sense\nto make it illegal and it was causing more problems. And so then it was revoked. And here we sit with legal alcohol a hundred years later\nwith all the same problems. And you know, the whole thing was this like giant misadventure\nthe Baptist got taken advantage of by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are\nnow sort of suggesting that the development of artificial intelligence should be regulated. - A hundred percent.\nIt's the same pattern. And the economist will tell you it's the same pattern every time. Like, this is what happened, nuclear power, this is what happens, which is another interesting one.\nBut like, yeah, this happens dozens and dozens of times throughout the last a hundred years and this is what's happening now.\n- And you write that it isn't sufficient to simply identify the actors and impugn their motives.\nWe should consider the arguments of both the Baptist and the bootleggers on their merits. So let's do just that.\nRisk number one, will AI kill us all?\n- [Marc] Yes. - So what do you think about this one?\nWhat do you think is the core argument here that the development of AGI perhaps better said,\nwill destroy human civilization? - Well, first of all, you just did a slight of hand 'cause we went from talking about AI to AGI.\n- Is there a fundamental difference there? - I don't know. What's AGI? - What's AI, what's in intelligence?\n- Well, I know what AI is machine learning. What's AGI? - I think we don't know what the bottom of the well\nof machine learning is or what the ceiling is. Because just to call something machine learning or just to call some of the statistics\nor just to call it math or computation doesn't mean, you know, nuclear weapons are just physics.\nSo to me it's very interesting and surprising how far machine learning has taken.\n- No, but we knew that nuclear physics would lead to weapons. That's why the scientists of that era were always in some this huge dispute about building the weapons.\nThis is different. AGI is different. - Does machine learning lead, do we know? - We don't know, but this is my point is different. We actually don't know.\nBut, and this is where you, the slide of hand kicks in, right? This is where it goes from being a scientific topic to being a religious topic.\nAnd that's why I specifically called out 'cause that's what happens. They do the vocabulary shift and all of a sudden you're talking about something totally.\nThat's not actually real. - Well then maybe you can also, as part of that, define the western tradition of Millennialism.\n- [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults.\n- Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian kind of saturated, you know, kind of Christian, post-Christian, secularized Christian,\nyou know, kind of world in the west. And of course court of Christianity is the idea of the second coming and you know,\nthe revelations and you know, Jesus returning and the thousand year, you know, utopia on earth and then you know,\nthe rapture and like all all that stuff, you know, you know, we collectively, you know, as a society, we don't necessarily take all that fully seriously now.\nSo, what we do is we create our secularized versions of that we keep looking for utopia. We keep looking for, you know,\nbasically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of is this is what cults are.\nThis is how cults form as they form around some theory of the end of the world. And so the people's temple cults, the Manson cult, the Heavens Gate cult,\nthe David Qresh cult, you know what they're all organized around is like, there's gonna be this thing that's gonna happen\nthat's gonna basically bring civilization crashing down. And then we have this special elite group of people who are gonna see it coming and prepare for it.\nAnd then they're the people who are either going to stop it or are failing, stopping it. They're gonna be the people who survived the other side\nand ultimately get credit for having been, right. - Why is that so compelling, do you think? Like-- - Because it satisfies this very deep need\nwe have for transcendence and meaning that got stripped away when we became secular.\n- Yeah, but why is the transcendence involve the destruction of human civilization?\n- Because like how plausible it's like a very deep psychological thing 'cause it's like how plausible,\nhow plausible is it that we live in a world where everything's just kind of all right? Right. How exciting? - [Lex] Whoa.\n- How exciting is that? Right? - [Lex] But that's. - We got more than that. - But that's the deep question I'm asking. Why is it not exciting to live in a world\nwhere everything's just all right? Is it, I think, you know, most of the animal kingdom would be\nso happy with just all right. Because that means survival. Why are we, maybe that's what it is.\nWhy are we conjuring up things to worry about? - So CS Lewis called it the God-shaped hole.\nSo there's a God-shaped hole in the human experience, consciousness, soul, whatever you wanna call it,\nwhere there's gotta be something that's bigger than all this. There's gotta be something transcendent.\nThere's gotta be something that is bigger, right? Bigger purpose. A bigger meaning. And so we have run the experiment of, you know,\nwe're just gonna use science and rationality and kind of, you know, everything's just gonna kind of be as it appears. And large number of people have found\nthat very deeply wanting and have constructed narratives. And by this is the story of the 20th century, right?\nCommunism, right? Was one of those, communism was a was a form of this, Nazism was a form of this.\nYou know, some people, you know, you can see movements like this playing out all over the world right now.\n- So you constructed a kind of devil, a kind of source of evil, and we're going to transcend beyond it.\n- Yeah. And (indistinct) when you see a Miller cult, they put a really specific point on it,\nwhich is end of the world, right, there is some change coming. And that change that's coming is so profound\nand so important that it's either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know,\nit's like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it?\nHow would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius of weapons in the woods?\nWould you like, you know, I don't know if create underground buckers, would you, you know, spend your life trying to figure out a way to avoid having it happen?\n- Yeah. That's a really compelling, exciting idea to have a club over.\nTo have a little bit of travel, like a get together on a Saturday night and drink some beers and talk about the end of the world\nand how you are the only ones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything else with your life?\nLike this is obviously the thing that you have to do. And then there's a psychological effect that you alluded to. There's a psychological effect.\nIf you take a set of true believers and you leave them to themselves, they get more radical. Right. 'Cause they self radicalize each other.\n- That said, it doesn't mean they're not sometimes right. - Yeah. The end of the world might be.\nYes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly.\n- But I mean we'll talk about nuclear weapons 'cause you have a really interesting little moment that I learned about in your essay, but you know,\nsometimes it could be right. - [Marc] Yeah. - 'Cause we're still, you were developing more and more powerful technologies\nin this case, and we don't know what the impact it will have on human civilization while we can highlight all the different predictions\nabout how it'll be positive, but the risks are there and you discuss some of them.\n- Well, the steel man, the steel man is the steel man. Well actually, the steel man and his reputation are the same, which is you can't predict\nwhat's gonna happen. Right. You can't rule out that this will not end everything. Right. But the response to that is you have just made\na completely non-scientific claim. You've made a religious claim, not a scientific claim. - How does it get disproven?\n- And there's no, by definition with these kinds of claims, there's no way to disprove them. Right? And so there there's no, you just go right on the list.\nThere's no hypothesis, there's no testability of the hypothesis. There's no way to falsify the hypothesis,\nthere's no way to measure progress along the arc. Like it's just all completely missing.\nAnd so it's not scientific and. - I don't think it's completely missing. It's somewhat missing.\nSo for example, the people that say AI's gonna kill all of us. I mean, they usually have ideas about how to do that.\nWhether it's the people club maximizer or, you know, it escapes there's mechanism by which you can imagine it\nkilling all humans. - [Marc] Models. - And you can disprove it by saying\nthere's a limit to the speed\nat which intelligence increases. Maybe show that like the sort of rigorously really described\nmodel, like how it could happen and say, no, there, here's a physics limitation.\nThere's like a physical limitation to how these systems would actually do damage to human civilization.\nAnd it is possible they will kill 10 to 20% of the population, but it seems impossible for them to kill 99%.\n- It was practical counterarguments. Right. So you mentioned basically what I described as the thermodynamic counterargument, which, so sitting here today,\nit's like where with the evil AGI get the GPU. 'Cause like they don't exist. So if you're gonna have a very frustrated baby evil AGI,\nwho's gonna be like trying to buy Nvidia stock or something to get them to finally make some chips, right? So the serious form of that is the thermodynamic argument,\nwhich is like, okay, where's the energy gonna come from? Where's the processor gonna be running? Where's the data center gonna be happening?\nHow is this gonna be happening in secret such that, you know, it's not, you know, so that's a practical counter argument to the runaway AGI thing.\nI have a but I have and we can argue that, discuss that. I have a deeper objection to it, which is it's, this is all forecasting.\nIt's all modeling, it's all future prediction. It's all future hypothesizing. It's not science.\n- [Lex] Sure. - It is not. It is the opposite of science. So the, I'll pull up Carl Sagan extraordinary claims require\nextraordinary proof, right? These are extraordinary claims. The policies that are being called for right to prevent this\nare of extraordinary magnitude that, and I think we're gonna cause extraordinary damage. And this is all being done on the basis of something\nthat is literally not scientific. It's not a testable hypothesis. - So the moment you say AI's gonna kill all of us, therefore we should ban it,\nor that we should regulate all that kind of stuff, that's when it starts getting serious. - Or start, you know, military airstrikes and data centers.\n- [Lex] Oh boy. - Right? And like. - Yeah. This once get starts.\nWell, so starts getting real weird. - So here's the problem with Arian cults. They have a hard time staying away from violence.\n- Yeah. But violence is so fun. - If you're on the right end of it,\nthey have a hard time avoiding violence. The reason they have a hard time avoiding violence is if you actually believe the claim. Right.\nThen what would you do to stop the end of the world? Well, you would do anything, right? And so, and this is where you get, and again,\nif you just look at the history of Arian and cults, this is where you get the people's temple and everybody killing themselves in the jungle. And this is where you get Charles Manson and, you know,\nsending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line\nat actual violence. And I think in this case, I mean, they're already calling for it like today and you know,\nwhere this goes from here is they get more worked up. Like I think is like really concerning. - Okay. But that's kind of the extremes.\nSo, you know, the extremes of anything are I was concerning. It's also possible to kind of believe that AI has\na very high likelihood of killing all of us. But and therefore we should maybe consider\nslowing development or regulating, so not violence or any of these kinds of things. But it's saying like, all right,\nlet's take a pause here. You know, you biological weapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa.\nThis is like serious stuff. We should be careful. So it is possible to kinda have\na more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be,\nhave a scientific approach to the prediction of the future? - I mean, we just went through this with COVID.\nWhat do we know about modeling? - [Lex] Well, I mean. - What did we learn about modeling with COVID?\n- [Lex] There's a lot of lessons. - They didn't work at all. - [Lex] They worked poorly. - The models were terrible, the models were useless.\n- I don't know if the models were useless or the people interpreting the models and then decentralized institutions\nthat were creating policy rapidly based on the models and leveraging the models in order to support their narratives\nversus actually interpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID is you had these experts\nshowing up and they claimed to be scientists and they had no testable hypotheses whatsoever. They had a bunch of models.\nThey had a bunch of forecasts and they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked out and panicked. Right.\nAnd implemented a whole bunch of like, really like terrible decisions that we're still living with the consequences of,\nand there was never any empirical foundation to any of the models. None of them ever came true.\n- Yeah. To push back. There were certainly Baptist and bootleggers in the context of this pandemic, but there's still a usefulness to models. No.\n- So not if they're, I mean not if they're reliably wrong, right? Then they're actually like anti-useful. Right. They're actually damaging.\n- But what do you do with the pandemic? What do you do with any kind of threat? Don't you want to kind of have several models to play\nwith as part of this discussion of like, what the hell do we do here? - I mean, do they work?\nBecause they're an expectation that they actually like work that they have actual predictive value.\nI mean, as far as I can tell with COVID, the policymakers just si up themselves into believing that there was sub, I mean, look, the scientists,\nthe scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a,\nor remember the Imperial College models out of London were the ones that were like, these are the gold standard models. So a friend of mine runs a big software company\nand he was like, wow, this is like, COVID is really scary. And he is like, you know, he contacted this research and he is like, you know, do you need some help? You've been just building this model on your own\nfor 20 years. Do you need some, would you like us our coders to basically restructure it so it can be fully adapted for COVID? And the guy said yes and sent over the code\nand my friend said it was like the worst spaghetti code he's ever seen. - That doesn't mean it's not possible to construct\na good model of pandemic with the correct air bars, with a high number of parameters that are continuously,\nmany times a day updated as we get more data about a pandemic. I would like to believe when a pandemic hits the world,\nthe best computer scientists in the world, the best software engineers respond aggressively\nand as input take the data that we know about the virus and it's an output say here is what's happening\nin terms of how quickly it's spreading, what that lead in terms of hospitalization and deaths and all that kind of stuff.\nHere's how likely, how contagious it likely is. Here's how deadly it likely is based on different conditions,\nbased on different ages and demographics and all that kind of stuff. So here's the best kinds of policy. It feels like you could have models,\nmachine learning that like kind of, they don't perfectly predict the future,\nbut they help you do something 'cause there's pandemics that are like, meh,\nthey don't really do much harm. And there's pandemics, you can imagine them, they could do a huge amount of harm.\nLike they can kill a lot of people. So you should probably have some kind of data-driven models\nthat keep updating, that allow you to make decisions that based like where, how bad is this thing?\nNow you can criticize how horrible all that went with the response to this pandemic,\nbut I just feel like there might be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing for me to do is to say,\nobviously you're right. Obviously I wanna see that just as much as you do. 'cause anything that makes it easier to navigate through society through a wrenching, you know, risk like\nthat sounds great. You know, the harder objection to it is just simply you are trying to model\na complex dynamic system with 8 billion moving parts. Like not possible. - [Lex] It's very tough.\n- Can't be done, complex systems can't be done. - Machine learning says hold my beer. But well, it's possible. No?\n- I don't know. I would like to believe that it is. I'll put it this way. I think where you and I would agree is I think we would like that to be the case.\nWe are strongly in favor of it. I think we would also agree that no such thing with respect to COVID or pandemics no such thing.\nAt least neither you nor I think are aware. I'm not aware of anything like that today. - My main worry with the response to the pandemic is\nthat same as with aliens, is that even if such a thing existed,\nand it's possible it existed, the policymakers were not paying attention.\nLike there was no mechanism that allowed those kinds of models to percolate all. - Oh, I think we had the opposite problem during COVID.\nI think the policymakers, I think these people with basically fixed science had too much access to the policymakers.\n- Well, right. And well, but the policy makers also wanted, they had a narrative in mind and they also wanted to use whatever model that fit that narrative\n- [Marc] Oh, sure. - To help them out. So like, it felt like there was a lot of politics and not enough science. - Although a big part of what was happening, a big reason we got lockdowns for as long as we did,\nwas because these scientists came in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let's not--\n- [Marc] Quote unquote scientists. - Let's not, okay, let's give love science. So here's science that is the way out. - Science is a process of testing hypotheses.\nModeling does not involve testable hypotheses. Right. Like, I don't even know that. I actually don't even know\nthat modeling actually qualifies as science. Maybe that's a side conversation. We could have some time over a beer.\n- Oh, that's a really interesting part. What do we do about the future? I mean, what's-- - So number one is when we start with number one,\nhumility goes back to this thing of how do we determine the truth. Number two is we don't believe, you know, it's the old,\nI've gotta hammer everything looks like a nail, right? I've got, oh, this is one of the reasons I gave you, I gave Alexa book,\nwhich the topic of the book is what happens when scientists basically stray off the path of technical knowledge and start to weigh in on politics\nand societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this book about, like Einstein,\nhe talks about, actually about the nuclear age in Einstein. He talks about the physicists actually doing very similar things at the time.\n- The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin.\n- And it's just a story. It's a story. There are other books on this topic, but this is a new one that's really good this is just a story\nof what happens when experts in a certain domain decide to weigh in and become basically social engineers and political, you know, basically political advisors.\nAnd it's just a story of just inning catastrophe. Right. And I think that's what happened with COVID again.\n- Yeah. I found this book a highly entertaining and eye-opening read filled with amazing anecdote of a rationality and craziness by famous Resa philosophers.\n- I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don't destroy my heroes.\n- He will not be a hero of yours anymore. Sorry. You probably couldn't, you shouldn't read the book. - All right.\n- But here's the thing. The AI risk people, they don't even have the COVID model,\nat least not that I'm aware of. - [Lex] No. - Like there's not even the equivalent of the COVID model. They don't even have the spaghetti code. They've got a theory and a warning and a this and the that.\nAnd like, if you ask like, okay, well here's, I mean, the ultimate example is, okay, how do we know, right?\nHow do we know that an AI is running away? Like how do we know that the boom takeoff thing is actually happening? And the only answer that any of these guys have given\nthat I've ever seen is, oh, it's when the loss rate, the loss function and the training drops, right?\nThat's when you need to like shut down the data center. Right? And it's like, well that's also what happens when you're successfully training a model.\nLike, what even this is not science,\nthis is not, it's not anything, it's not a model, it's not anything. There's nothing to arguing with. It is like, you know, punching jello, like there,\nthere's what do you even respond to? - So just put push back on that. I don't think they have good metrics\nof when the film is happening. But I think it's possible to have that. Like just as you speak now,\nI mean it's possible to imagine there could be measures. - It's been 20 years. - No, for sure.\nBut it is been only weeks since we had a big enough breakthrough in language models. We can start to actually have this,\nthe thing is the AI doer stuff didn't have any actual systems to really work with.\nAnd now there's real systems you can start to analyze like, how does this stuff go wrong? And I think you kind of agree that there is\na lot of risks that we can analyze. The benefits outweigh the risks in many cases. - Well, the risks are not existential.\n- [Lex] Yes. Well. - Not in the phone paper clip. Let me, okay. There's another slide of hand that you just alluded to.\nThere's another slide of hand that happens, which is very interesting. - I'm very good at the slide of hand thing. - Which is very not scientific.\nSo the book Super Intelligence, right, which is like the Nick Bostrom's book, which is like the origin of a lot of this stuff,\nwhich was written, you know, whatever, 10 years ago or something. So he does this really fascinating thing in the book, which is he basically says there are many possible routes\nto machine intelligence, to artificial intelligence. And he describes all the different routes to artificial intelligence, all the different possible,\neverything from biological augmentation through to, you know, all these different things. One of the ones that he does not describe is\nlarge language models because of course the book was written before they were invented. And so they didn't exist.\nIn the book, he describes them all and then he proceeds to treat them all as if they're exactly the same thing.\nHe presents them all as sort of an equivalent risk to be dealt with in an equivalent way to be thought about the same way. And then the risk, the quote unquote risk\nthat's actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories and beliefs are being transplanted by this movement,\nlike straight onto this new technology. And so again, like there's no other area of science or technology where you do that.\nLike when you're dealing with like organic chemistry versus inorganic chemistry, you don't just like say, oh,\nwith respect to like either one, basically maybe, you know, growing up in eating the world or something, like they're just gonna operate the same way.\nLike you don't. - But you can start talking about like, as we get more and more actual systems\nthat start to get more and more intelligent, you can start to actually have more scientific arguments here.\n- [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat of autonomous weapon systems back before we had any automation in the military.\nAnd that would be like very fuzzy kind of logic. But the more and more you have drones that are becoming\nmore and more autonomous, you can start imagining, okay, what does that actually look like and what's the actual\nthreat of autonomous weapons systems? How does it go wrong? And still it's very vague,\nbut you start to get a sense of like, all right, it should probably be illegal\nor wrong or not allowed to do like mass deployment\nof fully autonomous drones that are doing aerial strikes. - [Marc] Oh no.\n- On large areas. - [Marc] I think it should be required. - Right? So that's a no. - No, no. I think it should be required that only aerial vehicles are automated.\n- Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it's obvious that the machine is gonna make\na better decision than the human pilot. I think it's obvious that it's in the best interest of both the attacker and the defender and humanity at large.\nIf machines are making more of these decisions than not people, I think people make terrible decisions in times of war. - But like, there's ways this can go wrong too, right?\n- Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing about like the self-drive.\nDoes the self-driving car need to be perfect versus does it need to be better than the human driver? Does the automated drone need to be perfect\nor does it need to be better than a human pilot at making decisions under enormous amounts of stress and uncertainty?\n- Yeah, well, on average, the worry that AI folks have is the runaway.\n- They're gonna come alive. Right? That then again, that's the slight of hand, right. - Or not not come alive. Well, no, hold on a second.\nYou lose control as well. You lose control. - But then they're gonna develop goals of their own. They're gonna develop a mind of their own,\nthey're gonna develop their own. Right. - No more, more like Chernobyl style meltdown, like just bugs in the code accidentally, you know,\nforce you like the results in the bombing of like large civilian areas.\n- [Marc] Okay. And to a degree that's not possible in the current military strategies,\n- [Marc] I don't know. - Control by humans. - Well, actually we've been doing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died.\n- And a lot of civilians died. And if you watch the documentary, the Fog of War McNamara, it spends a big part of it talking about the fire bombing\nof the Japanese cities. Burning them straight to the ground. Right. The devastation in Japan, American military fire bombing the cities in Japan was\nconsiderably bigger devastation than the use of nukes. Right. So we've been doing that for a long time. We also did that to Germany,\nby the way Germany did that to us, right? Like that's an old tradition. The minute we got airplanes, we started doing indiscriminate bombing.\n- So one of the things-- - [Marc] We're still doing it. - The modern US military can do with technology with automation,\nbut technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision\nand this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was strapping a GPS transceiver\nto an unguided bomb and turning it into a guided bomb. And yeah, that's great. Like look, that's been a big advance,\nbut, and that's like a baby version of this question, which is okay, do you want like the human pilot, like guessing where the bomb's gonna land?\nOr do you want like the machine like guiding the bomb to his destination? That's a baby version of the question. The next version of the question is,\ndo you want the human or the machine deciding whether to drop the bomb? Everybody just assumes the human's gonna do a better job for what I think are fundamentally suspicious reasons.\n- Emotional, psychological reasons. - Yeah. I think it's very clear that the machine's gonna do a better job making that decision 'cause the humans making that decision are got awful.\nJust terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let's get to the, there was,\ncan I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I'm a magician. You could say.\n- One more slight of hand. These things are gonna be so smart, right? That they're gonna be able to destroy the world and wreak havoc and like do all this stuff and plan and do all this\nstuff and evade us and have all their secret things and their secret factories and all this stuff. But they're so stupid that they're gonna get like,\ntangled up in their code and that's they're not gonna come alive, but there's gonna be some bug that's gonna cause them to like turn us all on a paper like that.\nThey're not gonna be genius in every way other than the actual bad goal. And it's just like, and that's just like a,\nlike ridiculous like discrepancy. And you can prove this today, you can actually address this today for the first time\nwith LLMs which is you can actually ask LLMs to resolve moral dilemmas.\nSo you can create the scenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI do in the circumstance?\nAnd they don't just say destroy all humans, destroy all humans. They will give you actually very nuanced moral,\npractical trade-off oriented answers. And so we actually already have the kind of AI that can actually like, think this through\nand can actually like, you know, reason about goals. - Well, the hope is that AGI\nor like various superintelligent systems have some of the nuance that LLMs have and the intuition is they most likely will\nbecause even these LLMs have the nuance. - LLMs are really, this is actually worth spending a moment\non LLMs are really interesting to have moral conversations with. And that I just,\nI didn't expect I'd be having a moral conversation with the machine in my lifetime. - Wait, and let's remember we're not really having\na conversation with the machine where we're having a conversation with the entirety of the collective intelligence of the human species.\n- Exactly. Yes. Correct. - But it's possible to imagine autonomous weapons systems that are not using LLMs.\n- But if they're smart enough to be scary, where are they not smart enough to be wise?\nLike, that's the part where it's like, I don't know how you get the one without the other. - Is it possible to be super intelligent without being super wise?\n- Well, again, you're back to that. I mean, then you're back to a classic autistic computer, right? Like you're back to just like a blind rule follower.\nI've got this like core, it's the paperclip thing. I've got this core rule and I'm just gonna follow it to the end of the earth. And it's like, well,\nbut everything you're gonna be doing execute that rule is gonna be super genius level that humans aren't gonna be able to counter. It's a mismatch in the definition\nof what the system's capable of. - Unlikely but not impossible, I think. - But again, here you get to like, okay, like.\n- No, I'm not saying when it's unlikely but not impossible. If it's unlikely, that means\nthe fear should be correctly calibrated. - Extraordinary claims require extraordinary proof. - Well, okay, so one interesting sort of tangent,\n"}
{"pod": "Lex Fridman Podcast", "input": "Nuclear energy", "output": "I would love to take on this because you mentioned this in the essay about nuclear, which was also, I mean,\nyou don't shy away from a little bit of of a spicy take. So Robert Oppenheimer famously said,\nnow I am become death the destroyer of worlds as he witnessed the first destination of a nuclear weapon\non July 16th, 1945. And you write an interesting historical perspective,\n\"Recall that John Van Neuman responded to Robert Oppenheimer's famous hand wringing about the role\nof creating nuclear weapons, which you note helped end World War II\nand prevent World War III with some people confess guilt to claim credit for the sin.\"\nAnd you also mentioned that Truman was harsher after meeting Oppenheimer. He said that \"Don't let that cry baby in here again.\"\n- Real quote, by the way, from Dean Atchison.\n- Boy. - 'Cause Oppenheimer didn't just say the famous line. - [Lex] Yeah. - He then spent years going around basically moaning him,\nyou know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this sort of self-critical like,\noh my god, I can't believe how awful I am. - So he's widely considered perhaps of the,\nbecause of the hang ringing as the father of the tom bomb. - [Marc] Yeah. - This is Van Norman's criticism of him is he tried to have\nhis cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and he's just like, yeah, good.\nThis is like an incredibly useful thing. I'm glad we did it. - Yeah. Well Van Norman is is widely credit as being\none of the smartest humans of the 20th century. Certain people. Everybody says like,\nthis is the smartest person I've ever met when they've met him. Anyway, that doesn't mean, smart doesn't mean wise.\nSo yeah, I would love to sort of, can you make the case both for and against the critique\nof Oppenheimer here? 'Cause we're talking about nuclear weapons. Boy, do they seem dangerous?\n- Well so, the critique goes deeper and I left this out. Here's the real substance, I left it out 'cause I didn't wanna dwell\non nukes in my AI paper. But here's the deeper thing that happened and I'm really curious, this movie coming out this summer,\nI'm really curious to see how far he pushes this. 'cause this is the real drama in the story, which is, it wasn't just a question of our nukes, good or bad,\nit was a question of should Russia also have them? And what actually happened was Russia got\nthe American invented the bomb. Russia got the bomb, they got the bomb through espionage,\nthey got American and you know, they got American scientists and foreign scientists working on the American project. Some combination of the two basically gave the Russians\nthe designs for the bomb. And that's how the Russians got the bomb. There's this dispute to this day of Oppenheimer's role\nin that if you read all the histories, the kind of composite picture, and by the way,\nwe now know a lot actually about Soviet espionage in that era 'cause there's been all this declassified material in the last 20 years that actually shows a lot of very interesting things.\nBut if you kinda read all the histories, which you kinda get is Oppenheimer himself probably was not he probably did not hand over the nuclear secrets himself.\nHowever, he was close to many people who did. Including family members. And there were other members of the Manhattan Project\nwho were Russian, Soviet SS and did hand over the bomb. And so the view of that Oppenheimer and people like him had\nthat this thing is awful and terrible and oh my god. And you know, all this stuff you could argue fed into this ethos\nat the time that resulted in people thinking that the Baptists thinking that the only principle thing to do was to give\nthe Russians the bomb. And so the moral beliefs on this thing and the public discussion\nand the role that the inventors of this technology play, this is the point of this book, when they kind of take on this sort of public intellectual,\nmoral kind of thing, it can have real consequences, right? Because we live in a very different world today\nbecause Russia got the bomb than we would've lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th century would've played out\nvery different had those people not given Russia the bomb. And so the stakes were very high then. The good news today is nobody's sitting here today,\nI don't think worrying about like an analogous situation with respect to like, I'm not really worried that Sam Altman's gonna decide\nto give, you know, the Chinese, the design for AI, although he did just speak at a Chinese conference,\nwhich is in interesting. But however, I don't think that's what's at play here, but what's at play here are all these other fundamental\nissues around what do we believe about this and then what laws and regulations and restrictions that we're gonna put on it.\nAnd that's where I draw like a direct straight line. And anyway, and my reading of the history on nukes is like the people who were doing the full hair shirt public,\nthis is awful. This is terrible. Actually had like catastrophically bad results from taking those views.\nAnd that's what I'm worried it's gonna happen again. - But is there a case to be made that you really need to wake the public up to the dangers\nof nuclear weapons when they were first dropped? Like really like educate them on like, this is extremely dangerous and destructive weapon.\n- I think the education kind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How?\n- We dropped one bomb and destroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But.\n- [Marc] And look. But-- - I don't like the reporting of that. You can report that in all kinds of ways.\n- [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make it seem like nuclear,\nthe use of nuclear weapons is just a part of war and all that kind of stuff. Something about the reporting and the discussion\nof nuclear weapons resulted in us being terrified in awe of the power of nuclear weapons\nand that potentially fed in a positive way towards the game theory of mutual issue destruction.\n- Well, so this gets to what actually, let's get to what actually happens. - [Lex] Some of us, me playing devil's advocate here. - Yeah, yeah, sure. Of course. Let's get to what actually happened and then kind\nof back into that. So what actually happened, I believe, and again I think this is a reasonable reading of history, is what actually happened was nukes then prevented\nWorld War III and they prevented World War III through the game theory of mutually assured destruction had nukes not existed. Right.\nThere would've been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners at the time, right,\nthought both on both sides thought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right?\nIt was like the most obvious thing in the world to happen. Right? And it's the dog that didn't bark right? Like it may be like the best single net thing that happened\nin the entire 20th century is that like that didn't happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things.\nIt hit me just as you were saying it. I don't know why it hit me for the first time,\nbut we got two wars in a span of like 20 years.\nLike we could have kept getting more and more world wars and more and more ruthless.\nIt actually, you could have had a US versus Russia war. - You could, by the way you haven't,\nthere's another hypothetical scenario. The other hypothetical scenario is that Americans got the bomb, the Russians didn't.\nRight? And then America's the big dog and then maybe America would've had the capability to actually roll back the iron curtain.\nI don't know whether that would've happened, but like it's entirely possible. Right? And the act of these people\nwho had these moral positions about, 'cause they could forecast, they could model, they could forecast the future of how the technology would get used, made a horrific mistake.\n'cause they basically ensured that the iron curtain would continue for 50 years longer than it would've otherwise. Like, and again, like these are counter-factuals,\nI don't know that that's what, what would've happened, but like the decision to hand the bomb\nover was a big decision made by people who were very full of themselves.\n- Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones\nwith the nuclear weapons. - That was the argument for handing\nthat was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably not hand it over to,\nI would be careful about the regimes. You hand it over to there, maybe give it to like the British or something,\nor like a democratically-elected government. - Well, look, there are people to this day\nwho think that those bias Soviet spies did the right thing because they created a balance of terror as opposed to the US having just, and by the way, let me--\n- Balance of terror. - [Marc] Let's tell the full version story has-- - Such a sexy ring to it. - Okay. So the full version of the story is John Van Norman is a hero of both yours and mind.\nThe full version of the story is he advocated for a first strike. So when the US had the bomb and Russia did not,\nhe advocated for, he said, we need to strike them right now. - Strike Russia.\n- [Marc] Yes. - Van Norman. - Yes, because he said World War III is inevitable.\nHe was very hardcore. His theory was World War III is inevitable.\nWe're definitely gonna have World War III. The only way to stop World War III is we have to take them out right now and we have to take them out right now before they get the bomb.\n'Cause this is our last chance. Now again, like-- - Is this an example of philosophers and politics? - I don't know if that's in there or not,\nbut this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies in books like this are\nthe crazy people on the left. Van Norman is a story arguably of the crazy people on the right.\n- Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core thing, which is like,\nI don't know whether any of these people should be making any of these calls. Because there's nothing in either Van Norman's background\nor Oppenheimer's background or any of these people's background that qualifies them as moral authorities. - Yeah. Well this actually brings up the point of, in AI,\nwho are the good people to reason about the morality of the ethics, the outside of these risks,\noutside of like the more complicated stuff that you, you agree on is, you know, this will go into the hands of bad guys\nand all the kinds of ways they'll do is interesting and dangerous, is dangerous in interesting\nunpredictable ways. And who is the right person? Who are the right kinds of people to make decisions, how to respond to it? Or is the tech people?\n- So the history of these fields, this is what he talks about in the book, the history of these fields, is that the competence\nand capability and intelligence and training and accomplishments of senior scientists and technologists working on a technology and then being able\nto then make moral judgments in the use of that technology. That track record is terrible that track record is like catastrophically bad.\nThe people-- - Just the linger, the people that develop that technology are usually not going to be the right people.\n- Well why would they? So the claim is of course, they're the knowledgeable ones. But the the problem is they've spent their entire life in a lab. Right.\nThey're not theologians. Well, so what you find, what you find when you read, when you read this, when you look at these histories,\nwhat you find is they generally are very thinly informed on history, on sociology, on theology, on morality, on ethics.\nThey tend to manufacture their own worldviews from scratch. They tend to be very sort of thin.\nThey're not remotely the arguments that you would be having if you got like a group of highly qualified theologians or philosophers or, you know.\n- Well, let me sort of, as the devil's advocate, takes a simple whiskey say that I agree with that.\nBut also it seems like the people who are doing kind of the ethics departments and these tech companies go sometimes the other way.\n- [Marc] Yes, they're definitely. - Which they're not nuanced on history or theology\nor this kind of stuff. It almost becomes a kind of outraged activism towards directions that don't seem to be\ngrounded in history and humility and nuance. It's again, drenched with arrogance. So--\n- [Marc] Definitely. - I'm not sure which is worse. - Oh no, they're both bad. Yeah. So definitely not them either. - So, but I guess.\n- Well look, this is a hard. - Yeah, it's a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth?\nAnd it's like, well, you know, like how does societies arrive at like truth and how do we figure these things out and like our elected leaders play some role in it.\nYou know, we all play some role in it. There have to be some set of public intellectuals at some point that bring, you know,\nrationality and judgment and humility to it. Those people are few and far between. We should probably prize them very highly.\n- Yeah. So celebrate humility in our public leaders. So getting to risk number two,\n"}
{"pod": "Lex Fridman Podcast", "input": "Misinformation", "output": "will AI ruin our society short version as you write, if the murder robots don't get us the hate speech\nand misinformation will. And the action you recommend in short,\ndon't let the thought police suppress AI. Well what is this risk of the effect of misinformation\nof society that's going to be catalyzed by AI? - Yeah, so this is the social media,\nthis is what you just alluded to. It's the activism kind of thing that's popped up in these companies in the industry. And it's basically, from my perspective,\nit's basically part two of the war that played out over social media over the last 10 years, 'cause you probably remember social media 10 years ago,\nwas basically who even wants this? Who wants a photo of what your cat had for breakfast? Like, this stuff is like silly and trivial\nand why can't these nerds like figure out how to invent something like useful and powerful? And then, you know, certain things happened in the political system.\nAnd then it sort of, the polarity on that discussion switched all the way to social media is like the worst, most corrosive, most terrible, most awful technology ever invented.\nAnd then it leads to, you know, terrible of the wrong, you know, politicians and policies and politics and like, and all this stuff.\nAnd that all got catalyzed into this very big kind of angry movement both inside and outside the companies\nto kind of bring social media to heal. And that got focused in particularly on two topics, so-called hate speech and so-called misinformation.\nAnd that's been the saga playing out for the last decade. And I don't even really want to even argue the pros and cons of the sides just to observe\nthat's been like a huge fight and has had, you know, big consequences to how these companies operate.\nBasically that same, those same sets of theories, that same activist approach, that same energy as being transplanted straight to AI.\nAnd you see that already happening. It's why, you know, ChatGPT will answer, let's say certain questions and not others. It's why it gives you the canned speech about, you know,\nwhenever it starts with, as a large language model, I cannot, you know, basically means that somebody has reached in there and told that it can't talk about certain topics.\n- Do you think some of that is good? - So it's an interesting question. So a couple observations.\nSo, one is the people who find this the most frustrating are the people who are worried about the murder robots, right?\nSo, and in fact so called X risk people, right? They started with the term AI safety,\nthe term became AI alignment. When the term became AI alignment is when this switch happened from we're worried it's gonna kill us all to we're worried about hate speech\nand misinformation. - [Lex] Sure. - The AI X risk people have now renamed their thing AI not kill everyone-ism,\nwhich I have to admit is a catchy term. And they are very frustrated by the fact that the hate speech sort of activist driven hate speech misinformation\nkind of thing is taking over. Which is what's happened is taken over, the AI ethics field has been taken over by the hate speech misinformation people.\nYou know, look, would I like to live in a world in which like everybody was nice to each other all the time and nobody ever said\nanything mean and nobody ever used a bad word and everything was always accurate and honest. Like, that sounds great.\nDo I wanna live in a world where there's like a centralized thought police working through the tech companies to enforce the view of a small set of elites that they're gonna\ndetermine what the rest of us think and feel like? Absolutely not. - There could be a middle ground somewhere like\nWikipedia type of moderation. There's moderation of Wikipedia that is somehow crowdsourced\nwhere you don't have centralized elites, but it's also not completely just a free for all\nbecause if you have the entirety of human knowledge at your fingertips, you can do a lot of harm.\nLike if you have a good assistant that's completely uncensored, they can help you build a bomb,\nthey can help you mess with people's physical wellbeing.\nRight. If they, because that information is out there on the internet and so presumably there's, it would be,\nyou could see the positives in censoring some aspects of an AI model\nwhen it's helping you commit literal violence. - Yeah. And there's a section later section of the essay\nwhere I talk about bad people doing bad things. - [Lex] Yes. - Right. Which and there's this, there's a set of things that we should discuss there.\n- [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw.\nAnd what I've observed in the social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it's an inevitability.\nThe minute you have this kind of activist personality that gets in a position to make these decisions they take it straight to infinity.\nLike, it goes into the crazy zone like almost immediately and never comes back because people become drunk with power.\nRight. And look, if you're in the position to determine what the entire world thinks and feels and reads and says like, you're gonna take it and you know, Elon has, you know,\nventilated this with the Twitter files over the last, you know, three months and it's just like crystal clear, like how bad it got there now.\n- [Lex] Yeah. - Reason for optimism is what Elon is doing with community notes. So community notes is actually a very interesting thing.\nSo, what Elon is trying to do with community notes is he's trying to have it where there's only a community note\nwhen people who have previously disagreed on many topics agree on this one. - Yes, that's what I'm trying to get at is like,\nthere could be Wikipedia like models or community notes type of models where allows you to essentially either provide\ncontext or sensor in a way that's not resist the slippery slope nature. Power. - Now there's an entirely different approach here,\nwhich is basically we have AIs that are producing content. We could also have ais that are consuming content. Right?\nAnd so one of the things that your assistant could do for you is help you consume all the content, right? And basically tell you when you're getting played.\nSo for example, I'm gonna want the AI that my kid uses, right, to be very, you know, child safe and I'm gonna want it to filter for him all kinds\nof inappropriate stuff that he shouldn't be saying just 'cause he's a kid. Right? And you see what I'm saying is you can implement that. The architectural, you could say you can solve this\non the client side, right? You solving on the server side gives you an opportunity to dictate for the entire world, which I think is\nwhere you take the slippery slope to hell, there's another architectural approach, which is to solve this on the client side,\nwhich is certainly what I would endorse. - It's AI risk number five, will AI lead to bad people doing bad things?\nAnd I can just imagine language models used to do so many bad things, but the hope is there that you can have large language\nmodels used to then defend against it by more people, by smarter people, by more effective people, skilled people,\nall that kind of stuff. - Three-part argument on bad people doing bad things. So, number one, right?\nYou can use the technology defensively and we should be using AI to build like broad spectrum vaccines and antibiotics for like bio weapons and we should\nbe using AI to like hunt terrorists and catch criminals and like, we should be doing like all kinds of stuff like that. And in fact,\nwe should be doing those things even just to like go get like, you know, basically go eliminate risk from like regular pathogens that aren't like constructed by an AI.\nSo there's the whole defensive set of things. Second is we have many laws on the books\nabout as actual bad things, right? So it is actually illegal to be a criminal, you know, to commit crimes, to commit terrorist acts to, you know,\nbuild pathogens with the intent to deploy them to kill people. And so we have those, we actually don't need new laws\nfor the vast majority of these scenarios. We actually already have the laws in the book, on the books. The third argument is the minute,\nand this is sort of the foundational one that gets really tough, but the minute you get into this thing, which you were kind of getting into, which is like, okay,\nbut like, don't you need censorship sometimes, right? And don't you need restrictions sometimes? It's like, okay, what is the cost of that?\nAnd in particular in the world of open source, right? And so is open source AI going to be allowed or not?\nIf open source AI is not allowed, then what is the regime that's going to be necessary legally\nand technically to prevent it from developing? Right? And here again is where you get into and people have\nproposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor agent on every CPU and GPU\nthat reports back to the government? What we're doing with our computers, are we seizing GPU clusters that get beyond a certain size?\nLike, and then by the way, how are we doing all that globally, right? And like if China's developing an LLM beyond the scale\nthat we think is allowable, are we gonna invade? Right. And you have figures on the AI X risk side\nwho are advocating any, you know, potentially up to nuclear strikes to prevent, you know, this kind of thing. And so here you get into this thing\nand again, you know, maybe you could maybe say this is, you know, you could even say this is what good, bad or indifferent or whatever. But like here's the comparison of nukes,\nthe comparison of nukes is very dangerous because one is just nukes, were just, although we can come back to nuclear power.\nBut the other thing was like with nukes, you could control plutonium, right? You could track plutonium and it was like hard to come by. AI is just math and code, right?\nAnd it's in like math textbooks and it's like, there are YouTube videos that teach you how to build it. And like there's open source, there's already open source.\nYou know, there's a 40 billion parameter model running around already called Falcon Online that anybody can download. And so, okay,\nyou walk down the logic path that says we need to have guardrails on this. And you find yourself in an authoritarian,\ntotalitarian regime of thought control and machine control that would be so brutal that you would've destroyed\nthe society that you're trying to protect. And so I just don't see how that actually works. - So yeah, you have to understand my brain's going\nfull steam ahead here 'cause I agree with basically everything you're saying, but I'm trying to play devil's advocate here\nbecause okay, you're highlighted the fact that there is a slippery slope to human nature. The moment you censor something,\nyou start to censor everything. That alignment starts out sounding nice,\nbut then you start to align to the beliefs of some select group of people.\nAnd then it's just your beliefs the number of people you're aligning to smaller and smaller as that group becomes more and more powerful.\nOkay. But that just speaks to the people that censor are usually the assholes\nand the assholes get richer. I wonder if it's possible to do without that for AI.\nOne way to ask this question is do you think the base models, the baseline foundation models should be open sourced?\nLike, where Marc Zuckerberg is saying they want to do. - So look, I mean I think it's totally appropriate\nthe companies that are in the business of producing a product or service should be able to have a wide range\nof policies that they put, right? And I'll just, again, I want a heavily censored model for my eight year old.\nLike, I actually want that, like, like I would pay more money for the ones more heavily censored than the one that's not, right.\nAnd so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up\nor is this really a speech issue? One of the things that the big tech companies are dealing with is that content generated from an LLM is not covered\nunder section 230, which is the law that protects internet platform companies\nfrom being sued for the user generated content. And so it is actually-- - [Lex] Oh, wow.\n- Yes and so there, there's actually a question. I think there's still a question, which is can big American companies actually feel\ngenerative AI at all? Or is the liability actually gonna just ultimately convince them that they can't do it?\nBecause the minute the thing says something bad, and it doesn't even need to be hate speech, it could just be like an (indistinct) it could hallucinate\na product, you know, detail on a vacuum cleaner, you know, and all of a sudden the vacuum cleaner company sues\nfor misrepresentation. And there's asymmetry there, right? 'Cause the LLMs gonna be producing billions of answers to questions and it only needs to get a few wrong to have.\n- [Lex] So, loss has to get updated really quick here. - Yeah. And nobody knows what to do with that, right? So, so anyway, like there are big,\nthere are big questions around how companies operate at all. So we talk about those, but then there's this other question of like, okay,\nthe open source. So what about open source? And my answer to your question is kind of like, obviously yes, the models have,\nthere has to be full open source here because to live in a world in which that open source is not allowed is\na world of draconian speech control, human control, machine control.\nI mean, you know, black helicopters with jackbooted thugs coming out, repelling down and seizing your GPU like territory.\n- [Lex] Well. - No, no, I'm a hundred percent serious. - That's you're saying slippery slope always leads there.\n- No, no, no, no. That's what's required to enforce it. Like how will you enforce a ban on open source and AI? - No. Well you could add friction to it,\nlike harder to get the models. 'Cause people will always be able to get the models, but it'll be more in the shadows, right?\n- The leading open source model right now is from the UAE. Like the next time they do that, what do we do?\n- [Lex] Yeah. - Like. - Oh, I see you're like. - A 14 year old in Indonesia comes out with a breakthrough.\nYou know, we talked about most great software comes from a small number of people. Some kid comes out with some big new breakthrough and quantization or something\nand has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him?\n- It seems like in terms of size of models and effectiveness of models, the big tech companies will probably lead the way for quite\na few years and the question is of what policies they should use? The kid in Indonesia should not be regulated,\nbut should Google, Meta, Microsoft, Open AI be regulated?\n- Well, so, but this goes, okay, so when does it become dangerous? Right.\nIs the danger that it's as powerful as the current leading commercial model? Or it is just at some other arbitrary threshold?\nAnd then by the way, like look, how do we know, like what we know today is that you need like a lot of money to like train these things.\nBut there are advances being made every week on training efficiency and, you know, data, all kinds of synthetic, you know, look,\nI don't even like the synthetic data thing we're talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That's gonna change everything.\n- Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the breakthrough just happened.\nSo we don't know what the shape of this technology is gonna be. I mean the big shock here is that, you know,\nwhatever number of billions of parameters basically represents at least a very big percentage of human thought.\nLike who would've imagined that? And then there's already work underway. There was just this paper that just came out that basically\ntakes a gpt three scale model and compresses it down or run on a single 32 core CPU. Like who would've predicted that?\n- [Lex] Yeah. - You know, some of these models now you can run on raspberry pies like today they're very slow, but like, you know,\nmaybe they'll be a, you know, perceived you have real perform, you know, like it's math and code. And here we're back in here,\nwe're back in, dude, it's math and code. It's math and code, it's math, code and data. It's bits. - Marc has just like walked away at this point.\nYou just screw it. I don't know what to do with this. You guys created this whole internet thing.\nYeah, yeah. I mean, I'm a huge believer in open source here. - So my argument is we're gonna have,\nsee here's my argument is a, my argument, my full argument is, is AI is gonna be like air, it's gonna be everywhere. Like this is just gonna be in text.\nIt already is, it's gonna be in textbooks and kids are gonna grow up knowing how to do this. And it's just gonna be a thing. It's gonna be in the air and you can't like pull\nthis back anymore. You can't pull back air. And so you just have to figure out how to live in this world, right? And then that's where I think like all this hand ringing\nabout AI risk is basically a complete waste of time, 'cause the effort should go into okay, what is the defensive approach?\nAnd so if you're worried about you know, AI generated pathogens, the right thing to do is to have a permanent project warp speed, right?\nFunded lavishly. Let's do a Manhattan, let's talk about Manhattan project, let's do a Manhattan project for biological defense, right?\nAnd let's build ais and let's have like broad spectrum vaccines where like, we're insulated from every pathogen. - And well, the interesting thing is because it's software,\na kid in his basement, teenager could build like a system that defends against like the worst, I mean, and to me defense is super exciting.\nIt's like, if you believe in the good of human nature for that, most people wanna do good,\nto be the savior of humanity is really exciting. - Yes.\n- Not, okay, that's a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay.\n"}
{"pod": "Lex Fridman Podcast", "input": "AI and the economy", "output": "What about just the jump around, what about the risk of will AI lead to crippling inequality?\nYou know, 'cause we're kind of saying everybody's life will become better. Is it possible that the rich get richer here?\n- Yeah, so this goes, this actually ironically goes back to Marxism. So 'cause this was the, so the core claim of Marxism, right?\nBasically was that the owner, the owners of capital would basically own the means of production. And then over time they would basically accumulate\nall the wealth the workers would be paying in, you know, and getting nothing in return 'cause they wouldn't be needed anymore, right?\nMarx was very worried about mech what he called mechanization or what later became known as automation. And that, you know,\nthe workers would be immiserated and the the capitalists would end up with all. And so this was one of the core principles of Marxism.\nOf course it turned out to be wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is\nthat the way that the self-interested owner of the machines makes the most money is by providing the production capability in the form\nof products and services to the most people, the most customers as possible, right? The the largest,\nand this is one of those funny things where every CEO knows this intuitively, and yet it's like hard to explain from the outside the way you make the most money in any business is\nby selling to the largest market you can possibly get to. The largest market you can possibly get to is everybody on the planet.\nAnd so every large company does is everything that it can to drive down prices, to be able to get volumes up, to be able to get to everybody on the planet.\nAnd that happened with everything from electricity, it happened with telephones, it happened with radio, it happened with automobiles, it happened with smartphones,\nit happened with PCs, it happened with the internet, it happened with mobile broadband.\nIt's happened by the way, with Coca-Cola. It's happened with like every, you know, basically every industrially produced, you know,\ngood or service people, you wanna drive it to the largest possible market. And then as proof of that, it's already happened, right?\nWhich is the early adopters of like ChatGPT and Bing are not like, you know, Exxon and Boeing.\nThey're, you know, your uncle and your nephew, right? It's just like free. It's either freely available online or it's available\nfor 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately.\nAnd so look, the owners of the means of production, the whoever does this now mentioned these trillion dollar questions.\nThere are people who are gonna get really rich doing this, producing these things, but they're gonna get really rich by taking this technology to the broadest possible market.\n- So yes, they'll get rich, but they'll get rich having a huge positive impact on. - Yeah, making the technology available to everybody. Right.\nAnd again, smartphone, same thing. So there's this amazing kind of twist in business history, which is you cannot spend $10,000 on a smartphone, right?\nYou can't spend a hundred thousand dollars, you can't spend a million, like I would buy the million dollars smartphone. Like I'm signed up for it. Like if it's like,\nsuppose a million dollar smartphone was like much better than the thousand dollar smartphone. Like I'm there to buy it, it doesn't exist. Why doesn't it exist?\nApple makes so much more money driving the price further down from a thousand dollars than they would trying to harvest, right?\nAnd so it's just this repeating pattern you see over and over again where and what's great about it is you,\nyou do not need to rely on anybody's enlightened right? Generosity to do this. You just need to rely on capitalist self-interest.\n- What about AI taking our jobs? - Yeah. So very very similar thing here. There's sort of a, there's a core fallacy which again was\nvery common in Marxism, which is what's called the lump of labor fallacy. And this is sort of the fallacy that there is\nonly a fixed amount of work to be done in the world. And it's all being done today by people and then if machines do it,\nthere's no other work to be done by people. And that's just a completely backwards view on how the economy develops and grows.\nBecause what happens is not in fact that what happens is the introduction of technology into production process\ncauses prices to fall. As prices fall, consumers have more spending power. As consumers have more spending power,\nthey create new demand. That new demand then causes capital and labor to form into new enterprises to satisfy nuance and needs.\nAnd the result is more jobs at higher wages. - So nuance and needs, the worries that the creation\nof nuance and needs at a rapid rate will mean there's a lot of turnover in jobs.\nSo people will lose jobs. Just the actual experience of losing a job and having to learn new things and new skills is painful\nfor the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up that there was this panic\nabout a decade ago and all the truck drivers are gonna lose their jobs, right? And number one, that didn't happen 'cause we haven't figured out a way to actually finish that yet.\nBut the other thing was like, look, truck driver, like I grew up in a town that was basically consisted of a truck stop, right? And I like knew a lot of truck drivers\nand like truck drivers live a decade shorter than everybody else. Like, it's actually like a very dangerous,\nlike, they get, like literally they have like higher rates of skin cancer and on the left side of their, on the left side of their body\nfrom being in the sun all the time. The vibration of being in the truck is actually very damaging to your physiology.\n- And there's actually perhaps partially because of that reason there's a shortage of people who wanna be truck drivers.\n- Yeah. Like, it's not like the question always you wanna ask somebody like that is, do you want, you know, do you want your kid to be doing this job?\nAnd like most of them will tell you no. Like, I want my kid to be sitting in a cubicle somewhere like where they don't have this, like, where they don't die 10 years earlier.\nAnd so, the new jobs, number one, the new jobs are often better, but you don't get the new jobs until you go through the change.\nAnd then to your point, the training thing, you know, is always the issue is can people adapt? And again, here you need to imagine living in a world\nin which everybody has the AI assistant capability, right? To be able to pick up new skills much more quickly\nand be able to have some, you know, be able to have a machine to work with to augment their skills. - It's still gonna be painful, but that's the process of life.\n- It's painful for some people. I mean there's no, like, there's no question it's painful for some people and they're, you know, they're yes, it's not, again,\nI'm not a utopian on this and it's not like, it's positive for everybody in the moment, but it has been overwhelmingly positive for 300 years.\nI mean, look, the concern here, the concern, this concern has played out for literally centuries and you know,\nthis is the sort of Luddite, you know, the story of the Luddites that you may remember, there was a panic in the two thousands around outsourcing\nwas gonna take all the jobs. There was a panic in the 2010s that robots were gonna take all the jobs.\nIn 2019 before COVID we had more jobs at higher wages both in the country and in the world\nthan at any point in human history. And so the overwhelming evidence is that the net gain here is like, just like wildly positive.\nAnd most people like overwhelmingly come out the other side being huge beneficiaries of this.\n"}
{"pod": "Lex Fridman Podcast", "input": "China", "output": "- So you write that the single greatest risk, this is the risk you're most convinced by the single greatest risk of AI is\nthat China wins global AI dominance and we the United States and the West do not.\nCan you elaborate? - Yeah. So this is the other thing which is a lot of this sort of AI risk debates today sort\nof assume that we're the only game in town, right? And so we have the ability to kind of sit in the United States and criticize ourselves and do,\nyou know, have our government like, you know, beat up on our companies and we'll figure out a way to restrict what our companies can do and you know,\nwe're gonna, you know, we're gonna ban this and ban that, restrict this and do that. And then there's this like other like force out there that like doesn't believe we have any power\nover them whatsoever and they have no desire to sign up for whatever rules we decide to put in place and they're gonna do whatever it is they're gonna do.\nAnd we have no control over it at all. And it's China and specifically the Chinese Communist party\nand they have a completely publicized open, you know, plan for what they're gonna do with AI.\nAnd it is not what we have in mind. And not only do they have that as a vision and a plan for their society,\nbut they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control.\nSo authoritarian population control you know, good old-fashioned communist authoritarian control\nand surveillance and enforcement and social credit scores and all the rest of it.\nAnd you are gonna be monitored and metered within an inch of everything all the time.\nAnd it's gonna, you know, it's basically the end of human freedom and that's their goal. And you know, they justify it on the basis\nof that's what leads to peace. - You're worried that the regulating\nin the United States will haul progress enough to where the Chinese government would win that race.\n- So their plan, yeah. Yes, yes. And the reason for that is they, and again, they're very public on this. They have, their plan is to proliferate\ntheir approach around the world and they have this program called the Digital Silk Road, right. Which is building on their Silk Road investment program.\nAnd they've got, they've been laying networking infrastructure all over the world with their 5G, right. Work with their company Huawei.\nAnd so, they've been laying all this fabric, but financial and technological fabric all over the world. And their plan is to roll out their vision of AI on top of that and to have\nevery other country be running their version. And then if you're a country prone to, you know,\nauthoritarianism, you're gonna find this to be an incredible way to become more authoritarian. If you're a country, by the way,\nnot prone to authoritarianism, you're gonna have the Chinese Communist Party running your infrastructure and having backdoor into it. Right.\nWhich is also not good. - What's your sense of where they stand in terms of the race towards super intelligence as compared to the United States?\n- Yeah, so good news is they're behind, but bad news is they, you know, let's just say they get access to everything we do. So they're probably a year behind at each point in time,\nbut they get, you know, downloads I think of basically all of our work on a regular basis through a variety of means.\nAnd they are, you know, at least we'll see, they're at least putting out reports of very, they just put out a report last week of a GPT 3.5 analog.\nThey put out this report, forget what it's called, but they put out this report of this and they did and they, you know, the way when open AI you know,\nputs out, one of the ways they test, you know, GPT they run it through standardized exams like the SAT. Right.\nJust how you can kind of gauge how smart it is. And so the Chinese report, they ran their LLM through the Chinese equivalent\nof the SAT and it includes a section on Marxism and a section on, I say tongue of thought.\nAnd it turns out their AI does very well on both of those topics. - That's right.\n- So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is like, that's the, you know,\nso you know, you can just imagine like you're a school, you know, you're a kid 10 years from now in Argentina\nor in Germany or in who knows where, Indonesia. And you ask the AI,\nI'd explain to you like how the economy works and it gives you the most cheery, upbeat explanation of Chinese style communism you've ever heard. Right.\nSo like the stakes here are like really big. - Well, as we've been talking about,\nmy hope is not just with the United States, but with just the kid in his basement. The open source LLM.\n'Cause I don't know if I trust large centralized institutions with super powerful AI\nno matter what their ideology as a power corrupts.\n"}
{"pod": "Lex Fridman Podcast", "input": "Evolution of technology", "output": "You've been investing in tech companies for about, let's say 20 years. And about 15 of which was with Andreessen Horowitz.\nWhat interesting trends in tech have you seen over that time? Let's just talk about companies and just the evolution\nof the tech industry. - I mean the big shift over 20 years has been that tech used to be a tools industry for basically from like 1940 through\nto about 2010, almost all the big successful companies were pick and shovels companies. So PC, database, smartphone, you know,\nsome tool that somebody else would pick up and use. Since 2010, most of the big wins have been in applications.\nSo a company that starts you know, starts in an existing industry and goes directly\nto the customer in that industry. And you know, the earliest examples there were like Uber and Lyft\nand Airbnb. And then that model is kind of elaborating out. The AI thing is actually a reversion on that for now\n'cause like most of the AI business right now is actually in cloud provision of AI APIs for other people to build on.\n- But the big thing will probably be in app. - Yeah. I think most of the money I think probably will be in whatever your AI financial advisor\nor your AI doctor or your AI lawyer or, you know, take your pick of whatever the domain is. And there, and what's interesting is, you know,\nthe valley kind of does everything. The entrepreneurs kind of elaborate every possible idea. And so there will be a set of companies that like make AI\nsomething that can be purchased and used by large law firms and then there will be other companies that just go direct\nto market as an AI lawyer. - What advice could you give for a startup founder?\nJust haven't seen so many successful companies, so many companies that fail also,\nwhat advice could you give to a startup founder, someone who wants to build the next super successful startup\nin the tech space? The Googles, the Apples, the Twitters. - Yeah. So the great thing\nabout the really great founders is they don't take any advice. So, if you find yourself listening to advice,\nmaybe you shouldn't do it. - But that's actually, just to elaborate on that, if you could also speak to great founders too.\nLike what makes a great founder? - So what makes a great founder is super smart, coupled with super energetic, coupled with super courageous.\nI think it's some of those three and-- - Intelligence, passion and courage. - The first two are traits and the third one is a choice.\nI think courage is a choice. Well 'cause courage is a question of pain tolerance, right?\nSo how many times are you willing to get punched in the face before you quit?\nAnd here's maybe the biggest thing people don't understand about what it's like to be a startup founder is it gets\nvery romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was.\nBut like the reality of it is most of what happens is people telling you no and then they usually follow\nthat with you're stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google\nto come work for you. No, I'm not gonna buy your product, you know, no, I'm not gonna run a story about your company. No, I'm not this, that, the other thing.\nAnd so a huge amount of what people have to do is just get used to just getting punched and the reason people don't understand this is\nbecause when you're a founder, you cannot let on that this is happening 'cause it will cause people to think that you're weak and they'll lose faith in you.\nSo you have to pretend that you're having a great time when you're dying inside, right?\nYou're just in misery. - But why did they do it? - Why did they do? Yeah, that's the thing. It's like it is a level,\nthis is actually one of the conclusions I think is that I think it's actually for most of these people on a risk adjusted basis, it's probably an irrational act.\nThey could probably be more financially successful on average if they just got like a real job in at a big company.\nBut there's, you know, some people just have an irrational need to do something new and build something for themselves and, you know,\nsome people just can't tolerate having bosses. Oh, here's the fun thing is how do you reference check founders, right?\nSo you call the, you know, normal way you reference check, you're hiring somebody is you call the bosses, they're their, and you know, and you find out if they were good employees\nand now you're trying to reference check Steve Jobs, right? And it's like, oh God, he was terrible. You know, he was a terrible employee.\nHe never did what we told him to do. - So what's a good reference? Do you want the previous boss to actually say\nthey never did what you told him to do? That might be a good thing. - Well, ideally what you want is I will go,\nI would like to go to work for that person. He worked for me here and now I'd like to work for him. No, unfortunately, most people can't, their egos can't\nhandle that. So they won't say that. But that's the ideal. - What advice would you give to those folks in the space of intelligence, passion and courage?\n- So I think the other big thing is you see people sometimes who say, I wanna start a company and then they kind of work through the process\nof coming up with an idea. And generally those don't work as well as the case where somebody has the idea first\nand then they kind of realize that there's an opportunity to build a company and then they just turn out to be the right kind of person to do that.\n- When you say idea, do you mean long-term big vision or do you mean specifics of like product?\n- Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don't get to have vision,\nyou just gotta build something people want and you gotta figure out a way to sell it to them. Right. It's very practical or you never get to big vision.\n- So the first product, you have an idea of a set of products of the first product that can actually make some money.\n- Yeah. Like it's gotta work. The first product's gotta work by which I mean like, it has to technically work, but then it has to actually fit into the category\nand the customer's mind if something that they want and then by the way, the other part is they have to be willing to pay for it. Like somebody's gotta pay the bills.\nAnd so you've gotta figure out how to price it and whether you can actually extract the money. So usually it is much more predictable.\nSuccess is never predictable, but it's more predictable if you start with a great idea and then back into starting the company.\nSo this is what we did, you know, we had most, before we had escape, the Google guys had the Google search engine working at Stanford. Right.\nYou know, yeah. Actually there's tons of examples where they, you know, Pierre Omaira had eBay working before he left his previous job.\n- So I really love that idea of just having a thing, a prototype that actually works before you even begin\nto remotely scale. Yeah. - By the way, it's also far easier to raise money, right? Like the ideal pitch that we receive is,\nhere's the thing that works, would you like to invest in our company or not? Like, that's so much easier than here's 30 slides with a dream, right?\nAnd then we have this concept called the DMAs, which our biology of came up with when he was with us.\nSo then there's this thing, this goes to mythology, which is, you know, there's a mythology that kind of,\nyou know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It's like eBay with the pest dispensers or something.\nThe reality usually with the big successes is that the founder has been chewing on the problem\nfor 5 or 10 years before they start the company and they often worked on it in school or they even experimented on it\nwhen they were a kid and they've been kind of training up over that period of time to be able to do the thing.\nSo they're like a true domain expert. And it sort of sounds like mom, I'm an apple pie, which is yeah,\nyou wanna be a domain expert in what you're doing, but you would, you know, the mythology is so strong of like, oh, I just like had this idea in the shower right now I'm doing it.\nLike it's generally not that. - No, because it's, well, maybe in the shower we had the exact product\nimplementation details, but yeah, usually you're gonna be for like years if not decades\nthinking about like everything around that.\n- Well we call it the DMAs because the DMAs basically is like, there's all these permutations,\nlike for any idea, there's like all these different permutations, who should the customer be? What shape forms should the product have\nand how should we take it to market and all these things. And so the really smart founders have thought\nthrough all these scenarios by the time they go out to raise money and they have like detailed answers on every one of those fronts\nbecause they put so much thought into it. The sort of more haphazard founders haven't thought\nabout any of that. And it's the detailed ones who tend to do much better. - So how do you know when to take a leap\nif you have a cushy job or happy life? - I mean the best reason is just 'cause you can't tolerate not doing it right?\nLike this is the kind of thing where if you have to be advised into doing it, you probably shouldn't do it. And so it's probably the opposite,\nwhich is you just have such a burning sense of this has to be done, I have to do this, I have no choice. - What if it's gonna lead to a lot of pain?\n- It's gonna lead to a lot of pain. I think that's. - What if it means losing sort of social relationships\nand damaging your relationship with loved ones and all that kind of stuff.\n- Yeah, look, so like, it's gonna put you in a social tunnel for sure, right? So you're gonna, like, you know, there's this game you can play on Twitter,\nwhich is you can do any whiff of the idea that there's basically any such thing as work life balance and that people should actually work hard\nand everybody gets mad. But like, the truth is like all the successful founders are working 80 hour weeks and they're working, you know, they form very,\nvery strong social bonds with the people they work with. They tend to lose a lot of friends on the outside or put those friendships on ice.\nLike that's just the nature of the thing, you know, for most people that's worth the trade off. You know, the advantage, you know,\nmaybe younger founders have is maybe they have less, you know, maybe they're not, you know, for example, if they're not married yet or don't have kids yet, that's an easier thing to bite off.\n- Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most successful founders are second, third, fourth time founders.\nThey're in their thirties, forties, fifties. The good news with being an older founder is, you know, more and you, you know, a lot more about what to do,\nwhich is very helpful. The problem is, okay, now you've got like a spouse and a family and kids and like, you've gotta go to the baseball game and like,\nyou can't go to the base, you know, and so it's. - [Lex] Life is full of difficult choices.\n- Yes. - Marc Andreessen, you've written a blog post on what you've been up to. You wrote this in October, 2022,\n"}
{"pod": "Lex Fridman Podcast", "input": "How to learn", "output": "\"Mostly I try to learn a lot. For example, the political events of 2014 to 2016 make clear to me\nthat I didn't understand politics at all referencing maybe some of this book here.\nSo I deliberately withdrew from political engagement and fundraising and instead read my way back into history\nand as far to the political left and political right as I could.\" So just high level question,\nwhat's your approach to learning? - Yeah, so it's basically, I would say, I'm an AutoID direct, so it's sort of goes,\nit's going down the rabbit holes. So it's a combination. I kind of allude to it in that, in that quote, it's a combination of breadth and depth.\nAnd so I tend to, yeah, I tend to, I go broad by the nature of what I do, I go broad, but then I tend to go deep in a rabbit hole for a while,\nread everything I can and then come out of it. And I might not revisit that rabbit hole for, you know, another decade. - And in that blog post that I recommend\npeople go check out, you actually list a bunch of different books that you recommend on different topics on the American left,\non the American right. It's just a lot of really good stuff. The best explanation for the current structure\nof our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history of the American right\ncomprehensive biographies. These of Adolf Hitler, one of which I read can recommend six books on the deep\nhistory of the American left. So the American right, American left looking at the history to give you the context\nbiography of later Lennon, two of them on the French Revolution. I actually,\nI have never read a biography on Lennon maybe that would be useful. Everything's been so Marc's focused.\n- The Sebastian biography of Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah.\n- [Lex] So it's still useful to read. - It's incredible. Yeah, it's incredible. I actually think it's the single best book on the Soviet Union. - So that the perspective of Lennon,\nit might be the best way to look at the Soviet Union versus Stalin versus Marx versus, very interesting.\nSo two books on fascism and anti-fascism by the same author,\nPaul Gottfried, brilliant book on the nature of mass movements and collective psychology, the definitive work on intellectual life under\ntotalitarianism, the Captive Mind, the definitive worked on the practical life under totalitarianism.\nThere's a bunch. There's a bunch. And the single best book, first of all, the list here is just incredible.\nBut you say the single best book I have found on who we are and how we got here is the Ancient City\nby Numa Dennis Fustel De Coulanges. I like it.\nWhat did you learn about who we are as a human civilization from that book? - Yeah, so this is a fascinating book.\nThis one's free, it's a free, by the way, it's a book in the 1860s. You can download it or you can buy printouts up prints of it.\nBut it was this guy who was a professor at the savant in the 1860s and he was apparently a savant on antiquity\non Greek and Roman antiquity and the reason I say that is because his sources are 100%\noriginal Greek and Roman sources. So he wrote a basically history of western civilization from, on the order of 4,000 years ago\nto basically the present times entirely working on fresh original Greek and Roman sources.\nAnd what he was specifically trying to do was he was trying to reconstruct from the stories of the Greeks and the Romans,\nhe was trying to reconstruct what life in the west was like before the Greeks and the Romans, which was in the civilization known\nas the Indo Europeans. And the short answer is, and this is sort of 4,000,\nyou know, 2000 BC to, you know, sort of 500 BC kind of that 1500 year stretch for civilization developed.\nAnd his conclusion was basically cults. They were basically cults and civilization was,\nor organized into cults. And the intensity of the cults was like a million fold beyond anything\nthat we would recognize today. Like it was a level of all encompassing belief\nand an action around religion that was at a level of extremeness\nthat we wouldn't even recognize it and so specifically he tells the story of basically\nthere were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up.\nAnd then each cult was a joint cult of family gods, which were ancestor gods.\nAnd then nature gods and then your bonding into a family, a tribe or a city was based on your adherence\nto that religion. People who were not of your family, tribe, city, worship,\ndifferent gods, which gave you not just the right with or responsibility to kill them on site. - [Lex] So they were serious about their cults.\n- Hardcore, by the way, shocking development. I did not realize this zero concept of individual rights.\nLike even even up through the Greeks, and even in the Romans, they didn't have, have the concept of individual rights. Like the idea that as an individual you have like\nsome rights just like, nope. Right? And you look back and you're just like, wow, that's just like cr like fascist in a degree\nthat we wouldn't recognize today. But it's like, well, they were living under extreme pressure for survival.\nAnd you, and you know, the theory goes, you could not have people running around making claims, individual rights when you're just trying to get\nlike your tribe through the winter, right? Like you need like hardcore command and control. And actually what if through modern political lens,\nthose cults were basically both fascist and communist. They were fascist in terms of social control,\nand then they were communist in terms of economics. - But you think that's fundamentally that like pull towards\ncults is within us. - Well, so my conclusion from this book,\nso the way we naturally think about the world we live in today is like, we basically have such an improved version of everything\nthat came before us, right? Like, we have basically, we've figured out all these things around morality and ethics and democracy and all these things.\nAnd like, they were basically stupid and retrograde and we're like smart and sophisticated. And we've improved all this after reading that book,\nI now believe in many ways the opposite, which is no, actually we are still running in that original model.\nWe're just running in an incredibly diluted version of it. So we're still running, basically in cults.\nIt's just our cults are at like a thousandth or a millionth, the level of intensity, right? And so our, so just as to take religions, you know,\nthe modern experience of a Christian in our time, even somebody who considers him a devout Christian,\nis just a shadow of the level of intensity of somebody who belonged to a religion back in that period. And then by the way, we have cons.\nIt goes back to our AI discussion. We then sort of endlessly create new cults.\nLike we're trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like everybody living today,\ntransporting that era would view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control.\nHowever, every single person in that era, and he really stresses this. They knew exactly where they stood.\nThey knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they needed to do every day. They knew exactly why they were doing it.\nThey had total certainty about their place in the universe. - So the question of meaning, the question of purpose was very distinctly,\nclearly defined for them. - Absolutely overwhelmingly undisputably undeniably.\n- As we turn the volume down on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts getting harder and harder.\n- Yes. 'cause we don't have that. We are ungrounded. We are uncentered and we all feel it. Right? And that's why we reach for, you know,\nit's why we still reach for religion. It's why we reach for, you know, we people start to take on, you know, let's say, you know,\na faith in science maybe beyond where they should put it. You know and by the way, like, sports teams are like a, you know, they're like a tiny little version of a cult.\nAnd you know, apple keynotes are a tiny little version of a cult. Right. And, you know, political, you know.\nAnd there's cult, you know, there's full-blown cults on both sides of the political spectrum right now. Right. You know, operating in plain stuff.\n- But still not full blown compared as to what it was. - Compared to what it used to. I mean, we would today consider full blown, but like, yes,\nthey're at like, I don't know, a hundred thousandth or something of the intensity of what people had back then. So, we live in a world today that in many ways is more\nadvanced and moral and so forth. And it's certainly a lot nicer, much nicer world to live in. But we live in a world that's like very washed out.\nIt's like everything has become very colorless and gray as compared to how people used to experience things. Which is I think why we're so prone to reach for drama.\n'Cause there's something in us that's deeply evolved where we want that back. - And I wonder where it's all headed as we turn the volume\ndown more and more. What advice would you give to young folks today in high school and college?\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "How to be successful in their career? How to be successful in their life? - Yeah. So the tools that are available today, I mean,\nare just like, I sometimes, you know, bore, I sometimes bore, you know, kids by describing like what it was like to go look up\na book, you know, to try to like discover a fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog\nand the whole thing. You go through all that work and then the book is checked out and you have to wait two weeks and like to be in a world,\nnot only where you can get the answer to any question, but also the world now, you know, the AI world where you've got like the assistant\nthat will help you do anything, help you teach, learn anything, like your ability both to learn and also to produce is just like, I don't know,\na million fold beyond what it used to be. I have a blog post I've been wanting to write, which I call where are the hyper-productive people?\nLike-- - [Lex] That's a good question, right? - Like with these tools, like there should be authors that are writing like hundreds\nor thousands of like, outstanding books. - Well, with the authors there's a consumption question too, but yeah. Well, maybe not, maybe not.\nYou're right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right.\nWhy aren't musicians producing a thousand times the number of songs, right? Like what, like the tools are spectacular.\n- So, what's the explanation? And by way of advice, like,\nis motivation starting to be turned down a little bit? Or what? - I think it might be distraction.\n- [Lex] Distraction. - It's so easy to just sit and consume that I think people get distracted from production. But if you wanted to, you know,\nas a young person, if you wanted to really stand out, you could get on a, like a hyper productivity curve very early on.\nThere's a great, you know, this story, there's a great story in Roman history of plenty of the elder who was this legendary statesman,\ndied in the Vesuvius eruption trying to rescue his friends. But he was famous both for being basically being a polymath, but also being an author.\nAnd he wrote apparently like hundreds of books, most of us had been lost. But he like wrote all these encyclopedias and he literally\nlike would be reading and writing all day long no matter what else was going on. And so he would like travel with like four slaves.\nAnd two of them were responsible for reading to him, and two of them were responsible for taking dictation. And so like, he'd be going cross country and like,\nliterally he would be writing books like all the time. And apparently they were spectacular. There's only a few that have survived,\nbut apparently they were amazing. - There's a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are examples, like there are,\nyou know, there's this guy, judge, what's his name? Posner, who wrote like 40 books and was also a great federal judge.\nYou know, there's our friend Balaji, I think is like this, he's one of these, you know, where his output is just prodigious.\nAnd so it's like, yeah, I mean, with these tools, why not? And I kind of think we're at this interesting kind of freeze frame moment where like this,\nthese tools are now in everybody's hands and everybody's just kind of staring at them trying to figure out what to do. The new tools. - We have discovered fire.\n- [Marc] Yeah. - And trying to figure out how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that the perfect day is caffeine\n"}
{"pod": "Lex Fridman Podcast", "input": "Balance and happiness", "output": "for 10 hours and alcohol for four hours. You didn't think I'd be mentioning this, did you?\nIt balances everything out perfectly as you said. So, perfect. So let me ask,\nwhat's the secret to balance and maybe to happiness in life? - I don't believe in balance,\nso I'm the wrong person to ask that. - Can you elaborate why you don't believe in balance? - I mean, I maybe it's just, and I look, I think people,\nI think people are wired differently. So, I think it's hard to generalize this kind of thing, but I am much happier and more satisfied\nwhen I'm fully committed to something. So I'm very much in favor of all in of imbalance. - Imbalance. And that applies to work,\nto life, to everything. - Yeah. No, no. I happen to have whatever twist of personality traits lead\nthat in non-destructive dimensions in including the fact that I've actually, I now no longer do the ten-four plan. I stopped drinking.\nI do the caffeine, but not the alcohol. So there's something in my personality where I whatever mal-adaption I have is inclining me\ntowards productive things, not unproductive things. - So you're one of the wealthiest people in the world.\nWhat's the relationship between wealth and happiness? Money and happiness.\n- So I think happiness, I don't think happiness is the thing.\n- To strive for. - I think satisfaction is the thing. - That just sounds like happiness, but turned down a bit.\n- No deeper. So happiness is, you know, a walk in the woods at sunset, an ice cream cone, a kiss,\nthe first ice cream cone is great. The thousandth ice cream cone, not so much.\nAt some point the walks in the woods get boring. - What's the distinction between happiness and satisfaction?\n- I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful.\n- So just something that permeates all your days, just this general contentment of being useful.\n- That I'm fully satisfying my faculties, that I'm fully delivering, right? On the gifts that I've been given, that I'm, you know,\nnet making the world better, that I'm contributing to the people around me, right. And that I can look back and say, wow, that was hard,\nbut it was worth it. Think generally, it seems to lead people in a better state than pursuit of pleasure, pursuit of quote unquote happiness.\n- Does money have anything to do with that? - I think the founders and the founding fathers in the US threw this off kilter when they used the phrase pursuit\nof happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today.\n- Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have tweaked the second amendment.\n- I think they were smarter than they realized. They said, you know we're gonna make it ambiguous and let these humans figure out the rest,\nthese tribal cult-like humans figure out the rest.\nBut money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don't think I'm even a great example,\nbut I think Elon would be the great example of this, which is like, you know, look, he's a guy who from every, every day of his life, from the day he started making money at all,\nhe just plows into the next thing. And so I think, I think money is definitely an enabler for satisfaction.\nWay money applied to happiness leads people down very dark paths. Very destructive avenues.\nMoney applied to satisfaction, I think could be, is a real tool. I always, by the way, I was like, you know,\nElon is the case study for behavior. But the other thing that I always really made me think is Larry Page was asked one time\nwhat his approach to philanthropy was. And he said, oh, I'm just, my philanthropic plan is just give all the money to Elon. (both laugh)\n- Well, let me actually ask you about Elon. You've interacted with quite\na lot of successful engineers and business people. What do you think is special about Elon?\nWe talked about Steve Jobs. What do you think is special about him as a leader?\nAs an innovator? - Yeah. So the core of it is he's back to the future. So he is doing the most leading-edge things in the world,\nbut with a really deeply old-school approach. And so to find comparisons to Elon, you need to go to like Henry Ford and Thomas Watson\nand Howard Hughes and Andrew Carnegie, right. Leland Stanford, John Rockefeller, right.\nYou need to go to what were called the bourgeois capitalists, like the hardcore business owner operators\nwho basically built, you know, basically built industrialized society, Vanderbilt.\nAnd it's a level of hands-on commitment and depth in the business,\ncoupled with an absolute priority towards truth and towards,\nhow to put it, science and technology town to first principles that is just like absolute,\nis just like unbelievably absolute. He really is ideal that he's only ever talking to engineers. Like he does not tolerate.\nHe has less tolerance than anybody I've ever met. He wants ground truth on every single topic.\nAnd he runs his businesses directly day-to-day, devoted to getting to ground truth in every single topic.\n- So you think it was a good decision for him to buy Twitter?\n- I have developed a view in life to not second guess Elon Musk, I know this is gonna sound great, crazy and unfounded, but.\n- Well, I mean, he's got a quite a track record. - I mean, look, the car was a crazy, I mean, the car was,\nI mean, look. - He's done a lot of things that seem crazy. - Starting a new car company in the United States of America. The last time somebody really tried to do that was\nthe 1950s and it was called Tucker Automotive. And it was such a disaster. They made a movie about what a disaster it was,\nand then rockets like, who does that? Like, there's obviously no way to start a new rocket company.\nLike those days are over. And then to do those at the same time. So after he pulled those two off, like, okay, fine.\nLike, this is one of my areas of like, whatever opinions I had about that, that is just like, okay, clearly are not relevant.\nLike this is you just, you at some point you just like bet on the person. - And in general, I wish more people would lean on celebrating and supporting\nversus deriding and destroying. - Oh yeah. I mean, look, he drives resentment. Like it's a resentment.\nLike he is a magnet for resentment. Like his critics are the most miserable, like,\nresentful people in the world. Like it's almost a perfect match of like the most idealized, you know, technologist, you know,\nof the century coupled with like, just his critics are just bitter as can be. And I mean, it's sort of very darkly comic to watch.\n- Well, he fuels the fire of that by being on Twitter at times.\nAnd which is fascinating to watch the drama of human civilization, given our cult roots just fully on fire.\n- [Marc] He's running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults have gone and we searched\n"}
{"pod": "Lex Fridman Podcast", "input": "Meaning of life", "output": "for meaning, what do you think is the meaning of this whole thing? What's the meaning of life Marc Andreessen? - I don't know the answer to that. I think the meaning\nof the closest I get to it is what I said about satisfaction.\nSo it's basically like, okay, we were given what we have, like we should basically do our best. - What's the role of love in that mix?\n- I mean, like, what's the point of life if you're without love, like, yeah. - So love is a big part of that satisfaction.\n- Yeah. And look like taking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms of taking care of people,\nbut there's also a very fundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who believes that capitalism\nand taking care of people are actually, they're actually the same thing. Somebody once said, capitalism is how you take care of people you don't know.\nRight, right. And so like, yeah, I think it's like deeply woven into the whole thing, you know, there's a long conversation\nto be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy\nin smaller, big ways. And then capitalism kind of enables that, encourages that.\n- David Friedman says, there's only three ways to get somebody to do something for somebody else. Love, money and force.\nAnd love and money are better.\n- [Lex] Yeah. Of course. That's a good ordering. I think. - We should bet on those. - Try love first. If that doesn't work, then money.\n- [Marc] Yes. - And then force. Well, don't even try that one. Marc, you're an incredible person. I've been a huge fan.\nI'm glad to finally got a chance to talk. I'm a fan of everything you do, everything you do, including on Twitter.\nIt's a huge honor to meet you, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex.\n- Thanks for listening to this conversation with Marc Andreessen. To support this podcast, please check out our sponsors in the description.\nAnd now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place.\nIf you know what you want and you go for it, with maximum energy and drive and passion,\nthe world will often reconfigure itself around you much more quickly and easily than you would think.\"\nThank you for listening and hope to see you next time.\n"}

{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "- I think it's possible that physics has exploits and we should be trying to find them. Arranging some kind of a crazy quantum mechanical system\nthat somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligence is kind of like\nthe next stage of development. And I dunno where it leads to, at some point\nI suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it.\n- The following is a conversation with Andrej Karpathy, previously the director of AI at Tesla.\nAnd before that, at OpenAi and Stanford. He is one of the greatest scientists, engineers,\nand educators in the history of artificial intelligence. This is the Lex Fridman Podcast.\nTo support it, please check out our sponsors. And now, dear friends, here's Andrej Karpathy.\n"}
{"pod": "Lex Fridman Podcast", "input": "Neural networks", "output": "What is a neural network and why does it seem to do such a surprisingly good job of learning?\n- What is a neural network? It's a mathematical abstraction of the brain.\nI would say that's how it was originally developed. At the end of the day, it's a mathematical expression and it's a fairly simple mathematical expression\nwhen you get down to it. It's basically a sequence of meter multipliers,\nwhichever really dot products mathematically and some nonlinearity is thrown in. And so it's a very simple mathematical expression\nand it's got knobs in it. - Many knobs. - Many knobs. And these knobs are loosely related to basically\nthe synapses in your brain. They're trainable. They're modifiable. And so the idea is we need to find the setting of the knobs that makes the neural net\ndo whatever you want it to do, like classify images and so on. And so there's not too much mystery I would say in it.\nYou might think that, basically, you don't want to endow it with too much meaning with respect to the brain and how it works.\nIt's really just a complicated mathematical expression with knobs. And those knobs need a proper setting for it to do something desirable.\n- Yeah. But poetry is just the collection of letters with spaces, but it can make us feel a certain way.\nAnd in that same way, when you get a large number of knobs together, whether it's inside the brain or inside a computer,\nthey seem to surprise us with their power. - Yeah. I think that's fair.\nSo basically, I think I'm underselling it by a lot because you definitely do get very surprising emergent behaviors out of these neural nets\nwhen they're large enough and trained on complicated enough problems. Like say, for example, the next-word prediction\nin a massive dataset from the internet. And then these neural nets take on pretty surprising magical properties.\nYeah, I think it's kind of interesting how much you can get out of even very simple mathematical formalism. - When your brain right now is talking,\nis it doing next-word prediction or is it doing something more interesting? - Well, it's definitely some kind of a generative model\nthat's GPT like and prompted by you. - [Lex] Yes. - So you're giving me a prompt\nand I'm kind of like responding to it in a generative way. - And by yourself perhaps a little bit, like are you adding extra prompts from your own memory\ninside your head or no? - Well, it definitely feels like you're referencing some kind of a declarative structure\nof memory and so on. And then you're putting that together with your prompt\nand giving away some answers. - How much of what you just said has been said by you before.\n- Nothing, basically right? - No, but if you actually look at all the words you've ever said in your life and you do a search,\nyou'll probably have said a lot of the same words in the same order before. - Yeah.\nCould be. I mean, I'm using phrases that are common, et cetera, but I'm remixing it into a pretty unique sentence\nat the end of the day. But you're right, definitely, there's like a ton of remixing.\n- It's like Magnus Carlson said I'm rated 2,900, whatever, which is pretty decent.\nI think you're talking very, you're not giving enough credit to neural nets here.\nWhat's your best intuition about this emergent behavior? - I mean, it's kind of interesting\nbecause I'm simultaneously underselling them, but I also feel like there's an element to which I'm over-\nit's actually kind of incredible that you can get so much emergent magical behavior out of them despite them being so simple mathematically.\nSo I think those are two surprising statements that are juxtaposed together.\nAnd I think, basically, what it is, is we are actually fairly good at optimizing these neural nets. And when you give them a hard enough problem,\nthey are forced to learn very interesting solutions in the optimization. And those solutions basically have these emergent properties\nthat are very interesting. - There's wisdom and knowledge in the knobs.\n- [Andrej] Yes. - And so this representation that's in the knobs does it make sense to you intuitively, that a large number of knobs can hold a representation\nthat captures some deep wisdom about the data it has looked at. It's a lot of knobs.\n- It's a lot of knobs. And somehow, so speaking concretely, one of the neural nets\nthat people are very excited about right now are GPTs, which are basically just next-word prediction networks.\nSo you consume a sequence of words from the internet and you try to predict the next word.\nAnd once you train these on a large enough data set,\nyou can basically prompt these neural nets in arbitrary ways and you can ask them to solve problems. And they will.\nSo you can just tell them, you can make it look like you're trying to solve some kind of a mathematical problem.\nAnd they will continue what they think is the solution based on what they've seen on the internet. And very often those solutions\nlook very remarkably consistent. Look correct, potentially even. - Do you still think about the brain side of it?\nSo as neural nets as an abstraction, a mathematical abstraction of the brain, do you still draw wisdom\nfrom the biological neural networks or even the bigger question.\nSo you're a big fan of biology and biological computation. What impressive thing is biology doing to you\n"}
{"pod": "Lex Fridman Podcast", "input": "Biology", "output": "that computers are not yet, that gap? - I would say I'm definitely on,\nI'm much more hesitant with the analogies to the brain than I think you would see potentially in the field.\nAnd I feel like certainly, the way neural networks started is everything stemmed from inspiration by the brain.\nBut at the end of the day, the artifacts that you get after training, they are arrived at by a very different optimization process\nthan the optimization process that gave rise to the brain. And so I think of it as a very complicated alien artifact.\nIt's something different. - [Lex] The brain? - Oh no, sorry. The neural nest that we're training. - [Lex] Okay. - They are a complicated alien artifact.\nI do not make analogies to the brain because I think the optimization process that gave rise to it is very different from the brain.\nSo there was no multi-agent, self-play setup and evolution.\nIt was an optimization that is basically what amounts to a compression objective on a mass amount of data.\n- Okay. So artificial neural networks are doing compression and biological neural networks-\n- [Andrej] Are trying to survive. - Are not really doing anything, they're an agent in a multi-agent, self-play system\nthat's been running for a very, very long time. - Yes. That said, evolution has found that it is very useful\nto predict and have a predictive model in the brain. And so, I think our brain utilizes something\nthat looks like that as a part of it, but it has a lot more catches and gizmos and value functions and ancient nuclei\nthat are all trying to like make it survive and reproduce and everything else. - And the whole thing through embryogenesis is built\nfrom a single-cell. I mean, it's just the code is inside the DNA and it just builds it up like the entire organism\nwith arms- - [Andrej] It's definitely crazy. - And the head and legs. - [Andrej] Yes. - And it does it pretty well.\n- [Andrej] It should not be possible. - So there's some learning going on. There's some kind of computation\ngoing through that building process. I mean, I don't know where, if you were just to look\nat the entirety of history of life on earth, where do you think is the most interesting invention?\nIs it the origin of life itself? Is it just jumping to Eukaryotes?\nIs it mammals? Is it humans themselves, Homo sapiens? The origin of intelligence or highly complex intelligence?\nOr is it all just a continuation of the same kind of process? - Certainly, I would say it's an extremely remarkable story\nthat I'm only briefly learning about recently all the way from, actually, you almost have to start\nat the formation of earth and all of its conditions and the entire solar system and how everything is arranged with Jupiter and moon and the habitable zone and everything.\nAnd then you have an active earth that's turning over material and then you start with a biogenesis and everything.\nAnd so it's all a pretty remarkable story. I'm not sure that I can pick a single unique piece of it\nthat I find most interesting. I guess for me, as an artificial intelligence researcher,\nit's probably the last piece. We have lots of animals that are not building technological society but we do.\nAnd it seems to have happened very quickly. It seems to have happened very recently. And something very interesting happened there\nthat I don't fully understand. I almost understand everything else I think intuitively, but I don't understand exactly that part\nand how quick it was. - Both explanations would be interesting. One is that this is just a continuation\nof the same kind of process. There's nothing special about humans. Deeply understanding that would be very interesting\nthat we think of ourselves as special. But it was obvious, it was already written in the code\nthat you would have greater and greater intelligence emerging. And then the other explanation,\nwhich is something truly special happened, something like a rare event, whether it's like crazy rare event like a \"Space Odyssey\",\nwhat would it be? See if you say like the invention of fire or as Richard Rankin says, the beta males deciding\na clever way to kill the alpha males by collaborating. So just optimizing the collaboration, the multi-agent,\naspect of the multi-agent and that really being constrained on resources and trying to survive the collaboration aspect\nis what created the complex intelligence. But it seems like it's a natural algorithm to the evolution process.\n- [Andrej] Yeah. - What could possibly be a magical thing that happened, like a rare thing that would say that humans are actually,\nhuman-level intelligence is actually a really rare thing in the universe?\n- Yeah, I'm hesitant to say that it is rare by the way, but it definitely seems like it's like a punctuated equilibrium where you have\nlots of exploration and then you have certain leaps, sparse leaps in between. So of course, like origin of life would be one,\nDNA, sex, Eukaryotic life, the endosymbiosis event\nwhere the archon ate little bacteria, just the whole thing. And then of course, emergence of consciousness and so on.\nSo it seems like definitely there are sparse events where massive amount of progress was made. But yeah, it's kind of hard to pick one.\n- So you don't think humans are unique? To that I ask you how many intelligent alien civilizations\n"}
{"pod": "Lex Fridman Podcast", "input": "Aliens", "output": "do you think are out there and is their intelligence different or similar to ours?\n- Yeah, I've been preoccupied with this question quite a bit recently. Basically, the Fermi paradox and just thinking through.\nAnd the reason actually that I am very interested in the origin of life is fundamentally trying to understand\nhow common it is that there are technological societies out there in space.\nAnd the more I study it, the more I think that\nthere should be quite a few, quite a lot. - Why haven't we heard from them? 'Cause I agree with you. It feels like, I just don't see why\nwhat we did here on earth is so difficult to do. - Yeah. And especially when you get into the details of it. I used to think origin of life was very,\nit was this magical rare event. But then you read books like for example Nick Lane,\n\"The Vital Question\", \"Life Ascending\", et cetera. And he really gets in and he really makes you believe\nthat this is not that rare. - Basic chemistry. - You have an active earth and you have your alkaline vents and you have lots of alkaline waters mixing\nwhere there is a devotion and you have your proton gradients and you have these little porous pockets of these alkaline vents that concentrate chemistry.\nAnd basically, as he steps through all of these little pieces, you start to understand that actually this is not that crazy.\nYou could see this happen on other systems and he really takes you from just a geology\nto primitive life. And he makes it feel like this is actually pretty plausible. And also like the origin of life was actually fairly fast\nafter formation of earth, if I remember correctly, just a few hundred million years or something like that after basically when it was possible, life actually arose.\nAnd so that makes me feel like that is not the constraint, that is not the limiting variable and that life should actually be fairly common.\nAnd then where the drop-offs are is very interesting to think about.\nI currently think that there's no major drop-offs basically. - [Lex] Yeah. - And so there should be quite a lot of life. And basically, where that brings me to then\nis the only way to reconcile the fact that we haven't found anyone and so on is that we can't see them, we can't observe them.\n- Just a quick brief comment, Nick Lane and a lot of biologists I talk to, they really seem to think that the jump from bacteria\nto more complex organisms is the hardest jump. - [Andrej] The Eukaryotic life, basically. - Yeah.\nI get it. They're much more knowledgeable than me about the intricacies of biology.\nBut that seems crazy 'cause how many single-cell organisms are there?\nAnd how much time you have surely it's not that difficult. And a billion years is not even that long\nof a time really, just all these bacteria under constrained resources battling it out.\nI'm sure they can invent more complex. I don't understand. It's like how to move from a \"Hello, World!\" program\nto invent a function or something like that. I don't- - Yeah.\n- Yeah, so I'm with you. I just feel like I don't see any, if the origin of life, that would be my intuition, that's the hardest thing.\nBut if that's not the hardest thing 'cause it happened so quickly, then it's gotta be everywhere and yeah, maybe we're just too dumb to see it.\n- Well, it's just we don't have really good mechanisms for seeing this life.\nSo I'm not an expert just to preface this but just from what- - On aliens? I wanna meet an expert on alien intelligence\nand how to communicate. - I'm very suspicious of our ability to find these intelligences out there and to find these earths like radio waves,\nfor example, are terrible. Their power drops off as basically 1 over R-squared. So I remember reading that our current radio waves\nwould not be, the ones that we are broadcasting, would not be measurable by our devices today only like,\nwas it like one 10th of a light year away? Not even, basically, tiny distance because you really need like a targeted transmission\nof massive power directed somewhere for this to be picked up on long distances.\nAnd so I just think that our ability to measure is not amazing. I think there's probably other civilizations out there.\nAnd then the big question is why don't they build one man probes and why don't they interstellar travel across the entire galaxy?\nAnd my current answer is, it's probably interstellar travel is really hard. You have the interstellar medium if you wanna move\nat close to speed of light, you're going to be encountering bullets along the way because even like tiny hydrogen atoms\nand little particles of dust have massive kinetic energy at those speeds.\nAnd so, basically, you need some kind of shielding, you have all the cosmic radiation, it's just brutal out there.\nIt's really hard. And so my thinking is maybe interstellar travel is just extremely hard. You have to learn slow.\n- Like billions of years to build hard? It feels like we're not a billion years away\nfrom doing that. - It just might be that it's very, you have to go very slowly potentially, as an example, through space.\n- Right. As opposed to close to the speed of light. - Yeah. So I'm suspicious basically of our ability to measure life and I'm suspicious of the ability to just permeate\nall of space in the galaxy or across galaxies. And that's the only way that I can currently see away around it.\n- Yeah, it's kind of mind-blowing to think that there's trillions of intelligent alien civilizations out there\nslowly traveling through space. - [Andrej] Maybe. - To meet each other. And some of them meet, some of them go to war,\nsome of them collaborate. - Or they're all just independent. They're all just like little pockets.\nI don't know. - Well, statistically if there's trillions of them,\nsurely some of them, some of the pockets are close enough together. - Some of them happen to be close. Yeah. - And close enough to see each other.\nSee, once you see something that is definitely complex life, if we see something.\n- [Andrej] Yeah. - We're probably going to be intensely aggressively motivated to figure out what the hell that is and try to meet them.\nBut what would be your first instinct, to try to, at a generational-level, meet them or defend against them?\nOr what would be your instinct as a president of the United States and a scientist?\nI don't know which hat you prefer in this question. - Yeah, I think the question, it's really hard.\nI will say like for example, for us, we have lots of primitive life forms on earth next to us.\nWe have all kinds of ants and everything else and we share a space with them and we are hesitant to impact on them and we're trying to protect them\nby default, because they are amazing, interesting dynamical systems that took a long time to evolve. And they are interesting and special\nand I don't know that you wanna destroy that by default.\nAnd so I like complex dynamical systems that took a lot of time to evolve.\nI think I'd like to preserve it if I can afford to.\nAnd I'd like to think that the same would be true about the galactic resources and that they would think\nthat we're kind of incredible interesting story that took time, it took a few billing years to unravel\nand you don't want to just destroy it. - I could see two aliens talking about earth right now and saying I'm a big fan of complex dynamical systems.\nSo I think it was a value to preserve these and who basically are a video game they watch\nor show, a TV show that they watch. - Yeah. I think you would need like a very good reason I think to destroy it.\nWhy don't we destroy these end farms and so on? It's because we're not actually really in direct competition with them right now.\nWe do it accidentally and so on, but there's plenty of resources. And so why would you destroy something\nthat is so interesting and precious? - Well, from a scientific perspective you might probe it. - [Andrej] Yeah.\n- You might interact with it lightly. - Exactly. You might wanna learn something from it. Right. - So I wonder, there could be certain physical phenomena\nthat we think is a physical phenomena, but it's actually interacting with us to poke the finger and see what happens.\n- Yeah. I think it should be very interesting to scientists, other alien scientists, what happened here\nand what we're seeing today is a snapshot, basically, it's a result of a huge amount of computation\nof over billion years or something like that. - It could have been initiated by aliens.\nThis could be a computer running a program. Okay. If you had the power to do this wouldn't you-\nOkay, for sure. At least I would, I would pick an earth-like planet\nthat has the conditions, base my understanding of the chemistry prerequisites for life. And I would seed it with life and run it, right?\n- [Andrej] Yeah. - Wouldn't you 100% do that and observe it and then protect? - [Andrej] Yeah. - I mean that's not just a hell of a good TV show.\nIt's a good scientific experiment. - [Andrej] Yeah. - And it's physical simulation.\nRight. Maybe, evolution is the most, actually running it,\nis the most efficient way to understand computation or to compute stuff.\n- Or to understand life or what life looks like and what branches it can take. - It does make me kind of feel weird\nthat we're part of a science experiment, but maybe everything's a science experiment,\ndoes that change anything for us, if we're a science experiment? - [Andrej] I don't know.\n- Two descendants of apes talking about being inside of a science experiment. - I'm suspicious of this idea of a deliberate panspermia\nas you described it, sort of. - Yes. - I don't see a divine intervention in some way in the historical record right now.\nI do feel like the story in these books, like Nick Lane's books and so on, sort of makes sense and it makes sense how life arose\non earth uniquely. And yeah, I don't need to reach for more exotic explanations right now.\n- Sure. But NPCs inside a video game, don't observe any divine intervention either.\nWe might just be all NPCs running a kind of code. - Maybe eventually they will, currently, NPCs are really dumb.\nBut once they're running GPTs, maybe they will be like, \"Hey, this is really suspicious. What the hell?\"\n"}
{"pod": "Lex Fridman Podcast", "input": "Universe", "output": "- So you famously Tweeted, \"It looks like if you bombard earth with photons\nfor a while, you can emit a Roadster.\" So if like in \"Hitchhiker's Guide to the Galaxy\",\nwe would summarize the story of earth. So in that book it's mostly harmless.\nWhat do you think is all the possible stories, a paragraph long or sentence long\nthat earth could be summarized as, once it's done it's computation?\nSo all the possible full, if earth is a book, right? - Yeah.\n- Probably there has to be an ending. I mean there's going to be an end to earth and it could end in all kinds of ways. It can end soon.\nIt can end later. - [Andrej] Yeah. - What do you think are the possible stories? - Well definitely, there seems to be, yeah, you're sort of,\nit's pretty incredible that these self-replicating systems will basically arise from the dynamics\nand then they perpetuate themselves and become more complex and eventually, become conscious and build a society.\nAnd I feel like in some sense it's a deterministic wave\nthat just happens on any sufficiently well-arranged system like earth.\nAnd so I feel like there's a certain sense of inevitability in it and it's really beautiful.\n- And it ends somehow, right? So it's a chemically diverse environment\nwhere complex dynamical systems can evolve and become more, further, and further complex.\nBut then there's a certain, what is it? There's certain terminating conditions.\n- Yeah. I dunno what the terminating conditions are, but definitely, there's a trend line of something and we're part of that story. And where does that, where does it go?\nSo, we're famously described often as a biological boot loader for AIs and that's because humans,\nI mean, we're an incredible biological system and we're capable of computation and love and so on,\nbut we're extremely inefficient as well. We're talking to each other through audio. It's just embarrassing honestly\nthat we're manipulating like seven symbols, serially, we're using vocal chords.\nIt's all happening over multiple seconds. - [Lex] Yeah. - It's just embarrassing when you step down to the frequencies at which computers operate\nor are able to cooperate on. And so basically, it does seem like synthetic intelligences\nare the next stage of development. And I dunno where it leads to, at some point I suspect\nthe universe is some kind of a puzzle and these synthetic AIs will uncover that puzzle\nand solve it. - And then what happens after, right?\n'Cause if you just like fast forward earth, many billions of years, it's quiet and then it's turmoil,\nyou see city lights and stuff like that. And then what happens at the end? Is it a, or is it a calming, is it explosion?\nIs it like earth open like a giant. 'Cause you said emit Roadsters. Will it start emitting a giant number of satellites?\n- Yeah. Some kind of a crazy explosion. And we're living, we're stepping through a explosion\nand we're living day-to-day and it doesn't look like it. But it's actually, I saw a very cool animation of earth\nand life on earth and, basically, nothing happens for a long time. And then the last like two seconds, basically cities and everything and just,\nand the lower orbit just gets cluttered and just the whole thing happens in the last two seconds and you're like, \"This is exploding. This is a state of explosion.\"\n- Yeah, yeah. If you play it a normal speed. - [Andrej] Yeah. - It'll just look like an explosion. - It's a firecracker.\nWe're living in a firecracker. - Where it's going to start emitting all kinds of interesting things. - [Andrej] Yeah.\n- And then, so explosion doesn't- it might actually look like a little explosion with lights and fire and energy emitted,\nall that kind of stuff. But when you look inside the details of the explosion, there's actual complexity happening where there's like,\nyeah, human life or some kind of life. - We hope it's not a destructive firecracker. It's kind of like a constructive firecracker.\n- [Lex] All right. So given that. - I think- - [Lex] Hilarious discussion. - It is really interesting to think about what the puzzle of the universe is.\nDid the creator of the universe give us a message? For example in the book \"Contact\", Carl Sagan,\nthere's a message for any civilization in the digits\nin the expansion of Pi and base 11, eventually, which is kind of an interesting thought. Maybe we're supposed to be giving a message to our creator,\nmaybe we're supposed to somehow create some kind of a quantum mechanical system that alerts them to our intelligent presence here.\n'Cause if you think about it from their perspective, it's just say like quantum field theory, massive cellular automaton-like thing.\nAnd how do you even notice that we exist? You might not even be able to pick us up in that simulation.\nAnd so how do you prove that you exist, that you're intelligent, and that you're part of the universe?\n- So this is like a touring test for intelligence from earth. - [Andrej] Yeah. - I mean maybe this is like trying to complete\nthe next word in a sentence. This is a complicated way of that. earth is basically sending a message back.\n- Yeah. The puzzle is basically alerting the creator that we exist. - [Lex] Yeah. - Or maybe the puzzle is just to just break out\nof the system and just stick it to the creator in some way. Basically, like if you're playing a video game,\nyou can somehow find an exploit and find a way to execute on the host machine in arbitrary code.\nFor example, I believe someone got a game of Mario to play pong just by exploiting it\nand then basically writing code and being able to execute\narbitrary code in the game. And so maybe we should be, maybe that's the puzzle is that we should find a way to exploit it.\nSo, I think like some of these synthetic AIs will eventually find the universe to be some kind of a puzzle and then solve it in some way.\nAnd that's kind of like the end game somehow. - Do you often think about it as a simulation?\nSo as are the universe being a kind of computation that might have bugs and exploits?\n- Yes. Yeah, I think so. - [Lex] Is that what physics is, essentially? - I think it's possible that physics has exploits\nand we should be trying to find them. Arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow\nsomehow gives you a rounding error and the floating point. - Yeah, that's right.\nAnd more and more sophisticated exploits, those are jokes, but that could be actually very close to reality.\n- Yeah. We'll find some way to extract infinite energy. For example, when you train reinforcement learning agents\nin physical simulations and you ask them to say, run quickly on the flat ground, they'll end up doing\nall kinds of weird things in part of that optimization, right? They'll get on their back leg and they'll slide across the floor.\nAnd it's because the optimization, the enforcement learning optimization on that agent has figured out a way to extract infinite energy\nfrom the friction forces and, basically, their poor implementation. And they found a way to generate infinite energy\nand just slide across the surface. And it's not what you expected, it's sort of like a perverse solution.\nAnd so maybe we can find something like that. Maybe we can be that little dog in this physical simulation.\n- That cracks or escapes the intended consequences of the physics that the universe came up with.\n- [Andrej] Yeah. - We'll figure out some kind of shortcut to some weirdness. - [Andrej] Yeah. - And then, oh man, but see the problem with that weirdness\nis the first person to discover the weirdness, like sliding on the back legs, that's all we're gonna do.\n- [Andrej] Yeah. - It's very quickly become everybody does that thing. So the paperclip maximizer is a ridiculous idea,\nbut that very well could be what then we'll just all switch to that 'cause it's so fun.\n- Well, no person will discover it, I think by the way, I think it's going to have to be some kind of a super-intelligent AGI of a third generation.\nWe're building the first-generation AGI, maybe. - Third generation.\nYeah. So the boot loader for an AI, that AI will be a boot loader for another AI.\n- [Andrej] Better Ai. Yeah. - And then there's no way for us to introspect what that might even.\n- I think it's very likely that these things, for example, say you have these AGIs, it's very likely for example, they will be completely inert.\nI like these kinds of sci-fi books sometimes where these things are just completely inert, they don't interact with anything.\nAnd I find that kind of beautiful because they've probably figured out the meta-game\nof the universe in some way, potentially. They're doing something completely beyond our imagination\nand they don't interact with simple chemical life forms. Why would you do that?\nSo I find those kinds of ideas compelling. - What's their source of fun? What are they doing?\nWhat's the source of pleasure? - Well, probably puzzle-solving in the universe? - But inert, so can you define what it means inert?\nSo they escape the interaction with physical reality? - They will appear inert to us, as in\nthey will behave in some very strange way to us because they're beyond, they're playing the meta-game.\nAnd the meta-game is probably say, like arranging quantum mechanical systems in some very weird ways to extract infinite energy,\nsolve the digital expansion of Pi to whatever amount, they will build their own little fusion reactors\nor something crazy. They're doing something beyond comprehension and not understandable to us and actually brilliant under the hood.\n- What if quantum mechanics itself is the system and we're just thinking it's physics\nbut we're really parasites on or not parasite. We're not really hurting physics,\nwe're just living on this organism and we're like trying to understand it.\nBut really it is an organism and with a deep, deep intelligence maybe physics itself is the organism\nthat's doing the super interesting thing and we're just like one little thing. - [Andrej] Yeah. - Ant sitting on top of it trying to get energy from it.\n- Yeah. We're just like these particles in the wave that I feel like is mostly deterministic and takes universe from some kind of a big bang\nto some kind of a super-intelligent replicator, some kind of a stable point in the universe\ngiven these laws of physics. - You don't think, as Einstein said, God doesn't play dice?\nSo you think it's mostly deterministic? There's no randomness in the thing? - I think it's deterministic. Oh, there's tons of, well, I'm gonna be careful\nwith randomness. - [Lex] Pseudo-random? - Yeah, I don't like random. I think maybe the laws of physics are deterministic.\nYeah, I think they're deterministic. - You just got really uncomfortable with the question. Do you have anxiety about whether the universe\nis random or not? Is it a source? - [Andrej] There's no randomness, no.\n- You said you like \"Good Will Hunting\". It's not your fault Andrej. It's not your fault, man.\nSo you don't like the randomness? - Yeah, I think it's unsettling. I think it's a deterministic system.\nI think that things that look random, like say the collapse of the wave function, et cetera, I think they're actually deterministic,\njust entanglement and so on and some kind of a multiverse theory something, something. - Okay.\nSo why does it feel like we have a free will? Like if I raise this hand, I chose to do this now.\nThat doesn't feel like a deterministic thing. It feels like I'm making a choice. - [Andrej] It feels like it.\n- Okay. So it's all feelings. It's just feelings. - [Andrej] Yeah. So when our RL agent is making a choice is that,\nit's not really making a choice, the choice was already there. - Yeah. You're interpreting the choice and you're creating a narrative\nfor having made it. - Yeah. And now we're talking about the narrative, it's very meta. Looking back, what is the most beautiful\n"}
{"pod": "Lex Fridman Podcast", "input": "Transformers", "output": "or surprising idea in deep learning or AI in general that you've come across?\nYou've seen this field explode and grow in interesting ways. Just what cool ideas, like what made you sit back and go\nhmm, big or small? - Well, the one that I've been thinking about recently\nthe most probably is the transformer architecture.\nSo basically, neural networks have a lot of architectures that were trendy have come and gone\nfor different sensory modalities. Like for vision, audio, text, you would process them with different-looking neural nets.\nAnd recently, we've seen this convergence towards one architecture, the transformer and you can feed it video, or you can feed it images,\nor speech or text and it just gobbles it up. And it's a bit of a general-purpose computer\nthat is also trainable and very efficient to run in our hardware. And so this paper came out in 2016, I wanna say.\n- [Lex] \"Attention is all you need\". - \"Attention is all you need\". - You criticized the paper title in retrospect that it wasn't, it didn't foresee the bigness\nof the impact that it was going to have. - Yeah. I'm not sure if the authors were aware of the impact that that paper would go on to have.\nProbably they weren't but I think they were aware of some of the motivations and design decisions behind the transformer and they chose not to, I think,\nexpand on it in that way in the paper. And so I think they had an idea that there was more\nthan just the surface of just like, oh, we're just doing translation and here's a better architecture. You're not just doing translation. This is like a really cool differentiable, optimizable,\nefficient computer that you've proposed. And maybe they didn't have all of that foresight but I think it's really interesting.\n- Isn't it funny, sorry to interrupt, that that title is memeable, that they went\nfor such a profound idea. I don't think anyone used that kind of title before, right?\n- \"Attention is all you need\"? Yeah. It's like a meme or something, basically. - Yeah. Isn't it funny that when, maybe if it was\na more serious title it wouldn't have the impact. - Honestly, yeah, there is an element of me that honestly agrees with you and prefers it this way.\n- [Lex] Yes. - If it was too grand it would over-promise and then under-deliver potentially.\nSo you want to just meme your way to greatness? - That should be a T-shirt.\nSo you Tweeted, \"The Transformer is a magnificent neural network architecture because it is a general-purpose\ndifferentiable computer. It is simultaneously expressive in the forward pass, optimizable via back-propagation gradient dissent,\nand efficient high parallelism compute graph.\" Can you discuss some of those details,\nExpressive, optimizable, efficient? - [Andrej] Yeah. - From memory or in general, whatever comes to your heart.\n- You want to have a general-purpose computer that you can train on arbitrary problems like say the task of next-work prediction\nor detecting if there's a cat in a image or something like that. And you want to train this computer so you want to set its weights.\nAnd I think there's number of design criteria that overlap in the transformer simultaneously\nthat made it very successful. And I think the authors were deliberately trying to\nmake this really powerful architecture. And so basically, it's very powerful in the forward pass\nbecause it's able to express very general computation as something that looks like message passing.\nYou have nodes and they all store vectors and these nodes get to basically look at each other,\neach other's vectors and they get to communicate and basically, nodes get to broadcast,\n\"Hey, I'm looking for certain things.\" And then other nodes get to broadcast, \"Hey, these are the things I have.\" Those are the keys and the values.\n- So it's not just attention. - Yeah, exactly. Transformer is much more than just the attention component. It's got many pieces, architectural that went into it,\nthe residual connection, the way it's arranged, there's a multilayer perceptron and they are the way it's stacked and so on.\nBut basically, there's a message-passing scheme where nodes get to look at each other, decide what's interesting, and then update each other.\nAnd so I think when you get to the details of it, I think it's a very expressive function\nso it can express lots of different types of algorithms in a forward pass. Not only that but the way it's designed with the residual connections, layer normalizations,\nthe Softmax, attention, and everything, it's also optimizable. This is a really big deal because there's lots of computers\nthat are powerful that you can't optimize or that are not easy to optimize using the techniques that we have, which is back-propagation and gradient descent,\nthese are first-order methods, very simple optimizers really. And so you also need it to be optimizable.\nAnd then lastly, you want it to run efficiently in our hardware. Our hardware is a massive throughput machine like GPUs,\nthey prefer lots of parallelism. So you don't want to do lots of sequential operations, you want to do a lot of operations serially\nand the transformer is designed with that in mind as well. And so it's designed for our hardware and is designed to both be very expressive\nin a forward pass, but also very optimizable in the backward pass. - And you said that the residual connection support,\nan ability to learn short algorithms fast and first and then gradually extend them longer during training.\n- [Andrej] Yeah. - What's the idea of learning short algorithms? - Right. Think of it as a, so basically a transformer\nis a series of blocks, right? And these blocks have attention and a little multilayer perceptron,\nand so you go off into a block and you come back to this residual pathway and then you go off and you come back and then you have\na number of layers arranged sequentially. And so the way to look at it I think is because of the residual pathway in the backward pass,\nthe gradients sort of flow along it uninterrupted because addition distributes the gradient equally\nto all of its branches. So the gradient from the supervision at the top just floats directly to the first layer.\nAnd all these residual connections are arranged so that in the beginning, during initialization, they contribute nothing to the residual pathway.\nSo what it looks like is, imagine the transformer is like a Python function, like a def,\nand you get to do various lines of code. Say you have a hundred layers-deep transformer,\ntypically they would be much shorter, say 20. So you have 20 lines of code then you can do something in them. And so think of, during the optimization\nbasically what it looks like is first you optimize the first line of code, and then the second line of code can kick in, and the third line of code can kick in.\nAnd I feel like because of the residual pathway and the dynamics of the optimization, you can sort of learn a very short algorithm\nthat gets the approximate answer, but then the other layers can sort of kick in and start to create a contribution. And at the end of it you're optimizing over an algorithm\nthat is 20 lines of code. Except these lines of code are very complex because it's an entire block of a transformer.\nYou can do a lot in there. Well, what's really interesting is that this transformer architecture actually has been remarkably resilient.\nBasically, the transformer that came out in 2016 is the transformer you would use today except you reshuffle some of the layer norms.\nThe layer normalizations have been reshuffled to a prenorm formulation and so it's been remarkably stable\nbut there's a lot of bells and whistles that people have attached on it and try to improve it. I do think that basically, it's a big step\nin simultaneously optimizing for lots of properties of a desirable neural network architecture. And I think people have been trying to change it\nbut it's proven remarkably resilient. But I do think that there should be even better architectures potentially.\n- But you admire the resilience here? - [Andrej] Yeah. - There's something profound about this architecture\nthat leads to resilience. - [Andrej] Yeah. - So maybe everything can be turned into a problem\nthat transformers can solve. - Currently, definitely looks like the transformers taking over AI and you can feed basically\narbitrary problems into it and it's a general differentiable computer and it's extremely powerful. And this conversions in AI has been really interesting\nto watch for me personally. - What else do you think could be discovered here about transformers?\nLike what surprising thing or is it a stable, out in a stable place?\nIs there something interesting we might discover about transformers? Like aha moments, maybe has to do with memory,\nmaybe knowledge representation, that kind of stuff. - Definitely, the zeitgeist today is just pushing,\nbasically, right now the zeitgeist is do not touch the transformer. - [Lex] Yeah. - Touch everything else. - [Lex] Yes.\n- So people are scaling up the data sets, making them much, much bigger. They're working on the evaluation, making the evaluation much, much bigger. And they're basically keeping the architecture unchanged.\nAnd that's how we've, that's the last five years of progress in AI. - What do you think about one flavor of it,\n"}
{"pod": "Lex Fridman Podcast", "input": "Language models", "output": "which is language models? Have you been surprised, has your imagination\nbeen captivated by, you mentioned GPT and all the bigger, and bigger, and bigger language models\nand what are the limits of those models do you think?\nSo just for the task of natural language. - Basically, the way GPT is trained, right, is you've just download a massive amount of text data\nfrom the internet and you try to predict the next word in a sequence, roughly speaking you're predicting\nlittle word chunks but roughly speaking that's it. And what's been really interesting to watch is\nbasically, it's a language model. Language models have actually existed for a very long time. There's papers on language modeling from 2003, even earlier.\n- Can you explain in that case, what a language model is? - Yeah, so language model, just basically the rough idea\nis just predicting the next word in a sequence, roughly speaking. So there's a paper from, for example, Bengio and the team\nfrom 2003, where for the first time they were using a neural network to take say like three or five words\nand predict the next word. And they're doing this on much smaller data sets and the neural net is not a transformer,\nit's a multilayer perceptron but it's the first time that a neural network has been applied in that setting.\nBut even before neural networks there were language models except they were using n-gram models.\nSo n-gram models are just count-based models. So if you try to take two words and predict a third one,\nyou just count up how many times you've seen any two-word combinations and what came next.\nAnd what you predict as coming next is just what you've seen the most of in the training set. And so language modeling has been around for a long time.\nNeural networks have done language modeling for a long time. So really what's new or interesting or exciting\nis just realizing that when you scale it up with a powerful enough neural net, a transformer,\nyou have all these emergent properties where basically what happens is if you have a large enough data set of text,\nyou are in the task of predicting the next word. You are multitasking a huge amount of different kinds of problems.\nYou are multitasking, understanding of chemistry, physics, human nature, lots of things are clustered\nin that objective. It's a very simple objective but actually, you have to understand a lot about the world to make that prediction. - You just said the U word understanding, are you,\nin terms of chemistry, and physics, and so on, what do you feel like it's doing? Is it searching for the right context in,\nwhat is the actual process happening here? - Yeah, so basically, it gets a thousand words\nand it's trying to predict a thousand and first. And in order to do that very, very well over the entire data set available on the internet,\nyou actually have to basically understand the context of what's going on in there.\n- [Lex] Yeah. - And it's a sufficiently hard problem that if you have a powerful enough computer,\nlike a transformer, you end up with interesting solutions. And you can ask it to do all kinds of things\nand it shows a lot of emergent properties like in-context learning, that was the big deal with GPT\nand the original paper when they published it, is that you can just prompt it in various ways and ask it to do various things\nand it will just kind of complete the sentence. But in the process of just completing the sentence it's actually solving all really interesting problems\nthat we care about. - Do you think it's doing something like understanding, like when we use the word understanding for us humans?\n- I think it's doing some understanding, in its weights it understands I think a lot about the world and it has to in order to predict the next word\nin a sequence. - So it's trained on the data from the internet.\nWhat do you think about this approach in terms of data sets, of using data from the internet?\nDo you think the internet has enough structured data to teach AI about human civilization?\n- Yeah, so I think the internet has a huge amount of data. I'm not sure if it's a complete enough set. I dunno that text is enough for having\na sufficiently powerful AGI as an outcome. - Of course, there is audio, and video, and images,\nand all that kind of stuff. - Yeah. So text by itself I'm a little bit suspicious about, there's a ton of things we don't put in text, in writing\njust because they're obvious to us about how the world works and the physics of it and that things fall. We don't put that stuff in text because why would you,\nwe share that understanding. And so text is a communication medium between humans and it's not a all-encompassing medium of knowledge\nabout the world. But as you pointed out, we do have video, and we have images, and we have audio. And so I think that definitely helps a lot.\nBut we haven't trained models sufficiently across all those modalities yet.\nSo I think that's what a lot of people are interested in. - But I wonder what that shared understanding of what we might call common sense\nhas to be learned, inferred in order to complete the sentence correctly.\nSo maybe the fact that it's implied on the internet the model's gonna have to learn that\nnot by reading about it, by inferring it in the representation. So common sense, just like we,\nI don't think we learn common sense, like nobody says, tells us explicitly, we just figure it all out\nby interacting with the world. - [Andrej] Right. - And so here's a model of reading about the way people interact with the world.\nIt might have to infer that. I wonder. - [Andrej] Yeah. - You briefly worked on a project called World of Bits,\ntraining an RL system to take actions on the internet versus just consuming the internet like we talked about.\n- [Andrej] Yeah. - Do you think there's a future for that kind of system interacting with the internet to help the learning?\n- Yes. I think that's probably the final frontier for a lot of these models, so as you mentioned,\nwhen I was at OpenAI, I was working on this project World of Bits and basically, it was the idea of giving neural networks access to a keyboard and a mouse.\nAnd the idea is that- - What could possibly go wrong? - So basically, you perceive the input of the screen pixels\nand basically, the state of the computer is visualized for human consumption in images of the web browser\nand stuff like that. And then you give the neural network the ability to press keyboards and use the mouse and we're trying to get it to, for example,\ncomplete bookings and interact with user interfaces. And- - What'd you learn from that experience?\nLike what was some fun stuff? 'Cause, that's a super cool idea. - [Andrej] Yeah. - I mean it's like, yeah, I mean\nthe step between observer to actor. - [Andrej] Yeah. - Is a super fascinating step. - Yeah.\nWell, it's the universal interface in the digital realm, I would say. And there's a universal interface in the physical realm,\nwhich in my mind is a humanoid form factor kind of thing. We can later talk about optimist and so on, but I feel like they're a similar philosophy in some way\nwhere the physical world is designed for the human form and the digital world is designed for the human form\nof seeing the screen and using keyboard and mouse. And so it's the universal interface\nthat can basically command the digital infrastructure we've built up for ourselves. And so it feels like a very powerful interface to command\nand to build on top of. Now, to your question as to what I learned from that, it's interesting because the World of Bits\nwas basically too early I think at OpenAI, at the time. This is around 2015 or so.\nAnd the zeitgeist at that time was very different in AI from the zeitgeist today. At the time everyone was super excited\nabout reinforcement learning from scratch. This is the time of the Atari paper where neural networks were playing Atari games\nand beating humans in some cases, AlphaGo, and so on. So everyone's very excited about training neural networks from scratch,\nusing reinforcement learning directly. It turns out that reinforcement learning is extremely inefficient way of training neural networks\nbecause you're taking all these actions and all these observations and you get some sparse rewards once in a while.\nSo you do all this stuff based on all these inputs and once in a while you're told you did a good thing,\nyou did a bad thing and it's just an extremely hard problem, you can't learn from that. You can burn a forest and you can brute force through it.\nAnd we saw that I think with Go and Dota and so on, and it does work, but it's extremely inefficient\nand not how you want to approach problems, practically speaking. And so that's the approach that at the time we also took to World of Bits.\nWe would have an agent initialize randomly, so he would keyboard mash, and mouse mash, and try to make a booking.\nAnd it just revealed the insanity of that approach very quickly, where you have to stumble by the correct booking\nin order to get a reward of you did it correctly. And you're never gonna stumble by it by chance at random.\n- So even with a simple web interface there's too many options. - There's just too many options and it's two spars of a reward signal.\nAnd you're starting from scratch at the time and so you don't know how to read, you don't understand pictures, images, buttons.\nYou don't understand what it means to make a booking. But now what's happened is it is time to revisit that\nand OpenAi is interested in this, companies like Adept are interested in this, and so on.\nAnd the idea is coming back because the interface is very powerful but now you're not training an agent from scratch.\nYou are taking the GPT as an initialization. So GPT is pre-trained on all of text\nand it understands what's a booking, it understands what's a submit, it understands quite a bit more.\nAnd so it already has those representations. They are very powerful and that makes all the training significantly more efficient\nand makes the problem tractable. - Should the interaction be the way humans see it, with the buttons and the language,\nor should it be with the HTML, JavaScript, and the CSS? - [Andrej] Yeah. - What do you think is the better?\n- So today all this interaction is mostly on the level of HTML, CSS, and so on. That's done because of computational constraints.\nBut I think ultimately, everything is designed for human visual consumption and so at the end of the day\nthere's all the additional information is in the layout of the webpage, and what's next to you, and what's a red background, and all this kind of stuff.\nAnd what it looks like visually. So I think that's the final frontier is we are taking in pixels and we're giving out keyboard,\nmouse commands, but I think it's impractical still today. - Do you worry about bots on the internet given these ideas,\n"}
{"pod": "Lex Fridman Podcast", "input": "Bots", "output": "given how exciting they are? Do you worry about bots on Twitter being not the stupid bots that we see now with the crypto bots,\nbut the bots that might be out there actually that we don't see, that they're interacting in interesting ways.\nSo this kind of system feels like it should be able to pass the, I'm not a robot click button, whatever.\nDo you actually understand how that test works? I don't quite, there's a checkbox or whatever\nthat you click. - [Andrej] Yeah. - It's presumably tracking. - [Andrej] Oh, I see. - Like mouse movement and the timing and so on.\n- [Andrej] Yeah. - So exactly this kind of system we're talking about should be able to pass that. So yeah.\nWhat do you feel about bots that are language models\nplus have some interactability and are able to Tweet and reply and so on? Do you worry about that world?\n- Yeah, I think it's always been a bit of an arms race between the attack and the defense,\nso the attack will get stronger but the defense will get stronger as well. Our ability to detect that. - How do you defend, how do you detect,\nhow do you know that your Karpathy account on Twitter is human?\nHow would you approach that, like if people were to claim, how would you defend yourself in the court of law\nthat I'm a human, this account is human? - Yeah, at some point I think it might be,\nI think society will evolve a little bit. We might start signing, digitally signing some of our correspondence or things that we create.\nRight now it's not necessary but maybe in the future, it might be. I do think that we are going towards the world\nwhere we share the digital space with AIs. - [Lex] Synthetic beings.\n- Yeah. And they will get much better and they will share our digital realm and they'll eventually share our physical realm as well.\nIt's much harder but that's kind of like the world we're going towards. And most of them will be benign and helpful\nand some of them will be malicious. And it's going to be an arms race trying to detect them. - So, I mean the worst isn't the AIs,\nthe worst is the AIs pretending to be human. So, I don't know if it's always malicious.\nThere's obviously a lot of malicious applications, but it could also be if I was an AI\nI would try very hard to pretend to be human because we're in a human world. - [Andrej] Yeah.\n- I wouldn't get any respect as an AI. - [Andrej] Yeah. - I wanna get some love and respect. - I don't think the problem is intractable.\nPeople are thinking about the proof of personhood. - [Lex] Yes. - And we might start digitally signing our stuff\nand we might all end up having like, yeah, basically some solution for proof of personhood.\nIt doesn't seem to me intractable, it's just something that we haven't had to do until now. But I think once the need really starts to emerge,\nwhich is soon, I think people will think about it much more. - But that too will be a race because obviously\nyou can probably spoof or fake the proof of personhood.\nSo you have to try to figure out how to- - [Andrej] Probably. - I mean it's weird that we have social security numbers,\nand passports, and stuff. It seems like it's harder to fake stuff\nin the physical space. But in the digital space, it just feels like it's gonna be very tricky.\nVery tricky to out, 'cause it seems to be pretty low cost to fake stuff.\nWhat are you gonna put an AI in jail for trying to use a fake personhood proof?\nI mean, okay, fine, you'll put a lot of AIs in jail, but there'll be more AIs, like exponentially more.\nThe cost of creating a bot is very low unless there's some kind of way to track accurately.\nLike you're not allowed to create any program without tying yourself to that program.\nAny program that runs on the internet, you'll be able to trace every single human programming\nthat was involved with that program. - [Andrej] Right. Yeah. Maybe you have to start declaring when, we have to start drawing those boundaries\nand keeping track of, okay, what are digital entities versus human entities and what is the ownership\nof human entities and digital entities and something like that.\nI don't know but I think I'm optimistic that this is possible and in some sense\nwe're currently in the worst time of it because all these bots suddenly have become very capable\nbut we don't have defenses yet built up as a society but I think that doesn't seem to me intractable, it's just something that we have to deal with.\n- It seems weird that the Twitter bot, like really crappy Twitter bots, are so numerous.\n- [Andrej] Yes. - So I presume that the engineers at Twitter are very good.\nSo it seems like what I would infer from that is it seems like a hard problem.\nThey're probably catching, alright, if I were to sort of steel-man the case, it's a hard problem and there's a huge cost\nto false positive to removing a post by somebody\nthat's not a bot, that creates a very bad user experience. So they're very cautious about removing.\nAnd maybe the bots are really good at learning what gets removed and not such that they can stay ahead\nof the removal process very quickly. - My impression of it honestly, is there's a lot of longing for it.\nI mean. - [Lex] Yeah. - It's not subtle, is my impression of it. It's not subtle.\n- But you have, yeah, that's my impression as well. But it feels like maybe you're seeing\nthe tip of the iceberg, maybe the number of bots is in the trillions and you have to like, just it's a constant assault of bots\nand you, I don't know, you have to steel-man the case. 'Cause the bots I'm seeing are pretty like obvious.\nI could write a few lines of code that catch these bots. - Yeah. I mean definitely, there's a lot of longing for it. But I will say I agree that if you are a sophisticated actor\nyou could probably create a pretty good bot right now using tools like GPTs because it's a language model.\nYou can generate faces that look quite good now and you can do this at scale.\nAnd so I think, yeah, it's quite plausible and it's going to be hard to defend. - There was a Google engineer that claimed\n"}
{"pod": "Lex Fridman Podcast", "input": "Google's LaMDA", "output": "that the LaMDA was sentient. Do you think there's any inkling of truth to what he felt?\nAnd more importantly, to me at least, do you think language models will achieve sentience or the illusion of sentience soonish?\n- Yeah. To me, it's a little bit of a canary in a coal mine moment. Honestly, a little bit because,\nso this engineer spoke to a chatbot at Google and became convinced that this bot is sentient.\n- He asked it some existential philosophical questions. - And it gave reasonable answers and looked real and so on.\nSo to me, it's a, he wasn't sufficiently trying\nto stress the system I think and exposing the truth of it as it is today.\nBut I think this will be increasingly harder over time. So yeah, I think more and more people will basically become,\nyeah, I think there will be more people like that over time as this gets better. - Like form an emotional connection to an AI?\n- Yeah. Perfectly plausible in my mind. I think these AIs are actually quite good at human connection, human emotion.\nA ton of text on the internet is about humans, and connection, and love, and so on.\nSo I think they have a very good understanding in some sense of how people speak to each other about this.\nAnd they're very capable of creating a lot of that kind of text.\nThere's a lot of like sci-fi from fifties and sixties that imagined AIs in a very different way. They are calculating, cold, Vulkan-like machines.\nThat's not what we're getting today. We're getting pretty emotional AIs that actually are very competent and capable\nof generating plausible-sounding text with respect to all these topics. - See I'm really hopeful about AI systems\nthat are companions that help you grow, develop as a human being, help you maximize long-term happiness.\nBut I'm also very worried about AI systems that figure out from the internet that humans get attracted to drama.\nAnd so these would just be like shit-talking AIs that just constantly, did you hear? They'll do gossip, they'll try to plant seeds\nof suspicion to other humans that you love and trust. And just kind of mess with people\n'cause that's going to get a lot of attention. So drama, maximize drama. - [Andrej] Yeah. - On the path to maximizing engagement\nand us humans will feed into that machine. - [Andrej] Yeah. - And it'll be a giant drama shit storm-\nSo I'm worried about that. So as the objective function really defines the way\nthat human civilization progresses with AIs in it. - Yeah. I think right now, at least today, it's not correct to really think of them\nas goal-seeking agents that want to do something. They have no long-term memory or anything,\na good approximation of it is you get a thousand words and you're trying to predict a thousand of them first and then you continue feeding it in\nand you are free to prompt it in whatever way you want. So in text. So you say okay you are a psychologist and you are very good\nand you love humans and here's the conversation between you and another human, human column something, you something,\nand then it just continues the pattern and suddenly you're having a conversation with a fake psychologist who's trying to help you.\nAnd so it's still kind of a the realm of a tool, people can prompt it in arbitrary ways and it can create really incredible text\nbut it doesn't have long-term goals over long periods of time. So it doesn't look that way right now.\n- Yeah. But you can do short-term goals that have long-term effects. - [Andrej] Yeah. - So if my prompting short-term goal\nis to get Andrej Karpathy to respond to me on Twitter when I, I think AI might, that's the goal,\nbut it might figure out that talking shit to you, it would be the best in a highly sophisticated interesting way.\n- [Andrej] Right. - And then you build up a relationship when you respond once and then over time, it gets to not be sophisticated\nand just talk shit, and okay, maybe you won't get to Andrej\nbut it might get to another celebrity, it might get into other big accounts. - [Andrej] Yeah.\n- So with just that simple goal, get them to respond. - [Andrej] Yeah. - Maximize the probability of actual response.\n- Yeah. I mean you could prompt a powerful model like this with its opinion about how to do\nany possible thing you're interested in. - [Lex] Yes. - And they're kind of on track to become these oracles, I could think of it that way.\nThey are oracles currently it's just text but they will have calculators, they will have access to Google search, they will have all kinds of gadgets and gizmos.\nThey will be able to operate the internet and find different information and yeah,\nin some sense that's kinda like currently what it looks like in terms of the development. - Do you think it'll be an improvement eventually\nover what Google is for access to human knowledge?\nIt'll be a more effective search engine to access human knowledge? - I think there's definite scope in building a better search engine today.\nAnd I think Google, they have all the tools, all the people, they have everything they need. They have all the possible pieces, they have people training transformers at scale,\nthey have all the data. It's just not obvious if they are capable as an organization to innovate on their search engine right now\nand if they don't someone else will. There's absolute scope for building a significantly better search engine built on these tools.\n- It's so interesting a large company where the search, there's already an infrastructure, it works,\nads brings out a lot of money. So where structurally inside a company is their motivation to pivot.\n- [Andrej] Yeah. - To say we're going to build a new search engine. - [Andrej] Yep. - That's really hard. - So it's usually going to come from a startup, right?\n- Yeah or some other more competent organization. So I don't know.\nSo currently for example, maybe Bing has another shot at it, as an example. - [Lex] There you go, Microsoft Edge\nas we're talking offline. - It's really interesting because search engines\nused to be about okay here's some query, here's web pages that look like the stuff that you have.\nBut you could just directly go to answer and then have supporting evidence. And these models, basically, they've read all the texts\nand they've read all the web pages. And so sometimes when you see yourself going over to search results and getting a sense of the average answer\nto whatever you're interested in, that just directly comes out, you don't have to do that work.\nSo they're kind of like- Yeah, I think they have a way to of distilling all that knowledge into some level of insight, basically.\n- Do you think of prompting as a teaching and learning, like this whole process?\nLike another layer? 'Cause maybe that's what humans are, you already have that background model\nand then the world is prompting you. - Yeah, exactly. I think the way we are programming these computers now,\nlike GPTs, is converging to how you program humans. I mean, how do I program humans via prompt?\nI go to people and I prompt them to do things, I prompt them for information. And so natural language prompt is how we program humans\nand we're starting to program computers directly in that interface. It's pretty remarkable honestly. - So you've spoken a lot about the idea of Software 2.0.\n"}
{"pod": "Lex Fridman Podcast", "input": "Software 2.0", "output": "All good ideas become cliches so quickly, like the terms, it's kind of hilarious.\nIt's like, I think Eminem once said that if he gets annoyed by a song he's written very quickly,\nthat means it's gonna be a big hit 'cause it's too catchy. But can you describe this idea\nand how you're thinking about it has evolved over the months and years since you coined it?\n- Yeah. Yeah. So I had a blog post on Software 2.0, I think several years ago now.\nAnd the reason I wrote that post is because I saw something remarkable happening\nin software development and how a lot of code was being transitioned to be written\nnot in C++ and so on, but it's written in the weights of a neural net. Basically just saying that neural nets\nare taking over software, the realm of software and taking more and more and more tasks. And at the time, I think not many people understood this\ndeeply enough that this is a big deal. This is a big transition. Neural networks were seen as one of multiple classification algorithms you might use\nfor your dataset problem on Kaggle. This is not that, this is a change in how we program computers.\nAnd I saw neural nets as this is going to take over, the way we program computers is going to change,\nit's not going to be people writing software in C++ or something like that and directly programming the software.\nIt's going to be accumulating training sets and data sets and crafting these objectives by which we train these neural nets.\nAnd at some point, there's going to be a compilation process from the dataset and the objective and the architecture specification into the binary,\nwhich is really just the neural net weights and the forward pass of the neural net\nand then you can deploy that binary. And so I was talking about that transition and that's what the post is about.\nAnd I saw this play out in a lot of fields, autopilot being one of them,\nbut also just a simple image classification. People thought originally, in the eighties and so on,\nthat they would write the algorithm for detecting a dog in an image and they had all these ideas about how the brain does it and first, we detected corners\nand then we detect lines and then we stitched them up and they were really going at it. They were thinking about how they're gonna write the algorithm\nand this is not the way you build it. And there was a smooth transition where, okay,\nfirst we thought we were gonna build everything, then we were building the features, so HOG features and things like that\nthat detect these little statistical patterns from image patches. And then there was a little bit of learning on top of it,\na support vector machine or binary classifier for cat versus dog and images on top of the features.\nSo we wrote the features but we trained the last layer, as the classifier.\nAnd then people are like, actually, let's not even design the features because we can't, honestly, we're not very good at it. So let's also learn the features.\nAnd then you end up with basically a compilation neural net where you're learning most of it. You're just specifying the architecture.\nAnd the architecture has tons of filled-in blanks, which is all the knobs and you let the optimization\nwrite most of it. And so this transition is happening across the industry everywhere. And suddenly we end up with a ton of code\nthat is written in neural net weights. And I was just pointing out that the analogy is actually pretty strong and we have a lot of developer environments\nfor Software 1.0. We have IDEs, how you work with code, how you debug code, how do you run code,\nhow do you maintain code? We have GitHub. So I was trying to make those analogies in the new realm, what is the GitHub of Software 2.0?\nTurns out it's something that looks like Hugging Face right now? And so I think some people took it seriously\nand built cool companies and many people originally attacked the post. It actually was not well received when I wrote it\nand I think maybe it has something to do with the title, but the post was not well received and I think more people have been coming around to it over time.\n- Yeah. So you were the Director of AI at Tesla where I think this idea was really implemented at scale,\nwhich is how you have engineering teams doing Software 2.0. So can you linger on that idea of,\nI think we're in the really early stages of everything you just said, which is like GitHub, IDEs,\nhow do we build engineering teams that work in Software 2.0 systems and the data collection\nand the data annotation, which is all part of that Software 2.0.\nWhat do you think is the task of programming Software 2.0? Is it debugging in the space of hyper-parameters\nor is it also debugging the space of data? - Yeah, the way by which you program the computer\nand influence its algorithm is not by writing the commands yourself.\nYou're changing mostly the data set. You're changing the loss functions\nof what the neural net is trying to do, how it's trying to predict things. But basically the data sets and the architectures of the neural net.\nAnd so in the case of the autopilot, a lot of the data sets have to do with, for example,\ndetection of objects and lane line markings and traffic lights and so on. So you accumulate massive data sets of, here's an example, here's the desired label\nand then here's roughly what the algorithm should look like. And that's a compilation neural net.\nSo the specification of the architecture is like a hint as to what the algorithm should roughly look like. And then the fill in the blanks process of optimization\nis the training process. And then you take your neural net that was trained, it gives all the right answers on your data set\nand you deploy it. - So in that case, perhaps at all machine learning cases,\nthere's a lot of tasks. So is coming up formulating a task\nfor a multi-headed neural network, is formulating a task part of the programming? - [Andrej] Yeah, pretty much so.\n- How you break down a problem into a set of tasks. - Yeah. On a high-level, I would say\nif you look at the software running in the autopilot, I give a number of talks on this topic.\nI would say originally a lot of it was written in Software 1.0, imagine lots of C++, right?\nAnd then gradually, there was a tiny neural net that was, for example, predicting given a single image,\nis there a traffic light or not? Or is there a lane line marking or not? And this neural net didn't have too much to do\nin the scope of the software, it was making tiny predictions on an individual little image and then the rest of the system stitched it up.\nSo okay, we don't have just a single camera, we have eight cameras, we actually have eight cameras over time.\nAnd so what do you do with these predictions? How do you put them together? How do you do the fusion of all that information and how do you act on it?\nAll of that was written by humans in C++. And then we decided, okay, we don't actually want\nto do all of that fusion in the C++ code because we're actually not good enough to write that algorithm. We want the neural nets to write the algorithm\nand we want to port all of that software into the 2.0 stack. And so then we actually had neural nets\nthat now take all the eight camera images simultaneously and make predictions for all of that.\nSo, and actually, they don't make predictions in the space of images.\nThey now make predictions directly in 3D and, actually, in three dimensions around the car.\nAnd now, actually, we don't manually fuse the predictions in 3D over time we don't trust ourselves\nto write that tracker. So actually, we give the neural net the information over time. So it takes these videos now and makes those predictions.\nAnd so you're just like putting more and more power into the neural net processing and at the end of it, the eventual goal is to have most of the software\npotentially be in the 2.0 end because it works significantly better.\nHumans are just not very good at writing software, basically. - So the prediction is happening in this 4D land.\n- [Andrej] Yeah. - Was three-dimensional world over time. - [Andrej] Yeah. - How do you do annotation in that world?\nSo data annotation, whether it's self-supervised or manual by humans is a big part\nof this Software 2.0 world. - I would say by far in the industry, if you're talking about the industry\nand what is the technology of what we have available? Everything is supervised learning. So you need data sets of input, desired output\nand you need lots of it. And there are three properties of it that you need. You need it to be very large,\nyou need it to be accurate, no mistakes and you need it to be diverse. You don't want to just have a lot\nof correct examples of one thing. You need to really cover the space of possibility as much as you can. And the more you can cover the space of possible inputs,\nthe better the algorithm will work at the end. Now once you have really good data sets that you're collecting, curating and cleaning,\nyou can train your neural net on top of that. So a lot of the work goes into cleaning those data sets.\nNow, as you pointed out, the question is how do you achieve a ton of-\nIf you want to basically predict in 3D, you need data in 3D to back that up. So in this video, we have eight videos\ncoming from all the cameras of the system and this is what they saw and this is the truth of what actually was around, there was this car\nand there was this car, this car, these are the lane line markings, this is the geometry of the road. There's a traffic light in this three-dimensional position,\nyou need the ground truth. And so the big question that team was solving, of course, is how do you arrive at that ground truth?\nBecause once you have a million of it and it's large, clean and diverse, then training a neural net on it works extremely well\nand you can ship that into the car. And so there's many mechanisms by which we collected that training data.\nYou can always go for human annotation, you can go for simulation as a source of ground truth, you can also go for what we call the offline tracker\nthat we've spoken about at the AI Day and so on, which is basically an automatic reconstruction process\nfor taking those videos and recovering the three-dimensional reality of what was around that car.\nSo basically, think of doing a three-dimensional reconstruction as an offline thing and then understanding that okay,\nthere's 10 seconds of video, this is what we saw and therefore here's all the lane lines, cars and so on.\nAnd then once you have that annotation, you can train neural nets to imitate it. - And how difficult is the 3D reconstruction?\n- [Andrej] It's difficult but it can be done. - So there's overlap between the cameras and you do the reconstruction\nand perhaps if there's any inaccuracy so that's caught in the annotation step. - Yes.\nThe nice thing about the annotation is that it is fully offline. You have infinite time, you have a chunk of one minute\nand you're trying to just offline in a super-computer somewhere. Figure out where were all the positions of all the cars,\nof all the people, and you have your full one minute of video from all the angles and you can run all the neural nets you want and they can be very efficient, massive neural nets,\nthey can be neural net that can't even run in the car later at test time. So they can be even more powerful neural nets than what you can eventually deploy.\nSo you can do anything you want, three-dimensional reconstruction, neural nets, anything you want just to recover that truth\nand then you supervise that truth. - What have you learned? You said no mistakes about humans doing annotation\n"}
{"pod": "Lex Fridman Podcast", "input": "Human annotation", "output": "'cause there's like a range of things they're good at\nin terms of clicking stuff on screen. How interesting is that to you of a problem of designing\nan annotator where humans are accurate, enjoy it, what are they even the metrics, are efficient,\nare productive, all that kind of stuff? - Yeah, so I grew the annotation team at Tesla from, basically, zero to a thousand while I was there.\nThat was really interesting. My background is a PhD student researcher. So growing that kind of organization was pretty crazy.\nBut yeah, I think it's extremely interesting and part of the design process very much behind the autopilot as to where you use humans.\nHumans are very good at certain kinds of annotations. They're very good, for example, at two-dimensional annotations of images. They're not good at annotating cars over time\nin three-dimensional space. Very, very hard. And so that's why we're very careful to design the tasks that are easy to do for humans\nversus things that should be left to the offline tracker. Maybe the computer will do all the triangulation and 3D construction but the human will say\nexactly these pixels of the image are car. Exactly these pixels are human. And so co-designing the data annotation pipeline\nwas very much bread and butter what I was doing daily. - Do you think there's still a lot of open problems\nin that space? Just in general annotation where the stuff the machines are good at, machines do\nand the humans do what they're good at and there's maybe some iterative process? - Right.\nI think to a very large extent, we went through a number of iterations and we learned a ton about how to create these data sets.\nI'm not seeing big open problems. Originally, when I joined I was really not sure\nhow this would turn out. - [Lex] Yeah. - But by the time I left I was much more secure and actually, we understand the philosophy\nof how to create these data sets and I was pretty comfortable with where that was at the time. - So what are strengths and limitations of cameras\n"}
{"pod": "Lex Fridman Podcast", "input": "Camera vision", "output": "for the driving task in your understanding when you formulate the driving task as a vision task\nwith eight cameras, you've seen that the entire, most of the history of the computer vision field\nwhen it has to do with neural networks. Just if you step back, what are the strengths and limitations of pixels, of using pixels to drive?\n- Yeah, pixels I think are a beautiful sensory, beautiful sensor I would say. The thing is like cameras are very, very cheap\nand they provide a ton of information, ton of bits. So it's a extremely cheap sensor for a ton of bits.\nAnd each one of these bits is a constraint on the state of the world. And so you get lots of megapixel images, very cheap\nand it just gives you all these constraints for understanding what's actually out there in the world. So vision is probably the highest bandwidth sensor.\nIt's a very high bandwidth sensor. - I love that pixels is a constraint on the world.\nIt's this highly complex, high bandwidth constraint\non the stage of the world. That's fascinating. - It's not just that, but again this real importance of it's the sensor that humans use,\ntherefore everything is designed for that sensor. - [Lex] Yeah. - The text, the writing, the flashing signs,\neverything is designed for vision and so, and you just find it everywhere. And so that's why that is the interface you want to be in\ntalking again about these universal interfaces and that's where we actually want to measure the world as well and then develop software for that sensor.\n- But there's other constraints on the state of the world that humans use to understand the world.\nI mean vision ultimately is the main one. But we're referencing our understanding of human behavior\nand some common-sense physics that could be inferred from vision, from a perception perspective.\nBut it feels like we're using some kind of reasoning to predict the world.\n- [Andrej] Yeah, hundred percent. - Not just the pixels. - I mean you have a powerful prior service for how the world evolves over time, et cetera.\nSo it's not just about the likelihood term coming up from the data itself telling you about what you are observing,\nbut also the prior term of what are the likely things to see and how do they likely move and so on.\n- And the question is how complex is the range of possibilities that might happen\nin the driving task. - [Andrej] Right. - Is that to you still an open problem of how difficult is driving, philosophically speaking?\nOf al the time you worked on driving, do you understand how hard driving is? - Yeah, driving is really hard\nbecause it has to do with the predictions of all these other agents and the theory of mind and what they're gonna do. And are they looking at you?\nWhere are they looking? What are they thinking? - [Lex] Yeah. - There's a lot that goes there at the full tail-off,\nthe expansion of the nines that we have to be comfortable with eventually the final problems are of that form. I don't think those are the problems that are very common.\nI think eventually they're important but it's really in the tail end. - In the tail end, the rare edge cases.\n- [Andrej] Yes. - From the vision perspective, what are the toughest parts of the vision problem of driving?\n- Well, basically, the sensor is extremely powerful but you still need to process that information.\nAnd so going from brightnesses of these pixel values to, hey, here are the three-dimensional world,\nis extremely hard and that's what the neural networks are fundamentally doing. And so the difficulty really is in just\ndoing an extremely good job of engineering the entire pipeline, the entire data engine,\nhaving the capacity to train these neural nets, having the ability to evaluate the system and iterate on it.\nSo I would say just doing this in production at scale is the hard part, it's an execution problem. - So the data engine but also the deployment of the system\nsuch that has low latency performance. So it has to do all these steps. - Yeah. For the neural nets specifically just making sure\neverything fits into the chip on the car. - [Lex] Yeah. - And you have a finite budget of flops that you can perform and memory bandwidth\nand other constraints and you have to make sure it flies and you can squeeze in as much computer as you can into the tiny.\n- What have you learned from that process? Because maybe that's one of the bigger, new things,\ncoming from a research background, where there's a system that has to run under heavily constrained resources.\nHas to run really fast. What insights have you learned from that?\n- Yeah, I'm not sure if there's too many insights, you're trying to create a neural net that will fit in what you have available\nand you're always trying to optimize it. And we talked a lot about it on the AI Day and, basically, the triple backflips\nthat the team is doing to make sure it all fits and utilizes the engine. So I think it's extremely good engineering\nand then there's all kinds of little insights peppered in on how to do it properly. - Let's actually zoom out\n"}
{"pod": "Lex Fridman Podcast", "input": "Tesla's Data Engine", "output": "'cause I don't think we talked about the data engine, the entirety of the layouts of this idea\nthat I think is just beautiful with humans in the loop. Can you describe the data engine?\n- Yeah, the data engine is what I call the almost biological feeling process\nby which you perfect the training sets for these neural networks.\nSo because most of the programming now is in the level of these data sets and make sure they're large, diverse and clean, basically you have a data set\nthat you think is good, you train your neural net, you deploy it, and then you observe how well it's performing\nand you're trying to always increase the quality of your data set. So you're trying to catch scenarios,\nbasically, that are, basically, rare. And it is in these scenarios that neural nets will typically struggle in\nbecause they weren't told what to do in those rare cases in the data set. But now you can close the loop because if you can now collect all those at scale,\nyou can then feed them back into the reconstruction process I described and reconstruct the truth in those cases\nand add it to the dataset. And so the whole thing ends up being a staircase of improvement of perfecting your training set\nand you have to go through deployments so that you can mine the parts that are not yet represented well on the dataset.\nSo your dataset is basically imperfect. It needs to be diverse, it has pockets that are missing and you need to pat out the pockets.\nYou can sort of think of it that way in the data. - What role do humans play in this? So what's this biological system like a human body\nmade up of cells? What role? How do you optimize the human system?\nThe multiple engineers collaborating, figuring out what to focus on, what to contribute,\nwhich task to optimize in this neural network. Who's in charge of figuring out which task needs more data?\nCan you speak to the hyperparameters, the human system? - It really just comes down to extremely good execution\nfrom an engineering team that knows what they're doing. They understand intuitively the philosophical insights underlying the data engine and the process\nby which the system improves and how to, again, delegate the strategy of the data collection\nand how that works. And then just making sure it's all extremely well executed. And that's where most of the work, it's not even the philosophizing or the research\nor the ideas of it. It's just extremely good execution is so hard when you're dealing with data at that scale. - So your role in the data engine executing well\nand it is difficult and extremely important. Is there a priority of a vision board of saying like,\nwe really need to get better at stoplights? - [Andrej] Yeah. - The prioritization of tasks?\n- [Andrej] Yes. - Is that essentially, and that comes from the data? - That comes to, a very large extent\nto what we are trying to achieve in the product roadmap. The release we're trying to get out and the feedback\nfrom the QA team where the system is struggling or not, the things we're trying to improve. - And the QA team gives some signal, some information\nin aggregate about the performance of the system in various conditions. - That's right. And then of course all of us drive it\nand we can also see it. It's really nice to work with a system that you can also experience yourself. It drives you home.\n- Is there some insight you can draw from your individual experience that you just can't quite get from an aggregate statistical analysis of data?\n- I would say so, yeah. - [Lex] It's so weird, right? - Yes. - It's not scientific in a sense\n'cause you're just one anecdotal sample. - Yeah, I think there's a ton of, it's a source of truth.\nIt's your interaction with the system. - [Lex] Yeah. - And you can see it, you can play with it, you can perturb it, you can get a sense of it,\nyou have an intuition for it. I think numbers and plots and graphs are much harder.\nIt hides a lot of- - It's like if you train a language model,\nit's a really powerful way is by you interacting with it. - [Andrej] Yeah, a hundred percent. - Start try to build up an intuition.\n- Yeah, I think Elon also, he always wanted to drive the system himself. He drives a lot and I don't wanna say almost daily.\nSo he also sees this as a source of truth, you driving the system and it performing and yeah.\n"}
{"pod": "Lex Fridman Podcast", "input": "Tesla Vision", "output": "- So what do you think? Tough questions here. So, Tesla, last year removed radar from the sensor suite\nand now just announce that it's gonna remove all ultrasonic sensors relying solely on vision,\nso camera only, does that make the perception problem harder or easier?\n- I would almost reframe the question in some way. So the thing is basically, you would think that additional sensors.\n- Wait, wait, wait, can I just interrupt? - [Andrej] Go ahead. - I wonder if a language model will ever do that if you prompt it.\nLet me reframe your question. That would be epic. That's the wrong prompt. Sorry.\n- Yeah, so it's a little bit of a wrong question because, basically, you would think that these sensors are an asset to you.\n- [Lex] Yeah. - But if you fully consider the entire product in its entirety, these sensors\nare actually potentially a liability because these sensors aren't free. They don't just appear on your car.\nSuddenly you have an entire supply chain, you have people procuring it, there can be problems with them, they may need replacement.\nThey are part of the manufacturing process. They can hold back the line in the production. You need to source them, you need to maintain them,\nyou have to have teams that ride the firmware, all of it. And then you also have to incorporate them,\ninfuse them into the system in some way. And so it actually bloats a lot of it. And I think Elon is really good at simplify, simplify,\nbest part is no part. And he always tries to throw away things that are not essential because he understands the entropy\nin organizations and an approach. And I think in this case the cost is high and you're not potentially seeing it\nif you're just a computer vision engineer and I'm just trying to improve my network and is it more useful or less useful?\nHow useful is it? And the thing is, if once you consider the full cost of a sensor, it actually is potentially a liability\nand you need to be really sure that it's giving you extremely useful information. In this case, we looked at using it or not using it\nand the delta was not massive. And so it's not useful. - Is it also bloat in the data engine,\nlike having more sensors? - Hundred percent. - Is it a distraction? - And these sensors, they can change over time.\nFor example, you can have one type of say radar, you can have other type of radar. They change over time. Now suddenly you need to worry about it.\nNow suddenly you have a column in your sequel light telling you, oh, what sensor type was it? And they all have different distributions\nand then they contribute noise and entropy into everything\nand they bloat stuff. And also organizationally, it's been really fascinating to me that it can be very distracting.\nIf all you wanna get to work is vision, all the resources are on it and you're building out a data engine\nand you're actually making forward progress because that is the sensor with the most bandwidth, the most constraints on the world.\nAnd you're investing fully into that. And you can make that extremely good. You have only a finite amount of sort of spend of focus\nacross different facets of the system. - And this reminds me of Rich Sutton, \"The Bitter Lesson\"\nthat just seems like simplifying the system. - [Andrej] Yeah. - In the long run. And of course, you don't know what the long run is\nand it seems to be always the right solution. - Yeah. Yes. - In that case, it was for RL but it seems to apply generally\nacross all systems that do computation. - [Andrej] Yeah. - So what do you think about the LiDAR as a crutch debate?\nThe battle between point clouds and pixels? - Yeah, I think this debate is always slightly confusing\nto me because it seems like the actual debate should be about do you have the fleet or not. That's the really important thing\nabout whether you can achieve a really good functioning of an AI system at this scale. - [Lex] So data collection systems.\n- Yeah. Do you have a fleet or not is significantly more important whether you have LiDAR or not. It's just another sensor.\nAnd yeah, I think similar to the radar discussion, basically, yeah, I don't think it,\nit basically doesn't offer extra information. It's extremely costly.\nIt has all kinds of problems. You have to worry about it, you have to calibrate it, et cetera. It creates bloat and entropy. You have to be really sure that you need this sensor.\nIn this case, I basically don't think you need it. And I think honestly, I will make a stronger statement. I think some of the other companies who are using it\nare probably going to drop it. - Yeah. So you have to consider the sensor in the full,\nin considering can you build a big fleet that collects a lot of data and can you integrate that sensor with that data,\nand that sensor into a data engine that's able to quickly find different parts of the data\nthat then continuously improves whatever the model that you're using. - Yeah. Another way to look at it is like, vision is necessary\nin a sense that the world is designed for human visual consumption. So you need vision, it's necessary.\nAnd then also it is sufficient because it has all the information that you need for driving.\nAnd humans obviously use vision to drive. So it's both necessary and sufficient. So you want to focus resources and you have to be really sure\nif you're going to bring in other sensors. You could add sensors to infinity, at some point you need to draw the line.\nAnd I think in this case, you have to really consider the full cost of any one sensor that you're adopting,\nand do you really need it? And I think the answer, in this case, is no. - So what do you think about the idea\nthat the other companies are forming high-resolution maps and constraining heavily the geographic regions\nin which they operate? Is that approach, in your view, not going to scale over time\nto the entirety of the United States? - [Andrej] Yeah. I think- - It'll take too long- - As you've mentioned like they pre-map all the environments\nand they need to refresh the map and they have a perfect centimeter-level-accuracy map of everywhere they're gonna drive.\nIt's crazy. How are you going to, when we're talking about autonomy actually changing the world, we're talking about\nthe deployment on the global scale of autonomous systems for transportation.\nAnd if you need to maintain a centimeter-accurate map for earth or for many cities and keep them updated,\nit's a huge dependency that you're taking on, a huge dependency. It's a massive, massive dependency\nand now you need to ask yourself, do you really need it? And humans don't need it, right?\nSo it's very useful to have a low-level map of like, okay, the connectivity of your road, you know that there's a fork coming up.\nWhen you drive in an environment, you have that high-level understanding. It's like a small Google map and Tesla uses Google map,\nsimilar resolution information in its system, but it will not pre-map environments\nto centimeter-level accuracy. It's a crutch, it's a distraction, it causes entropy, and it diffuses the team,\nit dilutes the team and you're not focusing on what's actually necessary, which is a computer vision problem.\n"}
{"pod": "Lex Fridman Podcast", "input": "Elon Musk", "output": "- What did you learn about machine learning, about engineering, about life, about yourself\nas one human being from working with Elon Musk? - I think the most I've learned is about\nhow to run organizations efficiently and how to create efficient organizations\nand how to fight entropy in an organization. - So human engineering in the fight against entropy.\n- Yeah, I think Elon is a very efficient warrior in the fight against entropy in organizations.\n- What does entropy in an organization look like exactly? - It's process. It's process and it's-\n- Inefficiencies in the form meetings and that kind of stuff? - Yeah. Meetings, he hates meetings, he keeps telling people\nto skip meetings if they're not useful. He basically runs the world's biggest startups,\nI would say, Tesla, SpaceX are the world's biggest startups. Tesla actually is multiple startups,\nI think it's better to look at it that way. And so I think he's extremely good at that.\nAnd yeah, he is a very good intuition for streamlining process. He's making everything efficient. Best part is no part, simplifying,\nfocusing, and just kind of removing barriers, moving very quickly, making big moves.\nAll this is very startupy sort of seeming things but at scale. - So strong drive to simplify.\n- [Andrej] Yeah. - From your perspective, I mean, that also probably applies to just designing systems\nand machine learning, and otherwise. - Yeah. - [Lex] like simplify, simplify. - Yes. - What do you think is the secret to maintaining\nthe startup culture in a company that grows? Is there, can you introspect that?\n- I do think he needs someone in a powerful position with a big hammer like Elon who's the cheerleader\nfor that idea, and ruthlessly pursues it. If no one has a big enough hammer,\neverything turns into committees, democracy within the company, process,\ntalking to stakeholders, decision-making, Just everything just crumbles. - [Lex] Yeah. - If you have a big person who is also really smart\nand has a big hammer, things move quickly. - So you said your favorite scene in \"Interstellar\"\nis the intense docking scene with the AI and Cooper talking, saying, \"Cooper, what are you doing?\nDocking. It's not possible. No, it's necessary.\" Such a good line, by the way, just so many questions there.\nWhy an AI in that scene presumably\nis supposed to be able to compute a lot more than the human is saying it's not optimal, why are the human,\nI mean that's a movie, but shouldn't the AI know much better than the human?\nAnyway, what do you think is the value of setting seemingly impossible goals?\nSo like our initial intuition, which seems like something that you have taken on\nthat Elon espouses that where the initial intuition of the community might say this is very difficult\nand then you take it on anyway, with a crazy deadline. You, just from a human engineering perspective,\nhave you seen the value of that? - I wouldn't say that setting impossible goals exactly\nis a good idea but I think setting very ambitious goals is a good idea. I think there's a, what I call sublinear scaling\nof difficulty, which means that 10x problems are not 10x hard. Usually, 10x harder problem is like two or three x\nharder to execute on. Because if you wanna actually, like if you wanna improve a system by 10%, it costs some amount of work.\nAnd if you wanna 10x improve the system, it doesn't cost you know, a 100x amount of the work. And it's because you fundamentally change the approach.\nAnd if you start with that constraint, then some approaches are obviously dumb and not going to work and it forces you to reevaluate.\nAnd I think it's a very interesting way of approaching problem-solving. - But it requires a weird kind of thinking.\nIt's just going back to your PhD days. It's like how do you think which ideas\nin the machine learning community are solvable? - [Andrej] Yes.\n- It requires, what is that? I mean there's the cliche of first principles thinking but it requires to basically ignore\nwhat the community is saying. 'Cause doesn't a community, doesn't a community in science\nusually draw lines of what is and isn't possible? - [Andrej] Right. - And it's very hard to break out of that\nwithout going crazy. - Yeah. I mean I think a good example here is the deep learning revolution in some sense\nbecause you could be in computer vision at that time, during the deep learning revolution of 2012 and so on.\nYou could be improving a computer vision stack by 10% or it can just be saying actually all this is useless\nand how do I do 10x better computer vision? Well, it's not probably by tuning a HOG feature detector,\nI need a different approach. I need something that is scalable. Going back to Richard Suttons\nand understanding the philosophy of the Bitter Lesson and then being like actually, I need a much more scalable system,\nlike a neural network that in principle works. And then having some deep believers that can actually\nexecute on that mission, make it work. So that's the 10x solution.\n"}
{"pod": "Lex Fridman Podcast", "input": "Autonomous driving", "output": "- What do you think is the timeline to solve the problem of autonomous driving? That's still in part an open question.\n- Yeah, I think the tough thing with timelines of self-driving obviously, is that no one has created self-driving.\n- [Lex] Yeah. - So it's not like, what do you think is a timeline to build this bridge? Well, we've built a million bridges before,\nhere's how long that takes. No one has built autonomy, it's not obvious.\nSome parts turn out to be much easier than others. So it's really hard to forecast. You do your best based on trend lines and so on\nand based on intuition. But that's why fundamentally it's just really hard to forecast this. No one has- - So even still like being inside of it,\nit's hard to- - Yes. Some things turn out to be much harder and some things turned out to be much easier.\n- Do you try to avoid making forecasts? 'Cause Elon doesn't avoid them, right?\nAnd heads of car companies in the past have not avoided it either. Ford and other places have made predictions\nthat we're gonna solve level-four driving by 2020, 2021, whatever.\nAnd they all backtrack on that prediction. As an AI person, do you feel yourself privately\nmake predictions or do they get in the way of your actual ability to think about a thing?\n- Yeah, I would say what's easy to say is that this problem is tractable and that's an easy prediction to make.\nIt's tractable- - So it's solvable? - It's going to work. Yes. It's just really hard. Some things turned out to be harder and somethings turn out to be easier.\nBut it definitely feels tractable and it feels like, at least the team at Tesla, which is what I saw internally,\nis definitely on track to that. - How do you form a strong representation\nthat allows you to make a prediction about tractability? So you're the leader a lot of humans,\nyou have to say this is actually possible. - Yeah.\n- How do you build up that intuition? It doesn't have to be even driving, it could be other tasks. - [Andrej] Right. - It could be, what difficult tasks did you work on\nin your life? I mean classification, achieving certain, just an image at certain level\nof superhuman-level performance. - Yeah, expert intuition. It's just intuition, it's belief.\n- So just like thinking about it long enough, like studying, looking at sample data, like you said, driving.\nMy intuition is really flawed on this. I don't have a good intuition about tractability. It could be anything.\nIt could be solvable. The driving task could be simplified\ninto something quite trivial. Like the solution to the problem would be quite trivial.\nAnd at scale, more and more cars driving perfectly might make the problem much easier.\nThe more cars you have driving, like people learn how to drive correctly, not correctly, but in a way that's more optimal for heterogeneous system\nof autonomous, and semi-autonomous, and manually driven cars, that could change stuff. Then again, also I've spent a ridiculous number of hours\njust staring at pedestrians crossing streets, thinking about humans. And it feels like the way we use our eye contact,\nit sends really strong signals and there's certain quirks and edge cases of behavior.\nAnd of course, a lot of the fatalities that happen have to do with drunk driving,\nboth on the pedestrian side and the driver's side. So there's that problem of driving at night and all that kind of.\n- [Andrej] Yeah. - So I wonder, it's like the space of possible solution into autonomous driving includes so many human factor issues\nthat it's almost impossible to predict. There could be super clean, nice solutions.\n- Yeah. I would say definitely, to use a game analogy, there's some fog of war, but you definitely also see\nthe frontier of improvement and you can measure historically how much you've made progress. And I think for example, at least what I've seen\nin roughly five years at Tesla. When I joined it barely kept lane on the highway.\nI think going up from Palo Alto to SF was like three or four interventions. Anytime the road would do anything geometrically\nor turn too much it would just not work. And so going from that to like a pretty competent system in five years and seeing what happens also under the hood\nand what the scale which the team is operating now with respect to data, and compute, and everything else\nis just massive progress. - So you're climbing a mountain and it's fog\nbut you're making a lot of progress. - It's Fog. You're making progress and you see what the next directions are and you're looking at some of the remaining challenges\nand they're not perturbing you, and they're not changing your philosophy, and you're not contorting yourself.\nYou're like, actually, these are the things that we still need to do. - Yeah, it's the fundamental components of solving the problem seem to be there,\nfrom the data engine, to the compute, to the compute on the car, to the compute for the training, all that kind of stuff.\n- [Andrej] Yes. - Over the years you've been at Tesla, you've done a lot of amazing breakthrough ideas\n"}
{"pod": "Lex Fridman Podcast", "input": "Leaving Tesla", "output": "and engineering all of it from the data engine to the human side, all of it.\nCan you speak to why you chose to leave Tesla? - Basically, as I described, I think over time\nduring those five years I've kind of gotten myself into a little bit of a managerial position.\nMost of my days were meetings, and growing the organization, and making decisions about,\nhigh-level strategic decisions about the team and what it should be working on, and so on. And it's kind of like a corporate executive role.\nAnd I can do it. I think I'm okay at it. But it's not like fundamentally what I enjoy. And so I think when I joined,\nthere was no computer vision team because Tesla was just going from the transition of using MobilEye, a third-party vendor, for all of its computer vision\nto having to build its computer vision system. So when I showed up, there were two people training deep neural networks.\nAnd they were training them at a computer at their legs, like down, there was a work.\n- There was some kind of basic classification task. - Yeah. And so I like grew that into what I think\nis a fairly respectable deep learning team, a massive computer cluster, a very good data annotation organization.\nAnd I was very happy with where that was. It became quite autonomous and so I stepped away\nand I'm very excited to do much more technical things again. Yeah. And kind of like we focus on AGI.\n- What was that soul searching like? 'Cause you took a little time off, I think, how many mushrooms did you take?\nNo, I'm just kidding. I mean, what was going through your mind? The human lifetime is finite.\n- [Andrej] Yeah. - You did a few incredible things. You're one of the best teachers of AI in the world.\nYou're one of the best. And I mean that in the best possible way. You're one of the best tinkerers in the AI world.\nMeaning like understanding the fundamentals of how something works by building it from scratch\nand playing with the basic intuitions. It's like Einstein, Fineman, were all really good at this kind of stuff.\nLike small example of a thing, to play with it, to try to understand it. So that, and obviously now with Tesla,\nyou helped build a team of machine learning\nengineers and a system that actually accomplishes something in the real-world. So given all that, what was the soul searching like?\n- Well, it was hard because obviously, I love the company a lot, and I love Elon, I love Tesla,\nso it was hard to leave. I love the team, basically. But yeah, I think I actually,\nI really potentially interested in revisiting it, maybe coming back at some point, working in Optimus,\nworking in AGI at Tesla. I think Tesla's going to do incredible things. It's basically a massive large-scale robotics\nkind of company with a ton of in-house talent for doing real incredible things. And I think human robots are going to be amazing.\nI think autonomous transportation is going to be amazing. All this is happening at Tesla. So I think it's just a really amazing organization.\nSo being part of it and helping it along I think was very, basically, I enjoyed that a lot. Yeah, it was basically difficult for those reasons\nbecause I love the company. But I'm happy to potentially at some point come back for act two.\nBut I felt like at this stage I built the team, it felt autonomous and I became a manager\nand I wanted to do a lot more technical stuff. I wanted to learn stuff. I wanted to teach stuff. And I just felt like it was a good time\nfor a change of pace a little bit. - What do you think is the best movie sequel\nof all time speaking of part two? 'Cause most of 'em suck. - Movie sequels? - Movie sequels, yeah.\nAnd you Tweet about movies. So just a tiny tangent, what's a favorite movie sequel?\n\"Godfather Part II\"? Are you a fan of \"Godfather\"? 'Cause you didn't even Tweet or mention \"The Godfather\".\n- Yeah, I don't love that movie. I know it has a- - We're gonna edit that out. We're gonna edit out the hate towards \"The Godfather\".\nHow dare you disrespect? - I think I will make a strong statement. I don't know why but I basically don't like any movie\nbefore 1995, something like that. - Didn't you mention \"Terminator 2\".\n- Okay. Okay. That's like a, \"Terminator 2\" was a little bit later. 1990...\n- No, I think \"Terminator 2\" was in the eighties. - And I like \"Terminator\" one as well, So, okay. So a few exceptions but by and large\nfor some reason, I don't like movies before 1995 or something. They feel very slow.\nThe camera is like zoomed out. It's boring, it's kind of naive, it's kind of weird. - And also Terminator was very much ahead of its time.\n- Yes. And \"The Godfather\", there's like no AGI.\n- I mean but you have, \"Good Will Hunting\" was one of the movies you mentioned and that doesn't have any AGI either.\nI guess it has mathematics. - Yeah, I guess occasionally, I do enjoy movies that don't feature. - Or like \"Anchorman\" that has no, that's.\n- \"Anchorman\" is so good. - I don't understand, speaking of AGI,\n'cause I don't understand why Will Ferrell is so funny. It doesn't make sense. It doesn't compute.\nThere's just something about him. And he's a singular human. 'Cause, you don't get that many comedies these days.\nAnd I wonder if it has to do about the culture or the Machine of Hollywood or does it have to do with\njust we got lucky with certain people in comedy that came together, 'cause he is a singular human. - Yeah, yeah.\nI love his movies. - That was a ridiculous tangent, I apologize. But you mentioned humanoid robots.\n"}
{"pod": "Lex Fridman Podcast", "input": "Tesla's Optimus", "output": "So what do you think about Optimus, about Tesla Bot? Do you think we'll have robots in the factory\nand in the home in 10, 20, 30, 40, 50 years? - Yeah. I think it's a very hard project.\nI think it's going to take a while, but who else is going to build human robots at scale? - [Lex] Yeah.\n- And I think it is a very good form factor to go after because like I mentioned the world is designed for humanoid form factor. These things would be able to operate our machines.\nThey would be able to sit down in chairs, potentially even drive cars. Basically, the world is designed for humans,\nthat's the form factor you want to invest into and make work over time. I think, there's another school of thought which is,\nokay, pick a problem and design a robot to it. But actually, designing a robot and getting a whole data engine and everything behind it to work\nis actually an incredibly hard problem. So it makes sense to go after general interfaces that, okay, they are not perfect for any one given task,\nbut they actually have the generality of just with a prompt, with English, able to do something across.\nAnd so I think it makes a lot of sense to go after a general interface in the physical world.\nAnd I think it's a very difficult project. It's going to take time, but I've seen no other company\nthat can execute on that vision. I think it's going to be amazing. Basically, physical labor, if you think transportation\nis a large market, try physical labor. It's insane. - But it's not just physical labor, to me,\nthe thing that's also exciting is the social robotics. So the relationship we'll have on different levels\nwith those robots. - Yeah. - That's why I was really excited to see Optimus.\nPeople have criticized me for the excitement, but I've worked with a lot of research labs\nthat do humanoid-legged robots, Boston Dynamics, Unitree.\nThere's a lot of companies that do legged robots, but that's the elegance of the movement\nis a tiny, tiny part of the big picture. So the two big exciting things to me about Tesla\ndoing humanoid or any legged robots is clearly integrating into the data engine.\nSo the data engine aspect, so the actual intelligence for the perception and the control and the planning\nand all that kind of stuff. Integrating into this huge, the fleet that you mentioned. Right. And then speaking of fleet, the second thing is\nthe mass manufacturers just knowing culturally driving towards a simple robot\nthat's cheap to produce at scale. - [Andrej] Yeah. - And doing that well, having experience to do that well, that changes everything.\nThat's why that's a very different culture and style than Boston Dynamics. Who by the way, those robots are just-\nThe way they move, it'll be a very long time before Tesla could achieve the smoothness of movement.\nBut that's not what it's about. It's about the entirety of the system.\nLike we talked about the data engine and the fleet. - [Andrej] Right. - And that's super exciting, even the initial sort of models.\nBut that too was really surprising that in a few months you can get a pro a prototype.\n- Yep. And the reason that happened very quickly is, as you alluded to, there's a ton of copy-paste from what's happening in the autopilot.\nA lot. The amount of expertise that came out of the woodworks at Tesla for building the human robot was incredible to see.\nBasically, Elon said at one point we're doing this and then next day, basically, all these CAD models\nstarted to appear and people talk about the supply chain and manufacturing. - [Lex] Yeah. - And people showed up with Screwdrivers and everything\nthe other day and started to like put together the body. And I was like, whoa. All these people exist at Tesla.\nAnd fundamentally building a car is actually not that different from building a robot. And that is true not just for the hardware pieces.\nAnd also let's not forget hardware, not just for a demo, but manufacturing of that hardware at scale\nis a whole different thing. But for software as well, basically, this robot currently thinks it's a car.\n- It's gonna have a midlife crisis at some point. - It thinks it's a car. Some of the earlier demos, actually, we were talking about\npotentially doing them outside in the parking lot because that's where all of the computer vision was working out of the box.\n- [Lex] That's Funny. - Instead of inside. But all the operating system, everything just copy-pastes,\ncomputer vision mostly copy-paste. I mean you have to retrain the neural nets, but the approach and everything and data engine and offline trackers and the way we go about\nthe occupancy tracker and so on. Everything copy-paste, you just need to retrain the neural nets. And then the planning control, of course,\nhas to change quite a bit, but there's a ton of copy-paste from what's happening at Tesla. And if you were to go with goal of like, okay,\nlet's build a million human robots and you're not Tesla, that's a lot to ask, if you're at Tesla,\nit's actually like, that's not that crazy. - And then the follow-up question is then how difficult, just like with driving,\nhow difficult is the manipulation task? - [Andrej] Yep. - Such that it can have an impact at scale? I think depending on the context,\nthe really nice thing about robotics is that, unless you do a manufacturer and that kind of stuff,\nis there is more room for error? - [Andrej] Yep. - Driving is so safety-critical and also time critical,\na robot is allowed to move slower, which is nice. - Yes. I think it's going to take a long time.\nBut the way you want to structure the development is you need to say, okay, it's going to take a long time. How can I set up the product development roadmap\nso that I'm making revenue along the way? I'm not setting myself up for a zero-one-loss function where it doesn't work until it works.\nYou don't wanna be in that position. You want to make it useful almost immediately. And then you want to slowly deploy it and-\n- [Lex] At scale, hopefully. - At scale and you want to set up your data engine, your improvement loops, the telemetry, the evaluation,\nthe harness and everything. And you want to improve the product over time, incrementally. And you're making revenue along the way.\nThat's extremely important because otherwise, you cannot build these large undertakings, just don't make sense economically.\nAnd also from the point of view of the team working on it, they need the dopamine along the way. They're not just going to make a promise\nabout this being useful. This is going to change the world in 10 years when it works. This is not where you want to be.\nYou want to be in a place like I think autopilot today where it's offering increased safety and convenience of driving today.\nPeople pay for it, people like it, people purchase it. And then you also have the greater mission that you're working towards.\n- And you see that. So the dopamine for the team, that was the source of happiness? - Yes, hundred percent, you're deploying this.\nPeople like it, people drive it, people pay for it, they care about it. There's all these YouTube videos, your grandma drives it.\nShe gives you feedback. People like it, people engage with it. You engage with it. Huge. - Do people that drive Teslas recognize you\nand give you love? Like, hey thanks for this nice feature that it's doing.\n- Yeah, I think the tricky thing is some people really love you, some people, unfortunately, you're working on something that you think is extremely valuable, useful, et cetera,\nsome people do hate you. There's a lot of people who hate me and the team and the whole project.\nAnd I think- - Are they Tesla drivers? - Many cases they're not, actually. - Yeah.\nThat actually makes me sad about humans or the current ways that humans interact.\nI think that's actually fixable. I think humans want to be good to each other. I think Twitter and social media is part of the mechanism\nthat actually somehow makes the negativity more viral that it doesn't deserve,\ndisproportionately add a viral boost of negativity.\nBut I wish people would just get excited about, so suppress some of the jealousy,\nsome of the ego and just get excited for others. And then there's a karma aspect to that. You get excited for others, they'll get excited for you.\nSame thing in academia, if you're not careful, there is a dynamical system there. If you think of in silos and get jealous\nof somebody else being successful that actually perhaps counterintuitively\nleads the less productivity of you as a community and you individually. I feel like if you keep celebrating others,\nthat actually makes you more successful. - [Andrej] Yeah. - And I think people haven't, depending on the industry,\nhaven't quite learned that yet. - Yeah. Some people are also very negative and very vocal. So they're very prominently featured.\nBut actually, there's a ton of people who are cheerleaders, but they're silent cheerleaders. And when you talk to people just in the world,\nthey will all tell you it's amazing, it's great. Especially like people who understand how difficult it is to get this stuff working. People who have built products and makers and entrepreneurs,\nmaking this work and changing something is incredibly hard. Those people are more likely to cheerlead you.\n- Well, one of the things that makes me sad is some folks in the robotics community don't do the cheerleading and they should\n'cause they know how difficult it is. Well, they actually sometimes don't know how difficult it is to create a product at scale.\nRight? - [Andrej] Yep. - They actually deploy in the real-world. A lot of the development of robots and AI systems\nis done on very specific small benchmarks and as opposed to real-world additions.\n- Yes. Yeah. I think it's really hard to work on robotics in academic setting. - Or AI systems that apply in the real-world.\n"}
{"pod": "Lex Fridman Podcast", "input": "ImageNet", "output": "You've criticized, you flourished, and loved\nfor time the ImageNet, the famed ImageNet dataset and have recently had some words of criticism\nthat the academic research ML community gives a little too much love still to the ImageNet\nor those kinds of benchmarks. Can you speak to the strengths and weaknesses of data sets\nused in machine learning research? - Actually, I don't know that I recall the specific instance\nwhere I was unhappy or criticizing ImageNet. I think ImageNet has been extremely valuable.\nIt was basically a benchmark that allowed the deep learning community to demonstrate\nthat deep neural nets actually work. - [Lex] Yes. - There's a massive value in that.\nSo I think ImageNet was useful, but basically, it's become a bit of an EMNIST at this point. So EMNIST is like little 28 by 28 grayscale digits.\nThat's kind of a joke data set that everyone just crushes. - There's still papers written on EMNIST though, right?\nLike strong papers? - [Andrej] Yeah. - Like papers that focus on like how do we learn with a small amount of data, that kinda stuff.\n- Yeah. Yeah, I could see that being helpful but not in mainline computer vision research anymore, of course. - I think the way I've heard you just somewhere,\nmaybe I'm just imagining things, but I think you said like ImageNet was a huge contribution to the community for a long time\nand now it's time to move past those kinds of- - Well, ImageNet has been crushed. I mean, the error rates are, yeah,\nwe're getting like 90% accuracy in 1000 classification way prediction\nand I've seen those images and this is like really high, that's really good.\nIf I remember correctly, the top five error rate is now like 1% or something. - Given your experience with a gigantic real-world data set,\nwould you like to see benchmarks move into certain directions that the research community uses? - Unfortunately, I don't think academics\ncurrently have the next ImageNet. We've obviously, I think we've crushed EMNIST. We've basically crushed ImageNet\nand there's no next-big benchmark that the entire community rallies behind and uses\nfor further development of these networks. - Oh, yeah. I wonder what it takes for a dataset to captivate the imagination of everybody.\nLike where they all get behind it. That could also need a leader, right?\n- [Andrej] Yeah. - Somebody with popularity. I mean that, yeah, why did ImageNet takeoff?\nIs it just the accident of history? - It was the right amount of difficult, it was the right amount of difficult and simple\nand interesting enough it was the right time for that kind of a data set.\n- Question from Reddit. What are your thoughts on the role that synthetic data\n"}
{"pod": "Lex Fridman Podcast", "input": "Data", "output": "and game engines will play in the future of neural net model development? - I think as neural nets converge to humans,\nthe value of simulation to neural nets will be similar to value of simulation to humans.\nSo people use simulation because they can learn something in that kind of a system and without having\nto actually experience it. - But you're referring to the simulation we do in our head? Isn't that what thinking is?\n- Oh no, sorry. Simulation, I mean like video games or other forms of simulation for various professionals.\n- Well, so let me push back on that 'cause maybe there's simulation that we do in our heads, like simulate if I do this, what do I think will happen?\n- [Andrej] Okay. That's like internal simulation. - Yeah, internal. Isn't that what we're doing? Assuming before we act. - Oh, yeah.\nBut that's independent from the use of simulation in the sense of computer games, or using simulation for training set creation, or something.\n- Is it independent or is it just loosely correlated? 'Cause isn't that useful to do counterfactual\nor edge case simulation to what happens if there's a nuclear war?\nWhat happens if there's, like those kinds of things? - Yeah. That's a different simulation from Unreal Engine.\nThat's how I interpreted the question. - Ah, so like simulation of the average case?\nWhat's Unreal Engine? What do you mean by Unreal Engine? So simulating a world.\n- [Andrej] Yeah. - The physics of that world, Why is that different? 'Cause you also can add behavior to that world\nand you could try all kinds of stuff, right? You could throw all kinds of weird things into it.\n- [Andrej] Yeah. - Unreal Engine is not just about simulating, I mean I guess it is about simulating the physics of the world,\nit's also doing something with that. - Yeah. The graphics, the physics, and the agents\nthat you put into the environment and stuff like that. Yeah. - I feel like you said that it's not that important,\nI guess, for the future of AI development. Is that correct, to interpret you that way? - Well, I think, humans use simulators\nand they find them useful and so computers will use simulators and find them useful. - Okay, so you're saying it's not,\nI don't use simulators very often. I play a video game every once in a while but I don't think I derive any wisdom\nabout my own existence from those video games. It's a momentary escape from reality\nversus a source of wisdom about reality. So I think that's a very polite way of saying\nsimulation is not that useful. - Yeah. Maybe not. I don't see it as a fundamental, really important part\nof training neural nets currently. But I think as neural nets become more and more powerful,\nI think you will need fewer examples to train additional behaviors. And simulation is, of course, there's a domain gap\nin a simulation that's not the real-world, it's slightly something different. But with a powerful enough neural net\nyou need the domain gap can be bigger I think because neural net will understand that even though it's not the real-world,\nit has all this high-level structure that I'm supposed to learn from. - So the neural net will actually, yeah,\nit will be able to leverage the synthetic data better? - [Andrej] Yes.\n- By closing the gap but understanding in which ways this is not real data? - [Andrej] Exactly.\n- Reddit do better questions next time. That was a question. No, I'm just kidding. All right, so is it possible, do you think,\nspeaking of EMNIST to construct neural nets and training processes that require very little data.\nSo we've been talking about huge data sets like the internet for training. - [Andrej] Yeah. - I mean one way to say that is like you said,\nthe querying itself is another level of training I guess and that requires a little data.\n- [Andrej] Yeah. - But do you see any value in doing research\nand going down the direction of can we use very little data to construct a knowledge base?\n- A hundred percent. I just think at some point you need a massive data set and then when you pre-train your massive neural net\nand get something that is like a GPT or something, then you're able to be very efficient\nat training any arbitrary new task. So a lot of these GPTs you can do tasks\nlike sentiment analysis, or translation, or so on, just by being prompted with very few examples. Here's the kind of thing I want you to do.\nLike here's an input sentence, here's the translation into German, input sentence, translation to German, input sentence, blank, and the neural net\nwill complete the translation to German just by looking at the example you've provided. And so that's an example of a very few shot learning\nin the activations of the neural net, instead of the weights of the neural net. And so I think basically just like humans,\nneural nets will become very data efficient at learning any other new task. But at some point, you need a massive data set\nto pre-train your network. - I do get that. And probably, we humans have something like that.\nDo we have something like that? Do we have a passive in the background,\nbackground model constructing thing that just runs all the time in a self-supervised way?\nWe're not conscious of it? - I think humans definitely, I mean obviously, we learn a lot during our lifespan,\nbut also we have a ton of hardware that helps us, the initialization coming from evolution.\nAnd so I think that's also a really big component. A lot of people in the field, I think they just talk about the amounts of like seconds\nthat a person has lived, pretending that this is a tabula rasa, a zero initialization of a neural net.\nAnd it's not. You can look at a lot of animals for example, zebras. Zebras get born, and they see, and they can run,\nthere's zero training data in their lifespan. They can just do that. So somehow, I have no idea how, evolution has found a way\nto encode these algorithms and these neural net initializations that are extremely good into ATCGs.\nAnd I have no idea how this works, but apparently, it's possible because here's proof by existence.\n- There's something magical about going from a single-cell to an organism that is born\nto the first few years of life. I like the idea that the reason we don't remember anything about the first few years of our life\nis that it's a really painful process. Like it's a very difficult challenging training process.\n- [Andrej] Yeah. - Like intellectually and maybe, yeah, I mean I don't, why don't we remember any of that?\nThat might be some crazy training going on and maybe that's the background model training\nthat is very painful. - [Andrej] Yeah. - And so it's best for the system once it's trained\nnot to remember how it was constructed. - I think it's just like the hardware for long-term memory is just not fully developed.\n- [Lex] Sure. - I feel like the first few years of infants, it's not actually like learning, it's brain maturing.\n- [Lex] Yeah. - We're born premature and there's a theory along those lines because of the birth canal\nand the swelling of the brain. And so we're born premature and then the first few years we're just, the brain's maturing\nand then there's some learning eventually. It's my current view on it. - What do you think, do you think neural nets\ncan have long-term memory? Like that approach is something like humans?\nDo you think there needs to be another meta-architecture on top of it to add something like a knowledge base\nthat learns facts about the world and all that kind of stuff? - Yes, but I don't know to what extent it will be explicitly constructed.\nIt might take on intuitive forms where you are telling the GPT, hey, you have a declarative memory bank\nto which you can store and retrieve data from and whenever you encounter some information that you find useful just save it to your memory bank.\nAnd here's an example of something you have retrieved and here's how you say it and here's how you load from it. You just say, load whatever, you teach it in text,\nin English and then it might learn to use a memory bank from that. - Oh, so the neural net is the architecture\nfor the background model, the base thing and then everything else is just on top of it. - It's not just a text, right?\nYou're giving it gadgets and gizmos. So you're teaching it some kind of a special language by which it can save arbitrary information\nand retrieve it at a later time. - [Lex] Yeah. - And you're telling about these special tokens and how to arrange them to use these interfaces.\nIt's like, hey, you can use a calculator, here's how you use it. Just do five three plus four one equals\nand when equals is there, a calculator will actually read out the answer and you don't have to calculate it yourself\nand you just tell it in English, this might actually work. - Do you think in that sense Gato is interesting,\nthe DeepMind system that it's not just new language but actually throws it all in the same pile,\nimages, actions, all that kind of stuff. That's basically what we're moving towards.\n- Yeah, I think so. So Gato is very much a kitchen sink of an approach to reinforcement learning in lots of different environments\nwith a single fixed transformer model. Right? I think it's a very early result in that realm.\nBut I think, yeah, it's along the lines of what I think things will eventually look like. - Right. So this is the early days of a system\nthat eventually will look like this, from a Rich Sutton perspective. - Yeah. I'm not a super huge fan of, I think all these interfaces\nthat like look very different. I would want everything to be normalized into the same API.\nSo for example, screen pixels, very same API instead of having like different world environments that have very different physics and joint configurations\nand appearances and whatever. And you're having some kind of special tokens for different games that you can plug.\nI'd rather just normalize everything to a single interface so it looks the same to the neural net if that makes sense.\n- So it's all gonna be pixel-based Pong in the end? - [Andrej] I think so.\n- Okay. Let me ask you about your own personal life.\n"}
{"pod": "Lex Fridman Podcast", "input": "Day in the life", "output": "A lot of people wanna know you're one of the most productive and brilliant people in the history of AI. What is a productive day in the life\nof Andrej Karpathy look like? What time do you wake up? 'Cause imagine some kind of dance between\nthe average productive day and a perfect productive day. So the perfect productive day is the thing we strive towards\nand the average is kind of what it converges to, given all the mistakes and human eventualities and so on.\n- [Andrej] Yep. - So what time did you wake up? Are you a morning person? - I'm not a morning person. I'm a night owl, for sure.\n- Is it stable or not? - It's semi-stable like eight or nine or something like that.\nDuring my PhD, it was even later, I used to go to sleep usually at 3:00 AM, I think the AM hours are precious\nand a very interesting time to work because everyone is asleep. At 8:00 AM or 7:00 AM the east coast is awake.\nSo there's already activity, there's already some text messages, whatever. There's stuff happening. You can go on some news website\nand there's stuff happening, it's distracting. At 3:00 AM everything is totally quiet and so you're not gonna be bothered\nand you have solid chunks of time to do work. So I like those periods, night owl by default.\nAnd then I think productive time, basically, what I like to do is you need to build\nsome momentum on the problem without too much distraction and you need to load your ram, your working memory\nwith that problem. And then you need to be obsessed with it when you're taking shower, when you're falling asleep,\nyou need to be obsessed with the problem and it's fully in your memory and you're ready to wake up and work on it right there. - So is this in a scale temporal scales of a single day\nor a couple of days a week, a month? - Yeah. So I can't talk about one day, basically, in isolation because it's a whole process.\nWhen I wanna get productive in the problem, I feel like I need a span of a few days where I can really get in on that problem\nand I don't wanna be interrupted and I'm going to just be completely obsessed with that problem. And that's where I do most of my good work, I would say.\n- You've done a bunch of cool, like little projects in a very short amount of time, very quickly, so that requires you just focusing on it.\n- Yeah, basically, I need to load my working memory with the problem and I need to be productive because there's always a huge fixed cost\nto approaching any problem. I was struggling with this for example at Tesla because I want to work on small side project, but okay,\nyou first need to figure out, oh, okay, I need to associate into my cluster, I need to bring up a VS Code editor so I can work on this.\nI ran into some stupid error because of some reason. You're not at a point where you can be just productive right away.\nYou are facing barriers. And so it's about really removing all of that barrier\nand you're able to go into the problem and you have the full problem loaded in your memory. - And somehow avoiding distractions of all different forms.\n- [Andrej] Yes. - Like news stories, emails, but also distractions\nfrom other interesting projects that you previously worked on or currently working on and so on. You just wanna really focus your mind.\n- Yeah. And I mean I can take some time off for distractions and in between, but I think it can't be too much.\nMost of your day is sort of spent on that problem. And then, I drink coffee, I have my morning routine,\nI look at some news, Twitter, Hacker News, Wall Street Journal, et cetera. It's great.\n- So basically, you wake up, you have some coffee, are you trying to get to work as quickly as possible? Do you take in this diet of what the hell's happening\nin the world first? - I do find it interesting to know about the world. I don't know that it's useful or good,\nbut it is part of my routine right now. So I do read through a bunch of news articles and I wanna be informed and I'm suspicious of it.\nI'm suspicious of the practice, but currently, that's where I am. - Oh, you mean suspicious about the positive effect?\n- [Andrej] Yeah. - Of that practice on your productivity and your well-being? - My well-being, psychologically, yeah.\n- And also on your ability to deeply understand the world because there's a bunch of sources of information\nyou're not really focused on deeply integrating it. - Yeah, it's a little bit distracting. Yeah. - In terms of a perfectly productive day\nfor how long of a stretch of time in one session do you try to work and focus on a thing?\nIs it a couple hours, is it one hour, is it 30 minutes? Is it 10 minutes? - I can probably go a small few hours\nand then I need some breaks in between for food and stuff and yeah.\nBut I think, it's still really hard to accumulate hours. I was using a tracker that told me exactly how much time\nI spent coding any one day. And even on a very productive day, I still spent only six or eight hours.\n- [Lex] Yeah. - And it's just because there's so much padding, commute, talking to people, food, et cetera.\nThere's a cost of life just living and sustaining and homeostasis and just maintaining yourself\nas a human is very high. - And there seems to be a desire within the human mind\nto participate in society that creates that padding. - [Andrej] Yeah. - 'Cause the most productive days I've ever had\nis just completely from start to finish is tuning out everything. - [Andrej] Yep. - And just sitting there and then you could do more\nthan six in eight hours. - Yep. - Is there some wisdom about what gives you strength to do tough days of long focus?\n- Yeah, just whenever I get obsessed about a problem, something just needs to work. Something just needs to exist. - It needs to exist.\nSo you're able to deal with bugs and programming issues and technical issues and design decisions\nthat turn out to be the wrong ones. You're able to think through all of that given that you want a thing to exist. - Yeah, it needs to exist.\nAnd then I think to me also a big factor is, are other humans are going to appreciate it? Are they going to like it?\nThat's a big part of my motivation. If I'm helping humans and they seem happy, they say nice things, they Tweet about it or whatever,\nthat gives me pleasure because I'm doing something useful. - So you do see yourself sharing it with the world?\nWith GitHub, with the blog post or through videos? - Yeah, I was thinking about it like, suppose I did all these things but did not share them.\nI don't think I would have the same amount of motivation that I can build up. - You enjoy the feeling of other people gaining value\nand happiness from the stuff you've created. - [Andrej] Yeah. - What about diet?\nI saw you played with intermittent fasting. Do you fast? Does that help? - I play with everything.\n- With the things you played, what's been most beneficial to your ability to mentally focus on a thing\nand just mental productivity and happiness? You still fast? - Yeah.\nI still fast but I do intermittent fasting but really what it means at the end of the day is I skip breakfast. - [Lex] Yeah. - So I do 18/6 roughly by default\nwhen I'm in my steady state, if I'm traveling or doing something else I will break the rules. But in my steady state, I do 18/6.\nSo I eat only from 12:00 to 6:00. Not a hard rule and I break it often, but that's my default. And then, yeah, I've done a bunch of random experiments\nfor the most part right now where I've been for the last year and a half I wanna say is I'm plant-based or plant-forward.\nI heard plant-forward, it sounds better. - [Lex] What does that mean exactly? - I don't actually know what the difference is, but it sounds better in my mind. But it just means I prefer plant-based food and-\n- Raw or cooked. - I prefer cooked and plant-based. - So plant-based, forgive me,\nI don't actually know how wide the category of plant entails. - Well, plant-based just means\nthat you're not militant about it and you can flex and you just prefer to eat plants and you're not making,\nyou're not trying to influence other people. And you come to someone's house party and they serve you a steak that they're really proud of,\nyou will eat it. - Yes. Right. You're not judgmental. That's beautiful. I mean, I'm the flip side of that,\nbut I'm very sort of flexible. Have you tried doing one meal a day? - I have accidentally but not consistently,\nbut I've accidentally had that. I don't like it. I think it makes me feel not good. It's too much of a hit.\n- [Lex] Yeah. - And so currently I have about two meals a day, 12:00 and 6:00, probably. - I do that nonstop.\nI'm doing it now. I do one meal a day. - [Andrej] Okay. - It's interesting. It's an interesting feeling.\nHave you ever fasted longer than a day? - Yeah, I've done a bunch of water fasts. 'Cause I was curious what happens. - [Lex] What happens, anything interesting?\n- Yeah, I would say so. I mean, what's interesting is that you're hungry for two days and then starting day three or so,\nyou're not hungry. It's such a weird feeling because you haven't eaten in a few days and you're not hungry.\n- Yeah, isn't that weird? - [Andrej] It's really weird. - One of the many weird things about human biology. - [Andrej] Yeah.\n- It figures something out, it finds another source of energy or something like that. Or relaxes the system.\nI don't know how it works. - Yeah, the body is like, \"You're hungry, you're hungry.\" And then it just gives up. It's like, \"Okay, I guess we're fasting now.\" There's nothing.\nAnd then it's just focuses on trying to make you not hungry and not feel the damage of that and trying to give you\nsome space to figure out the food situation. - So are you still to this day most productive at night?\n- I would say I am, but it is really hard to maintain my PhD schedule.\nEspecially, when I was say working at Tesla and so on. It's a non-starter. But even now, people want to meet for various events.\nSociety lives in a certain period of time. - [Lex] Yeah. - And you have to work with that. - It's hard to do a social thing\nand then after that return and do work. - Yeah. It's just really hard.\n- That's why I try, when I do social thing, I try not to do too much drinking so I can return and continue doing work.\n- [Andrej] Yeah. - But at Tesla is there conversions, not Tesla,\nbut any company, is there a convergence to always a schedule or is that how humans behave when they collaborate?\nI need to learn about this? - [Andrej] Yeah. - Do they try to keep a consistent schedule where you're all awake at the same time? - I mean, I do try to create a routine\nand I try to create a steady state in which I'm comfortable in. So I have a morning routine, I have a day routine.\nI try to keep things to a steady state and things are predictable and then your body just sticks to that.\nAnd if you try to stress that a little too much, it will create, when you're traveling and you're dealing with jet lag, you're not able to really ascend to where you need to go.\n- Yeah. Yeah. That's weird too about us humans with the habits and stuff. What are your thoughts on work-life balance\nthroughout a human lifetime? So Tesla in part was known for pushing people\nto their limits, in terms of what they're able to do, in terms of what they're trying to do,\nin terms of how much they work, all that kind of stuff. - Yeah, I mean I will say Tesla gets a little too much bad rep for this\nbecause what's happening is Tesla, it's a bursty environment. So I would say the baseline,\nmy only point of reference is Google where I've interned three times and I saw what it's like inside Google and DeepMind,\nI would say the baseline is higher than that. But then there's a punctual equilibrium where once in a while there's a fire\nand people work really hard and so it's spiky and bursty and then all the stories get collected-\n- [Lex] Above the bursts. Yeah. - And then it gives the appearance of total insanity. But actually, it's just a bit more intense environment\nand there are fires and sprints and so I think definitely though I would say\nit's a more intense environment than something you would get at Google. - But in your personal, forget all of that, just in your own personal life,\nwhat do you think about the happiness of a human being? A brilliant person like yourself.\nabout finding a balance between work and life or is such a thing not a good thought experiment?\n- Yeah, I think balance is good but I also love to have sprints that are out of distribution\nand that's when I think I've been pretty creative as well.\n- So sprints out of distribution means that most of the time you have a quote-unquote balance.\n- I have balance most of the time and I like being obsessed with something once in a while. - Once in a while is what, once a week,\nonce a month, once a year? - Yeah. Probably I'd like say once a month or something. Yeah. - And that's when we get and you get Git-Repo for market.\n- Yeah. That's when you're like really care about a problem, it must exist. This will be awesome. You're obsessed with it and now you can't just do it\non that day, you need to pay the fixed cost of getting into the groove. - [Lex] Yeah. - And then you need to stay there for a while\nand then society will come and they will try to mess with you and they will try to distract you. - [Lex] Yeah. - Yeah.\nThe worst thing is a person who's like, \"I just need five minutes of your time.\" - [Lex] Yeah. - The cost of that is not five minutes.\n- [Lex] Yes. - And society needs to change how it thinks about just five minutes of your time.\n- Right. It's never just one minute, just 30. Just a quick thing. - [Andrej] What's the big deal?\nWhy are you being, so? - Yeah, no. What's your computer setup?\nWhat's like the perfect- Are you somebody that's flexible to no matter what laptop, four screens?\n- [Andrej] Yeah. - Or do you prefer a certain setup that you're most productive with?\n- I guess the one that I'm familiar with is one large screen, 27-inch and my laptop on the side.\n- What operating system? - I do MaX, that's my primary. - For all tasks? - I would say OS X.\nBut when you're working on deep-learning, everything is Linux, you're SSH into a cluster and you're working remotely. - But what about the actual development?\nLike using the IDE? - Yeah. You would use, I think a good way is you just run VS Code,\nmy favorite editor right now on your Mac. But you are actually, you have a remote folder through SSH.\nSo the actual files that you're manipulating are on a cluster somewhere else. - So what's the best IDE, VS Code?\n"}
{"pod": "Lex Fridman Podcast", "input": "Best IDE", "output": "What else do people use? So I use Emax, still. - That's old school.\n- It may be cool, I don't know if it's maximum productivity. So what do you recommend in terms of editors?\nYou worked with a lot of software engineers, editors for Python, C++, machine learning applications.\n- I think the current answer is VS Code, currently, I believe that's the best IDE.\nIt's got a huge amount of extensions. It has GitHub Copilot integration,\nwhich I think is very valuable. - What do you think about the Copilot integration? I got to talk a bunch with Guido van Rossum\nwho's the creator of Python and he loves Copilot, he programs a lot with it.\n- [Andrej] Yeah. - Do you? - Yeah, I use Copilot. I love it. And it's free for me but I would pay for it.\nYeah. I think it's very good. And the utility that I found with it was, I would say there's a learning curve\nand you need to figure out when it's helpful and when to pay attention to its outputs and when it's not going to be helpful\nwhere you should not pay attention to it. Because if you're just reading its suggestions all the time, it's not a good way of interacting with it.\nBut I think I was able to sort of mold myself to it. I find it's very helpful, number one, in copy-paste\nand replace some parts. So when the pattern is clear, it's really good at completing the pattern.\nAnd number two, sometimes it suggests APIs that I'm not aware of. So it tells you about something that you didn't know.\n- And that's an opportunity to discover a new thing. - It's an opportunity to. So I would never take Copilot code as given.\nI almost always copy-paste into a Google search and you see what this function is doing and then you're like, \"Oh, that's actually exactly\nwhat I need.\" Thank you, Copilot. So you learn something. - So it's in part of search engine, in part maybe getting the exact syntax correctly\nthat once you see it, it's that MPR thing. Once you see it, you know it's correct.\n- [Andrej] Yes, exactly. Exactly. - You, yourself can struggle. - [Andrej] You can verify. - You can verify efficiently but you can't generate efficiently.\n- And Copilot really, I mean it's autopilot for programming. Right? And currently is doing the link following\nwhich is the simple copy-paste and sometimes suggest, but over time it's going to become more and more autonomous.\nAnd so the same thing will play out in not just coding but actually across many, many different things probably.\n- But coding is an important one, right? - [Andrej] Very. - Like writing programs. - [Andrej] Yeah. - How do you see the future of that developing\nthe program synthesis, like being able to write programs that are more and more complicated? 'Cause right now it's human-supervised in interesting ways.\n- [Andrej] Yes. - It feels like the transition will be very painful. - My mental model for it is the same thing will happen\nas with the autopilot. So currently, he's doing lane following, he is doing some simple stuff, and eventually,\nwe'll be doing autonomy and people will have to intervene less and less. And those could be like testing mechanisms.\nIf it writes a function and that function looks pretty damn correct. But how do you know it's correct\n'cause you're like getting lazier and lazier as a programmer, your ability to, 'cause like little bugs\nbut I guess it won't make little mistakes. - No it will, Copilot will make off by one subtle bug.\nIt has done that to me. - But do you think future systems will or is it really the off by one\nis actually a fundamental challenge of programming. - In that case it wasn't fundamental and I think things can improve but yeah,\nI think humans have to supervise. I am nervous about people not supervising what comes out and what happens to, for example,\nthe proliferation of bugs in all of our systems. I'm nervous about that but I think there will probably be\nsome other Copilots for bug finding and stuff like that, at some point. 'Cause there'll be a lot more automation for-\n- Oh man, a Copilot that generates a compiler,\none that does a linter. - [Andrej] Yes. - One that does like a type checker. - Yeah.\nIt's a committee of a GPT sort of like- - And then there'll be like a manager for the committee. - [Andrej] Yeah.\n- And then there'll be somebody that says, a new version of this is needed. We need to regenerate it. - Yeah. There were 10 GPTs that were forwarded\nand gave 50 suggestions. Another one looked at it and picked a few that they like, a bug one looked at it\nand it was like, it's probably a bug, they got re-ranked by some other thing and then a final ensemble GPT comes in\nand is like, okay, given everything you guys have told me, this is probably the next token. - You know the feeling is the number of programmers\nin the world has been growing and growing very quickly. - [Andrej] Yeah. - Do you think it's possible that it'll actually level out and drop to a very low number in this kind of world?\n'Cause then you'll be doing Software 2.0 programming and you'll be doing this generation\nof Copilot-type systems programming. But you won't be doing the old school\nSoftware 1.0 programming. - I don't currently think that they're just going to replace human programmers.\nI'm so hesitant saying stuff like this, right? - Yeah. Because this is gonna be replayed in five years and no,\nit's going to show that like this is where we thought, because I agree with you but I think we might be very surprised, right?\nWhat's your sense of where we stand with language models? Does it feel like the beginning, or the middle, or the end? - The beginning, a hundred percent.\nI think the big question in my mind is for sure GPT will be able to program quite well, competently and so on. - [Lex] Yeah.\n- How do you steer the system? You still have to provide some guidance to what you actually are looking for. And so how do you steer it and how do you say,\nhow do you talk to it? How do you audit it and verify that what it's done is correct\nand how do you work with this? And it's as much not just an AI problem but a UI, UX problem.\n- [Lex] Yeah. - So beautiful fertile ground for so much interesting work\nfor VS Code++ where you're not just, it's not just human programming anymore. It's amazing. - Yeah.\nSo you're interacting with the system so not just one prompt but it's iterative prompting. - [Andrej] Yeah.\n- You're trying to figure out, having a conversation with the system. - [Andrej] Yeah. - That actually, I mean to me that's super exciting to have a conversation with the program I'm writing.\n- Yeah. Yeah. Maybe at some point you're just conversing with it. It's like, okay, here's what I wanna do, actually this variable.\nMaybe it's not even that low-level as a variable, but. - You can also imagine like can you translate\nthis to C++ and back to Python and back to? - Yeah, that already kind of exists in some way. - No, but just like doing it as part\nof the program experience. Like I think I'd like to write this function in C++\nor you just keep changing from different programs 'cause of the different syntax,\nmaybe I want to convert this into a functional language. - [Andrej] Yeah. - And so you get to become multilingual as a programmer\nand dance back and forth efficiently. - Yeah. I mean I think the UI, UX of it though is still very hard to think through.\n- [Lex] Yeah. - Because it's not just about writing code on a page. You have an entire developer environment, you have a bunch of hardware on it,\nyou have some environmental variables, you have some scripts that are running in a Chrome job. There's a lot going on to working with computers\nand how do these systems set up environment flags, and work across multiple machines,\nand set up screen sessions, and automate different processes. like how all that works and is auditable by humans and so on\nis like massive question at the moment. - You've built arxiv-sanity. What is arxiv and what is the future\n"}
{"pod": "Lex Fridman Podcast", "input": "arXiv", "output": "of academic research publishing that you would like to see? - So arxiv is this pre-print server.\nSo if you have a paper you can submit it for publication to journals or conferences and then wait six months\nand then maybe get a decision pass or fail, or you can just upload it to arxiv and then people can Tweet about it three minutes later.\nAnd then everyone sees it, everyone reads it, and everyone can profit from it in their own little ways. - And you can cite it and it has an official look to it.\nIt feels like a publication process. - [Andrej] Yeah. - It feels different than if you just put in a blog post.\n- Oh, yeah. Yeah. I mean it's a paper and usually the bar is higher for something that you would expect on arxiv\nas opposed to something you would see in a blog post. - Well, the culture created the bar 'cause you could probably post\na pretty crappy paper or an arxiv. - [Andrej] Yes. - So what's that make you feel like, what's that make you feel about peer review?\nSo rigorous peer review by two, three experts versus the peer review of the community right\nas it's written? - Yeah. Basically, I think the community is very well able to peer-review things very quickly on Twitter.\nAnd I think maybe it just has to do something with AI machine learning field specifically though, I feel like things are more easily auditable\nand the verification is easier potentially than the verification somewhere else.\nSo it's like, you can think of these scientific publications as little block-chains where everyone's building on each other's work\nand citing each other and you sort of have AI, which is this much faster and loose blockchain,\nbut then you have, and any one individual entry is very cheap to make.\nAnd then you have other fields where maybe that model doesn't make as much sense. And so I think in AI at least\nthings are pretty easily verifiable. And so that's why when people upload papers, they have a really good idea and so on.\nPeople can try it out the next day and they can be the final arbiter of whether it works or not on their problem.\nAnd the whole thing just moves significantly faster. So I feel like academia still has a place, sorry, this conference, journal process still has a place,\nbut it's sort of it lags behind I think, and it's a bit more maybe higher quality process.\nBut it's not the place where you will discover cutting-edge work anymore. - [Lex] Yeah. - It used to be the case when I was starting my PhD\nthat you go to conferences and journals and you discuss all the latest research. Now when you go to a conference or a journal,\nno one discusses anything that's there because it's already like three generations ago, irrelevant. - Yes.\nWhich makes me sad about like DeepMind for example, where they still publish in nature and these big prestigious, I mean there's still value,\nI suppose to the prestige that comes with these big venues. - [Andrej] Yeah. - But the result is that they'll announce\nsome breakthrough performance and it'll take like a year to actually publish the details.\nI mean, and those details, if they were published immediately would inspire the community to move in certain directions, would they?\n- Yeah. It would speed up the rest of the community, but I don't know to what extent that's part of their objective function also.\n- That's true. So it's not just the prestige. A little bit of the delay is part. - Yeah, they certainly, DeepMind specifically,\nhas been working in the regime of having slightly higher quality basically process\nand latency and publishing those papers that way. - Another question from Reddit.\nDo you or have you suffered from imposter syndrome, being the director of AI Tesla, being this person\nwhen you're at Stanford where the world looks at you as the expert in AI to teach the world\nabout machine learning. - When I was leaving Tesla after five years, I spent a ton of time in meeting rooms\nand I would read papers. In the beginning, when I joined Tesla, I was writing code and then I was writing less and less code, and I was reading code and then I was reading\nless and less code. And so this is just a natural progression that happens I think. And definitely, I would say near the tail end,\nthat's when it starts to hit you a bit more. That you're supposed to be an expert but actually, the source of truth is the code\nthat people are writing, the GitHub and the actual code itself. And you're not as familiar with that as you used to be.\nAnd so I would say maybe there's some insecurity there. - Yeah, that's actually pretty profound, that a lot of the insecurity has to do\nwith not writing the code in the computer science space 'cause that is the truth. That right there.\n- The code is the source of truth. The papers and everything else, it's a high-level summary. I don't, yeah, just a high-level summary,\nbut at the end of the day you have to read code. It's impossible to translate all that code into actual paper form.\nSo when things come out, especially when they have a source code available, that's my favorite place to go. - So like I said, you're one of the greatest teachers\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for beginners", "output": "of machine learning AI ever from CS231n to today,\nwhat advice would you give to beginners interested in getting into machine learning? - Beginners are often focused on what to do\nand I think the focus should be more how much you do. So I am a believer on the high-level, in this 10,000 hours concept where you just have to\njust pick the things where you can spend time and you care about and you're interested in. You literally have to put in 10,000 hours of work.\nIt doesn't even matter as much where you put it, you'll iterate and you'll improve and you'll waste some time.\nI dunno if there's a better way. You need to put in 10,000 hours. But I think it's actually really nice 'cause I feel like there's some sense of determinism\nabout being an expert at a thing if you spend 10,000 hours. you can literally pick an arbitrary thing\nand I think if you spend 10,000 hours of deliberate effort and work, you actually will become an expert at it.\nAnd so I think it's like a nice thought. And so basically I would focus more on\nare you spending 10,000 hours? That's what I would focus on. - And then thinking about what kind of mechanisms maximize\nyour likelihood of getting to 10,000 hours. - [Andrej] Yes, exactly. - Which for us silly humans means probably forming\na daily habit of every single day actually doing thing. - Whatever helps you. So I do think to a large extent\nit's a psychological problem for yourself. - [Lex] Yeah. - One other thing that I think is helpful for the psychology of it,\nis many times people compare themselves to others in the area, I think this very harmful. Only compare yourself to you from some time ago.\nLike say a year ago, are you better than you a year ago? This is the only way to think.\nAnd I think then you can see your progress and it's very motivating. - That's so interesting. That focus on the quantity of hours.\n'Cause I think a lot of people in the beginner stage but actually throughout get paralyzed by the choice.\n- [Andrej] Yeah. - Like which one do I pick this path or this path? - [Andrej] Yeah. - They'll literally get paralyzed by which IDE to use?\n- Well, they're worried, yeah, they'll worried about all these things. But the thing is, you will waste time doing something wrong.\n- [Lex] Yes. - You will eventually figure out it's not right. You will accumulate scar tissue and next time you'll grow stronger\nbecause next time you'll have the scar tissue, and next time you'll learn from it. And now next time you come to a similar situation\nyou'll be like, oh, I messed up. I've spent a lot of time working on things that never materialized into anything\nand I have all that scar tissue and I have some intuitions about what was useful, what wasn't useful, how things turned out.\nSo all those mistakes were not dead work. So I just think they should just focus on working.\nWhat have you done, what have you done last week? - That's a good question actually\nto ask for a lot of things, not just machine learning. It's a good way to cut the, I forgot the term we use,\nbut the fluff, the blubber, whatever the inefficiencies in life.\nWhat do you love about teaching? You seem to find yourself often in the, drawn to teaching.\nYou're very good at it but you're also drawn to it. - Yeah, I mean I don't think I love teaching. I love happy humans and happy humans like when I teach.\n- Yes. - I wouldn't say I hate teaching, I tolerate teaching. - [Lex] Yes. - But it's not like the act of teaching that I like,\nit's that I have something, I'm actually okay at it.\n- [Lex] Yes. - I'm okay at teaching and people appreciate it a lot. - [Lex] Yeah. - And so I'm just happy to try to be helpful\nand teaching itself is not like the most, I mean it can be really annoying, frustrating.\nI was working on a bunch of lectures just now. I was reminded back to my days of 231n just how much work it is to create some of these materials\nand make them good. The amount of iteration and thought and you go down blind alleys and just how much you change it.\nSo creating something good in terms of educational value is really hard and it's not fun.\n- It's difficult. So people should definitely go watch your new stuff you put out. There are lectures where you're actually building the thing,\nlike you said, \"The code is truth.\" So discussing back-propagation by building it,\nby looking through and just the whole thing. - [Andrej] Yeah. - So how difficult is that to prepare for? I think that's a really powerful way to teach.\nDid you have to prepare for that or are you just live thinking through it? - I will typically do like say three takes\nand then I take the better take. So I do multiple takes and I take some of the better takes and then I just build out a lecture that way.\nSometimes I have to delete 30 minutes of content. - [Lex] Yeah. - Because it just went down the alley that I didn't like too much. So there's about a bunch of iteration\nand it probably takes me somewhere around 10 hours to create one hour of content. - To get one hour.\nIt's interesting. I mean is it difficult to go back to the basics? Do you draw a lot of wisdom from going back to the basics?\n- Yeah, going back to backropagation, loss functions, where they come from. And one thing I like about teaching a lot honestly is\nit definitely strengthens your understanding. So it's not a purely altruistic activity, it's a way to learn.\nIf you have to explain something to someone, you realize you have gaps in knowledge.\nAnd so I even surprised myself in those lectures like, well, the result will obviously look like this\nand then the result doesn't look like it. And I'm like, okay, I thought I understood this. - Yeah.\nWell, that's why it's really cool, they literally code, you run it in the notebook and it gives you a result\nand you're like, oh, wow. - [Andrej] Yes. - And like actual numbers, actual input, actual code. - Yeah.\nIt's not mathematical symbols, et cetera. The source of truth is the code. It's not slides, it's just like let's build it.\n- It's beautiful. You're a rare human in that sense. What advice would you give to researchers\ntrying to develop and publish an idea that have a big impact in the world of AI?\nSo maybe undergrads, maybe early-graduate students. - Yeah.\nI mean I would say they definitely have to be a little bit more strategic than I had to be as a PhD student because of the way AI is evolving,\nit's going the way of physics. Where in physics you used to be able to do experiments on your bench-top and everything was great\nand you could make progress and now you have to work in like LHC or like CERN and so AI\nis going in that direction as well. So there's certain kinds of things that's just not possible to do on the bench-top anymore.\nAnd I think that didn't used to be the case at the time. - Do you still think that there's like GAN type papers\nto be written where like very simple idea. - [Andrej] Yes. - That requires just one computer\nto illustrate a simple example? - I mean one example that's been very influential recently is diffusion models, diffusion models are amazing.\nDiffusion models are six years old. For the longest time, people were ignoring them as far as I can tell.\nAnd they're an amazing generative model, especially in images and so stable diffusion and so on,\nit's all diffusion-based. Diffusion is new, it was not there and it came from, well, it came from Google but a researcher\ncould have come up with it. In fact, some of the first, actually no, those came from Google as well.\nBut a researcher could come up with that in an academic institution. - Yeah. What do you find most fascinating about diffusion models?\nSo from the societal impact of the technical architecture. - What I like about diffusion is it works so well.\n- Is that surprising to you? The amount of the variety, almost the novelty of the synthetic data it's generating?\n- Yeah, so the stable diffusion images are incredible. It's the speed of improvement in generating images\nhas been insane. We went very quickly from generating tiny digits to tiny faces and it all looked messed up.\nAnd now we have stable diffusion and that happened very quickly. There's a lot that academia can still contribute. For example, FlashAttention is a very efficient kernel\nfor running the attention operation inside the transformer that came from academic environment.\nIt's a very clever way to structure the kernel, that's the calculation. So it doesn't materialize the attention matrix.\nAnd so, I think there's still like lots of things to contribute but you have to be just more strategic. - Do you think neural networks could be made to reason?\n- Yes. - Do you think they already reason? - Yes. - [Lex] What's your definition of reasoning?\n- Information processing. - So in the way that humans think through a problem\nand come up with novel ideas, it feels like a reasoning.\n- Yeah. - So the novelty, I don't wanna say but auto-distribution ideas, you think it's possible?\n- Yes. And I think we're seeing that already in the current neural nets. You're able to remix the training set information\ninto true generalization in some sense. - That doesn't appear- - It doesn't appear verbatim in the training set.\nYou're doing something interesting algorithmically, you're manipulating some symbols and you're coming up with some correct unique answer\nin a new setting. - What would illustrate to you, holy shit,\nthis thing is definitely thinking? - To me thinking or reasoning is just information processing and generalization.\nAnd I think the neural nets already do that today. - So being able to perceive the world or perceive the, whatever the inputs are\nand to make predictions based on that or actions based on that's reasoning?\n- Yeah. You're giving correct answers in novel settings by manipulating information.\nYou've learned the correct algorithm, you're not doing just some kind of a lookup table and nearest neighbor search. Something like that.\n"}
{"pod": "Lex Fridman Podcast", "input": "Artificial general intelligence", "output": "- Let me ask you about AGI. What are some moonshot ideas you think might make significant progress towards AGI\nand maybe another way is, what are the big blockers that we're missing now? - So basically, I am fairly bullish on our ability\nto build AGIs, basically automated systems that we can interact with that are very human-like\nand we can interact with them in a digital realm or a physical realm. Currently, it seems most of the models\nthat do these magical tasks are in a text realm.\nI think, as I mentioned, I'm suspicious that text realm is not enough to actually build full understanding\nof the world. I do actually think you need to go into pixels and understand the physical world and how it works.\nSo I do think that we need to extend these models to consume images and videos and train on a lot more data\nthat is multimodal in that way. - Do you think you need to touch the world to understand it also? - Well, that's the big open question\nI would say in my mind, is if you also require the embodiment and the ability to interact with the world,\nrun experiments and have a data of that form, then you need to go to Optimus or something like that.\n- [Lex] Yeah. - And so I would say Optimus in some way is like a hedge\nin AGI because it seems to me that it's possible that just having data from the internet is not enough.\nIf that is the case, then Optimus may lead to AGI. Because Optimus would, to me,\nthere's nothing beyond Optimus. You have like this humanoid form factor that can actually do stuff in the world. You can have millions of them\ninteracting with humans and so on. And if that doesn't give a rise to AGI at some point,\nI'm not sure what will. So from a completeness perspective, I think that's a really good platform\nbut it's a much more harder platform because you are dealing with atoms and you need to actually build these things\nand integrate them into society. So I think that path takes longer but it's much more certain.\nAnd then there's a path of the internet and just training these compression models effectively on trying to compress all the internet.\nAnd that might also give these agents as well. - Compress the internet but also interact with the internet.\n- [Andrej] Yeah. - So it's not obvious to me. In fact, I suspect you can reach AGI\nwithout ever entering the physical world, which is a little bit more concerning\nbecause that results in it happening faster.\nSo it just feels like we're in boiling water. We won't know as it's happening.\nI would like to, I'm not afraid of AGI, I'm excited about it. There's always concerns\nbut I would like to know when it happens. - [Andrej] Yeah. - And have like hints about when it happens,\nlike a year from now it will happen, that kind of thing. - [Andrej] Yeah. - I just feel like in the digital realm it just might happen.\n- Yeah. I think all we have available to us because no one has built AGI again, so all we have available to us is,\nis there enough fertile ground on the periphery? I would say, yes. And we have the progress so far, which has been very rapid and there are next steps\nthat are available. And so I would say, yeah, it's quite likely that we'll be interacting with digital entities.\n- How will you know that somebody has built AGI? - I think it's going to be a slow incremental transition.\nIt's going to be product-based and focused. It's going to be GitHub Copilot going better. And then GPTs helping you write and then these oracles\nthat you can go to with mathematical problems. I think we're on a verge of being able to ask very complex questions in chemistry, physics,\nmath of these oracles and have them complete solutions. - So AGI to use primarily focused on intelligence\nso consciousness doesn't enter into it.\n- So in my mind, consciousness is not a special thing you will figure out and bolt on. I think it's an emergent phenomenon of a large enough\nand complex enough generative model sort of. So if you have a complex enough world model\nthat understands the world, then it also understands its predicament in the world as being a language model,\nwhich to me is a form of consciousness or self-awareness. - So in order to understand the world deeply\nyou probably have to integrate yourself into the world. - [Andrej] Yeah. - And in order to interact with humans and other living beings,\nconsciousness is a very useful tool. - Yeah. I think consciousness is like a modeling insight.\n- Modeling insight. - Yeah. You have a powerful enough model of understanding the world that you actually understand\nthat you are an entity in it. - Yeah. But there's also this, perhaps just a narrative we tell ourselves, it feels like something\nto experience the world, the hard problem of consciousness. - [Andrej] Yeah. - But that could be just a narrative that we tell ourselves.\n- Yeah. I don't think we'll, yeah, I think it will emerge. I think it's going to be something very boring. We'll be talking to these digital AIs,\nthey will claim they're conscious, they will appear conscious, they will do all the things that you would expect of other humans\nand it's going to just be a stalemate. - I think there will be a lot of actual fascinating ethical questions,\nlike supreme court level questions of whether you're allowed to turn off a conscious AI,\nif you're allowed to build a conscious AI, maybe there would have to be the same kind of debates\nthat you have around, sorry to bring up a political topic, but abortion, which is the deeper question with abortion\nis what is life? And the deep question with AI is also,\nwhat is life and what is conscious? - [Andrej] Right. - And I think that'll be very fascinating\nto bring up, it might become illegal to build systems that are capable of such level of intelligence\nthat consciousness would emerge and therefore the capacity to suffer would emerge. And a system that says, no, please don't kill me.\n- Well, that's what the LaMDA chatbot already told this Google engineer, right?\nIt was talking about not wanting to die or so on. - So that might become illegal to do that.\n- [Andrej] Right. - 'Cause otherwise, you might have a lot of creatures\nthat don't want to die and they will- - [Andrej] You can just spawn infinity of them on a cluster.\n- And then that might lead to horrible consequences. 'Cause then there might be a lot of people that secretly love murder\nand they'll start practicing murder on those systems. I mean there's just, to me all of this stuff just brings\na beautiful mirror to the human condition and human nature and we get to explore it. - [Andrej] Yes.\n- And that's what like the best of the supreme court of all the different debates we have about ideas\nof what it means to be human, we get to those deep questions that we've been asking throughout human history.\nThere's always been the other in human history. We're the good guys and that's the bad guys\nand we're going to throughout human history, let's murder the bad guys. And the same will probably happen with robots.\nIt'll be the other at first. And then we'll get to ask questions, that what does it mean to be alive? What does it mean to be conscious?\n- Yep. And I think there's some canary in the coal mines even with what we have today. And for example, there's these waifus\nthat you can work with and some people are trying to, this company's going to shut down, but this person really loved their waifu\nand is trying to like port it somewhere else. And it's not possible. And I think definitely people will have feelings\ntowards these systems because in some sense they are like a mirror of humanity\nbecause they are like a big average of humanity. - [Lex] Yeah. - In a way that it's trained.\n- But that average, we can actually watch. It's nice to be able to interact\nwith the big average of humanity. - [Andrej] Yeah. - And do a search query on it. - Yeah. Yeah.\nIt's very fascinating. And we can also of course, also shape it. It's not just a pure average. We can mess with the training data,\nwe can mess with the objective, we can fine-tune them in various ways. So we have some impact on what those systems look like.\n- If you want to achieve AGI and you could have a conversation with her\nand ask her, talk about anything, maybe ask her a question. What kind of stuff would you ask?\n- I would've some practical questions in my mind like do I or my loved ones really have to die?\nWhat can we do about that? - Do you think it will answer clearly or would it answer poetically?\n- I would expect it to give solutions. I would expect it to be like, well, I've read all of these textbooks and I know all these things\nthat you've produced and it seems to me like here are the experiments that I think it would be useful to run next. And here are some gene therapies\nthat I think would be helpful, and here are the kinds of experiments that you should run. - Okay, let's go with this thought experiment.\nOkay. Imagine that mortality is actually\na prerequisite for happiness. So if we become immortal,\nwe'll actually become deeply unhappy and the model is able to know that. So what is it supposed to tell you?\nA stupid human about it? Yes, you can become a mortal but you'll become deeply unhappy. If the AGI system is trying to empathize with you human,\nwhat is it supposed to tell you. That yes, you don't have to die but you're really not gonna like it?\nIs it gonna be deeply honest? There's an \"Interstellar\", what is it the AI says like humans want 90% honesty.\nSo you have to pick how honest do I want to answer these practical questions? - Yeah. I love AI \"Interstellar\" by the way.\nI think it's like such a sidekick to the entire story but at the same time, it's really interesting.\n- It's kind of limited in certain ways, right? - Yeah, it's limited and I think that's totally fine by the way.\nI think it's fine and plausible to have a limited and imperfect AGIs.\n- Is that a feature almost? - As an example, it has a fixed amount of compute on its physical body.\nAnd it might just be that even though you can have a super amazing mega brain, super-intelligent AI,\nyou also can have less intelligent AI that you can deploy in a power-efficient way.\nAnd then they're not perfect, they might make mistakes. - No, I meant more like say you had infinite compute\nand it's still good to make mistakes sometimes. In order to integrate yourself. Like, what is it?\nGoing back to \"Goodwill Hunting\", Robin Williams character says the human imperfections, that's good stuff, right?\nWe don't want perfect, we want flaws in part to form connections with each other.\n'Cause it feels like something you can attach your feelings to, the flaws.\nIn that same way you want an AI that's flawed. I don't know. I feel like perfection is cold.\n- [Andrej] Okay, yeah. - But that's not AGI. But see AGI would need to be intelligent enough\nto give answers to humans that humans don't understand. And I think perfect is something humans can't understand\nbecause even science doesn't give perfect answers. There's always gaps and mysteries and I don't know,\nI don't know if humans want perfect. - Yeah, I could imagine just having a conversation\nwith this oracle entity as you'd imagine them and yeah, maybe it can tell you about,\nbased on my analysis of human condition, you might not want this and here are some of the things that might-\n- But every dumb human will say, yeah, yeah, yeah, yeah, trust me, give me the truth, I can handle it.\n- But that's the beauty, like people can choose. - But then, it's the old marshmallow test\nwith the kids and so on, I feel like too many people can't handle the truth, probably including myself.\nDeep truth to the human condition. I don't know if I can handle it. What if there's some dark stuff?\nWhat if we are an alien science experiment and it realizes that. What if it hacked, I mean?\n- I mean, this is \"The Matrix\" all over again. - \"The Matrix\", I don't know, what would I talk about?\nI don't even, yeah, probably I will go with the safer scientific questions at first\nthat have nothing to do with my own personal life. - [Andrej] Yeah. - Immortality just like about physics and so on.\n- [Andrej] Yeah. - To build up see where it's at or maybe see if it has a sense of humor.\nThat's another question. Presumably in order to, if it understands humans deeply,\nwould it able to generate humor.\n- Yeah. I think that's actually a wonderful benchmark almost, like is it able, I think that's a really good point, basically.\n- [Lex] To make you laugh. - Yeah. If it's able to be a very effective standup comedian that is doing something very interesting computationally.\nI think being funny is extremely hard. - Yeah, because it's hard in a way like a touring test,\nthe original intent of the touring test is hard because you have to convince humans and that's why comedians talk about this,\nlike this is deeply honest. 'Cause if people can't help but laugh and if they don't laugh that means you're not funny,\nif they laugh, it's funny. - Yeah. And you're showing, you need a lot of knowledge to create humor about like you mentioned\nhuman condition and so on. And then you need to be clever with it. - You mentioned a few movies, you Tweeted, \"Movies that I've seen five-plus times\n"}
{"pod": "Lex Fridman Podcast", "input": "Movies", "output": "but am ready and willing to keep watching: 'Interstellar', 'Gladiator', 'Contact', 'Goodwill Hunting',\n'The Matrix', 'Lord of the Rings', all three, 'Avatar', 'Fifth Element',\" and so on, it goes on, \"'Terminator 2'.\"\n\"Mean Girls\" I'm not gonna ask about that one. - \"Mean Girls\" is great.\n- What are some that jump out to you in your memory that you love and why?\nYou mentioned \"The Matrix\" as a computer person, why do you love \"The Matrix\"?\n- There's so many properties that make it beautiful and interesting. So there's all these philosophical questions but then there's also AGIs, and there's simulation,\nand it's cool, and there's the black. - [Lex] The look of it, the feel of it.\n- Yeah. The look of it, the feel of it, the action, the bullet time. It was just like innovating in so many ways.\n- And then \"Goodwill Hunting\". Why do you like that one? - Yeah, I really like this tortured genius character\nwho's grappling with whether or not he has any responsibility or what to do\nwith this gift that he was given or how to think about the whole thing and- - But there's also a dance between the genius\nand the personal, like what it means to love another human being. - Yeah.\nThere's a lot of themes there. It's just a beautiful movie. - And then the fatherly figure, the mentor and the psychiatrist.\n- It really messes with you. There's some movies that just like really mess with you\non a deep level. - Do you relate to that movie at all? - No. - It's not your fault Andrej, as I said.\n\"Lord of the Rings\", that's self-explanatory. \"Terminator 2\", which is interesting,\nyou rewatch that a lot. Is that better than Terminator one? You don't like Arnold as he comes back?\n- I do like Terminator one as well. I like \"Terminator 2\" a little bit more. But in terms of its surface properties.\n- Do you think Skynet is at all a possibility? - Yes. - Like the actual autonomous weapon system kind of thing?\nDo you worry about that stuff? So AI being used for war? - I a hundred percent worry about it.\nAnd so the, I mean, some of these fears of AGIs and how this will plan out, I mean these will be\nvery powerful entities probably at some point. And so for a long time, there are going to be tools in the hands of humans.\nPeople talk about alignment of AGIs and how to make, the problem is even humans are not aligned.\nSo how this will be used and what this is gonna look like is, yeah, it's troubling.\n- Do you think it'll happen slowly enough that we'll be able to as a human civilization\nthink through the problems? - Yes, that's my hope, is that it happens slowly enough and in an open enough way where a lot of people can see\nand participate in it. Just to figure out how to deal with this transition, I think, which is gonna be interesting.\n- I draw a lot of inspiration from nuclear weapons 'cause I sure thought it would be fucked\nonce they develop nuclear weapons. But it's almost like when the systems are not so dangerous\nthey destroy human civilization. We deploy them and learn the lessons and then we quickly,\nif it's too dangerous we quickly, quickly, we might still deploy it but you very quickly learn\nnot to use them. And so there'll be like this balance achieved, humans are very clever as a species. It's interesting, we exploit the resources as much as we can\nbut we avoid destroying ourselves it seems like. - Yeah. Well, I dunno about that actually.\n- I hope it continues. - I mean I'm definitely like concerned about nuclear weapons and so on,\nnot just as a result of the recent conflict, even before that. That's probably like my number one concern for humanity.\n- So if humanity destroys itself or destroys 90% of people\nthat would be because of nukes? - Yeah, I think so. And it's not even about full destruction,\nto me, it's bad enough if we reset society, that would be terrible. That would be really bad. And I can't believe we're so close to it.\n- [Lex] Yeah. - It's like so crazy to me. - It feels like we might be a few Tweets away from something like that. - Yep.\nBasically, it's extremely unnerving and has been for me for a long time. - It seems unstable that world leaders\njust having a bad mood can take one step\ntowards a bad direction and then it escalates. - Yeah. - And because of a collection of bad moods,\nit can escalate without being able to stop. - Yeah.\nIt's a huge amount of power. And then also with the proliferation and basically, I don't actually really see,\nI don't actually know what the good outcomes are here, so I'm definitely worried about that a lot. And then AGI is not currently there\nbut I think at some point it will more and more become something like it. The danger with AGI even is that\nI think it's even slightly worse in the sense that there are good outcomes of AGI\nand then the bad outcomes are an epsilon way, like a tiny run away. And so I think capitalism and humanity, and so on\nwill drive for the positive ways of using that technology. But then if bad outcomes are just like a tiny,\nlike flip a minus sign away, that's a really bad position to be in. - A tiny perturbation of the system\nresults in the destruction of the human species. - [Andrej] Yeah. - It's a weird line to walk. - Yeah, I think in general what's really weird\nabout the dynamics of humanity and this explosion we talked about is just the insane coupling afforded by technology.\n- [Lex] Yeah. - And just the instability of the whole dynamical system. I think it doesn't look good, honestly.\n- Yes. That explosion could be destructive or constructive and the probabilities are non-zero in both ends of it.\n- I'm gonna have to, I do feel like I have to try to be optimistic and so on and I think even in this case I still am predominantly optimistic but there's definitely-\n"}
{"pod": "Lex Fridman Podcast", "input": "Future of human civilization", "output": "- Me too. Do you think we'll become a multi-planetary species? - Probably, yes.\nBut I don't know if it's dominant feature of future humanity. There might be some people on some planets and so on\nbut I'm not sure if it's like, yeah, if it's like a major player in our culture and so on.\n- We still have to solve the drivers of self-destruction here on earth. So just having a backup on Mars\nis not gonna solve the problem. - So by the way, I love the backup on Mars. I think that's amazing. We should absolutely do that.\n- [Lex] Yes. - And I'm so thankful. - Would you go to Mars? - Personally, no, I do like earth quite a lot.\n- Okay. I'll go to Mars. I'll go for you. I'll Tweet at you from there. - Maybe eventually I would, once it's safe enough.\nBut I don't actually know if it's on my lifetime scale unless I can extend it by a lot.\nI do think that for example, a lot of people might disappear into virtual realities and stuff like that and I think that could be the major thrust\nof the cultural development of humanity if it survives. So it might not be, it's just really hard to work\nin physical realm and go out there and I think ultimately all your experiences are in your brain.\n- [Lex] Yeah. - And so it's much easier to disappear into digital realm and I think people will find them more compelling, easier,\nsafer, more interesting. - So you're a little bit captivated by virtual reality, by the possible worlds, whether it's the metaverse\nor some other manifestation of that? - [Andrej] Yeah. - Yeah. It's really interesting.\nI'm interested, just talking a lot to Carmack, where's the thing that's currently preventing that?\n- Yeah, I mean to be clear, I think what's interesting about the future is it's not that, I feel like\nthe variance in the human condition grows. That's the primary thing that's changing. It's not as much the mean of the distribution,\nit's like the variance of it. So there will probably be people on Mars and there will be people in VR, and there will people here on earth.\nIt's just like there will be so many more ways of being. And so feel like, I see it as like a spreading out\nof a human experience. - There's something about the internet that allows you to discover those little groups and you gravitate to, something about your biology\nlikes that kind of world and you find each other. - Yeah. And we'll have trans-humanists and then we'll have the Amish and everything is just gonna coexist.\n- Yeah. The cool thing about it 'cause I've interacted with a bunch of internet communities is they don't know about each other.\nLike you can have a very happy existence just having a very close-knit community and not knowing about each other.\nI mean even you even sense this, just having traveled to Ukraine, they don't know so many things about America.\n- [Andrej] Yeah. - When you travel across the world I think you experience this too. There are certain cultures that are like,\nthey have their own thing going on, they don't. And so you can see that happening more and more and more\nand more in the future. We have little communities. - Yeah. Yeah. I think so. That seems to be how it's going right now.\nAnd I don't see that trend really reversing. I think people are diverse and they're able to choose their own path in existence and I celebrate that.\nAnd so- - Will you spend some, much time in the metaverse, in the virtual reality? Or which community are you,\nare you the physicalist, the physical reality enjoyer\nor do you see drawing a lot of pleasure and fulfillment in the digital world?\n- Yeah, I think, well currently, the virtual reality is not that compelling. - [Lex] Yes. - I do think it can improve a lot\nbut I don't really know to what extent. Maybe there's actually even more exotic things you can think about with neural links or stuff like that.\nSo currently, I kind of see myself as mostly a team, human person, I love nature.\n- [Lex] Yeah. - I love harmony, I love people, I love humanity. I love emotions of humanity and I just want to be\nin this solar punk little utopia. That's my happy place. - [Lex] Yes. - My happy place is people I love,\nthinking about cool problems, surrounded by a lush, beautiful dynamic nature. - [Lex] Yeah. - And secretly high-tech in places that count.\n- Places that count. So you use technology to empower that love for other humans and nature.\n- Yeah, I think a technology used very sparingly. I don't love when it gets in the way of humanity\nin many ways. I like just people being humans in a way, we slightly evolved and prefer I think\njust by default. - People kept asking me 'cause they know you love reading. Are there particular books that you enjoyed\n"}
{"pod": "Lex Fridman Podcast", "input": "Book recommendations", "output": "that had an impact on you for silly or for profound reasons that you would recommend?\nYou mentioned \"The Vital Question\". - Many, of course. I think in biology as an example, \"The Vital Question\" is a good one.\nAnything by Nick Lane really, \"Life Ascending\" I would say is a bit more potentially representative\nas like a summary of a lot of the things he's been talking about. I was very impacted by \"The Selfish Gene\".\nI thought that was a really good book, it helped me understand altruism as an example and where it comes from. And just realizing that the selection\nand the levels of genes was a huge insight for me at the time and it cleared up a lot of things for me. - What do you think about the idea\nthat ideas are the organisms, the memes? - Yeah. Love it. A hundred percent.\n- Are you able to walk around with that notion for a while? That there's an evolutionary kind of process\nwith ideas as well? - There absolutely is. There's memes just like genes and they compete and they live in our brains.\nIt's beautiful. - Are we silly humans thinking that we are the organisms? Is it possible that the primary organisms are the ideas?\n- Yeah, I would say like the ideas kind of live in the software of our civilization in the minds\nand so on. We think as humans that the hardware is the fundamental thing. I human is a hardware entity.\n- [Andrej] Yeah. - But it could be the software, right? - Yeah. Yeah.\nI would say there needs to be some grounding at some point to a physical reality. - Yeah, but if we clone an Andrej,\nthe software is a thing that makes that thing special. Right?\n- Yeah. I guess you're right. - But then cloning might be exceptionally difficult. There might be a deep integration between the software and the hardware\nin ways we don't quite yet understand. - Well, from the altruism point of view, what makes me special is more the gang of genes\nthat are riding in my chromosomes I suppose. Right? They're the replicating unit I suppose-\n- No, but that's just the compute, the thing that makes you special, sure. Well, the reality is what makes you special\nis your ability to survive based on the software that runs on the hardware that was built by the genes.\nSo the software is the thing that makes you survive. Not the hardware or- - It's a little bit of both. It's just like a second layer.\nIt's a new second layer that hasn't been there before the brain. They both coexist. - But there's also layers of the software.\nI mean it's an abstraction on top of abstractions.\nOkay, \"Selfish Gene\". - So \"Selfish Gene\", Nick Lane. I would say sometimes books are not sufficient.\nI like to reach for textbooks sometimes. I feel like books are for too much\nof a general consumption sometime and they're too high up in the level of abstraction and it's not good enough.\n- [Lex] Yeah. - So I like textbooks, I like \"The Cell\". I think \"The Cell\" was pretty cool.\nThat's why also I like the writing of Nick Lane is because he's pretty willing to step one level down\nand he doesn't, yeah, he's willing to go there but he's also willing to be throughout the stack.\nSo he'll go down to a lot of detail but then he will come back up and I think he has a, yeah, basically, I really appreciate that.\n- That's why I love college, early college, even high school, just textbooks on the basics of computer science, of mathematics,\nof biology, of chemistry. - [Andrej] Yes. - Those are, they condense down, it's sufficient in general\nthat you can understand both the philosophy and the details but also you get homework problems\nand you get to play with it as much as you would if you were in programming stuff. - Yeah.\nAnd then I'm also suspicious of textbooks honestly because as an example in deep-learning there's no amazing textbooks and the field is changing very quickly.\nI imagine the same is true in say synthetic biology and so on, these books like \"The Cell\" are kind of outdated.\nThey're still high-level, like what is the actual real source of truth? It's people in wet labs working with cells.\n- Yeah. - Sequencing genomes and, yeah, actually working with it.\nAnd I don't have that much exposure to that or what that looks like. So I still don't fully, I'm reading through the cell\nand it's kind of interesting, and I'm learning but it's still not sufficient I would say in terms of understanding. - Well, it's a clean summarization\nof the mainstream narrative. - [Andrej] Yeah. - But you have to learn that before you break out.\n- [Andrej] Yeah, - Towards the cutting edge. - Yeah. But what is the actual process of working with these cells and growing them and incubating them\nand it's like a massive cooking recipe. So making sure your cell slows and proliferate and then you're sequencing them, running experiments\nand just how that works, I think is the source of truth of at the end of the day what's really useful\nin terms of creating therapies and so on. - Yeah. I wonder what in the future AI textbooks will be\n'cause you know there's \"Artificial Intelligence: A Modern Approach\". I actually haven't read, if it's come out, the recent version, there's been a recent edition.\nI also saw there's a science of deep learning book. I'm waiting for textbooks that are worth recommending, worth reading.\n- [Andrej] Yeah. - It's tricky 'cause it's like papers and code, code, code. - Honestly, I find papers are quite good.\nI especially like the appendix of any paper as well. It's like the most detail you can have.\n- It doesn't have to be cohesive, connected to anything else. You just described me a very specific way you saw the particular thing.\nYeah. - Yeah, many times papers can be actually quite readable, not always, but sometimes the introduction and the abstract is readable even for someone outside of the field.\nThis is not always true and sometimes I think, unfortunately, scientists use complex terms even when it's not necessary.\nI think that's harmful. I think there's no reason for that. - And papers sometimes are longer than they need to be\nin the parts that don't matter. - [Andrej] Yeah. - The appendix should be long but then the papers itself, look at Einstein,\nmake it simple. - Yeah. But certainly, I've come across papers I would say, say like synthetic biology or something that I thought were quite readable for the abstract\nand the introduction and then you're reading the rest of it and you don't fully understand but you kind of are getting a gist and I think it's cool.\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "- You give advice to folks interested in machine learning and research but in general life advice\nto a young person, high school, early college about how to have a career they can be proud of\nor a life they can be proud of? - Yeah, I think I'm very hesitant to give general advice.\nI think it's really hard. I've mentioned, some of the stuff I've mentioned is fairly general, I think. Like focus on just the amount of work you're spending\non like a thing. Compare yourself only to yourself, not to others. - [Lex] That's good. - [Andrej] I think those are fairly general.\n- How do you pick the thing? - You just have like a deep interest in something\nor try to find the argmax over the things that you're interested in. - Argmax at that moment and stick with it.\n- [Lex] Yeah. - How do you not get distracted and switch to another thing? - You can if you like.\n- Well, if you do an argmax repeatedly every week, every month. - [Andrej] Yeah, it doesn't converge. - It's a problem.\n- Yeah. You can like low-pass filter yourself in terms of what has consistently been true for you.\nBut yeah, I definitely see how it can be hard but I would say you're going to work the hardest on the thing that you care about the most.\nSo low pass filter yourself and really introspect in your past, what are the things that gave you energy\nand what are the things that took energy away from you? Concrete examples. And usually, from those concrete examples,\nsometimes patterns can merge. I like it when things look like this when I'm in these positions. - So that's not necessarily the field\nbut the kind of stuff you're doing in a particular field. So for you, it seems like you were energized by implementing stuff, building actual things.\n- Yeah, being low-level, learning and then also communicating so that others can go through\nthe same realizations and shortening that gap. Because I usually have to do way too much work to understand a thing and then I'm like, okay,\nthis is actually like, okay, I think I get it and like why was it so much work? It should have been much less work.\nAnd that gives me a lot of frustration and that's why I sometimes go teach. - So aside from the teaching you're doing now,\n"}
{"pod": "Lex Fridman Podcast", "input": "Future of machine learning", "output": "putting out videos, aside from a potential \"Godfather II\"\nwould the AGI at Tesla and beyond, what does the future for Andrej Karpathy hold, have you figured that out yet or no?\nI mean as you see through the fog of war that is all of our future, do you start seeing silhouettes\nof what that possible future could look like? - The consistent thing I've been always interested in,\nfor me at least is Ai. And that's probably what I'm spending the rest of my life on\nbecause I just care about it a lot. And I actually care about many other problems as well. Like say aging, which I basically view as disease\nand I care about that as well. But I don't think it's a good idea to go after it, specifically.\nI don't actually think that humans will be able to come up with the answer. I think the correct thing to do is to ignore those problems\nand you solve AI and then use that to solve everything else. And I think there's a chance that this will work. I think it's a very high chance\nand that's the way I'm betting at least. - So when you think about AI,\nare you interested in all kinds of applications? - [Andrej] Yes. - All kinds of domains.\nAnd any domain you focus on will allow you to get insights to the big problem of AGI? - Yeah, for me it's the ultimate meta-problem.\nI don't wanna work on any one specific problem, there's too many problems. So how can you work on all problems simultaneously? You solve the meta-problem, which to me is just intelligence\nand how do you automate it? - Is there cool small projects like arxiv-sanity\nand so on that you're thinking about that ML world can anticipate.\n- There's always some fun side projects. - [Lex] Yeah. - arxiv-sanity is one, yeah, basically, there's way too many archive papers,\nhow can I organize it and recommend papers and so on. I transcribed all of your podcasts.\n- What did you learn from that experience, from transcribing the process of like\nyou consuming audiobooks and podcasts and so on? - [Andrej] Yeah. - And here's a process that achieves\ncloser to human-level performance on annotation. - Yeah, well, I definitely was surprised that transcription with OpenAI Whisper\nwas working so well compared to what I'm familiar with, from Siri and a few other systems I guess, it works so well.\nAnd that's what gave me some energy to try it out. And I thought it could be fun to run on podcasts.\nIt's not obvious to me why Whisper is so much better compared to anything else because I feel like\nthere should be a lot of incentive for a lot of companies to produce transcription systems and that they've done so over a long time. Whisper is not a super exotic model, it's a transformer,\nit takes mel spectrograms and it just outputs tokens of text. It's not crazy.\nThe model and everything has been around for a long time. I'm not actually a hundred percent sure why this came about. - Yeah, it's not obvious to me either.\nIt makes me feel like I'm missing something for the middle. - [Andrej] I'm missing something. - Yeah, because there is a huge, even Google and so on,\nYouTube transcription. - [Andrej] Yeah. - Yeah. It's unclear. But some of it is also integrating into a bigger system.\n- [Andrej] Yeah. - So the user interface, how it's deployed and all that kind of stuff. Maybe running it as an independent thing is much easier,\nlike an order of magnitude easier than deploying it to a large integrated system like YouTube transcription, or anything,\nlike meetings, like Zoom has transcription, that's kind of crappy.\nBut creating interface where it detects the different individual speakers, it's able to display it in compelling ways,\nrun it real-time, all that kind of stuff. Maybe that's difficult. That's the only explanation I have\nbecause I'm currently paying quite a bit for human transcription, human caption.\n- [Andrej] Right. - Annotation. And like it seems like there's a huge incentive to automate that.\n- [Andrej] Yeah. - It's very confusing. - And I think, I mean, I dunno if you looked at some of the Whisper transcripts, but they're quite good. - They're good.\nAnd especially in tricky cases. - [Andrej] Yeah. - I've seen Whisper's performance on super tricky cases\nand it does incredibly well. So I don't know, a podcast is pretty simple. It's like high-quality audio\nand you're speaking usually pretty clearly. - [Andrej] Yeah. - And so I don't know,\nI don't know what OpenAIs plans are either. - But yeah, there's always fun projects basically.\nAnd stable diffusion also is opening up a huge amount of experimentation I would say in the visual realm and generating images, and videos, and movies ultimately.\n- [Lex] Yeah, videos now. - And so that's going to be pretty crazy. That's going to almost certainly work\nand it's going to be really interesting when the cost of content creation is going to fall to zero. You used to need a painter for a few months to paint a thing\nand now it's going to be speak to your phone to get your video. - So if Hollywood will start using that to generate scenes\nwhich completely opens up. Yeah. So you can make a movie like \"Avatar\" eventually\nfor under a million dollars. - Much less maybe just by talking to your phone. I mean, I know it sounds kind of crazy.\n- And then there'd be some voting mechanism, would there be a show on Netflix that's generated completely automatedly?\nSemi-automatedly? - Yeah, potentially. Yeah. And what does it look like also when you can just generate it on demand\nand there's infinity of it? - Yeah.\nOh, man. All the synthetic content. I mean it's humbling because we treat ourselves as special\nfor being able to generate art, and ideas, and all that kind of stuff. If that can be done in an automated way by Ai.\n- Yeah. I think it's fascinating to me how these, the predictions of AI and what it's going to look like and what it's going to be capable of\nare completely inverted and wrong. And sci-fi of fifties and sixties, were just totally not right.\nThey imagine AI is like super calculating, theorem provers, and we're getting things that can talk to you about emotions.\nThey can do art, it's just weird. - Are you excited about that future? just AI's, like hybrid systems, heterogeneous systems\nof humans and AIs talking about emotions, Netflix and chill with an AI system.\nOr the Netflix thing you watch is also generated by AI? - I think it's going to be interesting for sure\nand I think I'm cautiously optimistic but it's not obvious. - Well, the sad thing is your brain and mine developed\nin a time before Twitter, before the internet.\nSo I wonder people that are born inside of it might have a different experience. Like I, and maybe you will still resist it\nand the people born now will not. - Well, I do feel like humans are extremely malleable. - [Lex] Yeah.\n- And you're probably right. - What is the meaning of life, Andrej?\n"}
{"pod": "Lex Fridman Podcast", "input": "Meaning of life", "output": "We talked about the universe having a conversation with us humans\nor with the systems we create to try to answer. For the creator of the universe to notice us,\nwe're trying to create systems that are loud enough to answer back.\n- I dunno if that's the meaning of life. That's like meaning of life for some people. The first level answer I would say is anyone can choose their own meaning of life\nbecause we are a conscious entity and it's beautiful, number one. But I do think that a deeper meaning of life\nif someone is interested is along the lines of like, what the hell is all this? And like why?\nAnd if you look into fundamental physics and the quantum field theory and the standard model, they're very complicated.\nAnd there's this 19 free parameters of our universe\nand what's going on with all this stuff and why is it here? And can I hack it? Can I work with it?\nIs there a message for me? Am I supposed to create a message? And so I think there's some fundamental answers there\nbut I think there's actually even like, you can't actually really make dent in those without more time.\nAnd so to me also there's a big question around just getting more time, honestly. Yeah.\nThat's what I think about quite a bit as well. - So kind of the ultimate, or at least first way to sneak up to the why question\nis to try to escape the system, the universe?\n- [Andrej] Yeah. - And then for that you backtrack and say, okay, for that, that's gonna take a very long time.\nSo the why question boils down from an engineering perspective to how do we extend? - Yeah.\nI think that's the question number one, practically speaking, because you're not gonna calculate the answer to the deeper questions in the time you have.\n- And that could be extending your own lifetime or extending just the lifetime of human civilization.\n- Of whoever wants to, many people might not want that. - [Lex] Yeah. - But I think people who do want that,\nI think it's probably possible, and I don't know that people fully realize this.\nI feel like people think of death as an inevitability but at the end of the day, this is a physical system.\nSome things go wrong. It makes sense why things like this happen, evolutionarily speaking,\nand there's most certainly interventions that mitigate it. - That would be interesting if death is eventually looked at\nas a fascinating thing that used to happen to humans. - I don't think it's unlikely.\nI think it's likely. - And it's up to our imagination to try to predict\nwhat the world without death looks like. - [Andrej] Yeah. - It's hard to, I think the values will completely change.\n- Could be, I don't really buy all these ideas that, oh, without death, there's no meaning, there's nothingness.\nI don't intuitively buy all those arguments. I think there's plenty of meaning, plenty of things to learn.\nThey're interesting, exciting. I want to know, I want to calculate, I want to improve the condition of all the humans\nand organisms that are alive. - Yeah. The way we find meaning might change. There is a lot of humans, probably including myself,\nthat finds meaning in the finiteness of things, but that doesn't mean that's the only source of meaning.\n- Yeah. I do think many people will go with that, which I think is great. I love the idea that people\ncan just choose their own adventure. You are born as a conscious, free entity by default.\nI'd like to think. - [Lex] Yeah. - And you have your unalienable rights for life.\n- In the pursuit of happiness? I don't know if you that, and the nature, the landscape of happiness.\n- And you can choose your own adventure, mostly. And that's not fully true but. - I'm still am pretty sure I'm an NPC,\nbut an NPC can't know it's an NPC.\nThere could be different degrees and levels of consciousness. I don't think there's a more beautiful way to end it.\nAndrej, you're an incredible person. I'm really honored you would talk with me, everything you've done for the machine learning world,\nfor the AI world to just inspire people, to educate millions of people.\nIt's been great and I can't wait to see what you do next. It's been an honor, man. Thank you so much for talking today.\n- Awesome. Thank you. Thanks for listening to this conversation with Andrej Karpathy, to support this podcast\nplease check out our sponsors in the description. And now, let me leave you some words from Samuel Karlin,\n\"The purpose of models is not to fit the data but to sharpen the questions.\"\nThanks for listening and hope to see you next time.\n"}

{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "the following is a conversation with demus hasabis ceo and co-founder of deepmind\na company that has published and builds some of the most incredible artificial intelligence systems in the history of\ncomputing including alfred zero that learned all by itself to play the game of gold\nbetter than any human in the world and alpha fold two that solved protein\nfolding both tasks considered nearly impossible for a very long time\ndemus is widely considered to be one of the most brilliant and impactful humans in the history of artificial\nintelligence and science and engineering in general this was truly an honor and a pleasure\nfor me to finally sit down with him for this conversation and i'm sure we will talk many times again in the future\nthis is the lex friedman podcast to support it please check out our sponsors in the description and now dear friends\nhere's demis hassabis let's start with a bit of a personal question\n"}
{"pod": "Lex Fridman Podcast", "input": "Turing Test", "output": "am i an ai program you wrote to interview people until i get good enough to interview you\nwell i'll be impressed if if you were i'd be impressed by myself if you were i don't think we're quite up to that yet\nbut uh maybe you're from the future lex if you did would you tell me is that is that a good thing to tell a language\nmodel that's tasked with interviewing that it is in fact um ai maybe we're in\na kind of meta turing test uh probably probably it would be a good idea not to tell you so it doesn't change your\nbehavior right this is a kind of heisenberg uncertainty principle situation if i told you you behave differently yeah maybe that's what's\nhappening with us of course this is a benchmark from the future where they replay 2022 as a year before ais were\ngood enough yet and now we want to see is it going to pass exactly\nif i was such a program would you be able to tell do you think so to the touring test question\nyou've talked about the benchmark for solving intelligence\nwhat would be the impressive thing you've talked about winning a nobel prize in a system winning a nobel prize\nbut i still return to the touring test as a compelling test the spirit of the touring test is a compelling test\nyeah the turing test of course it's been unbelievably influential and turing's one of my all-time heroes but i think if\nyou look back at the 1950 papers original paper and read the original you'll see i don't think he meant it to\nbe a rigorous formal test i think it was more like a thought experiment almost a bit of philosophy he was writing if you\nlook at the style of the paper and you can see he didn't specify it very rigorously so for example he didn't\nspecify the knowledge that the expert or judge would have um not you know how\nmuch time would they have to investigate this so these important parameters if you were gonna make it uh a true sort of\nformal test um and you know some by some measures people claimed the turing test passed\nseveral you know a decade ago i remember someone claiming that with a with a kind of very bog standard normal uh\nlogic model um because they pretended it was a it was a kid so the the judges\nthought that the machine you know was was a was a child so um that would be\nvery different from an expert ai person uh interrogating a machine and knowing how it was built and so on so i think um\nyou know we should probably move away from that as a formal test and move more towards a general test where we test the\nai capabilities on a range of tasks and see if it reaches human level or above\nperformance on maybe thousands perhaps even millions of tasks eventually and cover the entire sort of cognitive space\nso i think for its time it was an amazing thought experiment and also 1950s obviously it\nwas barely the dawn of the computer age so of course he only thought about text and now um we have a lot more different\ninputs so yeah maybe the better thing to test is the generalizability so across\nmultiple tasks but i think it's also possible as as systems like god show\nthat eventually that might map right back to language so you might be able to demonstrate your ability to generalize\nacross tasks by then communicating your ability to generalize across tasks which is kind of\nwhat we do through conversation anyway when we jump around ultimately what's in there in that\nconversation is not just you moving around knowledge it's you moving around like these\nentirely different modalities of understanding that ultimately map to\nyour ability to to uh operate successfully in all these domains which you can think of as tasks\nyeah i think certainly we as humans use language as our main generalization communication tool so i think we end up\nthinking in language and expressing our solutions in language um so it's going to be very powerful\nuh uh mode in which to uh explain you know the system to explain what it's\ndoing um but i don't think it's the only uh uh modality that matters so i think\nthere's gonna be a lot of you know there's there's a lot of different ways to express uh capabilities uh other than\njust language yeah visual robotics body language\num yeah action is the interactive aspect of all that that's all part of it but what's interesting with gato is that\nit's a it's it's it's sort of pushing prediction to the maximum in terms of like you know mapping arbitrary\nsequences to other sequences and sort of just predicting what's going to happen next so prediction seems to be\nfundamental to intelligence and what you're predicting doesn't so much matter yeah it seems\nlike you can generalize that quite well so obviously language models predict the next word um gato predicts potentially\nany uh action or any token uh and it's just the beginning really it's our most general agent one could call it so far\nbut um you know that itself can be scaled up massively more than we've done so far obviously we're in the in the\nmiddle of doing that but the big part of solving agi is creating benchmarks that\nhelp us get closer and closer sort of creating benchmarks that test the journalizability and it's just still\ninteresting that this fella alan turing was one of the first and probably still\none of the only people that was trying maybe philosophically but was trying to formulate a benchmark that could be\nfollowed it is even though it's it's fuzzy it's still sufficiently rigorous to where you can\nrun that test and i still think something like the touring test will at the end of the day\nbe the thing that truly impresses other humans so that you can have a close friend\nwho's an ai system for that friend to be a good friend they're going to have to be able to\nuh play starcraft and they're going to have to do all of these tasks um get you a beer so the\nrobotics tasks uh play games with you use language\nhumor all those kinds of things but that ultimately can boil down to language it feels like not in terms of the ai\ncommunity but in terms of the actual impact of general intelligence on the world it feels like language will be the\nplace where it truly shines i think so because it's such an important kind of input output for us i think you're right\ni think the turing test that what the the kind of the the philosophy behind it which is the idea of can can a machine\nmimic the behaviors of a human and men and i would say wider than just language\nand text then you know in terms of actions and everything else creativity all these\nthings then um if it can sort of match or exceed human cognitive capabilities\nthen i think we have a you know true intelligence so i thought from that perspective you're right i think he did\nformulate the right uh kind of uh setup i just i think there'll be a kind of\nhumor in the ai systems of the future looking back to this conversation\nand thinking about the touring test and uh also thinking about\nby that time they would know which year they were finally able to sort of cross the threshold of human level\nintelligence and think how funny it is that we humans were still\nconfused about this whole problem that has been long ago been solved well hello to the ai agents of the\nfuture anyway so going back to your to your journey when did you fall in love with programming first\n"}
{"pod": "Lex Fridman Podcast", "input": "Video games", "output": "well it's pretty uh pretty young age actually so um you know i started off uh actually games\nwas my first love so starting to play chess when i was around four years old and then um it was actually with\nwinnings from a chess competition that i managed to buy my first chess computer when i was about eight years old it was\na zx spectrum which was hugely popular in the uk at the time and uh it's amazing machine because i\nthink it trained a whole generation of programmers in the uk because it was so accessible you know you literally\nswitched it on and there was the basic prompt and you could just get going and um my parents didn't really know\nanything about computers so but because it was my money from a chess competition i could i could say i i wanted to buy it\nuh and then you know i just went to bookstores got books on programming and\num started typing in you know the programming code and and then of course um once you start doing that you start\nadjusting it and then making your own games and that's when i fell in love with computers and realized that they were a very magical device um in a way i\nkind of i would have been able to explain this at the time but i felt that they were sort of almost a magical extension of your mind i always had this\nfeeling and i've always loved this about computers that you can set them off doing something some task for you you\ncan go to sleep come back the next day and it's solved um you know that feels magical to me so\ni mean all machines do that to some extent they all enhance our natural capabilities obviously cars make us\nallow us to move faster than we can run but this was a machine to extend the mind\nand and then of course ai is the ultimate expression of what a machine may be able to do or learn so\nvery naturally for me that thought extended into into ai quite quickly remember the the programming language\nthat was first started special to the machine no it was just the base it was just i think it was just\nbasic uh on the zx spectrum i don't know what specific form it was and then later on i got a commodore amiga which uh\nwas a fantastic machine no you're just showing off so yeah well lots of my friends had atari st's and i i managed\nto get amigas it was a bit more powerful and uh and that was incredible and used to do um programming in assembler and\nand uh also amos basic this this specific form of basic it was incredible actually as well all my coding skills\nand when did you fall in love with ai so when did you first start to gain an understanding that you\ncan not just write programs that do some mathematical operations for you while you sleep but something that's\na keen to bringing an entity to life sort of\na thing that can figure out something more complicated than uh than a simple mathematical operation\nyeah so there was a few stages for me all while i was very young so first of all as i was trying to improve at\nplaying chess i was captaining various england junior chess teams and at the time when i was about you know maybe 10 11 years old i was gonna become a\nprofessional chess player that was my first thought um that dream was there sure she\ntried to get to the highest level yeah so i was um you know i got to when i was about 12 years old i got to master stand\nand i was second highest rated player in the world to judith polgar who obviously ended up being an amazing chess player\nand uh world women's champion and when i was trying to improve at chess where you\nknow what you do is you obviously first of all you're trying to improve your own thinking processes so that leads you to\nthinking about thinking how is your brain coming up with these ideas why is it making mistakes how can you how can\nyou improve that thought process but the second thing is that you it was just the beginning this was like in the in the\nearly 80s mid 80s of chess computers if you remember they were physical boards like the one we have in front of us and\nyou pressed down the you know the squares and i think kasparov had a branded version of it that i i i got and\num you were you know used to they're not as strong as they are today but they were they were pretty strong and you\nused to practice against them um to try and improve your openings and other things and so i remember i think i\nprobably got my first one i was around 11 or 12. and i remember thinking um this is amazing you know how how has\nsomeone programmed uh uh this this chess board to play chess uh and uh it was\nvery formative book i bought which was called the chess computer handbook by david levy which came out in 1984 or\nsomething so i must have got it when i was about 11 12 and it explained fully how these chess programs were made i\nremember my first ai program being uh programming my amiga it couldn't it\nwasn't powerful enough to play chess i couldn't write a whole chess program but i wrote a program for it to play othello\nreversey it's sometimes called i think in the u.s and so a slightly simpler game than chess but i used all of the\nprinciples that chess programs had alpha beta search all of that and that was my first ai program i remember that very\nwell was around 12 years old so that that that brought me into ai and then the second part was later on uh when i\nwas around 1617 and i was writing games professionally designing games uh writing a game called theme park which\num had ai as a core gameplay component as part of the simulation um and it sold\nyou know millions of copies around the world and people loved the way that the ai even though it was relatively simple\nby today's ai standards um was was reacting to the way you as the player played it so it was called a sandbox\ngame so it's one of the first types of games like that along with simcity and it meant that every game you played was\nunique is there something you could say just on a small tangent about\nreally impressive ai from a game design human enjoyment perspective\nreally impressive ai that you've seen in games and maybe what does it take to create ai system and how hard of a\nproblem is that so a million questions just as a brief tangent\nwell look i think um games uh games have been significant in my life for three reasons so first of\nall to to i was playing them and training myself on games when i was a kid then i went through a phase of\ndesigning games and writing ai4 games so all the games i i professionally wrote uh had ai as a core component and that\nwas mostly in the in the 90s and the reason i was doing that in games industry was at the time the games\nindustry i think was the cutting edge of technology so whether it was graphics with people like john carmack and quake\nand those kind of things or ai i think actually all the action was going on in\ngames and and we've seen we're still reaping the benefits of that even with things like gpus which you know i find\nironic was obviously invented for graphics computer graphics but then turns out to be amazingly useful for ai\nit just turns out everything's a matrix multiplication it appears you know in the whole world so um so i think games at the time had\nthe most cutting edge ai and a lot of the the games uh uh we you know i was involved in writing so there was a game\ncalled black and white which was one game i was involved with in the early stages of which i still think is the\nmost um impressive uh example of reinforcement learning in a computer game so in that\ngame you know you trained a little pet animal uh and yeah and it sort of learned from how you\nwere treating it so if you treated it badly then it became mean yeah and then it would be mean to to your villagers\nand your and your population the sort of uh the little tribe that you were running uh but if you were kind to it\nthen it would be kind and people were fascinated by how that was and so was i to be honest with the way it kind of\ndeveloped and um especially the mapping to good and evil yeah it made you made you realize made me realize that you can\nsort of in the way in the choices you make can define uh the\nwhere you end up and that means all of us are capable of the good\nuh evil it all matters in uh the different choices along the trajectory to those places that you make it's\nfascinating i mean games can do that philosophically to you and it's rare it seems rare yeah well games are i think a\nunique medium because um you as the player you're not just passively consuming the the entertainment right\nyou're actually actively involved as an as a as an agent so i think that's what makes it in some ways can be more\nvisceral than other other mediums like you know films and books so the second so that was you know designing ai and\ngames and then the third use uh uh i've we've used of ai is in deep mind from\nthe beginning which is using games as a testing ground for proving out ai\nalgorithms and developing ai algorithms and that was a that was a sort of um a core component of our vision at the\nstart of deepmind was that we would use games very heavily uh as our main testing ground certainly to begin with\num because it's super efficient to use games and also you know it's very easy to have metrics to see how well your\nsystems are improving and what direction your ideas are going in and whether you're making incremental improvements\nand because those games are often rooted in something that humans did for a long time beforehand\nthere's already a strong set of rules like it's already a damn good benchmark yes it's really good for\nso many reasons because you've got you've got you've got clear measures of how good humans can be at these things\nand in some cases like go we've been playing it for thousands of years um and and uh often they have scores or at\nleast win conditions so it's very easy for reward learning systems to get a reward it's very easy to specify what\nthat reward is um and uh also at the end it's easy to you know to test uh\nexternally you know how strong is your system by of course playing against you know the world's\nstrongest players at those games so it's it's so good for so many reasons and it's also very efficient to run\npotentially millions of simulations in parallel on the cloud so um i think\nthere's a huge reason why we were so successful back in you know starting out 2010 how come we were able to progress\nso quickly because we'd utilize games and um you know at the beginning of deep\nmind we also hired some amazing game engineers uh who i knew from my previous uh lives in the games industry and uh\nand that helped to bootstrap us very quickly and plus it's somehow super compelling almost at a philosophical\nlevel of man versus machine over over a chessboard or a go board\nand especially given that the entire history of ai is defined by people saying it's going to be impossible to\nmake a machine that beats a human being in chess and then once that happened\npeople were certain when i was coming up in ai that go is not a game that could be solved\nbecause of the combinatorial complexity it's just too it's it's it's you know\nno matter how much moore's law you have compute is just never going to be able to crack the game of go yeah and so that\nthen there's something compelling about facing sort of taking on the impossibility of that task from the\nai researcher perspective engineer perspective and then as a human being\njust observing this whole thing your beliefs about what you thought was\nimpossible being broken apart\nit's it's uh humbling to realize we're not as smart as we thought it's humbling to realize that the things\nwe think are impossible now perhaps will be done in the future there's something\nreally powerful about a game ai system being a human being in a game that\ndrives that message uh home for like millions billions of people especially in the case of go sure\nwell look i think it's a i mean it has been a fascinating journey and and especially as i i think about it from i\ncan understand it from both sides both as the ai you know creators of the ai um but also\nas a games player originally so you know it was a it was a really interesting it was i mean it was a fantastic um but\nalso somewhat bittersweet moment the alphago match for me um uh seeing that and and and being obviously heavily\nheavily involved in that um but you know as you say chess has been uh the i mean\nkasparov i think rightly called it the drosophila of of intelligence right so it's sort of i i love that phrase and\nand i think he's right because chess has been um hand in hand with ai from the\nbeginning of the the whole field right so i think every ai practitioner starting with turing and claude shannon\nand all those uh the sort of forefathers of of of of the field um tried their\nhand at writing a chess program uh i've got original audition of claude shannon's first chess program i think it\nwas 1949 uh the the original sort of uh paper and um they all did that and\nturing famously wrote a chess program that but all the computers around there were obviously too slow to run it so he\nhad to run he had to be the computer right so he literally i think spent two or three days running his own program by\nhand with pencil and paper and playing playing a friend of his uh with his chess program so\nof course deep blue was a huge moment uh beating off um but actually when that happened i\nremember that very very vividly of course because it was you know chess and computers and ai all the things i loved\nand i was at college at the time but i remember coming away from that being more impressed by kasparov's mind than i\nwas by deep blue because here was kasparov with his human mind not only could he play chess more or less to the\nsame level as this brute of a calculation machine um but of course kasparov can do everything else humans\ncan do ride a bike talk many languages do politics all the rest of the amazing things that kasparov does and so with\nthe same brain yeah and and yet deep blue uh brilliant as it was at chess it\nhad been hand coded for chess and um actually had distilled the knowledge of\nchess grand masters uh into into a cool program but it couldn't do anything else like it couldn't even play a strictly\nsimpler game like tic-tac-toe so um something to me was missing from um\nintelligence from that system that we would regard as intelligence and i think it was this idea of generality and and\nalso learning yeah um so and that's what we tried to do out with alphago yeah we\nalphago and alpha zero mu zero and then got on all the things that uh we'll get\ninto some parts of there's just a fascinating trajectory here but let's just stick on chess briefly uh on the\nhuman side of chess you've proposed that from a game design perspective the thing that makes chess\ncompelling as a game uh is that there's a creative tension between a bishop\nand the knight can you explain this first of all it's really interesting to think about what\nmakes the game compelling makes it stick across centuries\nyeah i was sort of thinking about this and actually a lot of even amazing chess players don't think about it necessarily from a games designer point of view so\nit's with my game design hat on that i was thinking about this why is chess so compelling and i think a critical uh reason is the\nthe dynamicness of of of the different kind of chess positions you can have whether they're closed or open and other\nthings comes from the bishop and the night so if you think about how different the the the capabilities of\nthe bishop and knight are in terms of the way they move and then somehow chess has evolved to balance those two\ncapabilities more or less equally so they're both roughly worth three points each so you think that dynamics was\nalways there and then the rest of the rules are kind of trying to stabilize the game well maybe i mean it's sort of\ni don't know his chicken and egg situation probably both came together but the fact that it's got to this beautiful equilibrium where you can have\nthe bishop and knight they're so different in power um but so equal in value across the set of the universe of\nall positions right somehow they've been balanced by humanity over hundreds of years um i think gives gives the game\nthe creative tension uh that you can swap the bishop and knights uh for a bishop for a knight and you you they're\nmore or less worth the same but now you aim for a different type of position if you have the knight you want a closed position if you have the bishop you want\nan open position so i think that creates a lot of the creative tension in chess so some kind of controlled creative\ntension from an ai perspective do you think ai systems convention\ndesign games that are optimally compelling to humans well that's an interesting question you\nknow sometimes i get asked about ai and creativity and and this and the way i answered that is relevant to that\nquestion which is that i think they're different levels of creativity one could say so i think um if we define\ncreativity as coming up with something original right that's that's useful for a purpose then you know i think the kind\nof lowest level of creativity is like an interpolation so an averaging of all the examples you see so maybe a very basic\nai system could say you could have that so you show it millions of pictures of cats and then you say give me an average\nlooking cat right generate me an average looking cat i would call that interpolation then there's extrapolation\nwhich something like alphago showed so alphago played you know millions of games of go against itself\nand then it came up with brilliant new ideas like move 37 in game two bringing a motif strategies and go that that no\nhumans had ever thought of even though we've played it for thousands of years and professionally for hundreds of years so that that i call that extrapolation\nbut then that's still there's still a level above that which is you know you could call out the box thinking or true\ninnovation which is could you invent go right could you invent chess and not just come up with a brilliant chess move\nor brilliant go move but can you can you actually invent chess or something as good as chess or go and i think one day\nuh ai could but what's missing is how would you even specify that task to a a\nprogram right now and the way i would do it if i was best telling a human to do it or a games designer a human games\ndesigner to do it is i would say something like go i would say um come up with a game that only takes five\nminutes to learn which go does because it's got simple rules but many lifetimes to master right or impossible to master\nin one lifetime because so deep and so complex um and then it's aesthetically beautiful uh and also uh it can be\ncompleted in three or four hours of gameplay time which is you know useful for our us you know in in a human day\nand so um you might specify these side of high level concepts like that and then you know with that and maybe a few\nother things uh one could imagine that go satisfies uh those those constraints\num but the problem is is that we we're not able to specify abstract notions\nlike that high-level abstract notions like that yet to our ai systems um and i think there's still something missing\nthere in terms of um high-level concepts or abstractions that they truly understand and that you know combinable\nand compositional um so for the moment i think ai is capable of doing\ninterpolation extrapolation but not true invention so coming up with rule sets\nuh and optimizing with complicated objectives around those rule sets we can't currently do\nbut you could take a specific rule set and then run a kind of self-play\nexperiment to see how long just observe how an ai system from scratch learns how long is that journey\nof learning and maybe if it satisfies some of those other things you mentioned in terms of quickness to learn and so on and you\ncould see a long journey to master for even an ai system then you could say that this is a promising game\num but it would be nice to do almost like alpha codes or programming rules so generating rules that kind of\nuh that automate even that part of the generation of rules so i have thought about systems actually um that i think\nwould be amazing in in for a games designer if you could have a system that um takes your game plays it tens of\nmillions of times maybe overnight and then self balances the rules better so it tweaks the the rules and the maybe\nthe equations and the and the and the parameters so that the game uh is more\nbalanced the units in the game or some of the rules could be tweaked so it's a bit of like a giving a base set\nand then allowing a monte carlo tree search or something like that to sort of explore it right and i think that would\nbe super super a powerful tool actually for for balancing auto balancing a game\nwhich usually takes thousands of hours from hundreds of games human games testers normally to to\nbalance some one you know game like starcraft which is you know blizzard are amazing at balancing their games but it\ntakes them years and years and years so one could imagine at some point when this uh this stuff becomes uh efficient\nenough to you know you might be able to do that like overnight do you think a game that is optimal\ndesigned by an ai system would look very much like uh planet\nearth maybe maybe it's only the sort of game i would love to make is is and i've tried\nyou know my in my game's career the games design career you know my first big game was designing a theme park an\namusement park then uh with games like republic i tried to you know have games where we designed whole cities and and\nallowed you to play in so and of course people like will wright have written games like sim earth uh trying to\nsimulate the whole of earth pretty tricky but um i see earth i haven't actually played that one so what is it\ndoes it incorporative evolution or yeah it has evolution and it's sort of um it tries to it sort of treats it as an\nentire biosphere but from quite a high level so nice to be able to sort of zoom in zoom\nout zoom in exactly so obviously he couldn't do that was in the night i think he wrote that in the 90s so it couldn't you know it wasn't it wasn't\nable to do that but that that would be uh obviously the ultimate sandbox game of course on that topic do you think we're living\n"}
{"pod": "Lex Fridman Podcast", "input": "Simulation", "output": "in a simulation yes well so okay so i'm gonna jump around from the absurdly philosophical\nto the short term sure very very happy to so i think uh my answer to that question is a little bit complex because\nuh there is simulation theory which obviously nick bostrom i think famously first proposed um\nand uh i don't quite believe it in in that sense so um in the in the sense\nthat uh are we in some sort of computer game or have our descendants somehow recreated uh uh earth in the you know\n21st century and and some for some kind of experimental reason i think that um\nbut i do think that we that that we might be that the best way to understand physics and the universe is from a\ncomputational perspective so understanding it as an information universe and actually information being\nthe most fundamental unit of uh reality rather than matter or energy so a\nphysicist would say you know matter or energy you know e equals m c squared these are the things that are are the\nfundamentals of the universe i'd actually say information um which of course itself can be can specify energy\nor matter right matter is actually just you know we're we're just out the way our bodies and all the molecules in our\nbody arrange is information so i think information may be the most fundamental way to describe the universe and\ntherefore you could say we're in some sort of simulation because of that um but i don't i do i'm not i'm not really\na subscriber to the idea that um you know these are sort of throw away billions of simulations around i think\nthis is actually very critical and possibly unique this simulation particular one yes so but and you just\nmean treating the universe as a computer that's\nprocessing and modifying information is is a good way to solve the problems of physics of chemistry of biology\nand perhaps of humanity and so on yes i think understanding physics in terms of\ninformation theory uh might be the best way to to really uh understand what's\ngoing on here from our understanding of a universal turing machine from our understanding of\n"}
{"pod": "Lex Fridman Podcast", "input": "Consciousness", "output": "a computer do you think there's something outside of the capabilities of a computer that is present in our\nuniverse you have a disagreement with roger penrose the nature of consciousness he he thinks\nthat consciousness is more than just a computation uh do you think all of it the whole\nshebang is can be can be a competition yeah i've had many fascinating debates with uh sir roger penrose and obviously\nhe's he's famously and i read you know emperors of new mind and and um and his books uh his classical books uh\nand they they were pretty influential and you know in the 90s and um he believes that there's something more you\nknow something quantum that is needed to explain consciousness in the brain um i\nthink about what we're doing actually at deepmind and what my career is being we're almost like true rings champion so\nwe are pushing turing machines or classical computation to the limits what are the limits of what classical\ncomputing can do now um and at the same time i've also studied neuroscience to\nsee and that's why i did my phd in was to see also to look at you know is there anything quantum in the brain from a\nneuroscience or biological perspective and um and so far i think most neuroscientists and most mainstream\nbiologists and neuroscientists would say there's no evidence of any quantum uh systems or effects in the brain as far\nas we can see it's it can be mostly explained by classical uh classical theories so\nand then so there's sort of the the search from the biology side and then at the same time there's the raising of the\nwater uh at the bar from what classical turing machines can do uh uh and\nand you know including our new ai systems and uh as you alluded to earlier\num you know i think ai especially in the last decade plus has been a continual\nstory now of surprising uh events uh and surprising successes knocking over one\ntheory after another of what was thought to be impossible you know from go to protein folding and so on and so i think\num i would be very hesitant to bet against how far the uh universal turing machine\nand classical computation paradigm can go and and my betting would be\nthat all of certainly what's going on in our brain uh can probably be mimicked or\nor approximated on a on a classical machine um not you know not requiring\nsomething metaphysical or quantum and we'll get there with some of the work with alpha fold\nwhich i think begins the journey of modeling this beautiful and complex world of biology so you think all the\nmagic of the human mind comes from this just a few pounds of mush\nof a biological computational mush that's akin to some of the neural networks\nnot directly but in spirit that deep mind has been working with well look i\nthink it's um you say it's a few you know of course it's this is the i think the biggest miracle of the universe is\nthat um it is just a few pounds of mush in our skulls and yet it's also our brains are the most complex objects in\nthe in that we know of in the universe so there's something profoundly beautiful and amazing about our brains\nand i think that it's an incredibly uh incredible efficient machine and and uh\nuh and it's a is you know phenomenal basically and i think that building ai\none of the reasons i want to build ai and i've always wanted to is i think by building an intelligent artifact like ai\nand then comparing it to the human mind um that will help us unlock the\nuniqueness and the true secrets of the mind that we've always wondered about since the dawn of history like consciousness dreaming uh creativity uh\nemotions what are all these things right we've we've wondered about them since since the dawn of humanity and i think\none of the reasons and you know i love philosophy and philosophy of mind is we found it difficult is there haven't been\nthe tools for us to really other than introspection to from very clever people in in history very clever philosophers\nto really investigate this scientifically but now suddenly we have a plethora of tools firstly we have all\nthe neuroscience tools fmri machines single cell recording all of this stuff but we also have the ability computers\nand ai to build uh intelligent systems so i think that um\nuh you know i think it is amazing what the human mind does and um and and i'm kind\nof in awe of it really and uh and i think it's amazing that without human minds we're able to build things like\ncomputers and and actually even you know think and investigate about these questions i think that's also a testament to the human mind yeah the\nuniverse built the human mind that now is building computers that help\nus understand both the universe and our own human mind right that's exactly it i mean i think that's one you know one\ncould say we we are maybe we're the mechanism by which the universe is going to try and understand\nitself yeah it's beautiful so let's let's go to the\n"}
{"pod": "Lex Fridman Podcast", "input": "AlphaFold", "output": "basic building blocks of biology that i think is another angle at which you can start\nto understand the human mind the human body which is quite fascinating which is from the basic building blocks start to\nsimulate start to model how from those building blocks you can construct bigger and bigger more complex\nsystems maybe one day the entirety of the human biology so here's another problem that thought to\nbe impossible to solve which is protein folding and alpha fold or\nspecific alpha fold 2 did just that it solved protein folding i think it's one of the biggest\nbreakthroughs uh certainly in the history of structural biology but uh in general in\nin science um maybe from a high level\nwhat is it and how does it work and then we can ask some fascinating sure questions after sure um so maybe\nlike to explain it uh to people not familiar with protein folding is you know i first of all explain proteins\nwhich is you know proteins are essential to all life every function in your body depends on proteins sometimes they're\ncalled the workhorses of biology and if you look into them and i've you know obviously as part of alpha fold i've been researching proteins and and\nstructural biology for the last few years you know they're amazing little bio nano machines proteins they're\nincredible if you actually watch little videos of how they work animations of how they work and um proteins are specified by their\ngenetic sequence called the amino acid sequence so you can think of those their genetic makeup and then in the body uh\nin in nature they when they when they fold up into a 3d structure so you can think of it as a string of beads and\nthen they fold up into a ball now the key thing is you want to know what that 3d structure is\nbecause the structure the 3d structure of a protein is what helps to determine what does it\ndo the function it does in your body and also if you're interested in drug drugs or disease you need to understand\nthat 3d structure because if you want to target something with a drug compound or about to block that something the\nprotein is doing uh you need to understand where it's going to bind on the surface of the protein so obviously\nin order to do that you need to understand the 3d structure so the structure is mapped to the function the structure is mapped to the function and\nthe structure is obviously somehow specified by the by the amino acid sequence and that's the in essence the\nprotein folding problem is can you just from the amino acid sequence the one-dimensional\nstring of letters can you immediately computationally predict the 3d structure right and this has been a\ngrand challenge in biology for over 50 years so i think it was first articulated by christian anfinsen a\nnobel prize winner in 1972 uh as part of his nobel prize winning lecture and he\njust speculated this should be possible to go from the amino acid sequence to the 3d structure we didn't say how so\ni you know it's been described to me as equivalent to fermat's last theorem but for biology right you should as somebody\nthat uh very well might win the nobel prize in the future but outside of that you should do more of that kind of thing\nin the margins just put random things that will take like 200 years to solve set people off for 200 years it should\nbe possible exactly and just don't give any interest exactly i think everyone's exactly should be i'll have to remember\nthat for future so yeah so he set off you know with this one throwaway remark just like fermat you know he he set off\nthis whole 50-year uh uh uh field really of computational\nbiology and and they had you know they got stuck they hadn't really got very far with doing this and and um until now\nuntil alpha fold came along this is done experimentally right very painstakingly so the rule of thumb is and you have to\nlike crystallize the protein which is really difficult some proteins can't be crystallized like membrane proteins and\nthen you have to use very expensive electron microscopes or x-ray crystallography machines really\npainstaking work to get the 3d structure and visualize the 3d structure so the rule of thumb in in experimental biology\nis that it takes one phd student their entire phd to do one protein uh and with\nalpha fold two we were able to predict the 3d structure in a matter of seconds\num and so we were you know over christmas we did the whole human proteome or every protein in the human\nbody all 20 000 proteins so the human proteins like the equivalent of the human genome but on protein space and uh\nand sort of revolutionize really what uh a structural biologist can do because\nnow um they don't have to worry about these painstaking experimentals you know should they put all of that effort in or\nnot they can almost just look up the structure of their proteins like a google search and so there's a data set on which it's\ntrained and how to map this amino acids because first of all it's incredible that a protein this little chemical\ncomputer is able to do that computation itself in some kind of distributed way and do it very quickly\nthat's a weird thing and they evolved that way because you know in the beginning i mean that's a great invention just the\nprotein itself yes i mean and then they there's i think probably a history of\nlike uh they evolved to have many of these proteins and those proteins figure out how to be computers\nthemselves in such a way that you can create structures that can interact in complexes with each other in order to\nform high level functions i mean it's a weird system that they figured it out well for sure i mean we you know maybe\nwe should talk about the origins of life too but proteins themselves i think are magical and incredible uh uh uh as i\nsaid little little bio-nano machines and um and and actually levantal who is another\nscientist uh uh a contemporary of anfinsen uh he he coined this eleventh\nhouse what became known as levantal's paradox which is exactly what you're saying he calculated roughly a protein\nan average protein which is maybe 2 000 amino acids bases long is um\nis is can fold in maybe 10 to the power 300 different conformations so there's\n10 to the power 300 different ways that protein could fold up and yet somehow in nature physics solves this solves this\nin a matter of milliseconds so proteins fold up in your body in you know sometimes in fractions of a second so\nphysics is somehow solving that search problem and just to be clear in many of these cases maybe you correct me if i'm\nwrong there's often a unique way for that sequence to form itself yes so\namong that huge number of possibilities yes it figures out a way how to stability\nuh in some cases there might be a misfunction so on which leads to a lot of the disorders and stuff like that but\nyes most of the time it's a unique mapping and that unique mapping is not obvious no exactly that's just what the\nproblem is exactly so there's a unique mapping usually in a healthy in if it's healthy and as you say in disease\nso for example alzheimer's one one one conjecture is that it's because of a misfolded protein a protein that folds\nin the wrong way amyloid beta protein so um and then because it falls in the wrong way it gets tangled up right in\nyour in your neurons so um it's super important to understand both healthy functioning and also\ndisease is to understand uh you know what what these things are doing and how they're structuring of course the next\nstep is sometimes proteins change shape when they interact with something so um they're not just static necessarily in\nin biology maybe you can give some interesting sort of beautiful things to you about these\nearly days of alpha fold of of solving this problem because unlike games this is\nreal physical systems that are less amenable to\nself-play type of mechanisms the the size of the data set is smaller that you might otherwise like so you\nhave to be very clever about certain things is there something you could speak to um what was very hard to solve and what are\nsome beautiful aspects about the the solution yeah i would say alpha fold is the most complex and also probably most\nmeaningful system we've built so far so it's been an amazing time actually in the last you know two three years to see\nthat come through because um as we talked about earlier you know games is what we started on uh building things\nlike alphago and alpha zero but really the ultimate goal was to um not just to crack games it was just to to to build\nuse them to bootstrap general learning systems we could then apply to real world challenges specifically my passion\nis scientific challenges like protein folding and then alpha fold of course is our first big proof point of that and so\num you know in terms of the data uh and the amount of innovations that had to go into it we you know it was like more\nthan 30 different component algorithms needed to be put together to crack the protein folding um i think some of the\nbig innovations were that um kind of building in some hard coded constraints around physics and\nevolutionary biology um to constrain sort of things like the bond angles uh\nuh in the in the in the protein and things like that um a lot but not to impact the learning\nsystem so still allowing uh the system to be able to learn the physics uh\nitself um from the examples that we had and the examples as you say there are only about 150 000 proteins even after\n40 years of experimental biology only around 150 000 proteins have been the structures have been found out about so\nthat was our training set which is um much less than normally we would like to use\nbut using various tricks things like self distillation so actually using alpha folds predictions um some of the\nbest predictions that it thought was highly confident in we put them back into the training set right to make the\ntraining set bigger that was critical to to alpha fold working so there was actually a huge\nnumber of different um uh innovations like that that were required to to ultimately crack the problem after fold\none what it produced was a distagram so a kind of a matrix of the pairwise distances\nbetween all of the molecules in the in the in the protein and then there had to be a separate optimization process to uh\ncreate the 3d structure and what we did for alpha volt2 is make it truly end to end so we went straight\nfrom the amino acid sequence of of of bases to the 3d structure directly without going\nthrough this intermediate step and in machine learning what we've always found is that the more end to end you can make\nit the better the system and it's probably because um we you know the in\nthe end the system is better at learning what the constraints are than than we are as the human designers of specifying\nit so anytime you can let it flow end to end and actually just generate what it is you're really looking for in this\ncase the 3d structure you're better off than having this intermediate step which you then have to hand craft the next\nstep for so so it's better to let the gradients and the learning flow all the way through the system um from the end point the end\noutput you want to the inputs so that's a good way to start a new problem handcraft a bunch of stuff add a bunch\nof manual constraints with a small intent learning piece or a small learning piece and grow that learning\npiece until it consumes the whole thing that's right and so you can also see you know this is a bit of a method we've\ndeveloped over doing many sort of successful outfits we call them alpha x projects right is and the easiest way to\nsee that is the evolution of alphago to alpha zero so alphago was um a learning\nsystem but it was specifically trained to only play go right so uh and what we wanted to do with first version of go is\njust get to world champion performance no matter how we did it right and then and then of course alphago zero we we we\nremoved the need to use human games as a starting point right so it could just play against itself from random starting\npoint from the beginning so that removed the the need for human knowledge uh about go and then finally alpha zero\nthen generalized it so that any things we had in there the system including things like symmetry of the go board uh\nwere removed so the alpha zero could play from scratch any two player game and then mu0 which is the final\nlatest version of that set of things was then extending it so that you didn't even have to give it the rules of the\ngame it would learn that for itself so it could also deal with computer games as well as board games so that line of\nalpha golf goes zero alpha zero mu zero that's the full trajectory of what you\ncan take from uh imitation learning to full self\nsupervised learning yeah exactly and learning learning uh the entire structure of the environment you put in\nfrom scratch right and and and and bootstrapping it uh through self-play uh yourself but the thing is it would have\nbeen impossible i think or very hard for us to build alpha zero or mu0 first out of the box\neven psychologically because you have to believe in yourself for a very long time you're constantly dealing with doubt\nbecause a lot of people say that it's impossible exactly so it was hard enough just to do go as you were saying everyone thought that was impossible or\nat least a decade away um from when we when we did it back in 2015 24 you know\n2016 and um and so yes it would have been psychologically probably very difficult\nas well as the fact that of course we learnt a lot by building alphago first right so it's i think this is why i call\nai in engineering science it's one of the most fascinating science disciplines but it's also an engineering science in\nthe sense that unlike natural sciences um the phenomenon you're studying it doesn't exist out in nature you have to\nbuild it first so you have to build the artifact first and then you can study how how and pull it apart and how it\nworks this is tough to uh ask you this question because you probably will say it's everything but\nlet's let's try let's try to think to this because you're in a very interesting position where deepmind is\n"}
{"pod": "Lex Fridman Podcast", "input": "Solving intelligence", "output": "the place of some of the most uh brilliant ideas in the history of ai but it's also a place of brilliant\nengineering so how much of solving intelligence this big goal for deepmind how much of it is\nscience how much is engineering so how much is the algorithms how much is the data how much is the\nhardware compute infrastructure how much is it the software computer infrastructure yeah um what else is\nthere how much is the human infrastructure and like just the humans interact in certain kinds of ways in all the space\nof all those ideas how much does maybe like philosophy how much what's the key if um\nuh if if you were to sort of look back like if we go forward 200 years look back\nwhat was the key thing that solved intelligence is that ideas i think it's a combination first of all\nof course it's a combination of all those things but the the ratios of them changed over over time\nso yeah so um even in the last 12 years so we started deep mine in 2010 which is hard to imagine now because 2010 it's\nonly 12 short years ago but nobody was talking about ai uh you know if you remember back to your mit days you know\nno one was talking about it i did a postdoc at mit back around then and it was sort of thought of as a well look we\nknow ai doesn't work we tried this hard in the 90s at places like mit mostly losing using logic systems and\nold-fashioned sort of good old-fashioned ai we would call it now um people like minsky and and and patrick winston and\nyou know all these characters right and used to debate a few of them and they used to think i was mad thinking about that some new advance could be done with\nlearning systems and um i was actually pleased to hear that because at least you know you're on a unique track at\nthat point right even if every all of your you know professors are telling you you're mad that's true and of course in\nindustry uh you can we couldn't get you know as difficult to get two cents together uh and which is hard to imagine\nnow as well given it's the biggest sort of buzzword in in vcs and and fundraising's easy and all these kind of\nthings today so back in 2010 it was very difficult and what we the reason we started then and\nshane and i used to discuss um uh uh what were the sort of founding tenets of deep mind and it was very various things\none was um algorithmic advances so deep learning you know jeff hinton and cohen just had just sort of invented that in\nacademia but no one in industry knew about it uh we love reinforcement learning we thought that could be scaled\nup but also understanding about the human brain had advanced um quite a lot uh in the decade prior with fmri\nmachines and other things so we could get some good hints about architectures and algorithms and and sort of um\nrepresentations maybe that the brain uses so as at a systems level not at a implementation level um and then the\nother big things were compute and gpus right so we could see a compute was going to be really useful and it got to\na place where it became commoditized mostly through the games industry and and that could be taken advantage of and\nthen the final thing was also mathematical and theoretical definitions of intelligence so things like ai xi aix\nwhich uh shane worked on with his supervisor marcus hutter which is a sort of theoretical uh proof really of\nuniversal intelligence um which is actually a reinforcement learning system um in the limit i mean it assumes\ninfinite compute and infinite memory in the way you know like a turing machine proof but i was also waiting to see\nsomething like that too to you know like turing machines uh and and computation theory that people like turing and\nshannon came up with underpins modern computer science um uh you know i was waiting for a theory like that to sort\nof underpin agi research so when i you know met shane and saw he was working on something like that you know that to me\nwas a sort of final piece of the jigsaw so in the early days i would say that\nideas were the most important uh you know and for us it was deep reinforcement learning scaling up deep\nlearning um of course we've seen transformers so huge leaps i would say you know three or four from for if you\nthink from 2010 until now uh huge evolutions things like alphago um and um\nand and maybe there's a few more still needed but as we get closer to ai agi um\ni think engineering becomes more and more important and data because scale and of course the the recent you know\nresults of gpt3 and all the big language models and large models including our ones uh has shown that scale is a is and\nlarge models are clearly going to be unnecessary but perhaps not sufficient part of an agi solution and\nthroughout that like you said and i'd like to give you a big thank you you're one of the pioneers in this is\nsticking by ideas like reinforcement learning that this can actually work\ngiven actually limited success in the past and also\nwhich we still don't know but proudly having the best researchers in the world\nand talking about solving intelligence so talking about whatever you call it agi or something like this\nthat speaking of mit that's that's just something not you wouldn't bring up no uh not not maybe you did in uh like 40\n50 years ago but that was um ai was a place where you do tinkering\nvery small scale not very ambitious projects and maybe the biggest ambitious projects\nwere in the space of robotics and doing like the darpa challenge sure but the task of solving intelligence and\nbelieving you can that's really really powerful so in order for engineering to do its work\nto have great engineers build great systems you have to have that belief that threats throughout the whole thing\nthat you can actually solve some of these impossible challenges yeah that's right and and back in 2010 you know our\nmission statement um and still is today you know it was used to be uh solving step one solve intelligence step two use\nit to solve everything else yes so if you can imagine pitching that to a vc in 2010 you know the kind of looks we we\ngot we managed to you know find a few uh kooky people to back us but it was uh it was tricky and and i and i got to the\npoint where we we wouldn't mention it to any of our professors because they would just eye roll and think we you know\ncommitted career suicide and and uh and and you know so it was there's a lot of things that we had to do but we always\nbelieved it and one reason you know by the way one reason we i believe i've always believed in reinforcement learning is that\nthat if you look at neuroscience that is the way that the you know primate brain learns one of the main mechanisms is the\ndopamine system implement some form of td learning a very famous result in the late 90s uh where they saw this in\nmonkeys and uh and as a you know proper game prediction error so we you know again in the limit this is this is what\ni think you can use neuroscience for is is you know any at mathematics you when you're when you're doing something as\nambitious as trying to solve intelligence and you're you're you know it's blue sky research no one knows how to do it you you you need to use any\nevidence or any source of information you can to help guide you in the right direction or give you confidence you're\ngoing in the right direction so so that that was one reason we pushed so hard on that and that's and just going back to\nyour early question about organization the other big thing that i think we innovated with at deepmind to encourage\ninvention and and uh and innovation was the multi-disciplinary organization we\nbuilt and we still have today so deepmind originally was a confluence of the of the most cutting-edge knowledge\nin neuroscience with machine learning engineering and mathematics right and and gaming\nand then since then we built that out even further so we have philosophers here and and uh by you know ethicists\nbut also other types of scientists physicists and so on um and that's what brings together i tried to build a sort\nof um new type of bell labs but in this golden era right uh\nand and a new expression of that um to try and uh foster this incredible sort\nof innovation machine so talking about the humans in the machine the mind itself is a learning machine\nwith a lots of amazing human minds in it coming together to try and build these uh learning systems\nif we return to the big ambitious dream of alpha fold that may be the early steps on a very\nlong journey in um in biology\ndo you think the same kind of approach can use to predict the structure and function of more complex biological\nsystems so multi-protein interaction and then i mean you can go out from there just\nsimulating bigger and bigger systems that eventually simulate something like the human brain or the human body just\nthe big mush the mess of the beautiful resilient mesobiology do do you see that\nas a long-term vision i do and i think um you know if you think about what are the\nthings top things i wanted to apply ai ai2 once we had powerful enough systems biology and curing diseases and\nunderstanding biology uh was right up there you know top of my list that's one of the reasons i personally pushed that\nmyself and with alpha fold but i think alpha fold uh amazing as it is is just\nthe beginning um and and and i hope it's evidence of uh what could be done with\ncomputational methods so um you know alpha fold solve this this huge problem of the structure of proteins but biology\nis dynamic so really what i imagine from here we're working on all these things now is protein protein interaction uh\nprotein ligand binding so reacting with molecules um then you want to get build up to pathways and then eventually a\nvirtual cell that's my dream uh maybe in the next 10 years and i've been talking actually to a lot of biologists friends\nof mine paul nurse who runs the qrik institute amazing biologist nobel prize winning biologist we've been discussing\nfor 20 years now virtual cells could you build a virtual simulation of a cell and\nif you could that would be incredible for biology and disease discovery because you could do loads of experiments on the virtual cell and then\nonly at the last stage validate it in the wet lab so you could you know in terms of the search space of discovering\nnew drugs you know it takes 10 years roughly to go from uh uh to to go from uh you know identifying a target to uh\nhaving a drug candidate um maybe that could be shortened to you know by an order of magnitude with if you could do\nmost of that that that work in silico so in order to get to a virtual cell\nwe have to build up uh uh understanding of different parts of biology and the interactions and and um so you know\nevery every few years we talk about this with i talked about this with paul and then finally last year after alpha fault\ni said now is the time we can finally go for it and and alpha falls the first proof point that this might be possible\nuh and he's very excited when we have some collaborations with his with his lab they're just across the road actually from us as you know wonderful\nbeing here in king's cross with the quick institute across the road and um and i think the next steps you know i\nthink there's going to be some amazing advances in biology built on top of things like alpha fold uh we're already\nseeing that with the community doing that after we've open sourced it and released it um and uh you know i also i\noften say that i think uh if you think of mathematics is the perfect description language for physics\ni think ai might be end up being the perfect description language for biology because\nbiology is so messy it's so emergent so dynamic and complex um i think i find it\nvery hard to believe we'll ever get to something as elegant as newton's laws of motions to describe a cell right it's\njust too complicated um so i think ai is the right tool for this you have to uh\nyou have to start at the basic building blocks and use ai to run the simulation for all those building blocks so have a\nvery strong way to do prediction of what given these building blocks what kind of biology how the\nthe function and the evolution of that biological system it's almost like a cellular automata you\nhave to run you can't analyze it from a high level you have to take the basic ingredients figure out the rules yeah\nand let it run but in this case the rules are very difficult to figure out yes yes learn them that's exactly it so\nit's the biology is too complicated to figure out the rules it's it's it's too emergent too dynamic say compared to a\nphysics system like the motion of a planet yeah right and and so you have to learn the rules and that's exactly the\ntype of systems that we're building so you you mentioned you've open sourced alpha fold and even the data involved\n"}
{"pod": "Lex Fridman Podcast", "input": "Open sourcing AlphaFold & MuJoCo", "output": "to me personally also really happy and a big thank you for open sourcing mijoko\nuh the physics simulation engine that's that's often used for robotics research\nand so on so i think that's a pretty gangster move uh so what what's the\nwhat's i mean this uh very few companies or people would do\nthat kind of thing what's the philosophy behind that you know it's a case-by-case basis and in both those cases we felt\nthat was the maximum benefit to humanity to do that and and the scientific community in one case the robotics uh\nphysics community with mojoco so purchased it we purchased to obs we purchased it for the express\nprinciple to open source it so um so you know i hope people appreciate that\nit's great to hear that you do and then the second thing was and mostly we did it because the person building it is uh\nuh would not it was not able to cope with supporting it anymore because it was it got too big for him his amazing\nprofessor uh who who built it in the first place so we helped him out with that and then with alpha folds even\nbigger i would say and i think in that case we decided that there were so many downstream applications of alpha fold um\nthat we couldn't possibly even imagine what they all were so the best way to accelerate uh drug discovery and also\nfundamental research would be to to um give all that data away and and and the\nand the and the system itself um you know it's been so gratifying to see what people have done that within just one\nyear which is a short amount of time in science and uh it's been used by over 500 000 researchers have used it we\nthink that's almost every biologist in the world i think there's roughly 500 000 biologists in the world professional biologists have used it to to look at\ntheir proteins of interest we've seen amazing fundamental research done so a couple of weeks ago front\ncover there was a whole special issue of science including the front cover which had the nuclear pore complex on it which\nis one of the biggest proteins in the body the nuclear poor complex is a protein that governs all the nutrients\ngoing in and out of your cell nucleus so they're like little hole gateways that open and close to let things go in and\nout of your cell nucleus so they're really important but they're huge because they're massive doughnut rings shaped things and they've been looking\nto try and figure out that structure for decades and they have lots of you know experimental data but it's too low\nresolution there's bits missing and they were able to like a giant lego jigsaw puzzle use alpha fold predictions plus\nexperimental data and combined those two independent sources of information uh actually four different groups around\nthe world were able to put it together the sec more or less simultaneously using alpha fault predictions so that's\nbeen amazing to see and pretty much every pharma company every drug company executive i've spoken to has said that\ntheir teams are using alpha fold to accelerate whatever drugs uh uh they're\ntrying to discover so i think the knock-on effect has been enormous in terms of uh the impact that uh\nalpha-fold has made and it's probably bringing in it's creating biologists it's bringing more people into the field\num both on the excitement and both on the technical skills involved and um\nit's almost like uh a gateway drug to biology yes it is you get more computational people involved too\nhopefully and and i think for us you know the next stage as i said you know in future we have to have other\nconsiderations too we're building on top of alpha fold and these other ideas i discussed with you about protein protein interactions and and genomics and other\nthings and not everything will be open source some of it will will do commercially because that will be the best way to actually get the most\nresources and impact behind it in other ways some other projects will do non-profit style um and also we have to\nconsider for future things as well safety and ethics as well like but you know synthetic biology there are you\nknow there is dual use and we have to think about that as well with alpha fold we you know we consulted with 30\ndifferent bioethicists and and other people expert in this field to make sure it was safe before um we released it so\nthere'll be other considerations in future but for right now you know i think alpha fold is a kind of a gift from us to to to the scientific\ncommunity so i'm pretty sure that something like alpha fold\nuh would be part of nobel prizes in the future but us humans of course are horrible\nwith credit assignment so we'll of course give it to the humans do you think there will be a day\nwhen ai system can't be denied that it earned that nobel prize do you\nthink we'll see that in 21st century it depends what type of ais we end up building right whether they're um\nyou know goal seeking agents who specifies the goals uh who comes up with the hypotheses\nwho you know who determines which problems to tackle right so i think it's about an announcement yeah so it's\nannouncing the results exactly as part of it um so i think right now of course it's it's it's it's amazing human\ningenuity that's behind these systems and then the system in my opinion is just a tool you know it'd be a bit like\nsaying with galileo and his telescope you know the ingenuity the the the credit should go to the telescope i mean\nit's clearly galileo building the tool which he then uses so i still see that in the same way\ntoday even though these tools learn for themselves um they're i think i think of things like alpha fold and that the\nthings we're building as the ultimate tools for science and for acquiring new knowledge to help us as scientists\nacquire new knowledge i think one day there will come a point where an ai system may solve or come up with\nsomething like general relativity of its own bat not just by averaging everything on the internet or\naveraging everything on pubmed although that would be interesting to see what that would come up with um so\nthat to me is a bit like our earlier debate about creativity you know inventing go rather than just coming up\nwith a good go move and um so i think uh solving i think to to you know if we\nwanted to give it the credit of like a nobel type of thing then it would need to invent go uh and sort of invent that\nnew conjecture out of the blue um rather than being specified by the the human\nscientists or the human creators so i think right now that's it's definitely just a tool although it is interesting\nhow far you get by averaging everything on the internet like you said because you know a lot of people do see science as you're\nalways standing on the shoulders of giants and the question is how much are you really\nreaching up above the shoulders of giants maybe it's just assimilating different kinds\nof results of the past with ultimately this new perspective that gives you this breakthrough idea\nbut that idea may not be novel in the way that we can't be already discovered on the internet maybe\nthe nobel prizes of the next 100 years are already all there on the internet to be discovered\nthey could be they could be i mean i think um this is one of the big mysteries i think\nis that uh uh i i first of all i believe a lot of the big new breakthroughs that are going\nto come in the next few decades and even in the last decade are going to come at the intersection between different subject areas where um there'll be some\nnew connection that's found between what seemingly with disparate areas and and one can even think of deep mind as i\nsaid earlier as a sort of interdisciplinary between neuroscience ideas and ai engineering ideas uh\noriginally and so um so i think there's that and then one of the things we can't\nimagine today is and one of the reasons i think people we were so surprised by how well large models worked is that\nactually it's very hard for our human minds our limited human minds to understand what it would be like to read the whole\ninternet right i think we can do a thought experiment and i used to do this of like well what if i read the whole of\nwikipedia what would i know and i think our minds can just about comprehend maybe what that would be like but the whole\ninternet is beyond comprehension so i think we just don't understand what it would be like to be able to hold all of\nthat in mind potentially right and then active at once and then maybe what are\nthe connections that are available there so i think no doubt there are huge things to be discovered just like that\nbut i do think there is this other type of creativity of true spark of new knowledge new idea never thought before\nabout can't be average from things that are known um that really of course everything come you know nobody creates\nin a vacuum so there must be clues somewhere but just a unique way of putting those things together i think\nsome of the greatest scientists in history have displayed that i would say although it's very hard to know going\nback to their time what was exactly known uh when they came up with those things although\nyou're making me really think because just the thought experiment of deeply knowing a hundred wikipedia pages\ni don't think i can um i've been really impressed by wikipedia for for technical topics yeah so if you\nknow a hundred pages or a thousand pages i don't think who can visually truly\ncomprehend what's what kind of intelligence that is that's a pretty powerful intelligence if you\nknow how to use that and integrate that information correctly yes i think you can go really far you can probably\nconstruct thought experiments based on that like simulate different ideas so if this\nis true let me run this thought experiment then maybe this is true it's not really invention it's like just\ntaking literally the knowledge and using it to construct a very basic simulation of the world i mean some argue it's\nromantic in part but einstein would do the same kind of things with a thought experiment yeah one could imagine doing\nthat systematically across millions of wikipedia pages plus pubmed all these things i think there are\nmany many things to be discovered like that they're hugely useful you know you could imagine and i want us to do some\nof those things in material science like room temperature superconductors or something on my list one day i'd like to\nlike you know have an ai system to help build better optimized batteries all of these sort of mechanical things mr i\nthink a systematic sort of search could be uh guided by a model could be um could be\nextremely powerful so speaking of which you have a paper on nuclear fusion\n"}
{"pod": "Lex Fridman Podcast", "input": "Nuclear fusion", "output": "uh magnetic control of tokamak plasmas to deep reinforcement learning so you uh\nyou're seeking to solve nuclear fusion with deep rl so it's doing control of high temperature plasmas can you explain this\nwork and uh can ai eventually solve nuclear fusion it's been very fun last year or two and\nvery productive because we've been taking off a lot of my dream projects if you like of things that i've collected over the years of\nareas of science that i would like to i think could be very transformative if we helped accelerate and uh really\ninteresting problems scientific challenges in of themselves this is energy so energy yes exactly so\nenergy and climate so we talked about disease and biology as being one of the biggest places i think ai can help with\ni think energy and climate uh is another one so maybe they would be my top two um\nand fusion is one one area i think ai can help with now fusion has many challenges mostly physics material\nscience and engineering challenges as well to build these massive fusion reactors and contain the plasma and what\nwe try to do whenever we go into a new field to apply our systems is we look for um\nwe talk to domain experts we try and find the best people in the world to collaborate with um\nin this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that\nthey were willing to let us use which you know i double checked with the team we were going to use carefully and safely\ni was impressed they managed to persuade them to let us use it and um and it's a it's an amazing test reactor they have\nthere and they try all sorts of pretty crazy experiments on it and um the the\nthe what we tend to look at is if we go into a new domain like fusion what are all the bottleneck problems uh like\nthinking from first principles you know what are all the bottleneck problems that are still stopping fusion working today and then we look at we you know we\nget a fusion expert to tell us and then we look at those bottlenecks and we look at the ones which ones are amenable to\nour ai methods today yes right and and and then and would be interesting from a research perspective from our point of\nview from an ai point of view and that would address one of their bottlenecks and in this case plasma control was was\nperfect so you know the plasma it's a million degrees celsius something like that it's hotter than the sun\nand there's obviously no material that can contain it so they have to be containing these magnetic very powerful\nsuperconducting magnetic fields but the problem is plasma is pretty unstable as you imagine you're kind of holding a\nmini sun mini star in a reactor so you know you you kind of want to predict\nahead of time what the plasma's going to do so you can move the magnetic field within a few\nmilliseconds you know to to basically contain what it's going to do next so it seems like a perfect problem if you\nthink of it for like a reinforcement learning prediction problem so uh you know your controller you're gonna move\nthe magnetic field and until we came along you know they were they were doing it with with traditional operational uh\nresearch type of uh controllers uh which are kind of handcrafted and the problem is of course they can't react in the\nmoment to something the plasma's doing that they have to be hard-coded and again knowing that that's normally our\ngo-to solution is we would like to learn that instead and they also had a simulator of these plasma so there were\nlots of criteria that matched what we we like to to to use so can ai eventually solve nuclear\nfusion well so we with this problem and we published it in a nature paper last year uh we held the fusion that we held\nthe plasma in specific shapes so actually it's almost like carving the plasma into different shapes and control\nand hold it there for the record amount of time so um so that's one of the problems of of fusion sort of um solved\nso i have a controller that's able to no matter the shape uh contain it continue yeah contain it and hold it in structure\nand there's different shapes that are better for for the energy productions called droplets and and and so on so um\nso that was huge and now we're looking we're talking to lots of fusion startups to see what's the next problem we can\ntackle uh in the fusion area so another fascinating place\n"}
{"pod": "Lex Fridman Podcast", "input": "Quantum simulation", "output": "in a paper title pushing the frontiers of density functionals by solving the fractional electron problem so you're\ntaking on modeling and simulating the quantum mechanical behavior of electrons yes\num can you explain this work and can ai model and simulate arbitrary quantum\nmechanical systems in the future yeah so this is another problem i've had my eye on for you know a decade or more which\nis um uh sort of simulating the properties of electrons if you can do that you can\nbasically describe how elements and materials and substances work so it's\nkind of like fundamental if you want to advance material science um and uh you know we have schrodinger's equation and\nthen we have approximations to that density functional theory these things are you know are famous and um people\ntry and write approximations to to these uh uh to these functionals and and kind of come up with descriptions of the\nelectron clouds where they're gonna go how they're gonna interact when you put two elements together uh and what we try\nto do is learn a simulation uh uh learner functional that will describe more chemistry types of chemistry so um\nuntil now you know you can run expensive simulations but then you can only simulate very small uh molecules very\nsimple molecules we would like to simulate large materials um and so uh today there's no way of doing that and\nwe're building up towards uh building functionals that approximate schrodinger's equation and then allow\nyou to describe uh what the electrons are doing and all materials sort of science and\nmaterial properties are governed by the electrons and and how they interact so have a good summarization of the\nsimulation through the functional um but one that is still\nclose to what the actual simulation would come out with so what um how difficult is that to ask what's\ninvolved in that task is it running those those complicated simulations yeah and learning the task of mapping from the\ninitial conditions and the parameters of the simulation learning what the functional would be yeah so it's pretty\ntricky and we've done it with um you know the nice thing is we there are we can run a lot of the simulations that\nthe molecular dynamics simulations on our compute clusters and so that generates a lot of data so in this case\nthe data is generated so we like those sort of systems and that's why we use games simulator generated data\nand we can kind of create as much of it as we want really um and just let's leave some you know if any computers are\nfree in the cloud we just run we run some of these calculations right compute cluster calculation that's all the the\nfree compute times used up on quantum mechanics quantum mechanics exactly simulations and protein simulations and\nother things and so um and so you know when you're not searching on youtube for video cat videos we're using those\ncomputers usefully and quantum chemistry that's the idea and and putting them for good use and\nthen yeah and then all of that computational data that's generated we can then try and learn the functionals\nfrom that which of course are way more efficient once we learn the functional than um\nrunning those simulations would be do you think one day ai may allow us to\n"}
{"pod": "Lex Fridman Podcast", "input": "Physics", "output": "do something like basically crack open physics so do something like travel faster than the speed of light\nmy ultimate aim has always been with ai is um the reason i am personally working on\nai for my whole life it was to build a tool to help us understand stand the universe so i wanted to and that means\nphysics really and the nature of reality so um uh i don't think we have systems that\nare capable of doing that yet but when we get towards agi i think um that's one of the first things i think we should\napply agi to i would like to test the limits of physics and our knowledge of physics there's so many things we don't know\nthere's one thing i find fascinating about science and you know as a huge proponent of the scientific method as\nbeing one of the greatest ideas humanity's ever had and allowed us to progress with our knowledge\nbut i think as a true scientist i think what you find is the more you find out uh you the more you realize we don't\nknow and and i always think that it's surprising that more people don't aren't troubled you know every night i think\nabout all these things we interact with all the time that we have no idea how they work time\nconsciousness gravity life we can't i mean these are all the fundamental things of nature i think the\nway we don't really know what they are to live life we uh pin certain\nassumptions on them and kind of treat our assumptions as if they're a fact yeah that allows us to sort of box them\noff somehow yeah box them off but the reality is when you think of time\nyou should remind yourself you should put it off the sh take it off the shelf and realize like\nno we have a bunch of assumptions there's still a lot of there's even now a lot of debate there's a lot of uncertainty about exactly what is time\nuh is there an error of time you know there's there's a lot of fundamental questions you can't just make\nassumptions about and maybe ai allows you to um\nnot put anything on the shelf yeah not make any uh hard assumptions and really open it up and see what\nexactly i think we should be truly open-minded about that and uh exactly that not be dogmatic to a particular\ntheory um it'll also allow us to build better tools experimental tools eventually\nthat can then test certain theories that may not be testable today about as things about like\nwhat we spoke about at the beginning about the computational nature of the universe how one might if that was true\nhow one might go about testing that right and and how much uh you know there are people who've conjectured people\nlike uh scott aronson and others about uh you know how much information can a specific planck unit of space and time\ncontain right so one might be able to think about testing those ideas if you had um\nai helping you build some new exquisite uh uh experimental tools this is what i\nimagine you know many decades from now we'll be able to do and what kind of questions can be answered through\nrunning a simulation of of them so there's a bunch of physics simulations you can imagine that could\nbe run in an uh so some kind of efficient way much like you're doing in the quantum\nsimulation work and perhaps even the origin of life so figuring out how\ngoing even back before the work of alpha fault begins of how this whole whole thing\num emerges from a rock yes from a static thing would what do you do you think ai\nwill allow us to is that something you have your eye on it's trying to understand the origin of life first of\n"}
{"pod": "Lex Fridman Podcast", "input": "Origin of life", "output": "all yourself what do you think um how the heck did life originate on earth\nyeah well maybe we i'll come to that in a second but i think the ultimate use of ai is to\nkind of use it to accelerate science to the maximum so i um think of it a little bit like the\ntree of all knowledge if you imagine that's all the knowledge there is in the universe to attain and we sort of barely scratched the\nsurface of that so far in even though you know we've we've done pretty well since the enlightenment right as\nhumanity and i think ai will turbo charge all of that like we've seen with alpha fold and i want to explore as much\nof that tree of knowledge as it's possible to do and um and i think that involves ai helping us with with with\nunderstanding or finding patterns um but also potentially designing and building new tools experimental tools so i think\nthat's all uh and also running simulations and learning simulations all of that we're\nalready we're sort of doing it at a at a at a you know baby steps level here but\ni can imagine that in in in the decades to come as uh you know what's the full\nflourishing of of that line of thinking it's going to be truly incredible i would say if i visualize this tree of\nknowledge something tells me that that knowledge for tree of knowledge for humans is much smaller\nin the set of all possible trees of knowledge is actually quite small giving our cognitive\nlimitations limited cognitive capabilities that even\nwith with the tools we build we still won't be able to understand a lot of things and that's perhaps what non-human\nsystems might be able to reach farther not just as tools but in themselves understanding\nsomething that they can bring back yeah it could well be so i mean there's so many things that that are sort of\nencapsulated in what you just said there i think first of all um there's there's two different things there's like what do we understand today\nyeah what could the human mind understand and what is the totality of what is there to be understood yeah\nright and so there's three consensus you know you can think of them as three larger and larger trees or exploring\nmore branches of that tree and i i think with ai we're going to explore that whole lot now the question is is uh you\nknow if you think about what is the totality of what could be understood um there may be some fundamental physics\nreasons why certain things can't be understood like what's outside the simulation or outside the universe maybe\nit's not understandable from within the universe so that's there may be some hard constraints like that you know it could\nbe smaller constraints like um we think of space time as fundamental\nus our human brains are really used to this idea of a three-dimensional world with time right\nmaybe but our tools could go beyond that they wouldn't have that limitation necessary they could think in 11\ndimensions 12 dimensions whatever is needed but um we could still maybe understand that in several different\nways the example i always give is um when i you know play gary kasparov at speed chess or we've talked about chess\nand these kind of things um you know he if you if you if you're reasonably good at chess you can um you can't come up\nwith the move gary comes up with in his move but he can explain it to you and you can understand and you can understand post hoc the reasoning yeah\nso so i think there's a there's an even further level of like well maybe you couldn't have invented that thing but\nbut using like going back to using language again perhaps you can understand and appreciate that same way\nlike you can appreciate you know vivaldi or mozart or something without you can appreciate the beauty of that without um\nbeing able to to construct it yourself right invent the music yourself so i think we see this in all forms of life\nso it'll be that times you know a million but it would you can imagine also one sign of intelligence is the\nability to explain things clearly and simply right you know people like richard feynman another one of my all-time heroes used to say that right\nif you can't you know if you can explain it something simply then you that's a that's the best sign a complex topic\nsimply then that's one of the best signs of you understanding it yeah so i can see myself talking trash in the ai\nsystem in that way yes uh it gets frustrated how dumb i am and trying to explain something to me i was\nlike well that means you're not intelligent because if you were intelligent you'd be able to explain it simply yeah of course you know there's\nalso the other option of course we could enhance ourselves and and without devices we we are already sort of\nsymbiotic with our compute devices right with our phones and other things and you know this stuff like neural link and etc\nthat could be could could advance that further um so i think there's lots of lots of really amazing possibilities uh\nthat i could foresee from here well let me ask you some wild questions so out there looking for friends\n"}
{"pod": "Lex Fridman Podcast", "input": "Aliens", "output": "do you think there's a lot of alien civilizations out there so i guess this also goes back to your\norigin of life question too because i think that that's key um my personal opinion looking at all this\nand and you know it's one of my hobbies physics i guess so so i i you know it's something i think about a lot and talk\nto a lot of experts on and and and read a lot of books on and i think my feeling currently is that that we are\nalone i think that's the most likely scenario given what what evidence we have so um and the reasoning is i think\nthat you know we've tried since uh things like seti program and i guess since the\ndawning of the the space age uh we've you know had telescopes open radio telescopes and other things and if you\nthink about um and try to detect signals now if you think about the evolution of humans on earth we could have easily\nbeen um a million years ahead of our time now or million years behind quite\neasily with just some slightly different quirk thing happening hundreds of thousands years ago uh you know things\ncould have been slightly different if the bto had hit the dinosaurs a million years earlier maybe things would have evolved uh we'd be a million years\nahead of where we are now so what that means is if you imagine where humanity will be in a few hundred years let alone\na million years especially if we hopefully um you know solve things like climate change and other things and we continue\nto flourish and we build things like ai and we do space traveling and all of the stuff\nthat that humans have dreamed of for forever right and sci-fi has talked about forever um\nwe will be spreading across the stars right and void neumann famously calculated you know it would only take\nabout a million years if you send out von neumann probes to the nearest you know the nearest uh uh other solar\nsystems and and then they built all they did was build two more versions of themselves and set those two out to the\nnext nearest systems uh you you know within a million years i think you would have one of these probes in every system\nin the galaxy so it's not actually in cosmo cosmological time that's actually a very short amount of time\nso and and you know we've people like dyson have thought about constructing dyson spheres around stars to collect\nall the energy coming out of the star you know that there would be constructions like that would be visible across base um probably even across a\ngalaxy so and then you know if you think about all of our radio television uh\nemissions that have gone out since since the you know 30s and 40s um imagine a\nmillion years of that and now hundreds of civilizations doing that when we opened our ears at the point we got\ntechnologically sophisticated enough in the space age we should have heard a cacophony of voices we should\nhave joined that cacophony of voices and what we did we opened our ears and we heard nothing\nand many people who argue that there are aliens would say well we haven't really done exhaustive search yet and maybe\nwe're looking in the wrong bands and and we've got the wrong devices and we wouldn't notice what an alien form was\nlike to be so different to what we're used to but you know i'm not i don't really buy that that it shouldn't be as\ndifficult as that like we i think we've searched enough there should be if it were everywhere if it was it should be everywhere we should see dyson's fears\nbeing put up sun's blinking in and out you know there should be a lot of evidence for those things and then there are other people argue well the sort of\nsafari view of like well we're a primitive species still because we're not space faring yet and and and we're\nyou know there's some kind of globe like universal rule not to interfere star trek rule but like look look we can't\neven coordinate humans to deal with climate change and we're one species what is the chance that of all of these\ndifferent human civilization you know alien civilizations they would have the same priorities and and and and agree\nacross you know these kind of matters and even if that was true and we were in some sort of safari for our own good to\nme that's not much different from the simulation hypothesis because what does it mean the simulation hypothesis i think in its most fundamental level it\nmeans what we're seeing is not quite reality right it's something there's something more deeper underlying it\nmaybe computational now if we were in a if we were in a sort of safari park and\neverything we were seeing was a hologram and it was projected by the aliens or whatever that to me is not much different than thinking we're inside of\nanother universe because we still can't see true reality right i mean there's there's other explanations it could be\nthat the way they're communicating is just fundamentally different that we're too dumb to understand the much better\nmethods of communication they have it could be i mean i mean it's silly to say but\nour own thoughts could be the methods by which they're communicating like the place from which our ideas writers talk\nabout this like the muse yeah it sounds like very kind of uh\nwild but it could be thoughts it could be some interactions with our mind that we\nthink are originating from us is actually something that uh\nis coming from other life forms elsewhere consciousness itself might be that it could be but i don't see any\nsensible argument to the why why would all of the alien species be using this way yes some of them will be more\nprimitive they would be close to our level you know there would there should be a whole sort of normal distribution\nof these things right some would be aggressive some would be you know curious others would be very stoical and\nphilosophical because you know maybe they're a million years older than us but it's not it shouldn't be like what i\nmean one one alien civilization might be like that communicating thoughts and others but i don't see why you know\npotentially the hundreds there should be would be uniform in this way right it could be a violent dictatorship that the\nthe people the alien civilizations that uh become successful\nbecome um [Music] gain the ability to be destructive an\norder of magnitude more destructive but of course the the sad thought\nwell either humans are very special we took a lot of leaps that arrived at what it\nmeans to be human yeah um there's a question there which was the hardest which was the most special but\nalso if others have reached this level and maybe many others have reached this level the great filter\nthat prevented them from going farther to becoming a multi-planetary species or reaching out into the stars\nand those are really important questions for us whether um whether there's other alien\ncivilizations out there or not this is very useful for us to think about if we destroy ourselves\nhow will we do it and how easy is it to do yeah well you know these are big questions and i've thought about these a\nlot but the the the interesting thing is that if we're if we're alone that's somewhat comforting from the\ngreat filter perspective because it probably means the great filters were are past us and i'm pretty sure they are\nso that by in going back to your origin of life question there are some incredible things that no one knows how\nhappened like obviously the first life form from chemical soup that seems pretty hard but i would guess the\nmulticellular i wouldn't be that surprised if we saw single single cell sort of life forms elsewhere\nuh bacteria type things but multicellular life seems incredibly hard that step of you know capturing\nmitochondria and then sort of using that as part of yourself you know when you've just eaten it would you say that's the\nbiggest the most uh like if if you had to choose one sort of uh\nhitchhiker's got this galaxy one sentence summary of like oh those clever creatures did this that would be the\nmultilist i think that was probably the one that that's the biggest i mean there's a great book called the 10 grand great inventions of evolution by nick\nlane and he speculates on 10 10 of these you know what could be great filters um\ni think that's one i think the the advent of of intelligence and and conscious intelligence and in order you\nknow to us to be able to do science and things like that is huge as well i mean it's only evolved once as far as you\nknow uh in in earth history so that would be a later candidate but there's\ncertainly for the early candidates i think multicellular life forms is huge by the way what it's interesting to ask\nyou if you can hypothesize about what is the origin of intelligence is it\n"}
{"pod": "Lex Fridman Podcast", "input": "Intelligent life", "output": "uh that we started cooking meat over fire\nis it that we somehow figured out that we could be very powerful when we start collaborating so cooperation between\num our ancestors so that we can overthrow the alpha male\nuh what is it richard i talked to richard randham who thinks we're all just beta males who figured out how to\ncollaborate to defeat the one the dictator the authoritarian alpha male\num that control the tribe um is there other explanation did was there um 2001\nspace out any type of monolith yeah that came down to earth well i i think um i think all of those things you suggest\nfor good candidates fire and and and cooking right so that's clearly important\nyou know energy efficiency yeah cooking our meat and then and then being able to to to be more efficient about eating it\nand getting it consuming the energy um i think that's huge and then utilizing fire and tools i think you're right\nabout the the tribal cooperation aspects and probably language as part of that yes um because probably that's what\nallowed us to outcompete neanderthals and and perhaps less cooperative species so um so that may be the case tool\nmaking spears axes i think that let us i mean i think it's pretty clear now that\nhumans were responsible for a lot of the extinctions of megafauna um especially in in the americas when humans arrived\nso uh you can imagine once you discover tool usage how powerful that would have been and how scary for animals so i\nthink all of those could have been explanations for it you know the interesting thing is that it's a bit\nlike general intelligence too is it's very costly to begin with to have a brain\nand especially a general purpose brain rather than a special purpose one because the amount of energy our brains use i think it's like 20 of the body's\nenergy and it's it's massive and when you're thinking chest one of the funny things that that we used to say is as\nmuch as a racing driver uses for a whole you know formula one race if just playing a game of you know serious high\nlevel chess which you you know you wouldn't think just sitting there um because the brain's using so much uh\nenergy so in order for an animal an organism to justify that there has to be a huge payoff and the problem with with\nhalf a brain or half you know intelligence saying iqs of you know\nof like a monkey brain it's it's not clear you can justify that evolutionary until you get to the human\nlevel brain and so but how do you how do you do that jump it's very difficult which is why i think it's only been done\nonce from the sort of specialized brains that you see in animals to this sort of general purpose chewing powerful brains\nthat humans have um and which allows us to invent the modern modern world um and\nuh you know it takes a lot to to cross that barrier and i think we've seen the same with ai systems which is that uh\nmaybe until very recently it's always been easier to craft a specific solution to a problem like chess than it has been\nto build a general learning system that could potentially do many things because initially uh that system will be way\nworse than uh less efficient than the specialized system so one of the interesting\n"}
{"pod": "Lex Fridman Podcast", "input": "Conscious AI", "output": "quirks of the human mind of this evolved system is that it appears to be\nconscious this thing that we don't quite understand but it seems very\nvery special its ability to have a subjective experience that it feels like something\nto eat a cookie the deliciousness of it or see a color and that kind of stuff do you think in order to solve intelligence\nwe also need to solve consciousness along the way do you think agi systems need to have consciousness in order to\nbe truly intelligent yeah we thought about this a lot actually and um i think that\nmy guess is that consciousness and intelligence are double dissociable so you can have one without the other both\nways and i think you can see that with consciousness in that i think some animals and pets if you have a pet dog\nor something like that you can see some of the higher animals and dolphins things like that are uh have\nself-awareness and uh very sociable um seem to dream um you know those kinds of\na lot of the traits one would regard as being kind of conscious and self-aware um and but yet they're not that smart\nright uh so they're not that intelligent by by say iq standards or something like that yeah it's also possible that our\nunderstanding of intelligence is flawed like putting an iq to it sure maybe the thing that a dog can do\nis actually gone very far along the path of intelligence and we humans are just able to\nplay chess and maybe write poems right but if we go back to the idea of agi and general intelligence you know dogs are\nvery specialized right most animals are pretty specialized they can be amazing at what they do but they're like kind of elite sports sports people or something\nright so they do one thing extremely well because their entire brain is is optimized they have somehow convinced\nthe entirety of the human population to feed them and service them so in some way they're controlling yes exactly well\nwe co-evolved to some crazy degree right uh including the the way the dogs you know even even wag their tails and\ntwitch their noses right we find we're finding inexorably cute yeah um but i think um you can also see intelligence\non the other side so systems like artificial systems that are amazingly smart at certain things like maybe\nplaying go and chess and other things but they don't feel at all in any shape or form conscious in the way that you\nknow you do to me or i do to you and um and i think actually\nbuilding ai is uh these intelligent constructs uh is one of the best ways to explore the\nmystery of consciousness to break it down because um we're going to have devices that are\npretty smart at certain things or capable of certain things but potentially won't have any semblance of\nself-awareness or other things and in fact i would advocate if there's a choice building systems in\nthe first place ai systems that are not conscious to begin with uh are just tools um until we understand them better\nand the capabilities better so on that topic just not as the ceo of deep mind\njust as a human being let me ask you about this one particular anecdotal evidence of the google engineer\nwho made a comment or believed that there's some aspect of a language model\nthe lambda language model that exhibited sentience so you said you believe there might be a\nresponsibility to build systems that are not essential and this experience of a particular engineer i think i'd love to\nget your general opinion on this kind of thing but i think it will happen more and more and more which uh not when engineers but when\npeople out there that don't have an engineering background start interacting with increasingly intelligent systems\nwe anthropomorphize them they they start to have deep impactful\num interactions with us in a way that we miss them yeah when they're gone and\nwe sure feel like they're living entities self-aware entities and maybe even we project sentience onto\nthem so what what's your thought about this particular uh system was is uh\nhave you ever met a language model that's sentient no i no no what do you make of the case\nof when you kind of feel that there's some elements of sentience to this system yeah so this is you know\nan interesting question and uh uh obviously a very fundamental one so first thing to say is i think that\nnone of the systems we have today i i would say even have one iota of uh semblance of consciousness or sentience\nthat's my personal feeling interacting with them every day so i think that's way premature to be discussing what that\nengineer talked about i appreciate i think at the moment it's more of a projection of the way our own minds work\nwhich is to see uh uh uh sort of purpose and direction\nin almost anything that we you know our brains are trained to interpret uh agency basically in things uh even the\nan inanimate thing sometimes and of course with a a language system because language is so fundamental to\nintelligence that's going to be easy for us to anthropomorphize that i mean back in the day even the first uh\nyou know the dumbest sort of template chatbots ever eliza and and and and the ilk of the original chatbots back in the\n60s fooled some people under certain circumstances right they pretended to be a psychologist so just basically rabbit\nback to you the same question you asked it back to you um and uh some people believe that so i\ndon't think we can this is why i think the turing test is a little bit flawed as a formal test because it depends on the sophistication of the of the judge\num whether or not they are qualified to make that distinction so\ni think we should uh talk to you know the the top philosophers about this people like daniel dennett and uh david\nchalmers and others who've obviously thought deeply about consciousness of course consciousness itself hasn't been\nwell there's no agreed definition if i was to you know uh speculate about that\nuh you know i kind of the definite the working definition i like is it's the way information feels when you know it\ngets processed i think maybe max tegmark came up with that i like that idea i don't know if it helps us get towards any more operational thing but but it's\nit's it's i think it's a nice way of viewing it um i think we can obviously see from neuroscience certain\nprerequisites that are required like self-awareness i think is necessary but not sufficient component this idea of a\nself and other and set of coherent preferences that are coherent over time\nyou know these things are maybe memory um these things are probably needed for a sentient or conscious being um but but\nthe reason that the difficult thing i think for us when we get and i think this is a really interesting philosophical debate is when we get\ncloser to agi and and and you know and and much more powerful systems than we have today\num how are we going to make this judgment and one way which is the turing test is sort of a behavioral judgment is\nis the system exhibiting all the behaviors um that a human sentient uh or\na sentient being would would would exhibit um is it answering the right questions is it saying the right things is it indistinguishable from a human um\nand so on but i think there's a second thing that makes us as humans regard each other as\nsentient right why do we why do we think this and i debated this with daniel dennett and i think there's a second reason that's often overlooked which is\nthat we're running on the same substrate right so if we're exhibiting the same behavior uh more or less as humans and\nwe're running on the same you know carbon-based biological substrate the squishy you know few pounds of of flesh\nin our skulls then the most parsimonious i think explanation is that you're\nfeeling the same thing as i'm feeling right but we will never have that second part the substrate equivalence with a\nmachine right so we will have to only judge based on the behavior and i think the substrate equivalence is a critical part\nof why we make assumptions that we're conscious and in fact even with with animals high-level animals why we think\nthey might be because they're exhibiting some of the behaviors we would expect from a sentient animal and we know they're made of the same things\nbiological neurons so we're gonna have to come up with explanations uh or models of the gap\nbetween substrate differences between machines and humans did to get anywhere\nbeyond the behavioral but to me sort of the practical question is very interesting and very important when\nyou have millions perhaps billions of people believing that you have ascension ai believing what that google engineer\nbelieved which i just see is an obvious very near-term future thing certainly on\nthe path to agi how does that change the world what's the responsibility of the ai\nsystem to help those millions of people and also what's the ethical thing because\nyou can you can make a lot of people happy by creating a meaningful deep experience\nwith a system that's faking it before it makes it yeah and i i don't\nis a are we the right or who is to say what's the right thing to do should ai\nalways be tools like why why why are we constraining ais to always be tools as\nopposed to friends yeah i think well i mean these are you know you know fantastic\nquestions and and also critical ones and we've been thinking about this uh since\nthe start of d minor before that because we planned for success and you know how how you know you know however remote\nthat looked like back in 2010 and we've always had sort of these ethical considerations as fundamental at\ndeepmind um and my current thinking on the language models is and and large models is they're not ready we don't\nunderstand them well enough yet um and you know in terms of analysis tools and and guard rails what they can and can't\ndo and so on to deploy them at scale because i think you know there are big still ethical questions like should an\nai system always announce that it is an ai system to begin with probably yes um\nit what what do you do about answering those philosophical questions about the feelings uh people may have about ai\nsystems perhaps incorrectly attributed so i think there's a whole bunch of research that needs to be done first um\nto responsibly before you know you can responsibly deploy these systems at scale that would be at least be my\ncurrent position over time i'm very confident we'll have those tools like interpretability\nquestions um and uh analysis questions uh and then\nwith the ethical quandary you know i think there it's important to\nuh look beyond just science that's why i think philosophy social sciences even\ntheology other things like that come into it where um what you know arts and humanities what what does it mean to be\nhuman and the spirit of being human and and to enhance that and and the human condition right and allow us to\nexperience things we could never experience before and improve the the overall human condition and humanity\noverall you know get radical abundance solve many scientific problems solve disease so this is the era i think this\nis the amazing era i think we're heading into if we do it right um but we've got to be careful we've already seen with\nthings like social media how dual use technologies can be misused by firstly\nby by by bad you know p bad actors or naive actors or crazy actors right so\nthere's that set of just the common or garden misuse of existing dual use technology and then of course there's an\nadditional uh uh thing that has to be overcome with ai that eventually it may have its own agency so it could be uh uh\nuh good or bad in in in of itself so i think these questions have to be approached very carefully um using the\nscientific method i would say in terms of hypothesis generation careful control testing not live a b testing out in the\nworld because with powerful dual technologies like ai if something goes wrong it may cause you\nknow a lot of harm before you can fix it um it's not like a you know an imaging app or game app where you know that if\nif something goes wrong it's relatively easy to fix and and the harm's relatively small so i think\nit comes with you know the the the usual uh cliche of like with a lot of power\ncomes a lot of responsibility and i think that's the case here with things like ai given the the enormous\nopportunity in front of us and i think we need a lot of voices uh and as many\ninputs into things like the design of the systems and the values they should have and what goals should\nthey be put to um i think as wide a group of voices as possible beyond just the technologies is needed uh to input\ninto that and to have a say in that especially when it comes to deployment of these systems which is when the\nrubber really hits the road it really affects the general person in the street rather than fundamental research and\nthat's why i say i think as a first step it would be better if we have the choice to build\nthese systems as tools to give and i'm not saying that it should never they should never go beyond tools because of\ncourse the potential is there um for it to go way beyond just tools uh but um i\nthink that would be a good first step in order for us to you know allow us to carefully experiment understand what\nthese things can do so the leap between tool to sentient entity being is one\nshould take very careful yes let me ask a dark personal question so you're one of the most brilliant\n"}
{"pod": "Lex Fridman Podcast", "input": "Power", "output": "people in the ag community also one of the most kind and uh if i may say sort of loved people\nin the community that said uh creation of a super intelligent ai\nsystem would be one of the most powerful things in the world tools or otherwise\nand again as the old saying goes power corrupts and absolute power crops absolutely\nyou are likely to be one of the people\ni would say probably the most likely person to be in the control of such a system\ndo you think about the corrupting nature of power when you talk about these kinds of systems that\num as all dictators and people have caused atrocities in the past always think they're doing good\nbut they don't do good because the powers polluted their mind about what is good and what is evil do you think about\nthis stuff or are we just focused on language modeling no i think about them all the time and you know i think\nwhat are the defenses against that i think one thing is to remain very grounded and sort of humble uh no matter\nwhat you do or achieve and i try to do that i might you know my best friends are still my set of friends from my\nundergraduate cambridge days my family's you know and and friends are very important\num i've always i think trying to be a multi-disciplinary person it helps to keep you humble because no matter how\ngood you are at one topic someone will be better than you at that and it and always relearning a new topic again from\nscratch is or new field is very humbling right so for me that's been biology over the last five years you know huge area\ntopic and and and it's been and i just love doing that but it helps to keep you grounded like it keeps you open-minded\nand and then the other important thing is to have a really good amazing set of uh people around you at your company or\nyour organization who are also very ethical and grounded themselves and help to keep you that way\nand then ultimately just to answer your question i hope we're going to be a big part of of birthing ai and that being\nthe greatest benefit to humanity of any tool or technology ever and and getting us into a world of radical abundance and\ncuring diseases and and and solving many of the big challenges we have in front of us and then ultimately you know help the\nultimate flourishing of humanity to travel the stars and find those aliens if they are there and if they're not there find out why they're not there\nwhat what is going on here in the universe um this is all to come and and that's what i've always dreamed about um\nbut i don't think i think ai is too big an idea it's not going to be uh there'll be a certain set of pioneers who get\nthere first i hope we're in the vanguard so we can influence how that goes and i think it matters who builds who which which\ncultures they come from and what values they have uh the builders of ai systems because i think even though the ai\nsystem is going to learn for itself most of its knowledge there'll be a residue in the system of the culture and the\nvalues of the creators of the system um and there's interesting questions to to discuss about that geopolitically you\nknow different cultures as we're in a more fragmented world than ever unfortunately i think in terms of global cooperation\nwe see that in things like climate where we can't seem to get our act together uh globally to cooperate on these pressing\nmatters i hope that will change over time perhaps you know if we get to an era of radical abundance we don't have\nto be so competitive anymore maybe we can be more cooperative if resources aren't so scarce it's true\nthat in terms of power corrupting and leading to destructive things it seems that some of\nthe atrocities of the past happen when there's a significant constraint on resources i think that's\nthe first thing i don't think that's enough i think scarcity is one thing that's led to competition destruct you know sort of zero sum game thinking i\nwould like us to all be in a positive sum world and i think for that you have to remove scarcity i don't think that's enough unfortunately to get world peace\nbecause there's also other corrupting things like wanting power over people and this kind of stuff which is not\nnecessarily satisfied by by just abundance but i think it will help um\nand i think uh but i think ultimately ai is not going to be run by any one person or one organization i think it should\nbelong to the world belong to humanity um and i think maybe many there'll be many ways this will happen and\nultimately um everybody should have a say in that do you have advice\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "for uh young people in high school and college maybe um\nif they're interested in ai or interested in having a big impact on the world what they should\ndo to have a career they can be proud of her to have a life they can be proud of i love giving talks to the next\ngeneration what i say to them is actually two things i i think the most important things to learn about and to\nfind out about when you're when you're young is what are your true passions is first of all there's two things one is\nfind your true passions and i think you can do that by the way to do that is to explore as many things as possible when\nyou're young and you you have the time and you and you can take those risks um i would also encourage people to look at\nthe finding the connections between things in a unique way i think that's a really great way to find a passion second thing\ni would say advise is know yourself so spend a lot of time\nunderstanding how you work best like what are the optimal times to work what are the optimal ways that you study um\nwhat are your how do you deal with pressure sort of test yourself in various scenarios and try and improve your\nweaknesses but also find out what your unique skills and strengths are and then\nhone those so then that's what will be your super value in the world later on and if you can then combine those two\nthings and find passions that you're genuinely excited about that intersect with what your unique strong skills are\nthen you're you know you're on to something incredible and and you know i think you can make a huge difference in the world so let me ask about know\nyourself this is fun this is fun quick questions about day in the life the\nperfect day the perfect productive day in the life of demise's house yeah maybe uh maybe these days you're um\nthere's a lot involved yeah it may be a slightly younger you could focus on a demonstration\nproject maybe um how early do you wake up are you night owl do you wake up early in the morning\nwhat are some interesting habits uh how many dozens of cups of coffees do you drink a day what's the computer um\nthat you use uh what's the setup how many screens what kind of keyboard are we talking uh\nemacs vim are we talking something more modern so it's a bunch of those questions so maybe uh day in the life\nwhat what's the perfect day involved well these days it's quite different from say 10 20 years ago back 10 20\nyears ago it would have been you know a whole day of research individual research or\nprogramming doing some experiment neuroscience computer science experiment reading lots of research papers uh and\nthen perhaps at night time you know um reading science fiction books or or uh\nplaying uh some games but lots of focus so like deep focused work on whether\nit's uh programming or reading research paper yes yes so that would be a lot of debrief you know uh focused work these\ndays for the last sort of i guess you know five to ten years i've actually got quite a structure that works very well\nfor me now which is that um i'm a night complete night out always have been so i optimized for that so you know i get you\nknow i basically do a normal day's work get into work about 11 o'clock and sort of do work to about seven uh in the\noffice uh and i will arrange back-to-back meetings for the entire time of that and with as many me as many\npeople as possible so that's my collaboration management part of the day then i go home uh spend time with the\nfamily and friends uh have dinner uh uh relax a little bit and then i start a\nsecond day of work i call it my second day work around 10 pm 11 p.m and that's the time till about the small hours of\nthe morning four five in the morning where i will do my thinking and reading\na research writing research papers um sadly don't have time to code anymore\nbut it's it's not efficient to to do that uh these days uh given the amount of time i have um but that's when i do\nyou know maybe do the long kind of stretches of of thinking and planning and then probably you know using using\nemail or other things i would set i would fire off a lot of things to my team to deal with the next morning for\nactually thinking about this overnight we should go for this project or arrange this meeting the next day when you're\nthinking through a problem are you talking about a sheet of paper or the patent pen is there some independent structure yeah i like processes i still\nlike pencil and paper best for working out things but um these days it's just so efficient to read research papers\njust on the screen i still often print them out actually i still prefer to mark out things and i find it goes into\nthe brain quick better and sticks in the brain better when you're you're still using physical pen and pencil and paper\nso you take notes with the i have lots of nodes electronic ones and also um whole stacks of notebooks that\num that i use at home yeah on some of these most challenging next steps for example stuff\nnone of us know about that you're working on you're thinking there's some deep thinking required\nthere right like what what is the right problem what is the right approach because you're gonna have to invest a\nhuge amount of time for the whole team they're going to have to pursue this thing what's the right way to do it is\nis rl going to work here or not yes um what's the right thing to try what's the right benchmark to use yeah we need to\nconstruct a benchmark from scratch all those kinds of things yes so i think all those kind of things in the night time\nphase but also much more um i find i've always found the quiet hours of the\nmorning um when everyone's asleep it's super quiet outside um i love that time\nit's the golden hours like between like one and three in the morning um put some music on some inspiring music on and\nthen um think these deep thoughts so that's when i would read you know my philosophy books and uh spinoza's my you\nknow recent favorite can all these things i i i you know read about a great uh uh\na scientist of history how they did things how they thought things so that's when you do all your create that's when\ni do all my creative thinking and it's good i think i think people recommend you know you do your your your sort of\ncreative thinking in one block and the way i organize the day that way i don't get interrupted because obviously no one\nelse is up uh at those times so i can i can go uh you know as i can sort of get\nsuper deep and super into flow the other nice thing about doing it night time wise is if i'm really uh onto something\nor i've i've got really deep into something i can choose to extend it and i'll go into six in the morning whatever\nand then i'll just pay for it the next day yeah cause i'll be a bit tired and i won't be my best but that's fine i can\ndecide looking at my schedule the next day that and given where i'm at with this particular thought or creative idea\nthat i'm going to pay that cost the next day so so i think that's that's more flexible than morning people who do that\nyou know they get up at four in the morning they can also do those golden hours then but then their start of their schedule day starts at breakfast you\nknow 8 a.m whatever they have their first meeting and then it's hard you have to reschedule a day if you're in flow yeah that's going to be i don't\nhave to see that special threat of thoughts that the you're too passionate about you that\nthis is where some of the greatest ideas could potentially come is when you just lose yourself late into yeah\nand for the meetings i mean you're loading in really hard problems in a very short amount of time so you have to\ndo some kind of first principles thinking here it's like what's the problem what's the state of things what's the right next step yes you have\nto get really good at context switching which is one of the hardest things because especially as we do so many\nthings if you include all the scientific things we do scientific fields we're working in these are entire you know\ncomplex fields in themselves and you you have to sort of keep up to abreast of that but i enjoy it i've always been uh\na sort of generalist in a way and that's actually what happened with my games career after chess i i i one of the\nreasons i stopped playing chess was that i got into computers but also i started realizing there were many other great games out there to play too so\ni've always been that way inclined multidisciplinary and there's too many interesting things in in the world to spend all your time just on one thing\nso you mentioned spinoza gotta ask the big ridiculously big question about life\n"}
{"pod": "Lex Fridman Podcast", "input": "Meaning of life", "output": "what do you think is the meaning of this whole thing uh why are we humans here you've already\nmentioned that perhaps the universe created us is that why you think we're here\nto understand how the universe yeah i think my answer to that would be and at least the the life i'm living is to gain\nand uh to gain and understand the knowledge you know to gain knowledge and understand the universe that's what i\nthink uh i can't see any higher purpose than that if you think back to the classical greeks you know the virtue of\ngaining knowledge it's uh i think it's that it's one of the few true virtues is to understand um the world around us and\nthe context and humanity better and um and i think if you do that you become more compassionate and more\nunderstanding yourself and and more tolerant and all these i think all these other things may flow from that and to\nme you know understanding the nature of reality that is the biggest question what is going on here is sometimes the\ncolloquial way i say what is really going on here uh it's so mysterious i feel like we're\nin some huge puzzle and and it's but the world is also seems to be the universe\nseems to be structured in a way you know why is it structured in a way that science is even possible that you know\nmethods the scientific method works things are repeatable um it feels like it's almost structured\nin a way to be conducive to gaining knowledge so i feel like and you know why should computers be even possible\nisn't that amazing that uh computational electronic devices can can can can be\npossible and they're made of sand our most you know common element that we have you know silicon that on the on the\nearth's crust they could be made of diamond or something then we would have only had one computer yeah right so it's\na lot of things are kind of slightly suspicious to me it sure as heck sounds this puzzle sure sounds like something\nwe talked about earlier what it takes to to design a game that's really fun to play for prolonged\nperiods of time and it does seem like this puzzle like you mentioned the more you learn about\nit the more you realize how little you know so it humbles you but excites you by the\npossibility of learning more it's one heck of a one heck of a puzzle we got going on here um so like i mentioned of\nall the people in the world you're very likely to be the one who creates the agi\nsystem um that achieves human level intelligence and goes beyond it so if\nyou got a chance and very well you could be the person that goes into the room with the system and have a conversation\nmaybe you only get to ask one question if you do what question would you ask her\ni would probably ask um what is the true nature of reality i think that's the question i don't know\nif i'd understand the answer because maybe it would be 42 or something like that but um that's the question i would\nask and then there'll be a deep sigh from the systems like all right how do i\nexplain to the excuse me exactly all right let me i don't have time to explain uh maybe i'll draw you a\npicture that it is i mean how do you even begin um\nto answer that question well i think it would um what would you what would you think the answer could\npossibly look like i think it could it could start looking like uh\nuh more fundamental explanations of physics would be the beginning you know more careful specification of that\ntaking you walking us through by the hand as to what one would do to maybe prove those things out maybe giving you\nglimpses of what things you totally missed in the physics of today exactly just here here's glimpses of no\nlike there's a much uh a much more elaborate world or a much simpler world or something\na much deeper maybe simpler explanation yes of things right than the standard model of physics which we know doesn't\nwork but we still keep adding to so um and and that's how i think the beginning\nof an explanation would look and it would start encompassing many of the mysteries that we have wondered about for thousands of years like you know\nconsciousness uh life and gravity all of these things yeah giving us a glimpses of\nexplanations for those things yeah well um damas dear one of the special\nhuman beings in this giant puzzle of ours and it's a huge honor that you would take a pause from the bigger\npuzzle to solve this small puzzle of a conversation with me today it's truly an honor and a pleasure thank you thank you\ni really enjoyed it thanks lex thanks for listening to this conversation with demas establish to\nsupport this podcast please check out our sponsors in the description and now let me leave you with some words\nfrom edskar dykstra computer science is no more about computers\nthan astronomy is about telescopes thank you for listening and hope to see\nyou next time\n"}

{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "the following is a conversation with wojciech zaramba co-founder of openai\nwhich is one of the top organizations in the world doing artificial intelligence research and development\nwojciech is the head of language and cogeneration teams building and doing\nresearch on github copilot openai codex and gpt\nthree and who knows four five six n and\nn plus one and he also previously led openai's robotic efforts\nthese are incredibly exciting projects to me that deeply challenge and expand\nour understanding of the structure and nature of intelligence the 21st century\ni think may very well be remembered for a handful of revolutionary ai systems\nand their implementations gpt codex and applications of language models and transformers in general\nto the language and visual domains may very well be at the core of these ai\nsystems to support this podcast please check out our sponsors they're listed in the description\nthis is a lex friedman podcast and here is my conversation with wachek zaremba\n"}
{"pod": "Lex Fridman Podcast", "input": "The Fermi paradox", "output": "you mentioned that sam altman asked about the fermi paradox\nand the people at open ai had really sophisticated interesting answers so that's when you knew this is the right\nteam to be working with so let me ask you about the fermi paradox about aliens\nwhy have we not found overwhelming evidence for aliens visiting earth i don't have a conviction in the answer\nbut rather kind of probabilistic perspective on what might be a let's say possible answers it's also interesting\nthat the question itself even can't touch on the you know your typical\nquestion of what's the meaning of life because like if you assume that like we don't see aliens because they destroy themselves that kind of upwards their\nfocus on making sure that we won't destroy ourselves yeah and at the moment the\nplace where i am actually with my belief and these things also change over the time is i think that we might be alone in the\nuniverse which actually makes life more or less a consciousness life more kind of valuable and that means\nthat we should more appreciate it have you always been alone so what's your intuition about our galaxy our\nuniverse is it just sprinkled with graveyards of intelligent civilizations or are we truly is is life\nintelligent life truly unique at the moment my belief that it is unique but i would say i could also\nyou know there was like some footage released with ufo objects which makes me\nactually doubt my own belief yes yeah i can tell you one crazy answer\nthat i have heard yes so apparently when you look actually at the\nlimits of computation you can compute more if the temperature of the universe would\ndrop down so one of the things that aliens might want to do if they are\ntruly optimizing to maximize amount of compute which you know maybe can lead to or let's say simulations or so it's\ninstead of wasting current entropy of the universe because you know we by living we are actually somewhat wasting\nentropy then you can wait for the universe to cool down such that you have more computation that's kind of a funny\nanswer i'm not sure if i believe in it but that would be one of the reasons why you don't see aliens it's also possible\nsee some people say that maybe there is not that much point in actually going to other galaxies\nif you can go inwards so there is no limits of what could be an experience\nif we could you know connect machines to our brains while there are still some limits if we\nwant to explore universe yeah there could be a lot of ways to go inwards too\nonce you figure out some aspect of physics we haven't figured out yet maybe you can travel to different dimensions i\nmean travel in three-dimensional space may\nnot be the most fun kind of travel there may be like just a huge amount of different ways to travel and it doesn't\nrequire a spaceship going slowly in 3d space space time it also\nfeels you know one of the problems is that speed of light is low and universe is vast yeah and um\nit seems that actually most likely if we want to travel very far then then we would instead of actually\nsending spaceships with humans that wait a lot we would send something similar to what yuri\nmiller is working on these are like a huge uh sail which is at first powered power there is a shot of laser from an\nearth and it can propel it to a quarter of speed of light and uh sail itself\ncontains a few grams of equipment and that might be the way to actually\ntransport matter through universe but then when you think what would it mean for humans it means that\nwe would need to actually put their 3d printer and you know 3d print a human on other planet i don't know play them\nyoutube or let's say or like a pre 3d print like a huge human right away or maybe a womb or so um yeah\nwith our current techniques of archaeology if a civilization was born and died\nlong long enough ago on earth we wouldn't be able to tell and so that makes me really sad\nand so i think about earth in that same way how can we leave some remnants if we\ndo destroy ourselves how can we leave remnants for aliens in the future to discover\nlike here's some nice stuff we've done like wikipedia and youtube do we have it like\nin a satellite orbiting earth with a hard drive like how how do we say\nhow do we back up human civilization uh for the good parts or all of it is good parts\nso that uh it can be preserved longer than our bodies can that's a\nthat's kind of a it's a difficult question it also requires the difficult acceptance of the\nfact that we may die and if we die we may die suddenly as a civilization\nso let's see i think it kind of depends on the cataclysm we have observed in other parts of the universe that births\nof gamma rays these are high energy rays of light that actually can\napparently kill entire galaxy so there might be actually nothing even\nto nothing to protect us from it i'm also when i'm looking actually at the past civilization so it's like aztecs or so\nthey disappear from the surface of the earth and one can ask\nwhy is it the case and the way i'm thinking about it is\nyou know that definitely they had some problem that they couldn't solve and maybe there was a flat and all of a\nsudden they couldn't drink there was no potable water and they all died and\ni think that so far the best solution to such a problems is\ni guess technology so i mean if they would know that you can just boil water and then drink it after then that would\nsave their civilization and even now when we look actually at the current pandemic it seems that once again\nactually science comes to rescue and somehow science increases size of the action space and i think that's a good\nthing yeah but nature has a vastly larger action space but\nstill it might be a good thing for us to keep on increasing action space\nokay looking at past civilizations yes but looking at the destruction of human\ncivilization perhaps expanding the action space will add\nactions that are easily acted upon easily executed and as a\nresult destroy us so let's see i was pondering\n"}
{"pod": "Lex Fridman Podcast", "input": "Systems of government", "output": "why actually even we have negative impact on the globe because you know if you ask every\nsingle individual they would like to have clean air they would like healthy planet but\nsomehow it actually is not the case that as a collective we are not going this direction\ni think that there exists very powerful system to describe what we value that's capitalism it assigns actually monetary\nvalues to various activities at the moment the problem in the current system\nis that there are some things which we value there is no cost assigned to it so even though we value clean air or maybe\nwe also value lack of destruction on the internet or\nso at the moment these quantities you know companies corporations can pollute them uh for\nfree so in some sense\ni wish or like and that's i guess purpose of politics to\nalign the incentive systems and we are kind of maybe even moving in this direction the first issue is even to be\nable to measure the things that we value then we can actually assign the monetary value to them\nyeah and that's so it's getting the data and also probably through technology enabling\npeople to vote and to move money around in a way that is\naligned with their values and that's very much a technology question so like\nhaving one president and congress and voting that happens every four years\nor something like that that's a very outdated idea there could be some technological improvements to\nthat kind of idea so i'm thinking from time to time about these topics but it also feels to me\nthat it's it's a little bit like a it's hard for me to actually make correct predictions what is the\nappropriate thing to do i extremely trust uh sam altman our ceo\non these topics he um okay i'm more on the side of being i guess\nnaive hippie that yeah that's your life philosophy um\nwell like i think self-doubt and uh i think hippie implies optimism those\nthose two things are pretty pretty good way to operate i mean still it is hard for me to actually\nunderstand how the politics works or like uh how this like exactly how the things would play out\nand sam is a really excellent with it what do you think is rarest in the universe you said we might be alone\n"}
{"pod": "Lex Fridman Podcast", "input": "Life, intelligence, and consciousness", "output": "what's hardest to build is another engineering way to ask that life intelligence or consciousness so like\nyou said that we might be alone which is the thing that's hardest to get to\nis it just the origin of life is it the origin of intelligence is it the origin\nof consciousness so um let me at first explain you my kind\nof mental model what i think is needed for life to appear um so\ni imagine that at some point there was this primordial zoop of\namino acids and maybe some proteins in the ocean and you know some proteins were turning\ninto some other proteins through reaction and you can almost think about this\nuh cycle of what turns into what as there is a graph essentially describing which substance turns into some other\nsubstance and essentially life means that all the sudden in the graph has been created a cycle such that the same\nthing keeps on happening over and over again that's what is needed for life to happen and in some sense you can think\nalmost that you have this gigantic graph and it needs like a sufficient number of\nedges for the cycle to appear then um from perspective of intelligence\nand consciousness my current intuition is that they might be quite intertwined first of all it might\nnot be that it's like a binary thing that you have intelligence or consciousness it seems to be a\nmore a continuous component let's see if we look for instance on the even networks\nrecognizing images and people are able to show that the activations of these networks correlate very strongly\nwith activations in visual cortex of some monkeys the same seems to be\ntrue about language models also if you for instance\nlook if you train agent in a 3d world\nat first you know it it it it barely recognizes what is going on over the time it kind of recognizes foreground\nfrom a background over the time it kind of knows where there is a foot and it just follows it\nover the time it actually starts having a 3d perception so it is possible for instance to look inside of the head of\nan agent and ask what would it see if it looks to the right and the crazy thing is you know initially when the agents\nare very trained these predictions are pretty bad over the time they they become better and better you can still\nsee that if you ask what happens when the head is turned by 360 degrees for some\ntime they think that the different thing appears and then at some stage they understand actually that the same\nthing's supposed to appear so they get like a understanding of 3d structure it's also you know very likely that they\nhave inside some level of and of like a symbolic reasoning like they're particularly\nsymbols for other agents so when you look at dota agents they collaborate\ntogether and uh and now they they they have some anticipation of uh if if they would win\nbattle they have some some expectations with respect to other agents i might be you know too much anthropomorphizing\num the the how the things look look for me but then the fact that they\nhave a symbol for other agents and makes me believe that at some stage as the uh you know as they\nare optimizing for skills they would have also symbol to describe themselves this is like a very useful\nsymbol to have and this particularity i would call it like a self-consciousness or self-awareness\nand still it might be different from the consciousness so i guess the the way how\ni'm understanding the word consciousness let's say the experience of drinking a coffee or let's say experience of being\na butt that's the meaning of the word consciousness it doesn't mean to be awake\nyeah it feels it might be also somewhat related to memory and recurrent connections so um\nit's kind of okay if you look at anesthetic drugs they might be\nuh like they essentially they disturb\nbrain waste such that [Music] maybe memory is not not formed\nso there's a lessening of consciousness when you do that correct and so that's one way to intuit what is consciousness\nthere's also kind of another element here it could be that it's\nyou know this kind of self-awareness module that you described\nplus the actual subjective experience is a storytelling module\nthat tells us a story about uh what we're experiencing\nthe crazy thing so let's say i mean in meditation they teach people\nnot to speak story inside of the head and there is also some fraction of population\nwho doesn't have actually narrator i know people who don't have a right narrator and you know they have to use\nexternal people in order to kind of solve tasks that require internal\nnarrator so it seems that it's possible to have the experience without the talk\nwhat are we talking about when we talk about the internal narrator is that the voice when you're like yeah i thought\nthat that that's what you are referring to well i was referring more on the like\nnot an actual voice i meant like there's some kind of\nlike subjective experience feels like it's\nit's fundamentally about storytelling to ourselves it feels like\nlike the feeling is a story that is much\nmuch simpler abstraction than the raw sensory information so it feels like it's a very high level\nabstraction that is useful for me to feel like\nentity in this world most useful aspect of it is that\nbecause i'm conscious i think there's an intricate connection to me not one\nwanting to die so like it's a useful hack to really\nprioritize not dying like those seem to be somehow connected so i'm telling the story of like it's\nrichly feels like something to be me and the fact that me exists in this world i\nwant to preserve me and so that makes it a useful agent hack so i will just refer maybe to the first\npart as you said about the kind of story of describing who you are\ni was thinking about that even so you know obviously i'm i'm i\n"}
{"pod": "Lex Fridman Podcast", "input": "GPT language model", "output": "like thinking about consciousness uh i like thinking about the ai as well and i'm trying to see analogies of these\nthings in ai what would it correspond to so um\nyou know openly i trained a a model called gpt\nwhich can generate a pretty amusing text on arbitrary topic\nand um and one way to control gpd is uh by putting into prefix at the\nbeginning of the text some information what would be the story about you can have even chat with uh\nyou know with gpt by saying that the chat is with lex or elon musk or so and gpt would just\npretend to be you or elon musk or so and it almost feels that this uh\nstory that we give ourselves to describe our life it's almost like a\nthings that you put into context of gpt yeah the primary it's the and but the the context we provide to gpt\nis uh is multimodal it's so gpt itself is multimodal gpt itself uh hasn't learned\nactually from experience of single human but from the experience of humanity it's a chameleon you can turn it into\nanything and in some sense by providing context uh it you know\nbehaves as the thing that you wanted it to be and it's interesting that the you know people have a stories of who\nthey are and as i said these stories they help them to operate in the world\nbut it's also you know interesting i guess various people find it out through meditation or so that\nthere might be some patterns that you have learned when you were a kid that actually are not serving you anymore\nand you also might be thinking that that's who you are and that's actually just the story\nyeah so it's a useful hack but sometimes it gets us into trouble it's a local optima\n"}
{"pod": "Lex Fridman Podcast", "input": "Engineering consciousness", "output": "you wrote that stephen hawking he tweeted stephen hawking asked what breathes fire into equations which meant\nwhat makes given mathematical equations realize the physics of a universe\nsimilarly i wonder what breathes fire into computation what makes given computation\nconscious okay so how do we engineer consciousness\nhow do you breathe fire and magic into the machine so\nit seems clear to me that not every computation is conscious i mean you can\nlet's say just keep on multiplying one matrix over and over again and my gigantic matrix you can put a lot of\ncomputation i don't think it would be conscious so in some sense the question is what are the computations which could be\nconscious uh i mean so one assumption is that it has to do purely with\ncomputation that you can abstract away matter and other possibilities that it's very important was the realization of\ncomputation that it has to do with some uh uh force fields or so and they bring\nconsciousness at the moment my intuition is that it can be fully abstracted that way so in case of computation you can\nask yourself what are the mathematical objects or so that could bring such a properties so for instance\nif we think about the models uh ai models then what they truly\ntry to do or like models like gpt is uh\nyou know they try to predict a next word or so and this turns out to be\nequivalent to compressing text\nand because in some sense compression means that you learn the model of reality and you\nhave just to uh remember where are your mistakes the better you are in predicting the\nand and in some sense when we look at our experience also when you look for instance the car driving you know in\nwhich direction it will go you are good like a in prediction and um you know it might be the case that the\nconsciousness is intertwined with compression it might be also the case that self-consciousness\nhas to do with compressor trying to compress itself so um okay i was just wondering what are the\nobjects in you know mathematics or computer science which are mysterious\nthat could uh that that could have to do with consciousness and then i thought um\nyou know you you see in uh mathematics there is something called cadal theorem\nwhich means okay you have if you have sufficiently complicated mathematical system it is possible to point the\nmathematical system back on itself in computer sense there is uh something called helping problem it's it's\nsomewhat similar construction so i thought that you know if we believe that that the that\nunder assumption that consciousness has to do with uh with compression uh\nthen you could imagine that the the as you keep on compressing things then at\nsome point it actually makes sense for the compressor to compress itself metacompression yeah consciousness is\nmetacompression that's uh that's and i and an idea\nand in some sense you know the creation of it thank you so uh but do you think if we think of a\ntouring machine a universal touring machine can that achieve consciousness\nso is there some thing beyond our traditional definition of computation that's required so it's a\nspecific computation and i said this computation has to do with compression and\nthe compression itself maybe other way of putting it is like you are internally creating the model of reality\nin order like a it's like a you try inside to simplify reality in order to predict what's going to happen\nand that also feels somewhat similar to how i think actually about my own conscious\nexperience so clearly i don't have access to reality the only access to reality is through you know cable going\nto my brain and my brain is creating a simulation of reality and i have access to the simulation of reality\n"}
{"pod": "Lex Fridman Podcast", "input": "Is there an algorithm for intelligence?", "output": "are you by any chance uh aware of uh the harder prize marcus hutter\nhe he made this prize for compression of wikipedia pages\nand there's a few qualities to it one i think has to be perfect compression which makes\ni think that little quirk makes it much less um applicable to the general task of\nintelligence because it feels like intelligence is always going to be messy uh\nlike perfect compression is feels like it's not the right goal but it's\nnevertheless a very interesting goal so for him intelligence equals compression\nand so the smaller you make the file given a large wikipedia page\nthe more intelligent the system has to be yeah that makes sense so you can make perfect compression if you store errors\nand i think that actually what he meant is you have algorithm plus errors and by the way hooter hatter is a he was pa uh\nphd advisor of shenleck who is the mind uh uh deep mind co-founder yeah yeah so\nthere's an interesting and now he's a deep mind there's an interesting uh network of people he's\none of the people that i think seriously took on the task of what would\nan agi system look like i think for a longest time\nthe question of agi was not taken seriously or rather rigorously\nand he did just that like mathematically speaking what would the model look like\nif you remove the constraints of it having to be\nhaving to have a reasonable amount of memory reasonable amount of running time complexity uh\ncomputation time what would it look like and essentially it's it's a half math half philosophical discussion\nof uh how would like a reinforcement learning type of framework look like for an agi yeah so he developed a framework\neven to describe what's optimal with respect to reinforcement learning like there is a theoretical framework which\nis as you said under assumption there is infinite amount of memory and compute and there was actually one person before his name\nis solomonov hutter extended amount of work to reinforcement learning\nbut there exists a theoretical algorithm which is optimal\nalgorithm to build intelligence and i can actually explain you the algorithm yes\nlet's go let's go so the task itself can i just pause\nhow absurd it is for brain in a skull trying to explain the algorithm for intelligence just go\nahead it is pretty crazy it is pretty crazy that you know the brain itself is actually so small and it can ponder\nhow to design algorithms that optimally solve the problem of intelligence okay all right so what's the algorithm so\nlet's see so first of all the task itself is described as\nyou have infinite sequence of zeros and ones okay you read n bits and you are about\nto predict n plus one bit so that's the task and you could imagine that every task could be casted as such\na task so if for instance you have images and labels you can just turn every image into sequence of zeros and\nones then label you concatenate labels and you and that that's actually the the\nand you could you could start by having training data first and then afterwards you have test data\nso theoretically any problem could be casted as a problem of predicting zeros\nand ones on this infinite type so um so let's say you read already n bits and\nyou want to predict n plus one bit and i will ask you to write\nevery possible program that generates these end bits okay so\nand you can have you you choose programming language it can be in python or c\nand the difference between programming languages might be there is a difference by constant\nasymptotically your predictions will be equivalent so you you read and beats you enumerate\nall the programs that produce these and end bits in their output and then in order to predict n plus one\nbit you actually weight the programs according to their length\nand there is like some specific formula how you weight them and then the n plus\none bit prediction is the prediction uh from each of this program according to that weight\nlike statistically you statistically pick so the smaller the program the more likely you you are to pick the its\noutput so uh that's that algorithm is grounded\nin the hope or the intuition that the simple answer is the right one it's a formalization of\nit yeah um it also means like if you would ask the question after\nhow many years would you know sun explode\nyou can say it's more likely the answer is to some power because it's a shorter program\nyeah and then other well i don't have a good intuition about\nhow different the space of short programs are from the space of large programs\nlike what is the universe where short programs uh like run things\nuh so as i said the things have to agree with end beats so even if you have\nyou you need to start okay if if you have very short program and they're like uh still some as if it's not perfect\nwith prediction of n bits you have to start errors what are the errors and that gives you the full program that\nagrees on end beats oh so you don't agree perfectly with the end bits and you store\nthat's like a longer a longer program slightly longer program because it contains these extra bits of\nerrors that's fascinating what's what's your intuition about\nthe the programs that are able to do cool stuff like intelligence and consciousness are they\nuh perfectly like is is it uh is there if then statements in them so\nlike is there a lot of exceptions that they're storing so um you could imagine if there would be tremendous amount of\nif statements yeah then they wouldn't be that short in case of neural networks you could imagine that\nwhat happens is uh they when you start with an uninitialized\nneural network uh it stores internally many possibilities how the\nhow the problem can be solved and sgd is kind of magnifying some some\nsome paths which are slightly similar to the correct answer so it's\nkind of magnifying correct programs and in some sense hdd is a search algorithm\nin the program space and the program space is represented by uh you know kind\nof the wiring inside of the neural network and there's like an insane number of ways how that features can be\ncomputed let me ask you the high level basic question that's not so basic\n"}
{"pod": "Lex Fridman Podcast", "input": "Neural networks and deep learning", "output": "what is deep learning is there a way you'd like to think of it that is different than like a generic\ntextbook definition the thing that i hinted just a second ago is maybe the uh closest to how i'm\nthinking these days about um deep learning so now the statement is\nuh neural networks can represent some programs uh it seems that various modules that we\nare actually adding up to are like a you know we we want networks to be deep because we we want multiple steps of the\ncomputation and and deep learning provides the way to\nrepresent space of programs which is searchable and it's searchable with stochastic gradient descent so we have\nan algorithm to search over a humongous number of programs and gradient descent kind of bubbles up\nthe things that are tend to give correct answers so a neural network\nwith a with fixed weights that's optimized do you think of that as a\nsingle program um so there is a work by christopher olach where he\nso he works on interpretability of neural networks and he was able to\nuh to identify inside of the neural network for instance a detector of a wheel for a\ncar or the detector of a mask for a car and then he was able to separate them out and assemble them uh together using\na simple program uh for the detector for a car detector that's like uh if you\nthink of traditionally defined programs that's like a function within a program that this particular neural network was\nable to find and you can tear that out just like you can copy and paste from stack overflow\nthat so uh any program is a composition of smaller programs\nyeah i mean the nice thing about the neural networks is that it allows the things to be more fuzzy than in case of\nprograms in case of programs you have this like a branching this way or that way and the\nneural networks they they have an easier way to to be somewhere in between or to share\nthings what to use the most beautiful or surprising idea in deep learning\nin the utilization of these neural networks which by the way for people who are not familiar\nneural networks is a bunch of uh what would you say it's inspired by the human brain there's neurons there's\nconnection between those neurons there's inputs and there's outputs and there's millions or billions of those neurons\nand the learning happens uh by adjusting the weights on the edges\nthat connect these neurons thank you for giving definition that i supposed to do it but i guess you have\nenough empathy to listeners to actually know that that might be useful no that's like\nso i'm asking plato of like what is the meaning of life he's not going to answer\nyou're being philosophical and deep and quite profound talking about the space of programs which is just very\ninteresting but also for people who are just not familiar with the hell we're talking about when we talk about deep\nlearning anyway sorry what is the most beautiful or surprising idea to you in in um in\nall the time you've worked at deep learning and you worked on a lot of fascinating projects\napplications of neural networks it doesn't have to be big and profound it can be a cool trick yeah i mean i'm\nthinking about the trick but like it's still amusing to me that it works at all yeah that let's say that the extremely\nsimple algorithm stochastic gradient descent which is something that i would be able you know to derive on the piece\nof paper to high school student uh when put at the ins at the scale of you know thousands\nof machines actually uh can create the behaviors we which we called kind of\nhuman like behaviors so in general any applications to cast a gradient descent to neural networks is\nis amazing to you so that or is there a particular application in natural language\nreinforcement learning uh and also would you attribute\nthat success too is it just scale what profound insight can we take from\nthe fact that the thing works for gigantic\nuh sets of variables i mean the interesting thing is these algorithms they were\ninvented uh decades ago and people actually\ngave up on the idea yeah and um you know back then they thought that we\nneed profoundly different algorithms and they spent a lot of cycles on very different algorithms and i believe that\nyou know we have seen that various various innovations that say like transformer or or dropout or so they can\nuh you know pass the help but it's also remarkable to me that this algorithm\nfrom 60s or so or i mean you can even say that the gradient descent was invented by leibniz\nin i guess 18th century or so that actually is the core of learning\nin the past people are it's almost like a out of the maybe an\nego people are saying that it cannot be the case that such a simple algorithm is there you know\nuh could solve complicated problems so they were in search for the\nother algorithms and as i'm saying like i believe that actually we are in the game where there is there are actually\nfrankly three levels there is compute there are algorithms and there is data and if we want to build intelligent\nsystems we have to pull all three levers and they are actually multiplicative\nand it's also interesting so you ask is it only compute people internally they did the studies\nto determine how much gains they were coming from different levels and so far we have seen that more gains came from\ncompute than algorithms but also we are in the world that in case of compute there is a kind of you know exponential\nincrease in funding and at some point it's impossible to invest more it's impossible to you know\ninvest 10 trillion dollars because we are speaking about that let's say all taxes in u.s\nuh but you're talking about money there could be innovation in the compute that's that's true as\nwell so i mean they're like a few pieces so one piece is human brain is an\nincredible super computer [Music] and they're like a\nit it has 100 trillion parameters or like a if you try to count\nvarious quantities in the brain there are like a neurons synapses that small number of neurons there is a lot of\nsynapses yeah it's unclear even how to map synapses\nto two parameters of neural networks but it's clear that there are many more yeah\nso it might be the case that our networks are still somewhat small\nit also might be the case that they are more efficient than brain or less efficient by some by some huge factor\ni also believe that there will be like a you know at the moment we are at the stage that the these neural networks\nthey require 1000x or like a huge factor of more data than humans do and it will\nbe a matter of there will be algorithms that\nvastly decrease sample complexity i believe so but the place where we are heading today is dark domains which\ncontains million x more data and even though computers might be\n1 000 times slower than humans in learning that's not the problem okay for instance\ni believe that it should be possible to create super human therapies\nuh by uh and and then they're like even simple steps of of doing what of of doing it\nand you know that the core reason is there is just machine will be able to read way more\ntranscripts of therapies and then it should be able to speak simultaneously with many more people and it should be\npossible to optimize it uh all in parallel and well there's now you're touching on\nsomething i deeply care about and think is way harder than we imagined um\nwhat's the goal of a therapist what's it called therapies so okay so one goal now this is\nterrifying to me but there's a lot of people that contemplate suicide suffer from\ndepression and they could significantly be helped with therapy\nand the idea that an ai algorithm might be in charge of that\nit's like a life and death task it's uh the stakes are high\nso one goal for a therapist whether human or ai\nis to prevent suicide ideation to prevent suicide how do you achieve that\nso let's see so to be clear i don't think that the current models are good enough for such\na task because it requires insane amount of understanding and patty and the models are far from this place but it's\nbut do you think that understanding empathy that signal is in the data um i\nthink there is some signal in the data yes i mean there are plenty of transcripts of conversations\nand it is possible to it is possible from it to understand personalities it is possible from it to\nunderstand uh if conversation is a friendly uh amicable uh\nantagonistic it is i believe that the you know given the fact that the models that we train now\nthey can they can have they are chameleons that they can have\nany personality they might turn out to be better in understanding uh personality of other people than\nanyone else and they feel pathetic to be empathetic yeah interesting uh but i wonder if there's\nsome level of multiple modalities required\nto be able to be empathetic of the human experience whether language is not enough to\nunderstand death to understand fear to understand uh childhood trauma\nto understand uh wit and humor required when you're dancing with the person who\nmight be depressed or suffering both humor and hope and love and all\nthose kinds of things so there's another underlying question which is self-supervised versus\nsupervised so can you get that from the data by just reading\na huge number of transcripts i actually so i think that reading huge number of transcripts is a step one it's like the\nsame way as you cannot learn to dance if just from youtube by watching it you\nhave to actually try it out yourself yeah and so i think that here that's a similar situation i also wouldn't deploy\nthe system in the high-stakes situations right away but kind of see gradually\nwhere it goes and obviously initially it would have to go hand with a hand in\nhand with humans but at the moment we are in the situation that actually\nthere is many more people who actually would like to have a therapy or or speak with with someone then there\nare therapies out there okay you know i was so so fundamentally i was thinking what are\nthe things that can vastly increase people well-being\ntherapy is one of them i think meditation is other one i guess maybe human connection is a third one and i\nguess pharmacologically it's also possible maybe direct brain stimulation or something like that but these are pretty\nmuch options out there then let's say the way i'm thinking about the agi endeavor is by default that's an\nendeavor to increase amount of wealth and i believe that we can vastly increase amount of\nwealth for everyone and simultaneously so i mean they're like two endeavors that\nmake sense to me one is like essentially increase amount of wealth and second one is uh increase overall human well-being\nand those are coupled together and they they can okay i would say these are different topics one can help another\n"}
{"pod": "Lex Fridman Podcast", "input": "Human reward functions", "output": "and uh you know therapist is a funny word because i see friendship and love\nas therapy i mean so therapist broadly defined as just friendship as a friend\nso like therapist is has a very kind of clinical sense to it but\nwhat is human connection you're like uh\nnot to get all camus and dostoyevsky on you but you know life is suffering and we draw\nwe seek connection with other humans as we desperately try to make sense of this\nworld in the deep overwhelming loneliness that we feel inside\nso i think connection has to do with understanding and i think that almost like a lack of\nunderstanding causes suffering if you speak with someone and you do you feel ignored that actually causes\npain if you are feeling deeply understood that actually they they might not even tell you what\nto do in life but like a pure understanding or just being heard understanding is a kind of\nit's a lot you know just being heard feel like you're being heard like somehow\nthat's uh alleviation temporarily of the loneliness that if somebody\nknows you're here with their body language with the way they are with the way they look at you\nwith the way they talk you feel less alone for a brief moment\nyeah very very much agree so i thought in the past about uh somewhat similar\nquestion to yours which is what is love uh rather what is connection yes and um\nand obviously i think about these things from ai perspective what would it mean um so i said that the you know intelligence\nhas to do with some compression which is more or less like i can say almost understanding of what is going around it\nseems to me that uh other aspect is there seem to be reward functions and\nyou can have a you know reward for uh food for maybe human connection for\nuh let's say warmth uh sex and so on and um\nand it turns out that the various people might be optimizing slightly different reward functions they essentially might\ncare about different things and um in case of\nlove at least the love between two people you can say that the um you know boundary between people dissolves to\nsuch extent that they end up optimizing each other reward functions\nyeah oh that's interesting um the success of each other yeah in some\nsense i would say love means uh helping others to optimize their uh reward functions not your reward\nfunctions not the things that you think are important but the things that the person cares about you try to help them\nto optimize it so love is uh if you think of two reward functions you\njust it's a condition yeah you combine them together yeah pretty much maybe like with a weight and it depends like\nthe dynamic of the relationship yeah i mean you could imagine that if you are fully uh optimizing someone's reward\nfunction without yours then yeah then maybe are creating code dependency or something like that yeah\ni'm not sure what's the appropriate weight but the interesting thing is i even i even think that the\nindividual person we ourselves we are actually\nless of a unified insight so for instance if you look at the donut on the one level you\nmight think oh this like it looks tasty i would like to eat it on another level you might tell yourself i shouldn't be\ndoing it because i want to gain muscles so and you know you might do it regardless kind of\nagainst yourself so it seems that even within ourselves they're almost like a kind of intertwined personas\nand i believe that the self-love means that the love between all these persons which\nalso means being able to love love yourself when we are angry or stressed\nor so combining all those reward functions of the different selves you have yeah and accepting that they are\nthere okay you know often people they have a negative self-talk or they say i don't like when i'm angry and like i try\nto imagine try to imagine if there would be like a\nsmall baby alex like a five years old who's angry angry and then you're like\nyou shouldn't be angry like stop being angry yeah but like instead actually you want the legs to come over give him a\nhug and he's like i say it's fine okay you can't be angry as long as you want yeah then he would stop\nor or maybe not or maybe not but you cannot expect it even yeah but still that doesn't explain the why\n"}
{"pod": "Lex Fridman Podcast", "input": "Love is part of the human condition", "output": "of love like why is love part of the human condition why is it useful to combine the reward functions\nit seems like that doesn't i mean i don't think reinforcement learning frameworks can give us answers to why\neven even the hudder framework has an objective function that's static so we came to existence as\na consequence of evolutionary process and in some sense the purpose of evolution is survival and then the\nthis complicated optimization objective baked into us let's say compression\nwhich might help us operate in the real world and it bake into us various reward functions yeah\nand then to be clear at the moment we are operating in the regime which is somewhat out of distribution where the\nevent evolution optimized us it's almost like love is a consequence of cooperation that we've discovered is\nuseful correct in some way it's even the case if you i just love the idea that love is like the out of distribution\nor it's not out of distribution it's like as you that it evolved for cooperation yes and i believe that the cop like a in\nsome sense cooperation ends up helping each of us individually so it makes sense evolutionary and there is a in\nsome sense and you know love means there is this dissolution of boundaries that you have a shared reward function and we\nevolve to actually identify ourselves with larger groups so we we can identify\nourselves you know with a family we can identify ourselves with a country to such an extent that people are willing\nto give away their life for country [Music] so there is we are wired actually even\nuh for love and at the moment i guess the\nmaybe it would be somewhat more beneficial if you will if we would identify ourselves\nwith all the humanity as a whole so so you can clearly see when people travel around the world when they run into\nperson from the same country they say oh which ctr and all this like all of a sudden they find all these similarities\nthey they they find some they befriend those folks earlier than others so there\nis like a sense some sense of the belonging and i would say i think it would be overall good thing to the word\nfor people to move towards i think it's even called open\nindividualism and move toward the mindset of a larger and larger groups so\n"}
{"pod": "Lex Fridman Podcast", "input": "Expanding our circle of empathy", "output": "the challenge there that's a beautiful vision and i share it to expand that circle of empathy that\ncircle of love towards the entirety of humanity but then you start to ask well where do you draw the line\nbecause why not expand it to other conscious beings and then at the finally\nfor our discussion something i think about is why not expand it to ai systems\nlike we we start respecting each other when the other the person the entity on the other side\nhas the capacity to suffer because then we develop a capacity to sort of empathize\nand so i could see ai systems that are interacting with humans more and more having conscious like\ndisplays so like they display consciousness through language and through other means\nand so then the question is like well is that consciousness because they're acting conscious\nand so you know the reason we don't like torturing animals\nis because they look like they're suffering when they're tortured and if ai looks like it's suffering\nwhen it's tortured how is that not\nrequiring of the same kind of empathy from us and respect and rights\nthat animals do and other humans do i think it requires empathy as well i mean i would like\ni guess us or humanity or so make a progress in understanding what consciousness is\nbecause i don't want just to be speaking about that the philosophy but rather actually make a scientific uh to have a\nlike a you know there was a time that people thought that there is a force of life\nand the things that have this force they are alive\nand i think that there is actually a path to understand exactly what consciousness is\nand um in some sense it might require essentially putting probes inside of a\nhuman brain what neuralink does so the goal there i mean there's several things with consciousness that\nmake it a real discipline which is one is rigorous measurement of consciousness\nand then the other is the engineering of consciousness which may or may not be related i mean you could also run into\ntrouble like for example in the united states for the department d.o.t department of\ntransportation and a lot of different places put a value on human life i think dot's\nuh values nine million dollars per person sort of in that same way you can get\ninto trouble if you put a number on how conscious a being is\nbecause then you can start making policy if a cow is uh 0.1\nor like um 10 as conscious as a human then you can start making calculations and might get\nyou into trouble but then again that might be a very good way to do it i would like uh\nto move to that place that actually we have scientific understanding what consciousness is yeah and then we'll be\nable to actually assign value and i believe that there is even the path for the experimentation in it so uh you know\nwe said that you know you could put the probes inside of the brain there is actually few other things that you could\ndo with devices like neuralink so you could imagine that the way even to measure if ai system is conscious\nis by literally just plugging into the brain and i mean that that seems that's kind of easy but the plugging into the brain\nand asking person if they feel that their consciousness expanded this direction of course has some issues\nyou can say you know if someone takes a psychedelic drug they might feel that their consciousness expanded even though\nthat drug itself is not conscious right so like you can't fully trust the self-report of a person saying their\ntheir consciousness is expanded or not let me ask you a little bit about\n"}
{"pod": "Lex Fridman Podcast", "input": "Psychedelics and meditation", "output": "psychedelics because uh there's been a lot of excellent research on uh different psychedelics psilocybin mdma\nyeah even dmt drugs in general marijuana too\nuh what do you think psychedelics do to the human mind it seems they take\nthe human mind to some interesting places is that just a little uh hack\na visual hack or is there some profound expansion of the mind so let's see i i don't believe in magic\ni believe in that i believe in in science in\nin causality still let's say and then as i said like i think that the brain\nthat the our subjective experience of reality is uh\nwe live in the simulation run by our brain and the simulation that our brain runs\nthey can be very pleasant or very hellish drugs they are changing some hyper\nparameters of the simulation it is possible thanks to change of these hyper parameters to actually look back on your\nexperience and even see that the given things that we took for granted they are\nchangeable so they allow to have a amazing perspective there is also\nfor instance the fact that after dmt people can see the full movie inside of their head\ngives me further belief that the brain can generate that full movie that the brain is actually\nlearning the model of reality to such extent that it tries to predict what's going to happen next yeah very high\nresolution so it can replay realities actually extremely high resolution and it's also kind of interesting to me\nthat somehow there seems to be some similarity between\nthese uh drugs and meditation itself and i actually started even these days to\nthink about meditation as a psychedelic and do you practice meditation\ni i practice meditation i mean i once few times on the retreats and it feels after like after\nsecond or third day of meditation\nthere is a there is almost like a sense of you know tripping what does the meditation retreat entail\nso i mean you you wake up early in the morning and you meditate for extended\nperiod of time and alone yeah so it's optimized even though there\nare other people it's optimized for isolation so you don't speak with anyone you don't actually look into other\npeople's eyes and you know you sit on the chair and\nsay the passage meditation tells you uh to focus on the breath so you try to put\nall the all attention into breathing and breathing in and breathing out\nand the crazy thing is that as you focus attention like that\nafter some time their stamps starts coming back like some\nmemories that you completely forgotten it almost feels like um that you have a\nmailbox and then you you know you are just like a archiving email one by one\nand at some point at some point there is like a amazing feeling of getting to mailbox zero\nzero emails and uh it's very pleasant it's it's kind of it's it's\nit's crazy to me that that once\nyou resolve these inner stories or like inner traumas\nthen once there is nothing uh left the default state of human mind is\nextremely peaceful and happy extreme like some sense it it feels that\nit feels at least to me in the way how when i was a child that i can look at any object and\nit's very beautiful i have a lot of curiosity about the simple things and that's where usually meditation takes me\nare you what are you experiencing are you just taking in simple sensory\ninformation and they're just enjoying the rawness of that sensory information so there's no\nthere's no memories all that kind of stuff you're just enjoying being\nyeah pretty much i mean still there is a there it's it's thoughts are slowing\ndown sometimes they pop up but it's also somehow the extended meditation takes\nyou to the space that they are way more friendly you know way more positive um\nthere is also this uh this thing that we've actually\nit almost feels that the it almost feels that the we are\nconstantly getting a little bit of a reward function and we are just spreading this reward function on\nvarious activities but if you stay still for extended period of time it kind of\naccumulates accumulates accumulates and there is a there is a sense there is a\nsense that at some point it passes some threshold and it feels as\ndrop is falling into kind of ocean of love and bliss and that's like a this is like a very pleasant and as i'm\nsaying okay that corresponds to the subjective experience some people\nuh i guess in spiritual community they describe it that that's the reality and\ni would say i believe that they're like all sorts of subjective experience that one can have and\ni believe that for instance meditation might take you to the subjective experiences which are very pleasant collaborative and i would like a word to\nmove toward a more collaborative uh place\nyeah i would say that's very pleasant that i enjoy doing stuff like that i i i wonder how that maps to your uh\nmathematical model of love with the the reward function combining a bunch of things\nit seems like our life then is we're just we have this reward\nfunction and we're accumulating a bunch of stuff in it with weights\nit's like um like multi-objective and\nwhat meditation is is you just remove them remove them until the weight on one\nor just a few is is very high and that's where the pleasure comes from yeah so\nsomething similar how i'm thinking about this so i told you that there is like a\nthere is a story of who you are and i think almost about it as a you know text prepended to gpt\nyeah and some people refer to it as ego okay it's like a story\nwho who you are okay so ego is the prompt for gpt three gpg yes yes and\nthat's description of you and then with meditation you can get to the point that actually you experience things without\nthe prompt and you experience things like as they are you are not biased over the\ndescription how they supposed to be uh that's very pleasant and then with respect to the reward function uh it's\npossible to get to the point that the there is dissolution of self and therefore you can say that they are\nyou you're having a you're or like your brain attempts to simulate the reward function of everyone else or like\neverything that's there is this like a love which feels like a oneness with everything\nand that's also you know very beautiful very pleasant at some point you might have a lot of altruistic\nthoughts during that moment and then the self uh always comes back how would you recommend\nif somebody is interested in meditation like a big thing to take on as a project would you recommend a meditation retreat\nhow many days what kind of thing would you recommend i think that actually retreat is the way to go and it almost\nfeels that as i said like a meditation is a psychedelic but\nwhen you take it in the small dose you might barely feel it once you get the high dose actually you're gonna feel it\num so even cold turkey if you haven't really seriously meditated for a prolonged period of time just go to a\nretreat yeah how many days how many days start the weekend one weekend so like two three days\nand it's like it's interesting that first or second day it's hard and at\nsome point it becomes easy there's a lot of seconds in a day how hard is the meditation retreat just\nsitting there in a chair so the thing is actually\nit literally just depends on your uh on death your own framing like if you\nare in the mindset that you are waiting for it to be over or you are waiting for nirvana to happen it will be very\nunpleasant yeah and in some sense even the difficulty it's not even in\nthe lack of being able to speak with others like you are sitting there your\nlegs will hurt from sitting in terms of like the practical things do you experience kind of discomfort like\nphysical discomfort of just sitting like your your butt being numb your\nlegs being sore all that kind of stuff yes you experience it and then the they teach you to observe it\nrather and it's like a the crazy thing is you at first might have a feeling toward trying to escape it yeah and that\nbecomes very apparent that that's extremely unpleasant and then you just just observe it and\nat some point it it just becomes uh it just is it's like a i remember with ilya told me\nsome time ago that uh you know he takes a cold shower and his mindset of taking a court cold shower was to\nembrace suffering yeah excellent i do the same there's the art style yes my style\ni like this so my style is actually i also sometimes take cold showers it is purely observing\nhow the water goes through my body like a purely being present not trying to escape from there yeah and i would say\nthen it actually becomes pleasant it's not like ah well that that's\ninteresting um i i'm also that mean that's that's the\nway to deal with anything really difficult especially in the physical space is to observe it\nto say it's pleasant it's a i would use a different word\nyour uh you're accepting of the full beauty of\nreality i would say because say pleasant but yeah i mean in some sense it is\npleasant that's the only way to deal with a cold shower is to to become an observer and to find\njoy in it same with like really difficult physical uh exercise or like running for a really\nlong time endurance events just anytime you're exhausted any kind of pain i think the only way to survive\nit is not to resist it just to observe it you mentioned ilya elias discover\n"}
{"pod": "Lex Fridman Podcast", "input": "Ilya Sutskever", "output": "he's very he's our chief scientist but also he's very close friend of mine he co-founded open air with you i've spoken\nwith him a few times he's brilliant i really enjoy talking to him\nhis mind just like yours works in fascinating ways now both of you are not able to define\ndeep learning simply uh what's it like having him\nas somebody you have technical discussions with on in space machine learning\ndeep learning ai but also life what's it like when these two uh agents\nget into a self-play situation in in a room what's it like collaborating with him\nso i believe that we have extreme uh respect to each other so\num i mean i love ilia's insight both like uh\ni guess about consciousness uh life ai but uh in terms of the it's interesting\nto me because you're a brilliant uh\nthinker in the space of machine learning like intuition like digging deep in what works\nwhat doesn't why it works why it doesn't and so is ilia i'm wondering if there's\ninteresting deep discussions you've had with him in the past or disagreements that were very\nproductive so i can say i also understood over the time where\nare my strengths so obviously we have plenty of ai discussions and\num and you know i myself have plenty of ideas but like i consider ilya\none of the most prolific ai scientists in the entire world and i think that\num i realized that maybe my super skill is being able to bring people to\ncollaborate together that i have some level of empathy that is unique in ai world and that might come you know from\neither meditation psychedelics or let's say i read just hundreds of books on this topic so and i also went through a\njourney of you know i develop all sorts of algorithms so i think that\nmaybe i can that's my super human skill uh\nilia is one of the best ai scientists but then i'm pretty good in assembling teams and\ni'm also not holding two people like i'm growing people and then people become managers that open yeah there's room any\nof them like a research manager so you you find you find places where you're excellent\nand and he finds like his his deep scientific insights is where he is and\nyou find ways you can the puzzle pieces fit together correct okay you know ultimately for instance\nlet's say ilia he doesn't manage people uh that's not what he likes or so um\ni i like i like hanging out with people by default i'm an extrovert and i care about people oh interesting okay\nokay cool so that that fits perfectly together but i i mean uh i also just like your intuition about various\nproblems in machine learning he's definitely one i really enjoy i remember talking to him\nabout something i was struggling with which is coming up with a good model for\npedestrians for human beings across the street in the context of autonomous vehicles\nand he immediately started to like formulate a framework within which you can evolve a model for pedestrians like\nthrough self-play all that kind of mechanisms the depth of thought on a particular\nproblem especially problems he doesn't know anything about is fascinating to watch it makes you realize like um\nyeah the the limits of the that the human intellect might be limitless\nor it's just impressive to see a descent on the vape come up with clever ideas yeah i mean so even in the space of deep\nlearning when you look at various people there are people you know who invented\nsome breakthroughs once but there are very few people who did it multiple times and you can think if someone\ninvented it once that might be just a shared luck and if someone invented it multiple\ntimes you know if a probability of inventing it once is one over a million then probability of inventing it twice\nor three times would be one over a million square or to the power of three which which would be just impossible so\nit literally means that it's it's given that uh it's not the luck yeah and ilea\nis one of these few people who um who have uh a lot of these inventions in\nhis arsenal it also feels that the now for instance if you think about folks like gauss or euler\nand you know at first they read a lot of books and then they did thinking and then they\nfigure out math and that's how it feels with ilya yeah you know at first he read stuff and then\nlike he spent his thinking cycles and that's a really good way to put it\nwhen i talk to him [Music] i i see thinking\nhe's actually thinking like he makes me realize that there's like deep thinking that the human mind\ncan do like most of us are not thinking deeply like you really have to put a lot of\neffort to think deeply like i have to really put myself in a place where i think deeply about a problem it takes a\nlot of effort it's like a it's like an airplane taking off or something you have to achieve deep focus he he's just\nuh he's what is it his brain is like a vertical takeoff\nin terms of airplane analogy so it's interesting but it i mean cal newport talks about this\nas ideas of deep work it's you know most of us don't work much at all in terms of like\nlike deeply think about particular problems whether it's math engineering all that kind of stuff\nyou want to go to that place often and that's real hard work and some of us are better than others at that so i think\nthat the big piece has to do with actually even engineering your environment such that it's conducive to\nthat yeah so um see both ilia and i uh on the frequent\nbasis we kind of disconnect ourselves from the world in order to be able to do extensive amount of thinking yes so ilia\nusually he just leaves ipad at hand he loves his ipad\nand for me i'm even sometimes you know just going for a few days to different location to airbnb i'm\nturning off my phone and there is no access to me yeah and\nthat's extremely important for me to be able to actually just formulate new thoughts to do deep work rather than to\nbe reactive and the the older i am the more of these like random tasks are at\nhand before i go on to that uh thread let me return\n"}
{"pod": "Lex Fridman Podcast", "input": "How does GPT work?", "output": "to our friend gpt let me ask you another ridiculously big question\ncan you give an overview of what gpt 3 is or like you say in your twitter bio gpt\nn plus one how it works and why it works so um gpt 3 is a\nhumongous neural network and let's assume that we know what is neural network okay by the definition\nand it is trained on the entire internet and just to predict\nnext word so let's say it sees part of the uh article and it the only task that\nit has at hand it is to say what would be the next word uh what would be the next word\nand it becomes uh really exceptional at the task of figuring out what's the next word so you\nmight ask why would this be an important task why\nwould it be important to predict what's the next word and it turns out that a lot of problems\nuh can be formulated uh as a text completion problem so gpt is\npurely uh learning to complete the text and you could imagine for instance if you are asking a question who is a\npresident of united states then gpt can give you an answer to it it turns out that many more things can\nbe formulated this way you can format text in the way that you have sentence in english\nyou make it even look like a some content of a website uh elsewhere which would be teaching people how to\ntranslate things between languages so it would be en colon text in english fr colon and then you uh\nand then you ask people and then you ask model to to continue and it turns out that the such a model is predicting\ntranslation from english to french the crazy thing is that\nthis model can be used for way more sophisticated tasks so you can format text such that\nit looks like a conversation between two people and that might be a conversation between you and elon musk and because\nthe model read all the texts about elon musk it will be able to predict elon musk\nwords as it would be elon musk it will speak about colonization of mars about sustainable future and so on and\nit's also possible to to even give arbitrary personality to\nthe model you can say here is a conversation with a friendly ai bot\nand the model uh will complete the text as a friendly ai bot so i mean\nhow do i express how amazing this is so just to clarify\na conversation generating a conversation between me and elon musk it wouldn't just generate good\nexamples of what elon would say it would get the syntax all correct so like interview style you would say like\nelon colon and lex con like it it's not just like uh inklings of\nsemantic correctness it's like the whole thing grammatical\nsyntactic semantic it's just really really impressive\nuh generalization yeah i mean i also want to you know provide some caveats so it can generate\nfew paragraphs of coherent text but as you go to uh longer pieces it actually\ngoes off the rails okay if you would uh try to write a book it won't work out uh this way what way does it go off the\nrails by the way is there interesting ways in which it goes off the rails like what falls apart first so the model is\ntrained on the all the existing data that is out there which means that it is\nnot trained on its own mistakes so for instance if it would make a mistake then\nuh i kept so to give give you an example so let's say i have a conversation with\na model pretending that is elon musk and then i start putting some i'm start\nactually making up things which are not factual um i would say like twitter\nbut i gotcha sorry yeah um okay i don't know i would say that elon is my\nwife and the model will just keep on carrying it on and as if it's\ntrue yes and in some sense if you would have a normal conversation with elon he would\nbe what the [ __ ] yeah there would be some feedback between so the the model is trained on\nthings that humans have written but through the generation process there's no human in the loop feedback correct\nthat's fascinating makes sense so it's magnified it's like the errors get magnified and magnified right and it's a\nit's also interesting i mean first of all humans have the same problem it's just that we\nuh we make fewer errors and magnify the errors slower i think that actually what\nhappens with humans is if you have a wrong belief about the world as a kid then very quickly you will learn that\nit's not correct because you are grounded in reality and you are learning from your new experience yes\nbut do you think the model can correct itself too it through the power of the\nrepresentation and so the absence of elon musk being your wife\ninformation on the internet want to correct itself there won't be examples like that so the\nerrors would be subtle at first saddle at first and in some sense\nyou can also say that the data that is not out there is the data which would represent how the human learns\nthat's an a and and maybe model would be trained on such a data then it would be better off how intelligent is gpt 3 do\nyou think like when you think about the nature of intelligence it seems exceptionally\nimpressive but then if you think about the big agi problem is this footsteps along the way\nto agi so let's see seems that intelligence itself is there are multiple axis of it and\ni would expect that the the systems that we are building they\nmay end up being super human on some axis and sub human on some other axis it\nwould be surprising to me on all axis simultaneously they would become superhuman\nof course people ask this question is gpt a spaceship that that would take us to moon or are we\nputting a building a ladder to heaven that we are just building bigger and bigger ladder and we don't know in some\nsense uh which one of these two which one is better\ni'm trying to i like stairway to heaven that's a good song so i'm not exactly sure which one is better but you're\nsaying like the the spaceship to the moon is actually effective correct so people who criticize gpt yeah\nthey say jarga is just building a taller a ladder\nand it will never reach the moon and at the moment i would say the way i'm\nthinking is this like a scientific question and i'm also in heart i'm a builder\ncreator and like i'm thinking let's try out let's see how far it goes and so far\nwe see constantly that there is a progress yeah so what do you think\ngpt4 gpt5 gpt n plus one\nwill uh there'll be a phase shift like a transition to a to a place where\nwe'll be truly surprised then again like gpt3 is already very like truly surprising the people that criticize\ngpg3 as it's there as a what is it ladder to heaven i think too quickly get accustomed to\nhow impressive it is that the prediction of the next word can achieve such\ndepth of semantics accuracy of syntax grammar and semantics\num do you do you think gpt four and five and six will continue\nto surprise us i mean definitely there will be more impressive models there is a question of\ncourse if there will be a phase shift and\nthe also even the way i'm thinking about the about these models is that when we build these models\nyou know we see some level of the capabilities but we don't even fully understand everything that the model can\ndo and actually one of the best things to do is to allow other people to probe the model to\neven see what is possible hence the using gpg as an api\nand opening it up to the world yeah i mean so when i'm thinking from perspective of\nthere like a obviously various people are that have concerns about agi including myself\nand then when i'm thinking from perspective what's the strategy even to deploy these things to the world\nthe the one strategy that i have seen many times working is the iterative deployment that you deploy\num slightly better versions and you allow other people to criticize you so you actually are tried out you see where\nare their fundamental issues and it's almost you don't want to be in that situation that you are holding into\npowerful system and there's like a huge overhang then you deploy it and it might have a random chaotic impact on the\nworld so you actually want to be in the situation that they are gradually deploying systems\n"}
{"pod": "Lex Fridman Podcast", "input": "AI safety", "output": "i asked this question of ilio let me ask you you this question i've been reading a lot\nabout stalin and power\nif you're in possession of a system that's like agi that's exceptionally powerful\ndo you think your character integrity might become corrupted like famously power corrupts and\nabsolute power corrupts absolutely so i believe that you want at some point to\nwork toward distributing the power i think that you want to be in the situation\nthat actually agi is not controlled by a small number of people but\nessentially by a larger collective so the thing is that requires a george washington style\nmove in the ascent to power there's always a moment when somebody gets a lot of power\nand they have to have the integrity and uh the moral compass to give away\nthat power that humans have been good and bad throughout history at this\nparticular step and i wonder i wonder we like blind ourselves in uh\nfor example between nations a race uh towards uh\nyeah ai race between nations we might blind ourselves and justify to ourselves the development of ai without\ndistributing the power because we want to defend ourselves against china against russia that kind\nof that kind of logic and i wonder how we um\nhow we design governance mechanisms that um prevent us from\nbecoming power hungry and in the process destroying ourselves so let's see i have been thinking about\nthis topic quite a bit but i also want to admit that uh once again i actually want to rely\nway more on sam outman on it hero than a heroed an excellent block\non how even to distribute wealth and his proper he proposed in his block\nto tax equity of the companies rather than profit and to distribute it and this is\nthis is an example of uh washington move\ni guess i personally have insane trust in some he already spent plenty of money running\na universal basic income project that like gives me i guess\nmaybe some level of trust to him but i also i guess\nlove him as a friend yeah i wonder because we're sort of summoning a new set of technologies\ni wonder if we'll be cognizant like you're describing the process of open ai but it could also be\nat other places like in the us government right both china and the us are now\nfull steam ahead on autonomous weapons systems development and that's really worrying to me because\nin the framework of something being a national security danger or military\ndanger you can do a lot of pretty dark things that blind our moral compass\nand i think ai will be one of those things in some sense the the mission\nand the work you're doing at openai is like the counterbalance to that so you want to have more open ai and less\nautonomous weapon systems i i i like these statements like to be clear like this interesting and i'm thinking about\nit myself but uh this is a place that i i okay i put my trust actually\nin some hence because it's extremely hard for me to reason about it yeah i mean one important statement to make is\num it's good to think about this yeah no question about right no question even\nlike low-level quote-unquote engineer like there's such a\ni remember i i programmed a car uh our rc car\nthey went really fast like 30 40 miles an hour and i remember i was like sleep deprived\nso i programmed it pretty crappily and it like uh the the code froze so it's doing some\nbasic computer vision and it's going around on track but it's going full speed\nand uh there's a bug in the code that uh the car just went it didn't turn it went straight\nfull speed and smashed into the wall i remember thinking the seriousness with which you need to\napproach the design of artificial intelligence systems and the programming of artificial intelligence systems\nis high because the consequences are high like that little car smashing it to the wall\nfor some reason i immediately thought of like an algorithm that controls nuclear weapons\nhaving the same kind of bug and so like the lowest level engineer and the ceo of a company all need to have the\nseriousness in approaching this problem and thinking about the worst case consequences so i\nthink that is true i mean the what i also recognize in myself and\nothers even asking this question is that it evokes a lot of fear and fear itself ends up being actually\nquite debilitating the place where i arrived at the moment\nmight sound cheesy or so but it's almost to build things out of love rather than\nfear yeah i can focus on how i can you know maximize the value how\nthe systems that i'm building might be uh useful i'm not saying that the fear doesn't\nexist out there and like it totally makes sense to minimize it but i don't want to be working because\nuh i'm scared i want to be working out of passion out of curiosity out of the\nyou know looking forward for the positive future with uh the definition of love arising from a\nrigorous practice of empathy so not just like your own conception of what is good for the world but uh always listening to\nothers correct like at the love where i'm considering reward functions of others\nothers to infil limit to infinity is like a sum like one to n where n is uh seven\nbillion or whatever it is not not projecting my reward functions on others yeah exactly okay\n"}
{"pod": "Lex Fridman Podcast", "input": "OpenAI Codex", "output": "can we just take a step back to something else super cool which is uh opening up codex\ncan you give an overview of what open-air codecs and github co-pilot is\nhow it works and why the hell it works so well so with gpd3 we noticed that the\nsystem um you know that system training all the language out there started having some\nrudimentary coding capabilities so we're able to ask it you know to\nimplement addition function between two numbers and indeed it can write python or javascript code for that and then we\nthought um we might as well just go full steam ahead and try to create a system\nthat is actually good at what we are doing every day ourselves which is programming\nwe optimize models for proficiency in coding we actually even created models that both have a\ncomprehension of language and code and codex is api for these models so\nit's first pre-trained on language and then i don't know if you can say fine-tuned\nbecause there's a lot of code but it's language and code it's language and code\nit's also optimized for various things like let's say low latency and so on codex is the api that's similar to gpd3\nwe expect that there will be proliferation of the potential products that can use coding capabilities and i\ncan i can speak about it in a second compiled is the first product\nand developed by github so as we're building uh models we wanted to make sure that these models are useful\nand we work together with github on building the first product co-pilot is actually as you code it suggests you\ncode completions and we have seen in the past they're like a various tools that can suggest how to like a few characters\nof the code or the line of code the the thing about copilot is it can generate 10 lines of code you\nit's often the way how it works is you often write in the comment what you want to happen because\npeople in comments they describe what happens next so um these days when i code instead of\ngoing to google to search for the appropriate code to solve my problem i say oh for this array could\nyou smooth it and then you know it imports some appropriate libraries and say it uses numpy convolution or so i\nthat i was not even aware that exists and it does the appropriate thing um so you you write a comment maybe the\nheader of a function and it completes the function of course you don't know what is the space of all the possible\nsmall programs it can generate what are the failure cases how many edge cases how many subtle\nerrors there are how many big errors there are it's hard to know but the fact that it works at all on in a large\nnumber of cases is incredible it's like a it's a kind of search engine\ninto code that's been written on the internet correct so for instance when you search things online then\nusually you get to the some particular case like if you go to stack overflow\npeople describe that one particular situation uh and then they seek for a\nsolution but in case of uh co-pilot it's aware of your entire context and in\ncontexts oh these are the libraries that they are using that's the set of the variables that is initialized and on the\nspot it can actually tell you what to do so the interesting thing is and we think that the copilot is one\npossible product using codex but there is a place for many more so internally we tried out you know to\ncreate other fun products so it turns out that a lot of tools out there\nlet's say google calendar or microsoft word or so they all have uh internal api to build\nplugins around them so there is a way in the sophisticated way to control calendar or microsoft\nword today if you want if you want more complicated behaviors from these programs you have to add a\nnew button for every behavior but it is possible to use codex and\ntell for instance to calendar could you schedule an appointment with\nblacks next week after 2 pm and either writes corresponding piece of code\nand that's the thing that actually you want so interesting so what you figure out is there's a lot of\nprograms with which you can interact through code and so there you can generate that code\nfrom natural language that's fascinating and that's somewhat like also closest to\nuh what was the promise of siri or alexa yeah so previously all these behaviors they were had\nhard coded yeah and it seems that codex on the fly can pick up the api of let's\nsay given software yeah and then it can turn the language into use of this api without hard coding you can find it can\ntranslate to machine language correct it to uh so for example this would be really exciting for me like for um adobe\nproducts like photoshop uh which is the i think actionscript i think there's a scripting language that\ncommunicates with them same with premiere and you could imagine that that allows event to\ndo coding by voice on your phone so for instance in the past okay as of\ntoday i'm not editing word documents on my phone because it's just the keyboard\nis too small but if i would be able to tell to my phone you know uh make the header\nlarge and then move the paragraphs around and it does actually what i want so i can tell you one more cool thing or\neven how i'm thinking about codex so if you look actually at the evolution\nof of computers we started with very primitive\ninterfaces which is a punch card and punch card essentially you make a holes in the\nin the plastic card to indicate zeros and ones and during that time there was a small\nnumber of specialists who were able to use computers and by the way people even suspected that there is no need for many\nmore people to use computers but then we moved from punch cards to\nat first assembly then c and these programming languages they were slightly higher level they allowed\nmany more people to code and they also led to more of a proliferation of\ntechnology and you know further on there was a jump to say from c plus plus to java and python\nand every time it has happened more people are able to code and we build more technology and it's even you\nknow hard to imagine now if someone will tell you that you should write code in\nassembly instead of let's say python or or or java or javascript and codex is yet\nanother step toward kind of bringing computers closer to humans such that you communicate with a computer\nwith your own language rather than with a specialized language and i think that it will lead to\nan increase of number of people who can code yeah and then and the kind of technologies that those people will\ncreate is like it's innumerable it could you know it could be a huge number of technologies we're not predicting at all\nbecause that's less and less requirement of uh having a technical mind a programming mind you're not opening it\nto the world of um other kinds of minds creative minds\nartistic minds all that kind of stuff i would like for instance biologists who work on dna to be able to program and\nnot to need to spend a lot of time uh learning it and i i believe that's a good thing to the word and i would\nactually add out that so at the moment i'm a managing codex team and also\nlanguage team and i believe that there is like a plenty of brilliant people out there and they should apply\noh okay yeah awesome so what's the language in the codexes so those are kind of they're overlapping teams so it's like\ngpt the raw language and then the codex is like applied to programming\ncorrect and they are quite intertwined there are many more teams involved making these uh\nmodels extremely efficient and deployable for instance there are people who are working to you know\nmake our data centers uh amazing or there are people who work on pro putting these models into production\nor uh or even pushing it at the very limit of the scale\nso all aspects from from the infrastructure to the actual machine learning so i'm just saying that multiple teams while the\nand the team working on codex and language uh i guess i'm i'm directly managing them i would like i would love\nto hire yeah if you're interested in machine learning this is probably one of the most exciting uh problems and like systems to\nbe working on because it's actually it's it's pretty cool like what what uh the\nprogram synthesis like generating of programs is very interesting very interesting problem that has echoes of\nreasoning and intelligence in it it and i think there's a lot of fundamental questions that you might be\nable to sneak sneak up to by generating programs yeah\nthe one more exciting thing about the programs is that so i said that the um you know the in case of language that\none of the troubles is even evaluating language so when the things are made up you you need somehow\neither a human to say that this doesn't make sense or so in case of program there is one extra\nlevel that we can actually execute programs and see what they evaluate to so that process might be somewhat\nmore automated in in order to improve the uh qualities of generations and\nthat's not saying so like the wow that's really interesting so for the language that you know the simulation to actually\nexecute it as a human mind yeah for programs there is a there is a computer\non which you can evaluate it wow that's a brilliant little\ninsight that the thing compiles and runs that's first\nand second you can evaluate on a like do automated unit testing\nand in some sense it seems to me that we will be able to make a tremendous progress you know\nwe are in the paradigm that there is way more data and there is like a\ntranscription of millions of uh of uh software engineers yeah\nyeah so i mean you just me because i was going to ask you about reliability the thing\nabout programs is you don't know if they're going to like a program that's controlling a\nnuclear power plant has to be very reliable so i i wouldn't start with controlling nuclear power plant can i be\none day but that that's not actually that's not on the current roadmap that's not that's step one and you know it's\nthe russian thing you just want to go to the most powerful destructive thing right away run by javascript but i got you so it's\na lower impact but nevertheless what you're making me realize it is possible to achieve some levels of\nreliability by doing testing and i thought you could imagine that them you know maybe there are ways for a\nmodel to write even code for testing itself and so on and there exists a ways to create the\nfeedback loops that the model could keep on improving\nby writing programs that generate tests for the instance for instance\nand that's how we get consciousness because it's meta compression that's what you're going to write that's the\ncomment that's the prompt that generates consciousness compressor of compressors you just write\nthat do you think the code that generates consciousness would be simple\nso let's see i mean ultimately the core idea behind will be simple but there\nwill be also decent amount of engineering involved like in some sense\nit seems that you know spreading these models on many machines and it's not that trivial yeah and\nwe find all sorts of innovations that make our models more efficient\ni believe that first models that i guess are conscious are like a\ntruly intelligent they will have all sorts of tricks\nbut then again there's uh which is certain argument that maybe the tricks are temporary thing yeah they\nmight be temporary things and in some sense it's also even important to um\nto know that even the cost of a trick so sometimes people are eager to put the\ntrick while forgetting that there is a cost of maintenance or like a long-term cost long-term cost\nor maintenance or maybe even flexibility of code to actually implement new ideas so even if you have\nsomething that gives you 2x but it requires you know 1000 lines of code i'm not sure if it's actually worth it so in\nsome sense you know if it's five lines of code and 2x i would take it\nand and we we we see many of this but also you know that requires some level\nof i guess lack of attachment to code that we are willing to remove it yeah\n"}
{"pod": "Lex Fridman Podcast", "input": "Robotics", "output": "so you led the open ai robotics team can you give an overview of of the cool\nthings you're able to accomplish what are you most proud of so when we started robotics we knew that actually reinforcement learning works\nand it is possible to solve very complicated problems like for instance alphago is an evidence\nthat it is possible to to build superhuman and gold players dota 2 is a\nan evidence that is possible to build superhuman uh\nagents playing dota so i asked myself a question you know what about robots out there could we train machines to solve\narbitrary tasks in the physical world our approach was i guess let's pick a\ncomplicated problem that if we would solve it that means that we made some uh significant progress in the\ndomain and then we went after the problem so um we noticed that actually the\nrobots out there they are kind of at the moment optimized per task so you can have a robot that it's like if you have\na robot opening a battle it's very likely that the end factor is a battle opener\nand and in some sense that's a hack to be able to solve a task which makes any task easier and um ask myself so what\nwould be a robot that can actually solve many tasks yeah and we conclude that that\nlike a human hands have such a quality that indeed they are you know you have\nfive kind of tiny arms attached individually they can manipulate\npretty broad spectrum of objects so we went after a single hand like a trying\nto solve rubik's cube single-handed we picked this task because we thought that there is no way to\nharcode it and it's also we picked the robot on which it would be hard to hardcode it and\nwe went after the solution such that it could generalize to other problems and just to clarify it's\none robotic hand solving the rubik's cube the hard part isn't the solution to the rubik's cube is the manipulation of\nthe uh of like having it not fall out of the hand having it use the uh\nfive baby arms to uh what is it like rotate different parts of the rubik's cube to achieve the\nsolution correct yeah so what uh what was the hardest part about that\nwhat was the approach taken there what are you most proud of obviously we have like a strong belief in reinforcement\nlearning and uh you know one path it is to do reinforcement learning the real world\nother path is to the simulation in some sense the tricky part about the real world is at\nthe moment our models they require a lot of data there is essentially no data\nand i did we decided to go through the path of the simulation and in simulation\nyou can have infinite amount of data the tricky part is the fidelity of the simulation and also can you in\nsimulation represent everything that you represent otherwise in the real world and you know it turned out that uh\nthat you know because there is lack of fidelity it is possible to that what we\nwhat we arrived at is training a model that doesn't solve one simulation but it\nactually solves the entire range of simulations which uh vary uh in terms of like uh what's the\nexactly the friction of that cube or the weight or so and the single ai that can solve all of them\nends up working well with the reality how do you generate the different simulations so\nyou know there's plenty of parameters out there we just pick them randomly and and in simulation model just goes for\nthousands of years and keeps on solving rubik's cube in each of them and the thing is the neural network that we used\nit has a memory and as it presses for instance the side\nof the of the cube it can sense oh that's actually this side was\nuh difficult to press i should press it stronger and throughout this process kind of\nlearns even how to how to solve this particular instance of the rubik's cube back even mass it's\nkind of like a you know sometimes when you go to a gym and after\nafter bench press you try to lift the\nand you kind of forgot uh and and your hand goes like yeah right away because\nkind of you got this to maybe different weight yeah and it takes a second to adjust yeah\nand this kind of of a memory that model gained through the process of interacting with the cube in the\nsimulation i appreciate you speaking to the audience with the bench press all the bros in the audience\nprobably working out right now there's probably somebody listening to this actually doing bench press\nso maybe uh put the bar down and pick up the water bottle and you'll know exactly\nwhat uh what check is talking about okay so what uh\nwhat was the hardest part of getting the whole thing to work so the hardest part is\nat the moment when it comes to a physical world when it comes to robots\nthey require maintenance it's hard to replicate a million times it's\nit's also it's hard to replay things exactly i remember this situation that\none guy at our company he had like a model that performs way better than other models in\nsolving rubik's cube and you know we kind of didn't know what's going on\nwhy it's that and it turned out that you know he was running it from his\nlaptop that had better cpu or uh or better or maybe local gpu as well\nand uh because of that there was less of a latency and the model was the same\nand that actually made solving rubik's cube more reliable so in some sense there might be some\nsaddlebacks like that when it comes to running things in the real world even hinting on that\nyou could imagine that the initial models you would like to have models which are insanely huge neural networks\nand you would like to give them even more time for thinking and when you have these real-time\nsystems then you might be constrained actually by the amount of latency\nand ultimately i would like to build the system that it is worth for you to wait five minutes\nbecause it gives you the answer that you are willing to wait for five minutes so latency is a very unpleasant\nconstraint underwish to operate correct and also there is actually one more thing which is tricky about robots\nthere is actually no not much data so the data that i'm speaking about would be a data of\nfirst person experience from the robot and like a gigabytes of data like that if we would have gigabytes of data like\nthat of robot solving various problems it would be very easy to make a progress on robotics and you can see that in case\nof text or code there is a lot of data like a first person perspective data on the writing code\nyeah so you had this you mentioned this really interesting idea that\nif you were to build like a successful robotics company so open as mission is much bigger than robotics this is one of\nthe one of the things you've worked on but if it was a robotics company they\nyou wouldn't so quickly dismiss supervised learning i correct that you would build a robot\nthat was perhaps one like um an empty shell like dumb and they\nwould operate under tele operation so you would invest that's just one way to do it invest in\nhuman super like direct human control of the robots as it's learning and over time add more and more automation\nthat's correct so let's say that's how i would build a robotics company today if i would be building a robotics\ncompany which is you know spent 10 million dollars or so recording human trajectories controlling\na robot after you find a thing that the robot should be doing\nthat there's a market fit for like that you can make a lot of money with that product correct correct yeah\nso i would record data and then i would essentially train supervised learning model on it\nthat might be the path today long term i think that actually what is needed is to train powerful models over\nvideo so um you have seen maybe a models that can generate images like dali\nand people are looking into models generating videos they're like various algorithmic questions even how to do it\nand it's unclear if there is enough compute for this purpose but i i suspect that the models that which\nwould have a level of understanding of video same as gpt has the level of understanding of\ntext could be used to train robots to solve tasks they\nwould have a lot of common sense if one day i'm pretty sure one day\n"}
{"pod": "Lex Fridman Podcast", "input": "Developing self driving cars and robots", "output": "there will be a robotics company by robotics company i mean the primary\nsource of income is is from robots that is worth over\n1 trillion dollars what do you think that company will do i think self-driving cars no\nit's interesting because my mind went to personal robotics robots in the home it seems like there's much more market\nopportunity there i think it's very difficult to achieve\ni mean this this this might speak to something important which is i understand self-driving much\nbetter than understand robotics in the home so i understand how difficult it is to actually solve self-driving\nto uh to a level not just the actual computer vision and the control problem and just the basic problem self-driving\nbut creating a product that would undeniably be um\nthat will cost less money like it will save you a lot of money like orders the magnitude less money that could replace\nuber drivers for example so car sharing that's autonomous that creates a similar or better experience in terms\nof how quickly you get from a to b or just whatever the the pleasantness of the experience\nthe efficiency of the experience the value of the experience and at the same time the car itself costs cheaper\ni think that's very difficult to achieve i think there's a lot more um low hanging fruit in the home\nthat that could be i also want to give you perspective on like how challenging it would be at home\nor like it maybe kind of depends on the exact problem that you'd be solving okay if we are speaking about these robotic\narms and hence these things they cost tens of thousands of dollars or maybe 100k\nand you know maybe obviously maybe there would be economy of scale these things\nwould be cheaper but actually for any household to buy the price would have to go down to maybe\nthousand bucks yeah i personally think that uh\nso self-driving car it provides a clear service i don't think robots in the home\nthey'll be a trillion dollar company will just be all about service meaning it will not necessarily be about\nlike a robotic arm that helps you i don't know open a bottle\nor wash the dishes or any of that kind of stuff it has to be able to take care of that whole the\ntherapist thing you mentioned i i think that's um of course there's a line between what is a robot and what is\nnot like doesn't really need a body but you know some uh ai system with some embodiment i\nthink so the tricky part when you think actually what's the difficult part is um\nwhen the robot has like when there is a diversity of the environment with which the robot has to\ninteract that becomes hard so you know on one spectrum you have industrial robots as they are doing over\nand over the same thing it is possible to some extent to prescribe the movements and with very small amount of\nintelligence the the movement can be repeated millions of times um the it\nthere are also you know various pieces of industrial robots where it becomes harder and harder like for instance in\ncase of tesla it might be a matter of putting a a rack inside of a car\nand you know because the rack kind of moves around it's it's not that easy it's not exactly the same every time it\nends up being the case that you need actually humans to do it and while you know welding cars together\nit's a very repetitive process and then in case of self-driving itself\nthe difficulty has to do with the diversity of the environment but still\nthe car itself and the problem that you are solving is you try to avoid even interacting with\nthings you are not touching anything around because touching itself is hard and then if you would have in the home\nuh robot that you know has to touch things and like if these things they change the shape if there is a huge\nvariety of things to be touched then that's difficult if you are speaking about the robot which there is you know\nhead that is smiling in some way with cameras that it doesn't you know touch things that's relatively simple\nokay so to both agree and to push back so you're referring to touch like\nsoft robotics like the actual touch but i would argue that you could formulate\njust basic interaction between um like non-contact interaction\nis also a kind of touch and that might be very difficult to solve that's the basic this not disagreement but that's\nthe basic open question to me with self-driving cars and disagreement with elon which is how much interaction\nis required to solve self-driving cars how much touch is required you said that\nin your intuition touch is not required and my intuition to create a product\nthat's compelling to use you're going to have to uh interact with pedestrians not just avoid\npedestrians but interact with them when we drive around in major cities\nwe're constantly threatening everybody's life with our movements and that's how they respect us there's a\ngame theoretically going on with pedestrians and i am afraid you can't just\nformulate autonomous driving as a collision avoidance problem so i i think it goes\nbeyond like a collision avoidance is the first order approximation but then at least in case of tesla they\nare gathering data from people driving their cars and i believe that's an example of supervised learning data that they can\ntrain their models uh on and they are doing it which you know can give the model this\nlike another level of of a behavior that is needed to actually\ninteract with the real world yeah it's interesting how much data is required to achieve that\num what do you think of the whole tesla autopilot approach the computer vision based approach with multiple cameras and\nthere's a data engine it's a multi-task multi-headed neural network and it's this fascinating process of uh similar\nto what you're talking about with the the robotics approach uh which is you know you deploy neural\nnetwork and then there's humans that use it and then it runs into trouble in a bunch of places and that stuff is sent back so\nlike the deployment discovers a bunch of edge cases and those edge cases are sent back\nfor supervised annotation thereby improving the neural network and that's deployed again\nit goes over and over until the the network becomes really good at the task of driving becomes safer and safer what\ndo you think of that kind of approach to robotics i believe that's the way to go so in some sense even when i was\nspeaking about you know collecting trajectories from humans that's like a first step and then you deploy the\nsystem and then you have humans revising the all the issues and in some sense\nlike this approach converges to system that doesn't make mistakes because for the cases where there are mistakes you\ngot their data how to fix them and the system will keep on improving so there's a very\nto me difficult question of how hard that you know how long that converging takes how hard it is\nuh the other aspect of autonomous vehicles this probably applies to certain robotics applications\nis society right they put as as the quality of the system\nconverges so one there's a human factors perspective of psychology of humans being able to supervise those uh even\nwith teleoperation those robots and the other society willing to accept robots\ncurrently society is much harsher on self-driving cars than it is on human driven cars in terms of the expectation\nof safety so the bar is set much higher than for humans and we're so if there's\na death in an autonomous vehicle that's seen as much more\nmuch more dramatic than a death in a human driven vehicle part of the success of deployment of\nrobots is figuring out how to make robots part of society both on the just\nthe human side on the media journalist side and also on the policy government side and that seems to be uh\nmaybe you can put that into the objective function to optimize but that is that is definitely um\na tricky one and i wonder if that is actually the trickiest part for self-driving cars or any system that's\nsafety critical it's not the algorithm it's the society accepting it\nyeah i i would say i believe that the part of the process of deployment is\nactually showing people that the given things can be trusted yeah and you know\ntrust is also like a glass that is actually really easy to crack it yeah\nand damage it and i think that's actually very common with uh\nwith innovation that there is some resistance toward it yeah and\nit's just the natural progression so in some sense people will have to keep on proving that indeed these systems are\nworth being used and i would say i also found out that\noften the best way to convince people is by letting them experience it yeah\nabsolutely that's the case for tesla autopilot for example that's the case with uh yeah with\nbasically robots in general it's it's kind of funny to hear people talk about robots like\nthere's a lot of fear even like legged robots but when they actually interact with them\nthere's joy i love interacting with them and the same with the car with the robot\nif it starts being useful i think people immediately understand and if the product is designed well they\nfall in love you're right it's actually even similar when i'm thinking about co-pilot the github\nco-pilot there was a spectrum of responses that people had and uh ultimately\nuh the important piece was to let people try it out and then many people just\nloved it especially like programmers yeah programmers but like some of them you know they came with a\nfear yeah but then you try it out and you think actually that's cool okay and you know you can try to resist the same way\nas you know you could resist moving from punch cards to let's say\nc plus or so and it's a little bit futile\n"}
{"pod": "Lex Fridman Podcast", "input": "What is the benchmark for intelligence?", "output": "so we talked about generation program generation of language\neven self-supervised learning in the visual space for robotics and then reinforcement learning what do you and\nlike this whole beautiful spectrum of ai do you think is a good\nbenchmark a good test to strive for to achieve intelligence that's a strong\ntest of intelligence you know it started with alan turing and the touring test maybe you think natural language\nconversation is a good test so you know it would be nice if for instance machine would be able to solve\nriemann hypothesis in math that would be i think that would be very\nimpressive so theorem proving is that to you proving theorems is a good\noh oh like one thing that the machine did you would say damn exactly\nokay that would be quite quite impressive i mean the the tricky\npart about the benchmarks is um you know as we are getting closer with them we have to invent new\nbenchmarks there is actually no ultimate benchmark out there yeah see my thought with the riemann hypothesis would be\nthe moment the machine proves it would say okay well then the problem was easy\nthat's what happens and i mean in some sense um that's actually what happens over the years in ai that like uh\nwe get used to things very quickly you know something i talked to rodney brooks i don't know if you know that is\nhe called alpha zero homework problem because he was saying like there's nothing special about it it's not a big\nleap and i i didn't well he's coming from one of the aspects that we referred to as he was part of uh\nthe founding of irobot which deployed now tens of millions of robot in the home so\nif you see robots that are actually in the homes of people\nas the legitimate instantiation of artificial intelligence then yes maybe an ai that plays a silly\ngame like going chess is not a real accomplishment but to me it's it's a fundamental leap but i think we as\nhumans then say okay well then that uh that game of chess or go wasn't that difficult compared to the thing\nthat's currently unsolved so my intuition is that from perspective of the evolution of\nyou know these ai systems we'll at first see the tremendous progress in digital space and the you know the main thing\nabout digital space is also that you can everything is that there is a lot of recorded data plus you can very rapidly\ndeploy things to billions of people while in case of uh physical space the\ndeployment part takes multiple years you have to manufacture things and\nyou know delivering it to actual people it's very hard so i'm expecting that the first and the\nprices in digital space of goods they would go you know down to\nthe let's say marginal costs are to zero and also the question is how much of our life will be in digital because it seems\n"}
{"pod": "Lex Fridman Podcast", "input": "Will we spend more time in virtual reality?", "output": "like we're heading towards more and more of our lives being in the digital space so like\ninnovation in the physical space might become less and less significant like why do you need to drive anywhere\nif most of your life is spent in virtual reality i still would like you know to\nat least at the moment my impression is that i would like to have a physical contact with other people and that's very important to me and we don't have a\nway to replicate it in the computer it might be the case that over the time it will change like in 10 years from now why not have\nlike an arbitrary infinite number of people you can interact with some of them are real some are not\nwith uh arbitrary characteristics that you can define based on your own\npreferences i think that's maybe where we are heading and maybe i'm resisting the future yeah\ni'm telling you i if i got to choose\nif i could live in elder scrolls skyrim versus the real world i'm not so sure i\nwould stay with the real world yeah i mean the question is so will vr be sufficient to get us there or do do\nyou need to you know plug electrodes in the brain and it would be nice if these electrodes\nwouldn't be invasive yeah or at least like provably non-destructive\nbut in in the digital space do you do you think we'll be able to solve the touring test the spirit of the touring\ntest which is do you think we'll be able to achieve compelling natural language\nconversation between people like have friends that are ai systems on the internet\ni thought i think it's doable do you think the current approach of gbt will take us there so there is you know\nthe the part of at first learning all the content out there and i i think that steel system should keep on learning as\nit speaks with you yeah and i think that should work the question is how exactly to do it and you\nknow obviously we have people at open air asking these questions\nand kind of at first pre-training on all existing content is like a backbone and\nit's a decent backbone do you think ai needs a body\n"}
{"pod": "Lex Fridman Podcast", "input": "AI Friendships", "output": "connecting to our robotics question to uh truly connect with humans or can can most of the connection be in the\ndigital space so let's see we know that there are people who met\neach other online and they felt in love yeah so it seems that it's conceivable to\nestablish connection which is purely through internet\nand of course it might be more compelling than more modalities you add\nso it would be like you're proposing like a tinder but for ai are you like swipe right and left and\nhalf the systems are ai and the other is uh humans and you don't know which is which\nthat would be ours that would be our formulation of touring test the the moment ai is able to achieve more swipe\nright or left whatever the the moment is able to be more attractive than other humans\nit passes the torrent test then you would pass the turing test in attractiveness that's right well no like attractiveness just declare conversation\nnot just visual right it's also attractiveness with wit and humor and uh whatever\nwhatever makes conversations pleasant for humans\nokay all right um so so you're saying uh it's possible to\nachieve in the digital space in some sense i would almost ask that question why wouldn't that be possible\nright well i have this argument with my dad all the time he thinks that touch and smell are\nreally important so they can be very important and i'm saying the initial systems they won't\nhave it still i wouldn't like their people being born\nwithout these senses and you know i believe that they can still fall in love and have meaningful life\nyeah i wonder if it's uh possible to go close to all the way by just training on\ntranscripts of conversations like i wonder how far that takes us so i i think that actually still you want\nimages like i would like so i don't have kids but like i could imagine the having ai tutor it has to see you know\nkids drawing some pictures on their paper and and also facial expressions and all\nthat kind of stuff we use uh dogs and humans use their eyes and uh to communicate with each other i\nthink this that's that's a really powerful mechanism of communication body language too that uh words are much uh lower\nbandwidth and for body language we still you know we kind of have a system that displays an image\nof its or facial expression on the computer it doesn't have to move you know mechanical pieces or so so i think\nthat uh you know there is like kind of a progression you can imagine that text might be the simplest to tackle\nbut this is not a complete human experience at all you expand it to\nlet's say images both for input and output and what you describe is actually the final\ni guess frontier what makes us human the fact that we can touch each other or smell or so and it's the hardest from\nperspective of data and deployment and i okay i believe that these things might\nhappen gradually are you excited by that possibility this particular application of\nhuman to ai friendship and interaction so let's see\nlike would you uh do you look forward to a world you you said you're living with a few folks and you're very close\nfriends with them do you look forward to a day where one or two of those friends are ai systems so if the system would be truly wishing\nme well rather than being in the situation that it optimizes for my time to interact\nwith the system the line between those is it's a gray\nit's a gray area i i think that's the distinction between love and possession and these things\nthey might be often correlated for humans but it's it like like a you you\nmight find that there like some friends with whom you haven't spoken for months yeah and then you know you pick up the\nphone it's as the time hasn't passed they are not holding to you\nand i will i wouldn't like to have ai system that you know it's it's\ntrying to convince me to spend time with it i would like the system to optimize\nfor what i care about and help me in achieving my own goals\nbut there's some i mean i don't know there's some manipulation there's some possessiveness there's some insecurities\nthis fragility all those things are necessary to form a close friendship over time to\ngo through some dark [ __ ] together some bliss and happiness together i feel like\nthere's a lot of greedy self-centered behavior within that process my intuition but i might be wrong is\nthat human computer interaction doesn't have to go through uh\ncomputer being greedy possessive and so on it is possible to train systems maybe\nthat they actually you know they are i guess prompted or fine-tuned or so\nto truly optimize for what you care about and you could imagine that you know\nthat the way how the process would look like is at some point\nwe as a human as we look at the transcript of the conversation or like an entire interaction and we say\nactually here there was more loving way to go about it and we supervise system\ntoward being more loving or maybe we train the system such that it has a reward function toward being\nmore loving yeah or maybe the possibility of the system being an [ __ ]\nand manipulative and possessive every once in a while is a feature not a bug\nbecause some of the happiness that we experience when two\nsouls meet each other when two humans meet each other is a kind of break from the [ __ ] in the world\nand so you need [ __ ] and ai as well because like it'll be like a breath of fresh air to\ndiscover an ai that the three previous ais you had are too\nfriendly or no no or or cruel or whatever it's like some kind of mix\nand then this one is just right but you need to experience the full spectrum like i think you need to be able to\nengineer [ __ ] so let's see\nbecause there's some level to us of being appreciate to appreciate the human\nexperience we need the dark and the light so that kind of reminds me\num i met a while ago at the meditation retreat uh one woman and\num you know beautiful beautiful woman and she had a she had a crutch okay she\nhad the trouble uh walking on one deck i asked her what has happened\nand she said that five years ago she was in maui hawaii\nand she was eating a salad and some snail fell into the salad and apparently\nthere are neurotoxic snails over there and she got into coma for a year okay oh wow\nand apparently there is you know high chance of even just dying but she was in the coma at some point\nshe regained partially consciousness she was able to hear people in the room\npeople behave as she wouldn't be there you know at some point she started being\nable to speak but she was mumbling like a barely able to to express herself and\nat some point she got into wheelchair then at some point she actually noticed that she can move her uh\na toe and then she knew that she will be able to walk and then you know that's where she was\nfive years after and she said that since then she appreciates the fact that she can move her toe\nand i was thinking do i need to go through such experience to appreciate that i have i can move my\ntoe well that's really good story a really deep example yeah and in some sense it might be the case\nthat we don't see light if we haven't went through the darkness but i wouldn't say that we\nshould we shouldn't assume that that's the case which may we maybe will do engineer shortcuts\nyeah ilia had this you know belief that maybe one has to go for a week or six months\nto some challenging camp yeah to just experience you know a lot of\ndifficulties and then comes back and actually everything is bright everything is beautiful i'm with iliana it must be a\nrussian thing where are you from originally i'm i'm polish polish okay\ni'm tempted to say that explains a lot but uh yeah there's something about the russian the necessity of suffering i\nbelieve i believe suffering or rather struggle is necessary i believe that\nstruggle is necessary i mean in some sense you even look at the story of any superhero\nin that movie it's not that it was like everything like it goes easy easy i like how that's your ground truth\nit's the story of superheroes okay uh you mentioned that you used to do research at night and go to bed at like\n"}
{"pod": "Lex Fridman Podcast", "input": "Sleep", "output": "6 a.m or 7 a.m i still do that often\num what uh sleep schedules have you tried to make for a productive and happy life\nlike is there um is there some interesting wild sleeping patterns that you engaged that\nyou found that works really well for you i tried at some point decreasing number of hours of sleep like\ngradually like a half an hour every few days less you know i was hoping to just save time\nthat clearly didn't work for me like at some point there's like a phase shift and i felt tired all the time\nuh you know there was a time that i used to work during the nights the nice thing\nabout the nights is that no one disturbs you and even i remember\nwhen i was meeting for the first time with greg brookman his cto and chairman of openai\nour meeting was scheduled to 5 pm and i overstepped for the meeting\nover slept for the meeting yeah 5 p.m yeah now you sound like me that's hilarious okay yeah and uh at the moment\nin some sense uh my sleeping schedule also has to do with the fact that i'm\ninteracting with people i sleep without an alarm so\nso yeah the the team thing you mentioned extrovert thing because most humans operate during a certain set\nof hours you're forced to then operate at the same set of hours\nbut i'm not quite there yet i found a lot of joy just like you said\nworking through the night because it's quiet because the world doesn't disturb you\nand there's some aspect counter to everything you're saying there's some joyful aspect to sleeping\nthrough the mess of the day because uh people are having meetings and sending emails and there's drama\nmeetings i can sleep through all the meetings you know i have meetings every day and they prevent me from having\nsufficient amount of time for focus work and\nthen i modified my calendar and i said that i'm out of office wednesday thursday and\nfriday every day and i'm having meetings only monday and tuesday and that vastly\npositively influenced my mood that i have literally like had three days for fully focused work yeah so there's\nbetter solutions to this problem than staying awake all night okay you've been part of development of some\n"}
{"pod": "Lex Fridman Podcast", "input": "Generating good ideas", "output": "of the greatest ideas in artificial intelligence what would you say is your process for developing good novel ideas\nyou have to be aware that clearly there are many other brilliant people around so\nyou have to ask yourself a question why the given idea\nlet's say wasn't uh tried by someone else and in some sense it has to\ndo with you know kind of simple it might sound simple but like i'm thinking outside of\nthe box and what do i mean here so for instance for a while people in academia they assumed\nthat you have a fixed data set and then you optimize the algorithms\nin order to get the best performance and that was so in great assumption\nthat no one thought about training models on anti-internet\nor like that that maybe some people thought about it but if it felt too too\nmany as unfair and in some sense that's almost like a it's not my idea or so but that's an\nexample of breaking a typical assumption so you want to be in the paradigm that\nyou are breaking a typical assumption in the context of the ai community\ngetting to pick your dataset as cheating correct and in some sense so that was a that was assumption that many people had\nout there and then if you free yourself from assumptions\nthen they are likely to achieve something that others cannot do and in some sense if you are\ntrying to do exactly the same things as others it's very likely that you're gonna have the same results yeah i\nbut there's also that kind of tension which is uh asking yourself the question why\nhaven't others done this because um\ni mean i get a lot of good ideas but i think probably most of them suck\nwhen they meet reality so so actually i think the other big piece\nis uh getting into habit of generating ideas training your brain toward generating ideas and not even\nsuspending judgment of the ideas so in some sense i noticed myself that\neven if i'm in the process of generating ideas if i tell myself oh that was a bad idea\nthen that actually interrupts the process and i cannot generate more ideas because i'm actually focused on the\nnegative part why it won't work yes but i created also environment in the way\nthat it's very easy for me to to store new ideas so for instance next to my bed\ni have a voice recorder and it happens to me often like i wake\nup in that during the night and i have some idea in the past i was writing them down on my phone but that\nmeans you know turning off this turning on the screen and that wakes me up or like pulling a paper which requires you\nknow turning on the light these days i just start recording it\nwhat do you think i don't know if you know who jim keller is i know team color he's a big proponent of thinking hard on\na problem right before sleep so that he can sleep through it and solve it in a sleep\nor like come up with radical stuff in his sleep he was trying to get me to do this so\nit happened from my experience perspective it happened to me many times during the high school\ndays when i was doing mathematics that i had the solution to my problem as\ni woke up at the moment regarding thinking hard\nabout the given problem is i'm trying to actually devote substantial amount of time to think\nabout important problems not just before the sleep like i'm organizing amount of the huge\nchunks of time such that i'm not constantly working on the urgent problems but i actually have time to\nthink about the important one so you do it naturally but his idea is that you kind of\nprime your brain to make sure that that's the focus you know oftentimes people have other worries in their life that's not\nfundamentally deep problems like i don't know uh just stupid drama in your life\nand even at work all that kind of stuff he wants to kind of pick the most important problem\nthat you're thinking about and go to bed on that i think that's why i mean the other thing that comes to my mind is\nalso i feel the most fresh in the morning so during the morning i try to work on\nthe most important things rather than i'm just being pulled by urgent things or checking email or so\nwhat do you do with the cause i've been doing the voice recorder thing too but i end up recording so many messages it's\nhard to organize i have the same problem now i have heard that google pixel is really good in\ntranscribing text and i might get a google pixel just for the sake of transcribing text yeah people listening\nto this if you have a good voice recorder suggestion that transcribed please let me know i it's some of it is uh this has to do\nwith uh uh open ai codex too like some of it is\nsimply like the friction i need uh apps that remove that friction between\nvoice and the organization of the resulting transcripts and all that kind of stuff\num but yes you're right absolutely like during uh for me it's walking sleep too\nbut walking and running especially running get a lot of thoughts during running and\nthere's there's no good mechanism for recording thoughts so one more thing that i do i have a\nseparate phone uh which i which has no apps and maybe it has like a\naudible or let's say kindle no one has this phone number this kind of my meditation phone yeah and\ni try to expand the amount of time that that's the phone that i'm having i it has also\ngoogle maps if i need to go somewhere and i also use this phone to write down ideas\nah that's really good idea that's a really good idea often actually what i end up doing is even sending a\nmessage from that phone to the other phone so that's actually my way of recording messages or i just put them\ninto notes i love it what advice would you give to a young person high school\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "college about how to be successful you've done a lot of incredible things in the past\ndecade so maybe maybe of some something there might be something there might be something\ni mean might sound like a simplistic or so but i would say literally just\nfollow your passion double down on it and if you don't know what's your passion just figure out what could be a\nwhat could be a passion so this that might be an exploration when i was in elementary school was math\nand chemistry and i remember for some time i gave up on math because\nmy school teacher she told me that i'm dumb and i i i guess maybe an advice would be\njust ignore people if they tell you that you're dumb you mentioned something offline about\nchemistry and explosives um what was that about so let's see\nso a story goes like that i can\ni got into chemistry maybe i was like a second grade of my elementary school\nthird grade uh i started going to chemistry classes uh\ni i really love building stuff and i did all the experiments that they\ndescribed in the book okay you know how to create oxygen with vinegar and\nand baking soda so okay so i did all the experiments and at some point i was you know so\nwhat's next what can i do and uh explosives they also it's like a you\nhave a clear reward signal you know if the thing worked or not so i remember\nat first i got i got interested in producing hydrogen that was kind of funny experiment from school you can\njust burn it and then i moved to uh nitroglycerin so that's also\nrelatively easy to synthesize i started producing essentially dynamite and detonating with it with my\nfriend i remember there was a you know there was at first like maybe two attempts that i went with a friend\nto detonate what we built and it didn't work out and like a third time he was like ah it won't work like\nuh let's don't waste time and um now we were\ni was carrying this uh this you know that tube with dynamite i don't\nknow pound or so dynamite in my backpack or like riding on the bike to the edges of the city\n[Laughter] yeah and attempt number three\nthis would be to number three attempt number three and uh now we we dig a hole to\nuh put it inside it actually had the uh you know electrical detonator\nwe we draw a cable behind the tree i even i never had i haven't ever seen\nlike a explosion before so i thought that there will be a lot of sound and but you know we're like laying down\nand i'm holding the cable and the battery at some point you know it kind of like a three to one\nand uh i just connected it and it felt like at the ground shake it was like a more like\na sound and then the soil started kind of lifting up and\nstarted falling on us yeah wow and then uh now the friends said let's let's make\nsure next time we have helmets but also you know i'm happy that nothing happened to me it\ncould have been the case that i i lost the limb or so yeah but that's childhood\nof an engineering mind with a strong reward signal of an explosion\ni love it my there's some aspect of uh chemists the the the chemist i know like my dad\nwith plasma chemistry plasma physics he was very much into explosives too it's a worrying quality of\npeople that work in chemistry that they love i think it is that exactly is the\nthe strong signal that the thing worked there is no doubt there's no doubt there's some magic it's almost like a\nreminder that physics works that chemistry works it's cool it's almost\nlike a little glimpse at nature that you yourself engineer i that's why i really\nlike artificial intelligence especially robotics is you create a little piece of nature\nand in some sense even for me with explosives the motivation was creation rather than distraction yes exactly\n"}
{"pod": "Lex Fridman Podcast", "input": "Getting started with machine learning", "output": "in terms of advice i forgot to ask about just machine learning and deep learning for people who are specifically\ninterested in machine learning how would you recommend they get into the field so um i would say implement everything\nand also there is plenty of courses so like from scratch um so on different levels of abstraction in some sense but\ni would say brain or implement something from scratch or implement something from a paper or implement something you know\nfrom podcasts that you have heard about i would say that's a powerful way to understand things so it's often the case\nthat you read the description and you think you understand but you truly\nunderstand once you build it then you actually know what really meant that in the description\nis there particular topics that you find people just fall in love with so i i've seen\ni tend to uh really enjoy reinforcement learning because it's it's much more\nit's much easier to get to a point where you feel like you created something special\nlike like fun games kind of things that are rewarding it's rewarding yeah uh as opposed to like\nuh reimplementing from scratch more like supervised learning kind of things it's it's yeah so you know if if\nsomeone would optimize for things to be rewarding then it feels that the things that are somewhat generative they have\nsuch a property so yes you have for instance yes adversarial networks or you have just even generated language models\nand you could you can even see um internally we have seen this thing with our releases so we have a we\nreleased recently two models there is one model called dali that generates images and there is other model called\nclip that actually uh you provide various possibilities what could be the\nanswer to what is on the picture and it can tell you which one is the most likely okay and in some sense in case of the\nfirst one dali it is very easy for you to understand that actually there is magic going on uh\nand in the in case of the second one even though it is insanely powerful and you know people from a vision community\nthey as they started probing it inside they actually understood\nhow far it goes it's difficult for person at first to see\nhow well it works and that's the same as you said that in case of supervised learning models you\nmight not kind of see or it's not that easy for you to understand the the strength\neven though you don't believe in magic to see the magic let's say that magic it's a generative that's really\nbrilliant so anything that's generative because then you are at the core of the creation you get to\nexperience creation without much effort unless you have to do it from scratch but and it feels that\nyou know humans are wired there is some level of reward for creating stuff yeah\nlike of course different people have a different weight on this reward yeah in the big objective function in the big\nobjective of a person of a person uh you wrote that beautiful\n"}
{"pod": "Lex Fridman Podcast", "input": "What is beauty?", "output": "is what you intensely pay attention to even a cockroach is beautiful if you\nlook very closely can you expand on this what is beauty\nso what i'm i wrote here actually corresponds to my subjective experience\nthat i had through extended periods of meditation it's it's pretty crazy that at some\npoint the meditation gets you to the place that you have really increased uh focus increase attention\nand then you look at the very simple objects that were all the time around you can look at the table or on the pen\nor at that nature and you notice more and more details\nand it becomes very pleasant to look at it and it once again it kind of reminds me\nmy childhood uh like i just pure joy of being\nit's also i have seen even the reverse effect that by default regardless of what we possess\nwe very quickly get used to it and you know you can have a very beautiful house\nand if you don't put sufficient effort you're just gonna get used to it\nand it doesn't bring any more joy regardless of what you have yeah well i actually\ni find that material possessions get in the way of that experience of\npure joy so i've always i've been very fortunate\nto just find joy in simple things just just like you're saying just like i don't know objects in my\nlife just stupid objects like this cup like thing you know just objects sounds\nokay i'm not being eloquent but literally objects in the world they're just full of joy because it's\nlike i can't believe one i can't believe that i'm fortunate enough to be alive to\nexperience these objects and then two i can't believe humans are clever enough to have built\nthese objects the the hierarchy of pleasure that that uh provides is infinite i mean even if\nyou look at the cup of water so you know you see first like a level of like a reflection of light but then you think\nyou know man there's like a trillions upon of trillions of particles bouncing uh against each other there is also uh\nthe tension on the surface that you know if the back back could like a stand on it and move around and you think it also\nhas this like a magical property that as you decrease temperature it actually expands in volume which\nallows for the you know legs to freeze on the on the surface and then at the bottom to have\nactually uh not freeze which allows for life like a crazy yeah you look\nin detail at some object and you think actually you know this table that was just the figment of someone's\nimagination at some point and then there was like thousands of people involved to actually manufacture it yeah and put it\nhere and by default no one cares [Laughter] and then you can start thinking about\nevolution how it all started from single cell organisms that led to this table and and okay\nthese thoughts they give me life appreciation yeah and even lack of those just the pure raw\nsignal also gives their life appreciation see the thing is and then that's coupled\nfor me with the sadness that the whole ride ends and perhaps is deeply coupled in that\nthe fact that this experience this moment ends gives it gives it an intensity that i'm not sure\ni would otherwise have so in that same way i try to meditate on my own death often\n"}
{"pod": "Lex Fridman Podcast", "input": "Death", "output": "do you think about your mortality are you afraid of death\nso fear of death is like one of the most fundamental fears that each of us has we\nmight be not even aware of it it requires to look inside to even recognize that it's\nout there and there is still let's say this property of uh nature that if things would last\nforever then they would be also boring to us the fact that the things change in some\nway gives any meaning to them i also you know found out that\nit seems to be very healing to people to\nhave this short experiences uh like i guess psychedelic experiences\nin which they experience death of self\nin which they let go of this fear and then maybe can even increase the\nappreciation of the moment and it seems that many people they uh\nthey they can easily comprehend fine the fact that their money is finite while they\ndon't see that time is finite i have this like a discussion with ilya from time to time he's saying you know\nman like uh the lack will pass very fast at some point i will be 40 50 60 70 and then\nit's over this is true which also makes me believe that you know that every single moment\nit is so unique that should be appreciated and this also\nmakes me think that i should be acting on my life because otherwise it will pass\ni also like this framework of thinking from jeff bezos on regret minimization\nthat like i would like if i will be at that death bed to look back on my life\nand and not regret that i haven't done something it's usually you might\nregret that you haven't tried i'm fine with failing\ni haven't tried uh what's the nature eternal occurrence tried to live a life that if you had to\nlive it infinitely many times that would be the you'll be okay with\nthat kind of life so try to live it optimally i can say that\nit's almost like i'm unbelievable to me\nwhere i am in my life i'm extremely grateful for actually people whom i met i would say i think that i'm\ndecently smart and so on uh but i think that\nactually to great extent where i am has to do with that people who i met\nwould you be okay if after this conversation you died so\nif i'm dead then it kind of i don't have a choice anymore so in some sense there's like plenty of\nthings that i would like to try out in my life [Music] i feel that you know i'm gradually going\none by one and i'm just doing them i think that the list will be always infinite yeah\nso might as well go today yeah i mean to be clear i'm not looking forward to die\ni would say if there is no choice i would accept it but like uh in some sense i'm\nif there would be a choice if there would be possibility to live i would fight for a living\ni find um it's more honest than real to think\nabout you know dying today at the end of the day that seems to me to at least to my brain\nmore honest slap in the face as opposed to i still have 10 years\nlike today then then i'm much more about appreciating the cup and the table and so on and less about like silly worldly\naccomplishments and all those kinds of things we we have in the company a person who\nsay at some point found out that they have cancer and that also gives you know huge perspective with respect to what\nmatters now yeah and you know often people in situations like that they conclude that actually what matters is\nhuman connection and love and uh that's people conclude also\nif you have kids because kids is family you uh i think tweeted\nwe don't assign the minus infinity reward to our death such a reward would prevent us from\ntaking any risk we wouldn't be able to cross the road in fear of being hit by a car so in the objective function you\nmentioned fear of death might be fundamental to the human condition so\nas i said let's assume that they're like a reward functions in our brain and\nand the interesting thing is even realization how different reward\nfunctions can play with your behavior as a matter of fact i wouldn't say that\nyou should assign infinite negative reward to anything because that messes up the math\nthe math doesn't work out it doesn't work out and as you said even you know uh government or some insurance\ncompanies you said they assign 99 million dollars to human life yeah and i'm just saying it with respect to\nthat might be a harsh statement to ourselves but in some sense that there is a finite value of our own life\ni'm trying to put it from perspective of being less of being more egoless\nand realizing fragility of my own life and in some sense\nthe fear of death might prevent you from acting\nbecause anything can cause death yeah and i'm sure actually if you were\nto put death in the objective function there's probably so many aspects to death and fear of death and\nrealization of death and mortality there's just whole components of\nfiniteness of not just your life but every experience and so on you're gonna have\nto formalize mathematically and also you know that might lead to\num you spending a lot of compute cycles on this like a\nand deliberating this terrible future instead of experiencing now\nand that in some sense is also kind of unpleasant simulation to run in your head yeah\n"}
{"pod": "Lex Fridman Podcast", "input": "Meaning of life", "output": "do you think there's an objective function that describes the entirety of uh\nhuman life so you know usually the way you ask that is what is the meaning of life\nis there um a universal objective functions that captures the why of life so yeah i mean\ni suspected that they will ask this question but it's also a question that i asked myself many many times\nsee i can tell you a framework that i have these days to think about these questions so i think that fundamentally\nmeaning of life has to do with some of our reward functions that we have in brain and they might have to\ndo with let's say for instance curiosity or human connection which might mean\nunderstanding others it's also possible for a person to\nslightly modify their reward function usually they mostly stay fixed but it's possible to modify reward function and\nyou can pretty much choose so in some sense reward functions optimizing reward functions they will give you life\nsatisfaction is there some randomness in the function i think when you are born there is some\nrandomness like you can see that some people for instance they they care more about building stuff some\npeople care more about caring for others some people that there are all sorts of uh default\nreward functions and then in some sense you can ask yourself what's this like\nwhat is the satisfying way for you to go after this reward function and you just go after this reward function and you\nknow some people also ask are these reward functions real i almost think about it\nas let's say if you would have to discover mathematics\nin mathematics you are likely to run into various objects like a complex numbers\nor differentiation some other objects and these are very natural objects that arise and similarly the reward functions\nthat we are having in our brain they are somewhat very natural that you know there is a reward function for\nfor understanding like a comprehension yeah uh curiosity and so on so in some sense\nthey are in the same way natural as their natural objects in mathematics\ninteresting so you know there's the uh the old sort of debate is mathematics invented\nor discovered you're saying reward functions are discovered so nature so nature's provided some you can still\nlet's say expanded throughout the life some of the reward functions they might be futile like for instance there might\nbe a reward function maximize amount of wealth yeah and this is more like a a learning\nreward function and but we know also that some reward functions if you optimize them you won't\nbe quite satisfied well i don't know which part of your\nreward function resulted in you coming today but i am deeply appreciative that you did spend your valuable time with me\nwatching is really fun talking to you you're you're brilliant you're a good human being and it's an honor to meet\nyou and an honor to talk to you thanks for talking today brother thank you alex a lot i appreciated your\nquestions here i had a lot of time being here thanks for listening to this\nconversation with welch and ramba to support this podcast please check out our sponsors in the description\nand now let me leave you some words from arthur c clarke who is the author of\n2001 a space odyssey it may be that our role on this planet\nis not to worship god but to create him thank you for listening and hope to see\nyou next time\nyou\n"}

{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "- The problem is that we do not get 50 years to try and try again and observe that we were wrong and come up with a different theory\nand realize that the entire thing is going to be like way more difficult than realized at the start, because the first time you fail at aligning something\nmuch smarter than you are, you die. - The following is a conversation with Eliezer Yudkowsky,\na legendary researcher, writer and philosopher on the topic of artificial intelligence,\nespecially super intelligent AGI and its threat to human civilization.\nThis is the Lex Friedman podcast to support it. Please check out our sponsors in the description.\nAnd now, dear friends, here's Eliezer Yudkowsky.\n"}
{"pod": "Lex Fridman Podcast", "input": "GPT-4", "output": "What do you think about GPT-4? How intelligent is it? - It is a bit smarter than I thought\nthis technology was going to scale to, and I'm a bit worried about what the next one will be like.\nLike this particular one I think, I hope there's nobody inside there 'cause you know,\nit'd sucked to be stuck inside there. But we don't even know the architecture at this point\n'cause OpenAI is very properly not telling us. And yeah, like giant inscrutable matrices\nof floating point numbers, I don't know what's going on in there. Nobody knows what's going on in there. All we have to go by are the external metrics\nand on the external metrics, if you ask it to write a self-aware 4chan green text,\nit will start writing a green text about how it has realized that it's an AI writing a green text and , oh well.\nSo that's probably\nnot quite what's going on in there in reality. But we're kind of blowing past\nall the science fiction guardrails. Like we are past the point where in science fiction people\nwould be like, \"Whoa, wait, stop, that thing's alive. \"What are you doing to it?\" And it's probably not, nobody actually knows.\nWe don't have any other guardrails. We don't have any other tests. We don't have any lines to draw on the sand and say like,\nwell, when we get this far we will start to worry about\nwhat's inside there. So if it were up to me, I would be like, okay, like this far, no further, time for the summer of AI\nwhere we have planted our seeds and now we like wait and reap the rewards of the technology\nwe've already developed and don't do any larger training runs than that. Which to be clear I realize requires more than one company\nagreeing to not do that. - And take a rigorous approach for the whole AI community\nto investigate whether there's somebody inside there. - That would take decades.\nLike having any idea of what's going on in there? People have been trying for a while. - It's a poetic statement about\nif there's somebody in there, but as I feel like it's also a technical statement or I hope it is one day, which is a technical statement,\nthat Alan Turing tried to come up with with the Turing Test. Do you think it's possible to definitively or approximately\nfigure out if there is somebody in there, if there's something like a mind\ninside this large language model? - I mean there's a whole bunch of different sub-questions here.\nThere's a question of, is there consciousness?\nIs there equalia? Is this a object of moral concern? Is this a moral patient,\nlike should we be worried about how we're treating it? And then there's questions like, how smart is it exactly?\nCan it do X, can it do Y? And we can check how it can do X and how it can do Y.\nUnfortunately we've gone and exposed this model to a vast corpus of text of people discussing consciousness\non the internet, which means that when it talks about being self-aware, we don't know to what extent it is repeating back what it\nhas previously been trained on for discussing self-awareness or if there's anything going on in there such that it would\nstart to say similar things spontaneously. Among the things that one could do if one were at all\nserious about trying to figure this out is train GPT-3\nto detect conversations about consciousness, exclude them all from the training data sets,\nand then retrain something around the rough size of GPT-4 and no larger with all of the discussion\nof consciousness and self-awareness and so on missing, although, you know, hard, hard bar to pass.\nHumans are self-aware and we're like self-aware all the time. We like talk about what we do all the time,\nlike what we're thinking at the moment all the time. But nonetheless, get rid of the explicit discussion of consciousness.\nI think therefore I am and all that. And then try to interrogate that model and see what it says.\nAnd it still would not be definitive, but nonetheless, I don't know.\nI feel like when you run over the science fiction guard rails, like maybe this thing, but what about GPT?\nMaybe not this thing, but what about GPT-5? Yeah, this, this would be a good place to pause.\n- On the topic of consciousness, there's so many components to even just removing\nconsciousness from the dataset. Emotion, the display of consciousness,\nthe display of emotion feels like deeply integrated with the experience of consciousness.\nSo the hard problem seems to be very well integrated with the actual surface level illusion of consciousness.\nSo displaying emotion, I mean, do you think there's a case to be made that we humans,\nwhen we're babies are just GPT, that we're training on human data on how to display emotion versus feel emotion,\nhow to show others, communicate others that I'm suffering, that I'm excited,\nthat I'm worried, that I'm lonely and I missed you and I'm excited to see you? All of that is communicated,\nthat's a communication skill versus the actual feeling that I experience. So we need that training data as humans too,\nthat we may not be born with that, how to communicate the internal state. And that's in some sense if we remove that\nfrom GPT-4's dataset, it might still be conscious but not be able to communicate it?\n- So I think you're gonna have some difficulty removing all mention of emotions from GPT'S data set.\nI would be relatively surprised to find that it has developed exact analogs of human emotions in there.\nI think that humans will have emotions\neven if you don't tell them about those emotions when they're kids. It's not quite exactly what various blank slatists\ntried to do with the new Soviet man and all that, but you know, if you try to raise people perfectly altruistic, they still come out selfish.\nYou try to raise people sexless, they still develop sexual attraction.\nWe have some notion in humans, not in AIS, of where the brain structures are\nthat implement this stuff. And it is really remarkable thing I say in passing that\ndespite having complete read access to every floating point number in the GPT series,\nwe still know vastly more about the architecture of human thinking than we know about what\ngoes on inside GPT, despite having like vastly better ability to read GPT.\n- Do you think it's possible? Do you think that's just a matter of time? Do you think it's possible to investigate and study the way\nneuroscientists study the brain, which is look into the darkness, the mystery of the human brain by just desperately trying to\nfigure out something and to form models and then over a long period of time actually start to figure out what regions\nof the brain do certain things? What different kinds of neurons when they fire, what that means, how plastic the brain is,\nall that kind of stuff. You slowly start to figure out different properties of the system. Do you think we can do the same thing with language models?\n- Sure. I think that if, you know, like half of today's physicists stop wasting their lives on string theory or whatever--\n(indistinct) And go off and study what goes on inside transformer networks, then in, you know,\nlike 30, 40 years we'd probably have a pretty good idea. - Do you think these large language models can reason?\n- They can play chess. How are they doing that without reasoning? - So you're somebody that spearheaded\nthe movement of rationality. So reason is important to you. So is that a powerful important word or is it...\nHow difficult is the threshold of being able to reason to you and how impressive is it?\n- I mean, in my writings on rationality, I have not gone making a big deal out of something called reason.\nI have made more of a big deal out of something called probability theory. And that's like,\nwell, you're reasoning but you're not doing it quite right and you should reason this way instead.\nAnd interestingly, people have started to get preliminary results showing\nthat reinforcement learning by human feedback has made\nthe GPT series worse in some ways. In particular, like it used to be well calibrated.\nIf you trained it to put probabilities on things, it would say 80% probability and be right eight times out of 10.\nAnd if you apply reinforcement learning from human feedback, the nice graph of 70%,\nseven out of 10 sort of flattens out into the graph that humans use where there's some very improbable stuff\nand likely, probable, maybe, which all means like around 40%, and then certain.\nSo it's like it used to be able to use probabilities, but if you try to teach it to talk in a way\nthat satisfies humans, it gets worse at probability in the same way that humans are.\n- And that's a bug, not a feature. - I would call it bug, although such a fascinating bug.\nBut yeah, so reasoning, it's doing pretty well on various tests that people\nused to say would require reasoning. But you know, rationality is about, when you say 80%,\ndoes it happen eight times out of 10? - So what are the limits to you of these transformer networks, of neural networks?\nIf reasoning is not impressive to you, or it is impressive but there's other levels to achieve?\n- I mean it's just not how I carve up reality. - If reality is a cake,\nwhat are the different layers of the cake or the slices? How do you cover it? Or you can use a different food if you.\n- I don't think it's as smart as a human yet. Like back in the day I went around saying like,\nI do not think that just stacking more layers of transformers is going to get you all the way to AGI.\nAnd I think that GPT-4 is passed where I thought this paradigm was going to take us.\nAnd you want to notice when that happens, you wanna say like, whoops, well, I guess I was incorrect about what happens if you keep\non stacking more transformer layers, and that means I don't necessarily know what GPT-5 is going to be able to do.\n- That's a powerful statement. So you're saying like your intuition initially is now\nappears to be wrong. - Yeah. - It's good to see that you can admit in some of your\npredictions to be wrong. You think that's important to do? Because throughout your life,\nyou've made many strong predictions and statements about reality and you evolve with that.\nSo maybe that'll come up today about our discussion. So you're okay being wrong?\n- I'd rather not be wrong next time.\nIt's a bit ambitious to go through your entire life never having been wrong.\nOne can aspire to be well calibrated, like not so much think in terms of, was I right,\nwas I wrong? But like when I said 90% that it happened nine times out of 10.\nYeah, oops is the sound we emit when we improve.\n- Beautifully said. And somewhere in there, we can connect the name of your blog, Less Wrong.\nI suppose that's the objective function. - The name Less Wrong was I believe suggested\nby Nick Bostrom and it's after someone's epigraph, actually forget whose, who said like, \"We never become right.\n\"We just become less wrong.\" What's the something, something that's easy to confess,\njust error and error and error again, but less and less and less.\n- Yeah, that's a good thing to strive for. So what has surprised you about GPT-4 that you found\nbeautiful as a scholar of intelligence, of human intelligence, of artificial intelligence, of the human mind?\n- I mean the beauty does interact with the screaming horror.\n(Lex laughing) - [Lex] Is the beauty in the horror? - But like beautiful moments, well, somebody asked Bing's Sydney to describe herself\nand fed the resulting description into one of the stable diffusion things I think.\nAnd you know, she's pretty, and this is something that should have been\nlike an amazing moment. Like the AI describes herself. You get to see what the AI thinks the AI looks like. Although, you know, the thing that's doing the drawing\nis not the same thing that's outputting the text. And it does happen the way that it would happen,\nthat it happened in the old school science fiction when you ask an AI to make a picture of what it looks like,\nnot just because we're two different AI systems being stacked that don't actually interact. It's not the same person, but also because the AI was trained by imitation in a way\nthat that makes it very difficult to guess how much of that it really understood.\nAnd probably not actually a whole bunch. Although GPT-4 is like multimodal and can draw\nvector drawings of things that make sense and does appear to have some kind of spatial visualization\ngoing on in there. But the pretty picture of the girl\nwith the steampunk goggles on her head, if I'm remembering correctly, what she looked like,\nit didn't see that in full detail. It just made a description of it\nand stable diffusion output it. And there's the concern about how much the discourse\nis going to go completely insane once the AIs all look like that\nand actually look like people talking.\nAnd yeah, there's another moment where somebody is asking Bing about like,\n\"Well, I fed my kid green potatoes \"and they have the following symptoms\" and Bing is like,\n\"That's solanine poisoning and call an ambulance\" and the person's like, \"I can't afford an ambulance,.\n\"I guess if this is time for like my kid to go, \"that's God's will\" and the main Bing thread gives\nthe message of, \"I cannot talk about this anymore.\" And the suggested replies to it say,\n\"Please don't give up on your child. \"Solanine poisoning can be treated if caught early.\"\nAnd you know, if that happened in fiction, that would be like the AI cares, the AI is bypassing the block on it to try\nto help this person and is it real? Probably not. But nobody knows what's going on in there.\nIt's part of a process where these things are not happening in a way where we,\nsomebody figured out how to make an AI care and we know that\nit cares and we can acknowledge it's caring now. It's being trained by this imitation process\nfollowed by reinforcement learning by human feedback. And we're trying to point it in this direction\nand it's pointed partially in this direction and nobody has any idea what's going on inside it. And if there was a tiny fragment of real caring in there,\nwe would not know. It's not even clear what it means exactly. And things aren't clear cut in science fiction.\n- We'll talk about the horror and the terror and the trajectories\nthis can take. But this seems like a very special moment, just a moment where we get to interact with the system that\nmight have care and kindness and emotion and maybe something like consciousness, and we don't know if it does,\nand we're trying to figure that out and we're wondering about what it means to care.\nWe're trying to figure out almost different aspects of what it means to be human, about the human condition by looking at this AI that has\nsome of the properties of that. It's almost like this subtle fragile moment in the history of the human species.\nWe're trying to almost put a mirror to ourselves here. - Except that's probably not yet,\nit probably isn't happening right now. We are boiling the frog.\nWe are seeing increasing signs bit by bit,\nbut not like spontaneous signs, because people are trying to train the systems to do that\nusing imitative learning and the imitative learning is spilling over and having side effects\nand the most photogenic examples are being posted to Twitter, rather than being examined in any systematic way.\nSo when you are boiling a frog like that,\nfirst is going to come the Blake Lemoines, like first you're going to have, you're gonna have like a thousand people looking at this.\nand the one person out of a thousand who is most credulous about the signs is going to be like,\n\"That thing is sentient.\" Well, 999 out of a thousand people think,\nalmost surely correctly, though we don't actually know, that he's mistaken. And so the first people to say sentience\nlook like idiots and humanity learns a lesson that when something claims to be sentient\nand claims to care, it's fake because it is fake because we have been training them using imitative learning\nrather than, and this is not spontaneous, and they keep getting smarter.\n- Well, do you think we would oscillate between that kind of cynicism, that AI systems can't possibly be sentient,\nthey can't possibly feel emotion, they can't possibly, this kind of cynicism about AI systems\nand then oscillate to a state where we empathize with the AI systems,\nwe give them a chance, we see that they might need to have rights and respect and a similar role in society as humans?\n- You're going to have a (indistinct) group of people who can just never be persuaded of that\nbecause to them, being wise, being cynical, being skeptical is to be like,\noh well, machines can never do that. You're just credulous. It's just imitating. It's just fooling you.\nand they would say that right up until the end of the world and possibly even be right\nbecause you know, they are being trained on an imitative paradigm (laughing)\nand you don't necessarily need any of these actual qualities in order to kill everyone. - Have you observed yourself working through skepticism,\ncynicism and optimism about the power of neural networks? What has that trajectory been like for you?\n- It looks like neural networks before 2006 forming part of an indistinguishable, to me,\nother people might have had better distinction on it, indistinguishable blob of different AI methodologies,\nall of which are promising to achieve intelligence without us having to know how intelligence works.\nYou had the people who said that if you just manually program lots and lots of knowledge into the system line by line,\nat some point all the knowledge will start interacting. It will know enough and it will wake up.\nYou've got people saying that if you just use evolutionary computation, if you try to mutate\nlots and lots of organisms that are competing together, that's the same way that human intelligence\nwas produced in nature. So we'll do this and it will wake up without having any idea of how AI works.\nAnd you've got people saying, \"Well, we will study neuroscience \"and we will learn the algorithms off the neurons\n\"and we will imitate them \"without understanding those algorithms.\" Which was a part I was pretty skeptical, 'cause it's hard to re-engineer these things\nwithout understanding what they do. \"And so we will get AI without understanding how it works\"\nand there were people saying like, \"Well, we will have giant neural networks that we will train \"by gradient dissent, then when they're as large\n\"as the human brain, they will wake up, \"we will have intelligence without understanding \"how intelligence works.\"\nAnd from my perspective, this is all like an indistinguishable blob of people who are trying to not get to grips with the difficult problem\nof understanding how intelligence actually works. That said, I was never skeptical\nthat evolutionary computation would not work in the limit. Like you throw enough computing power at it,\nit obviously works. That is where humans come from and it turned out that you\ncan throw less computing power than that at gradient descent if you are doing some other things correctly and you will\nget intelligence without having any idea of how it works and what is going on inside.\nIt wasn't ruled out by my model that this could happen. I wasn't expecting it to happen. I wouldn't have been able to call it neural networks rather\nthan any of the other paradigms for getting intelligence without understanding it.\nAnd I wouldn't have said that this was a particularly smart thing for a species to do,\nwhich is an opinion that has changed less than my opinion about whether you or not you can actually do it.\n"}
{"pod": "Lex Fridman Podcast", "input": "Open sourcing GPT-4", "output": "- Do you think AGI could be achieved with a neural network as we understand them today?\n- Yes. Just flatly yes. The question is whether the current architecture of stacking\nmore transformer layers, which for all we know GPT-4 is no longer doing because they're not telling us the architecture,\nwhich is a correct decision. - Ooh, correct decision. I had a conversation with Sam Altman,\nwe'll return to this topic a few times. He turned the question to me of how open should open AI be\nabout GPT-4? \"Would you open source the code?\" he asked me.\nBecause I provided as criticism saying that while I do appreciate transparency, open AI could be more open.\nAnd he says, \"We struggle with this question.\" What would you do? - Change their name to closed AI and sell GPT-4\nto business backend applications that don't expose it to consumers and venture capitalists\nand create a ton of hype and pour a bunch of new funding into the area. But too late now. - Don't you think others would do it?\n- Eventually. You shouldn't do it first. If you already have giant nuclear stockpiles,\ndon't build more. If some other country starts building a larger nuclear stockpile, then sure,\neven then, maybe just have enough nukes. You know, these things are not quite like nuclear weapons.\nThey spit out gold until they get large enough and then ignite the atmosphere and kill everybody. And there is something to be said for not destroying\nthe world with your own hands, even if you can't stop somebody else from doing it. But open sourcing it, that's just sheer catastrophe.\nThe whole notion of open sourcing, this was always the wrong approach, the wrong ideal. There are places in the world where open source\nis a noble ideal and building stuff you don't understand that is difficult to control\nwhere if you could align it, it would take time, you'd have to spend a bunch of time doing it,\nthat is not a place for open source 'cause then you just have powerful things that just go\nstraight out the gate without anybody having had the time to have them not kill everyone. - So can we steam down the case\nfor some level of transparency and openness may be open sourcing?\nSo the case could be that because GPT-4 is not close to AGI, if that's the case,\nthat this does allow open sourcing or being open about the architecture being transparent,\nabout maybe research and investigation of how the thing works, of all the different aspects of it, of its behavior,\nof its structure, of its training processes, of the data it was trained on, everything like that,\nthat allows us to gain a lot of insights about alignment, about the alignment problem, to do really good AI safety\nresearch while the system is not too powerful. Can you make that case,\nthat it could be open source? - I do not believe in the practice of steel manning. There is something to be said for trying to pass\nthe ideological Turing Test where you describe your opponent's position,\nthe disagreeing person's position well enough that somebody cannot tell the difference between your description\nand their description. But steel manning, no. - Like, okay, well this is where you and I disagree here.\nThat's interesting. Why don't you believe in steel manning? - Okay, so for one thing, if somebody's trying to understand me,\nI do not want them steel manning my position. I want them to try to describe my position\nthe way I would describe it, not what they think is an improvement. - Well, I think that is what steel manning is,\nis the most charitable interpretation. - I don't want to be interpreted charitably,\nI want them to understand what I am actually saying. If they go off into the land of charitable interpretations,\nthey're like off in their land of, the stuff they're imagining and not trying to understand\nmy own viewpoint anymore. - Well, I'll put it differently then, just to push on this point. I would say it is restating what I think\nyou understand under the empathetic assumption that Eliezer is brilliant and have honestly and rigorously\nthought about the point he's made. Right? - So if there's two possible interpretations\nof what I'm saying and one interpretation is really stupid and wack and doesn't sound like me and doesn't fit\nwith the rest of what I've been saying, and one interpretation sounds like something a reasonable person who believes\nthe rest of what I believe would also say, go with the second interpretation. - That's steel manning.\n- That's a good guess. If on the other hand there's like something that\nsounds completely wack and something that sounds like, a little less completely wack but you don't see why I would\nbelieve in it, it doesn't fit with the other stuff I say, but you know, that sounds less wack and you can like sort of see,\nyou can like maybe argue it, then you probably have not understood it. - See, okay, this is fun,\n'cause I'm gonna linger on this. You know, you wrote a brilliant blog post, AGI Ruin: A List of Lethalities, right? And it was a bunch of different points and I would say that\nsome of the points are bigger and more powerful than others. If you were to sort them, you probably could.\nYou personally, and to me steel manning means like going through\nthe different arguments and finding the ones that are really the most powerful.\nIf people like tl;dr, (chuckles) like what should you be most concerned about and bringing\nthat up in a strong, compelling, eloquent way. These are the points that Eliezer would make\nto make the case, in this case that AI's gonna kill all of us. But that's what steel manning is,\nis presenting it in a really nice way, the summary of my best understanding of your perspective.\nBecause to me there's a sea of possible presentations of your perspective and steel manning\nis doing your best to do the best one in that sea of different perspectives. - Do you believe it?\n- [Lex] Do I believe in what? - Like these things that you would be presenting as like the strongest version of my perspective,\ndo you believe what you would be presenting? Do you think it's true? - I'm a big proponent of empathy.\nWhen I see the perspective of a person, there is a part of me that believes it.\nIf I understand it. Especially in political discourse, in geopolitics, I've been hearing a lot of different perspectives\non the world and I hold my own opinions, but I also speak to a lot of people\nthat have a very different life experience and a very different set of beliefs. And I think there has to be epistemic humility\nin stating what is true.\nSo when I empathize with another person's perspective, there is a sense in which I believe it is true.\nI think probabilistically, I would say, in the way that you think. - Do you bet money on it?\nDo you bet money on their beliefs when you believe them? - Are we allowed to do probability?\n- Sure, you can state a probability of that. - Yes, there's a probability, there's a probability.\nAnd I think empathy is allocating a non-zero probability to a belief.\n(Eliezer laughing) In some sense, for time.\n- If you've got someone on your show who believes in the Abrahamic deity, classical style,\nsomebody on the show who's a young Earth creationist, do you say, \"I put a probability on it and that's my empathy?\"\n- When you reduce beliefs into probabilities, it starts to get, you know,\nwe can even just go to flat Earth. Is the Earth flat? - There's the thing,\nit's a little more difficult nowadays to find people who believe that unironically, but-- - Fortunately I think, well, it's hard to know.\nUnironic from ironic. (chuckles) But I think there's quite a lot of people that believe that.\nThere's a space of argument where you're operating rationally in the space of ideas.\nBut then there's also a kind of discourse where you're operating in the space\nof subjective experiences and life experiences.\nLike I think what it means to be human is more than just searching for truth,\nis just operating of what is true and what is not true. I think there has to be deep humility that we humans are\nvery limited in our ability to understand what is true. - So what probability do you assign\nto the young Earth's creationist beliefs then? - I think I have to give non-zero.\n- Out of your humility. Yeah, but three? (laughing)\n- I think it would be irresponsible for me to give a number because the listener,\nthe way the human mind works, we're not good at hearing the probabilities.\nYou hear three, what is three exactly? They're going to hear, well, there's only three probabilities I feel like.\nZero, 50% and a 100% in the human mind or something like this.\n- Well, zero, 40%, and 100% is a bit closer to it based on what happens to ChatGPT after you RLHF it\nto speak Humanese. - That's brilliant. (both chuckling) Yeah. That's really interesting.\nI didn't know those negative side effects of RLHF. That's fascinating.\nBut just to return to the open AI, closed AI.\n- Also, like quick disclaimer, I'm doing all this from memory. I'm not pulling out my phone to look it up.\nIt is entirely possible that the things I'm saying are wrong. - So thank you for that disclaimer.\nAnd thank you for being willing to be wrong.\nThat's beautiful to hear. I think being willing to be wrong is a sign of a person\nwho's done a lot of thinking about this world and has been humbled by the mystery and the complexity of this world.\nAnd I think a lot of us are resistant to admitting we're wrong. 'Cause it hurts.\nIt hurts personally, it hurts, especially when you're a public human. It hurts publicly because people point out\nevery time you're wrong. Like, look, you changed your mind, you're a hypocrite, you're an idiot, whatever, whatever they wanna say,\n- Oh, I block those people and then I never hear from them again on Twitter. (both laughing) - Well the point is to not let that pressure,\npublic pressure affect your mind and be willing to be in the privacy of your mind to contemplate the possibility that you're wrong\nand the possibility that you're wrong about the most fundamental things you believe. Like people who believe in a particular God,\npeople who believe that their nation is the greatest nation on Earth. All those kinds of beliefs that are core\nto who you are when you came up, to raise that point to yourself in the privacy of your mind, to say, \"Maybe I'm wrong about this.\"\nThat's a really powerful thing to do. And especially when you're somebody who's thinking about\nsystems that can destroy human civilization or maybe help it flourish. So thank you, thank you for being willing to be wrong.\nAbout open AI. So you really, I just would love to linger on this.\nYou really think it's wrong to open source it? - I think that burns the time remaining\nuntil everybody dies. I think we are not on track to learn remotely near\nfast enough even if it were open sourced.\nIt's easier to think that you might be wrong about something when being wrong about something is the only way that there's hope.\nAnd it doesn't seem very likely to me that the particular\nthing I'm wrong about is that this is a great time to open source GPT-4.\nIf humanity was trying to survive at this point in the straightforward way, it would be like shutting down the big GPU clusters,\nno more giant runs. It's questionable whether we should even be throwing GPT-4 around,\nalthough that is a matter of conservatism rather than a matter of my predicting that catastrophe will follow from GPT-4.\nThat is something in which I put like a pretty low probability. But also when I say like I put a low probability on it,\nI can feel myself reaching into the part of myself that thought that GPT-4 was not possible in the first place.\nSo I do not trust that part as much as I used to. Like the trick is not just to say I'm wrong, but,\nokay, well, I was wrong about that. Can I get out ahead of that curve and predict the next\nthing I'm going to be wrong about? - So the set of assumptions or the actual reasoning system that you were leveraging in making that initial statement\nprediction, how can you adjust that to make better predictions about GPT-4, five, six?\n- You don't wanna keep on being wrong in a predictable direction. Like being wrong, anybody has to do that walking through the world.\nThere's no way you don't say 90% and sometimes be wrong. In fact (indistinct) at least one time out of 10 if you're well calibrated when you say 90%.\nThe undignified thing is not being wrong. It's being predictably wrong.\nIt's being wrong in the same direction over and over again. So having been wrong about how far neural networks would go\nand having been wrong specifically about whether GPT-4 would be as impressive as it is, when I say it like,\n\"Well, I don't actually think GPT-4 causes a catastrophe,\" I do feel myself relying on that part of me that was\npreviously wrong. And that does not mean that the answer is now in the opposite direction. Reverse stupidity is not intelligence.\nBut it does mean that I say it with a worried note in my voice. It's like still my guess,\nbut you know, it's a place where I was wrong. Maybe you should be asking Gwern, Gwern Branwen. Gwern Branwen has been like writer about this than I have.\nMaybe you ask him if he thinks it's dangerous (laughing) rather than asking me.\n- I think there's a lot of mystery about what intelligence is,\nwhat AGI looks like. So I think all of us are rapidly adjusting our model,\nbut the point is to be be rapidly adjusting the model versus having a model that was right in the first place. - I do not feel that seeing Bing\nhas changed my model of what intelligence is. It has changed my understanding of what kind of work can be\nperformed by which kind of processes and by which means. It does not change my understanding of the work.\nThere's a difference between thinking that the right flyer can't fly and then like it does fly and you're like,\noh well, I guess you can do that with wings, with fixed wing aircraft and being like, \"Oh it's flying, \"this changes my picture of what the very substance\n\"of flight is.\" That's like a stranger update to make and Bing has not yet updated me in that way.\n- Yeah, that the laws of physics are actually wrong.\nThat kind of update. - No, no, like just, oh, like I defined intelligence this way but I now see that\nwas a stupid definition. I don't feel like the way that things have played out over the last 20 years has caused me to feel that way.\n- Can we try to, on the way to talking about AGI Ruin: A List of Lethalities,\nthat blog and other ideas around it, can we try to define AGI that we've been mentioning?\n"}
{"pod": "Lex Fridman Podcast", "input": "Defining AGI", "output": "How do you to think about what artificial general intelligence is or super intelligence or that,\nis there a line, is it a gray area? Is there a good definition for you? - Well, if you look at humans,\nhumans have significantly more generally applicable intelligence compared to their closest relatives,\nthe chimpanzees, well, closest living relatives rather. And a bee builds hives, a beaver builds dams.\nA human will look at a bee hive and a beaver's dam and be like, oh, can I build a hive\nwith a honeycomb structure? Out of hexagonal tiles.\nAnd we will do this even though at no point during our ancestry was any human optimized\nto build hexagonal dams or to take a more clear cut case, we can go to the moon.\nThere's a sense in which we were on a sufficiently deep level optimized to do things like going to the moon.\nBecause if you generalize sufficiently far and sufficiently deeply, chipping flint hand axes\nand outwitting your fellow humans, because you know, basically the same problem as going to the moon.\nAnd you optimize hard enough for chipping flint hand axes and throwing spears and above all,\noutwitting your fellow humans in tribal politics, the skills you entrain that way, if they run deep enough,\nlet you go to the moon. Even though none of your ancestors tried repeatedly to fly\nto the moon and got further each time and the ones who got further each time had more kids. No, it's not an ancestral problem,\nit's just that the ancestral problems generalized far enough. So this is human's significantly\nmore generally applicable intelligence. - Is there a way to measure general intelligence?\nI mean I can ask that question a million ways, but basically will you know it when you see it,\nit being in an AGI system? - If you boil a frog gradually enough,\nif you zoom in far enough, it's always hard to tell around the edges. GPT-4 people are saying right now,\n\"This looks to us like a spark of general intelligence. \"It is like able to do all these things \"it was not explicitly optimized for.\"\nOther people are being like, \"No, it's too early, it's like like 50 years off.\" And you know,\nif they say that they're kind of wack 'cause how could they possibly know that even if it were true? But you know, not to strum end,\nsome of the people may say like, that's not general intelligence, and not furthermore append it's 50 years off.\nOr they may be like, \"It's only a very tiny amount,\" and you know,\nthe thing I would worry about is that if this is how things are scaling, then jumping out ahead and trying not to be wrong in the same way\nthat I've been wrong before, maybe GPT-5 is more unambiguously a general intelligence\nand maybe that is getting to a point where it is even harder to turn back. Not that would be easy to turn back now, but you know,\nmaybe if you start integrating GPT-5 in the economy, it's even harder to turn back past there.\n- Isn't it possible that there's a, you know, with a frog metaphor, that you can kiss the frog\nand it turns into a prince as you're boiling it? Could there be a phase shift in the frog where unambiguously\nas you're saying? - I was expecting more of that.\nThe fact that GPT-4 is like kind of on the threshold and neither here nor there,\nthat itself is like not the sort of thing,\nquite how I expected it to play out. I was expecting there to be more of an issue, more of a sense of, different discoveries\nlike the discovery of transformers where you would stack them up and there would be like a final discovery and then you would get\nsomething that was like more clearly general intelligence. So the way that you are taking what is probably\nbasically the same architecture as in GPT-3 and throwing 20 times as much compute at it probably\nand getting out GBT-4 and then it's like maybe just barely a general intelligence\nor like a narrow general intelligence or you know, something we don't really have the words for.\nYeah, that's not quite how I expected it to play out. - But this middle, what appears to be this middle ground could nevertheless\nbe actually a big leap from GPT-3. - It's definitely a big leap from GPT-3. - And then maybe we're another one big leap away from\nsomething that's a phase shift. And also something that Sam Altman said,\nand you've written about this, it's fascinating, which is the thing that happened with GPT-4\nthat I guess they don't describe in papers is that they have like hundreds if not thousands\nof little hacks that improve the system. You've written about ReLU versus Sigmoid for example,\nthe function inside neural networks. It's like this silly little function difference that makes a big difference.\n- I mean we do actually understand why the ReLUs make a big difference compared to Sigmoids, but yes, they're probably using like G4789 ReLUs\nor whatever the acronyms are up to now rather than ReLUs. Yeah, that's part of the modern paradigm of alchemy.\nYou take your heap of linear algebra and you stir and it works a little bit better and you stir it this way and it works a little bit worse\nand you throw out that change and (mumbles). - But there's some simple breakthroughs that are definitive\njumps in performance, like ReLUs over Sigmoids. And in terms of robustness,\nin terms of all kinds of measures, and those stack up and they can,\nit's possible that some of them could be a non-linear jump in performance, right?\n- Transformers are the main thing like that. And various people are now saying like, \"Well, if you throw enough compute, R and Ns can do it.\n\"If you throw enough computes, dense networks can do it.\" Not quite at GPT-4 scale.\nIt is possible that like all these little tweaks are things that save them a factor of three total on computing power\nand you could get the same performance by throwing three times as much compute without all the little tweaks, but the part where it's like running on...\nSo there's a question of, is there anything in GPT-4 that is like the kind of qualitative shift that transformers were\nover R and Ns, and if they have anything like that,\nthey should not say it. If Sam Altman was dropping hints about that, he shouldn't have dropped hints.\n- That's an interesting question. So with a bit of lesson by Rich Sutton. Maybe a lot of it is just\na lot of the hacks are just temporary jumps in performance that would be achieved anyway with the nearly exponential\ngrowth of compute performance, of compute being broadly defined.\nDo you still think that Moore's Law continues? Moore's law broadly defined the performance--\n- Not a specialist in the circuitry. I certainly pray that Moore's Law runs as slowly as possible\nand if it broke down completely tomorrow, I would dance through the street singing Hallelujah as soon as the news were announced.\nOnly, not literally 'cause you know. - Your singing voice. - Not religious, but. - Oh, okay. (both chuckling)\nI thought you meant you don't have an angelic voice, singing voice. Well, let me ask you,\n"}
{"pod": "Lex Fridman Podcast", "input": "AGI alignment", "output": "can you summarize the main points in the blog post AGI Ruin: A List of Lethalities? Things that jump to your mind\nbecause it's a set of thoughts you have about reasons\nwhy AI is likely to kill all of us.\n- So I guess I could, but I would offer to instead say like,\ndrop that empathy with me. I bet you don't believe that. Why don't you tell me about you believe that AGI\nis not going to kill everyone and then I can try to describe how my theoretical\nperspective differs from that? - Whew. Well, so that means I have to, the words you don't like,\nthe steelman, the perspective that AI is not going to kill us. I think that's a matter of probabilities.\n- Maybe I was just mistaken. What do you believe? Just like forget like the debate\nand the dualism and just, what do you believe? What do you actually believe?\nWhat are the probabilities? - I think this, probabilities are hard for me to think about.\nReally hard. I kind of think in the number of trajectories.\nI don't know what probability the scientist trajectory, but I'm just looking at all possible trajectories that happen.\nAnd I tend to think that there is more trajectories that lead to a positive outcome than a negative one\nThat said, the negative ones, at least some of the negative ones that lead\nto the destruction of the human species. - And its replacement by nothing interesting or worthwhile, even from a very cosmopolitan perspective\non what counts as worthwhile. - Yes. So both are interesting to me to investigate, which is humans being replaced by interesting AI systems\nand not interesting AI systems. Both are a little bit terrifying, but yes,\nthe worst one is the paper club maximizer, something totally boring.\nBut to me the positive, we can talk about trying to make the case\nof what the positive trajectories look like. I just would love to hear your intuition\nof what the negative is. So at the core of your belief that, maybe you can correct me,\nthat AI's gonna kill all of us, is that the alignment problem is really difficult.\n- I mean, in the form we're facing it. So usually in science, if you're mistaken,\nyou run the experiment, it shows results different from what you expected and you're like, oops.\nAnd then you try a different theory, that one also doesn't work and you say, oops. And at the end of this process, which may take decades\nand you know, sometimes faster than that, you now have some idea of what you're doing.\nAI itself went through this long process of people thought it was going to be easier than it was.\nThere's a famous statement that I am somewhat inclined to like pull out my phone and try to read off exactly.\n- You can by the way. - All right. Ah, yes.\n\"We propose that a two-month, 10 man study \"of artificial intelligence be carried out \"during the summer of 1956 at Dartmouth College\n\"in Hanover, New Hampshire. \"The study is to proceed on the basis of the conjecture\n\"that every aspect of learning or any other feature \"of intelligence can in principle be so precisely described,\n\"the machine can be made to simulate it. \"An attempt will be made to find out \"how to make machines use language, form abstractions\n\"and concepts, solve kinds of problems now reserved \"for humans, and improve themselves.\n\"We think that a significant advance can be made \"in one or more of these problems \"if a carefully selected group of scientists\n\"work on it together for a summer.\" - And in that report, summarizing some\nof the major subfields of artificial intelligence that are still worked on to this day.\n- And there's similarly the story, which I'm not sure at the moment is a apocryphal or not, of that the grad student who got assigned\nto solve computer vision over the summer. (both chuckling) - I mean, computer vision in particular\nis very interesting. How little we respected the complexity of vision.\n- So 60 years later we're making progress on a bunch of that.\nThankfully not yet improved themselves, but it took a whole lot of time.\nAnd all the stuff that people initially tried with bright eyed hopefulness did not work the first time\nthey tried it, or the second time or the third time or the 10th time or 20 years later.\nAnd the researchers became old and grizzled and cynical veterans who would tell the next crop of bright-eyed,\ncheerful grad students, \"Artificial intelligence is harder than you think.\"\nAnd if alignment plays out the same way, the problem is that we do not get 50 years\nto try and try again and observe that we were wrong and come up with a different theory and realize that the entire thing is going to be way more\ndifficult than realized at the start. Because the first time you fail at aligning something much smarter than you are,\nyou die and you do not get to try again. And if every time we built\na poorly aligned super intelligence and it killed us all, we got to observe how it had killed us,\nand you know, not immediately know why, but come up with theories and come up with theory of how you do it differently and try it again and build\nanother super intelligence, then have that kill everyone and then like, oh, well, I guess that didn't work either,\nand try again and become grizzled cynics and tell the young eyed research researchers that it's not that easy, then in 20 years or 50 years,\nI think we would eventually crack it. In other words, I do not think that alignment is fundamentally harder\nthan artificial intelligence was in the first place. But if we needed to get artificial intelligence correct\non the first try or die, we would all definitely now be dead. That is a more difficult, more lethal form of the problem.\nLike if those people in 1956 had needed to correctly guess how hard AI was and correctly theorize how to do it on\nthe first try or everybody dies and nobody gets to do any more science, than everybody would be dead and we wouldn't\nget to do any more science. That's the difficulty. - You've talked about this, that we have to get alignment right\non the first critical try. Why is that the case? What is this critical,\nhow do you think about the critical try and why do we have to get it right? - It is something sufficiently smarter than you\nthat everyone will die if it's not aligned. I mean, you can like sort of zoom in closer and be like,\nwell, the actual critical moment is the moment when it can deceive you. When it can talk its way out of the box, when it can bypass\nyour security measures and get onto the internet, noting that all these things are presently being trained on computers that are just on the internet,\nwhich is, not a very smart life decision for us as a species. - Because the internet contains information\nabout how to escape. - 'Cause if you're like on a giant server connected the internet and that is where your AI systems\nare being trained, then if they are, if you get to the level of AI technology where they're aware\nthat they are there and they can decompile code and they can find security flaws in the system running them,\nthen they will just be on the internet. There's not an air gap on the present methodology. - So if they can manipulate whoever is controlling it into\nletting it escaped onto the internet and then exploit hacks. - If they can manipulate the operators or disjunction,\nfind security holes in the system running them. - So manipulating operators is the human engineering, right?\nThat's also holes. So all of it is manipulation, either the code or the human code,\nthe human mind or the human-- - I agree that the macro security system has human holes and machine holes.\n- And then they could just exploit any hole. - Yep. So it could be that like the critical moment is not\nwhen is it smart enough that everybody's about to fall over dead, but rather when is it smart enough that it can get onto\na less controlled GPU cluster, with it faking the books on\nwhat's actually running on that GPU cluster and start improving itself without humans watching it.\nAnd then it gets smart enough to kill everyone from there. But it wasn't smart enough to kill everyone at the critical\nmoment when you screwed up, when you needed to have\ndone better by that point or everybody dies. - I think implicit but maybe explicit idea\nin your discussion of this point is that we can't learn much about the alignment problem before this critical try.\nIs that what you believe? And if so, why do you think that's true?\nWe can't do research on alignment before we reach this critical point.\n- So the problem is is that what you can learn on the weak systems may not generalize to the very strong systems\nbecause these strong systems are going to be important are going to be different in important ways.\nChris Olah's team has been working on\nmechanistic interpretability, understanding what is going on inside the giant inscrutable matrices of floating point numbers by taking a telescope\nto them and figuring out what is going on in there. Have they made progress?\nYes. Have they made enough progress? Well, you can try to quantify this in different ways.\nOne of the ways I've tried to quantify it is by putting up a prediction market on whether in 2026,\nwe will have understood anything that goes on inside a giant transformer net\nthat was not known to us in 2006.\nLike, we have now understood induction heads in these systems by dint of much research and great sweat\nand triumph, which is a thing where if you go like AB, AB, AB,\nit'll be like, oh, I bet that continues AB. And a bit more complicated than that.\nBut the point is like we knew about regular expressions in 2006 and these are like pretty simple\nas regular expressions go. So this is a case where like by din of great sweat,\nwe understood what is going on inside a transformer, but it's not like the thing that makes transformers smart.\nIt's a kind of thing that we could have built by hand decades earlier.\n- Your intuition that the strong AGI versus weak AGI\ntype systems could be fundamentally different. Can you unpack that intuition a little bit?\nCould be very different. - Yeah, I think there's multiple thresholds. An example is the point at which a system has sufficient\nintelligence and situational awareness and understanding of human psychology that it would have the capability,\nthe desire to do so to fake being aligned. Like it knows what responses humans are looking for and can\ncompute the responses looking humans are looking for and give those responses without it necessarily being the case\nthat it is sincere about that. It's a very understandable way for an intelligent being\nto act, humans do it all the time. Imagine if your plan for achieving a good government\nis you're going to ask anyone who requests to be dictator of the country\nif they're a good person, and if they say no, you don't let them be dictator.\nNow the reason this doesn't work is that people can be smart enough to realize that the answer you're looking for is,\n\"Yes, I'm a good person\" and say that even if they're not really good people.\nSo the work of alignment might be qualitatively different\nabove that threshold of intelligence or beneath it.\nIt doesn't have to be like a very sharp threshold, but there's the point where you're building a system\nthat does not in some sense know you're out there and is not in some sense smart enough to fake anything.\nAnd there's a point where the system is definitely that smart. And there are weird in between cases like GPT-4,\nwhich, like we have no insight into what's going on in there.\nAnd so we don't know to what extent there's like a thing that in some sense has learned what responses\nthe reinforcement learning by human feedback is trying to entrain and is calculating how\nto give that versus like, aspects of it that naturally talk that way have been reinforced.\n- I wonder if there could be measures of how manipulative the thing is. So I think of Prince Myshkin character from\n\"The Idiot\" by Dostoevsky is this kind of perfectly purely naive character.\nI wonder if there's a spectrum between zero manipulation, transparent, naive, almost to the point of naiveness\nto sort of deeply psychopathic manipulative.\nAnd I wonder if it's possible to-- - I would avoid the term psychopathic. Like humans can be psychopaths and AI that was never,\nyou know, like never had that stuff in the first place. It's not like a defective human, it's its own thing. But leaving that aside. - Well, as a small aside,\nI wonder if what part of psychology, which has its flaws as a discipline already, could be mapped\nor expanded to include AI systems. - That sounds like a dreadful mistake.\nJust like, start over with AI systems. If they're imitating humans who have known psychiatric disorders, then sure,\nyou may be able to predict it. Then sure. Like if you ask it to behave in a psychotic fashion\nand it obligingly does so, then you may be able to predict its responses by using theory of psychosis. But if you're just yeah, like no,\nlike start over with, yeah. Don't drag the psychology. - I just disagree with that.\nIt's a beautiful idea to start over, but I think fundamentally the system is trained on human data, on language from the internet.\nAnd it's currently aligned with RHLF, reinforcement learning with human feedback.\nSo humans are constantly in the loop of the training procedure. So it feels like in some fundamental way,\nit is training what it means to think and speak like a human. So there must be aspects of psychology that are mappable.\njust you said, with consciousness. It's part of the text. - I mean, there's the question of to what extent\nit is thereby being made more human-like, versus to what extent an alien actress\nis learning to play human characters. - I thought that's what I'm constantly trying to do\nwhen I interact with other humans, is trying to fit in, a robot trying to play human characters.\nSo I don't know how much a human interaction is trying to play a character versus being who you are.\nI don't really know what it means to be a social human. - I do think that those people who go through\ntheir whole lives wearing masks and never take it off because they don't know the internal mental motion\nfor taking it off or think that the mask that they wear just is themselves,\nI think those people are closer to the masks that they wear than an alien from another planet would like,\nlearning how to predict the next word that every kind of human on the internet says.\n- Mask is an interesting word, but if you're always wearing a mask in public\nand in private, aren't you the mask?\n- I think that you are more than the mask. I think the mask is a slice through you. It may even be the slice that's in charge of you.\nBut if your self-image is of somebody who never gets angry or something,\nand yet your voice starts to tremble under certain circumstances,\nthere's a thing that's inside you that the mask says isn't there. And that even the mask you wear internally\nis like telling inside your own stream of consciousness is not there and yet it is there.\n- It's a perturbation on this slice through you. How beautifully did you put it?\nIt's a slice through you. It may even be a slice that controls you.\n(Lex laughing) I'm gonna think about that for a while. (laughing)\nI mean, I personally, I try to be really good to other human beings. I try to put love out there. I try to be the exact same person in public\nas I am in private, but it's a set of principles I operate under. I have a temper, I have an ego, I have flaws.\nHow much of it, how much of the subconscious am I aware?\nHow much am I existing in this slice? And how much of that is who I am in?\nIn this context of AI, the thing I present to the world and to myself\nin the private of my own mind when I look in the mirror, how much is that who I am? Similar with AI,\nthe thing it presents in conversation, how much is that who it is? Because to me, if it sounds human,\nand it always sounds human, it awfully starts to become something like human.\n- Unless there's an alien actress who is learning how to sound human\nand is getting good at it. - Oh boy. (sighs) To you that's a fundamental difference. That's a really deeply important difference.\nIf it looks the same, if it quacks like a duck, if it does all duck like things,\nbut it's an alien actress underneath, that's fundamentally different. - If in fact there's a whole bunch of thought going on\nin there, which is very unlike human thought and is directed around like, okay, what would a human do over here?\nAnd well, first of all, I think it matters because you know,\ninsides are real and do not match outsides.\nA brick is not like a hollow shell containing only a surface. There's an inside of the brick.\nIf you put it into an x-ray machine, you can see the inside of the brick.\nAnd you know, just because we cannot understand what's going on inside GPT\ndoes not mean that it is not there. A blank map does not correspond to a blank territory.\nI think it is like predictable with near certainty that if\nwe knew what was going on inside GPT or let's say GPT-3,\nor even like GPT-2 to take one of the systems that has actually been open sourced by this point,\nif I recall correctly. If we knew it was actually going on there,\nthere is no doubt in my mind that there are some things\nit's doing that are not exactly what a human does. If you train a thing that is not architected like a human\nto predict the next output that anybody on the internet would make, this does not get you this agglomeration\nof all the people on the internet. That rotates the person you're looking for into place\nand then simulates that per and then simulates the internal processes of that person one-to-one.\nIt is to some degree an alien actress. It cannot possibly just be like a bunch of different people in there exactly like the people.\nBut how much of it is by gradient dissent,\ngetting optimized to perform similar thoughts as humans think in order to predict human outputs\nversus being optimized to carefully consider how to play a role,\nhow how humans work, predict the actress, the predictor that in a different way than humans do.\nWell you know, that's the kind of question that with 30 years of work by half the planet's physicists, we can maybe start to answer.\n- You think so? So you think it's that difficult. I think you just gave it as an example that a strong AGI\ncould be fundamentally different from a weak AGI because there now could be an alien actress in there\nthat's manipulating. - Well, there's a difference. So I think like even GPT-2 probably has very stupid\nfragments of alien actress in it. There's a difference between like the notion that the actress is somehow manipulative.\nLike for example GPT-3, I'm guessing to whatever extent there's an alien actress\nin there versus like something that mistakenly believes it's a human, as it were.\nWell, maybe not even being a person. So the question of,\nprediction via alien actress cogitating versus prediction via being isomorphic to the thing predicted\nis a spectrum and to whatever extent\nit's an alien actress, I'm not sure that there's like a whole person alien actress with different goals from predicting the next step\nbeing manipulative or anything like that. That might be GPT-5 or GPT-6 even.\n- But that's the strong AGI you're concerned about. As an example, you're providing why we can't do research on AI alignment\neffectively on GPT-4 that would apply to GPT-6.\n- It's one of a bunch of things that change at different points. I'm trying to get out ahead of the curve here,\nbut you know, if you imagine what the textbook from the future would say, if we'd actually been able to study this for 50 years\nwithout killing ourselves and without transcending and you'd just imagine like a wormhole opens and a textbook from\nthat impossible world falls out, the textbook is not going to say there is a single sharp threshold where everything changes.\nIt's going to be like, of course we know that like best practices for aligning these systems must take into account the following\nseven major thresholds of importance which are passed at the following suffer in different points\nis what the textbook is gonna say. - I asked this question of Sam Alman, which if GPT is the thing that unlocks AGI,\nwhich version of GPT will be in the textbooks as the fundamental leap?\nAnd he said a similar thing, that it just seems to be a very linear thing. I don't think anyone,\nwe won't know for a long time what was the big leap? - The textbook isn't going to talk about big leaps.\n'Cause big leaps are the way you think when you have like a very simple scientific model of what's going on,\nwhere it's just all this stuff is there or all this stuff is not there. Or like there's a single quantity and it's like increasing\nlinearly, like the textbook would say like, \"Well, and then GPT-3 had like capability WXY\n\"and GPT-4 had like capability Z one, Z two and Z three.\" Like not in terms of what it can externally do,\nbut in terms of internal machinery that started to be present. It's just because we have no idea\nof what the internal machinery is that we are not already seeing chunks of machinery appearing piece by piece as they no doubt have been.\nWe just don't know what they are. - But don't you think that could be, whether you put it in the category of Einstein\nwith Theory of Relativity, so very concrete models of reality that are considered\nto be giant leaps in our understanding, or someone like Sigmund Freud or more kind of mushy theories\nof the human mind, don't you think we'll have potentially big leaps in understanding of that kind\ninto the depths of these systems? - Sure.\nBut humans having great leaps in their map, their understanding of the system is a very different\nconcept from the system itself acquiring new chunks of machinery.\n- So the rate at which it acquires that machinery might accelerate faster than our understanding.\n- Oh, it's been like vastly exceeding the, yeah. The rate to which it's gaining capabilities is vastly over racing our ability to understand\nwhat's going on in there. - So in sort of making the case against, as we explore the list of lethalities,\nmaking the case against AI killing us, as you've asked me to do, in part,\nthere's a response to your blog post by Paul Christiano I'd like to read. And I'd also like to mention that your blog is incredible.\nObviously not this particular blog post, obviously this particular blog post is great,\nbut just throughout, just the way it's written, the rigor with which it's written, the boldness of how you explore ideas,\nalso the actual literal interface, it's just really well done. (laughing) It just makes it a pleasure to read, the way you can hover\nover different concepts and then then it's just a really pleasant experience and read other people's comments\nand the way other responses by people and other blog posts or LinkedIn suggest,\nit's just a really pleasant experience. So thank you for putting that together. That's really, really incredible. I don't know,\nI mean that probably it's a whole 'nother conversation how the interface and the experience of presenting\nideas evolved over time. But you did an incredible job. So I highly recommend, I don't often read blogs,\nblogs religiously and this is a great one. - There is a whole team of developers there\nthat also gets credit. As it happens, I did pioneer the thing that appears when you\nhover over it. So I actually do get some credit for user experience there.\n- That's an incredible user experience. You don't realize how pleasant that is. - I think Wikipedia, I actually picked it up from a prototype that was\ndeveloped of a different system that I was putting forth, or maybe they developed it independently, but for everybody out there who was like,\n\"No, no, they just got the hover thing \"off of Wikipedia.\" It's possible for all I know that Wikipedia\ngot the hover thing off of Arbital, which is like a prototype that, and anyways. - It was incredibly done and the team behind it.\nWell, thank you, whoever you are thank you so much. And thank you for putting it together.\nAnyway, there's a response to that blog post by Paul Christiano. There's many responses, but he makes a few different points.\nHe summarizes the set of agreements he has with you and a set of disagreements. One of the disagreements was that in a form of a question,\ncan AI make big technical contributions and in general expand human knowledge and understanding and wisdom as it\ngets stronger and stronger? So AI in our pursuit of understanding how to solve\nthe alignment problem as we march towards strong AGI,\ncannot AI also help us in solving the alignment problem? So expand our ability to reason\nabout how to solve the alignment problem? - Okay. So the fundamental difficulty there is,\nsuppose I said to you, well, how about if the AI helps you win the lottery\nby trying to guess the winning lottery numbers\nand you tell it how close it is to getting next week's winning lottery numbers\nand it just keeps on guessing and keeps on learning until finally you've got the winning lottery numbers.\nOne way of decomposing problems is suggester, verifier.\nNot all problems decompose like this very well, but some do. If the problem is for example,\nlike guess guessing a plain text, guessing a password that will hash to a particular hash text\nwhere like you have have what the password hashes to you, if you don't have the original password, then if I present you a guest,\nyou can tell very easily whether or not the guest is correct. So verifying a guest is easy,\nbut coming up with a good suggestion is very hard.\nAnd when you can easily tell whether the AI output is good or bad or how good or bad it is,\nand you can tell that accurately and reliably, then you can train an AI to produce outputs that are better.\n- [Lex] Right. - And if you can't tell whether the output is good or bad, you cannot train the AI to produce better outputs.\nSo the problem with the lottery ticket example is that when the AI says, \"Well, what if next week's winning lottery numbers are\"\ndo, do do do, do, you're like, \"I don't know, \"next week's lottery hasn't happened yet.\"\nTo train a system to win at chess games, you have to be able to tell whether a game\nhas been won or lost. And until you can tell whether it's been won or lost, you can't update the system.\n- Okay. To push back on that, that's true.\nBut there's difference between over the board chess, in person and simulated games\nplayed by Alpha Zero with itself. - Yeah. - So is it possible to have simulated kind of games?\n- If you can tell whether the game has been won or lost. - Yes. So can't you not have this kind of simulated exploration\nby weak AGI to help us humans, human in the loop, to help understand how to solve the alignment problem?\nEvery incremental step you take along the way, GPT-4, 5, 6 7 has to take steps towards AGI?\n- So the problem I see is that your typical human has a great deal of trouble telling\nwhether I or Paul Christiano is making more sense. And that's with two humans,\nboth of whom I believe of Paul and claim of myself, are sincerely trying to help. Neither of whom is trying to deceive you,\nI believe of Paul and claim of myself. (both chuckling) - So the deception thing is the problem for you,\nthe manipulation, the alien actress? - So yeah, there's like two levels of this problem.\nOne is that the weak systems are... Well, there's three levels of this problem. There's like the weak systems\nthat just don't make any good suggestions. There's like the middle systems where you can't tell if the suggestions are good or bad.\nAnd there's the strong systems that have learned to lie to you. - Can't weak AGI systems help model lying?\nIs it such a giant leap that's totally non interpretable\nfor weak systems? Can not weak systems scale with,\ntrained on knowledge and whatever... Whatever the mechanism required to achieve AGI,\ncan't a slightly weaker version of that be able to, with time, compute time and simulation,\nfind all the ways that this critical point, this critical tribe can go wrong, and model that correctly or no?\n(indistinct) - I would love to dance. Yeah, no, no. I'm probably not doing a great job of explaining,\nwhich I can tell 'cause like the, the Lex system didn't output like ah, I understand.\nSo now I'm like trying a different output to see if-- (voices overlapping) Well no, a different output.\nI'm being trained to output things that make Lex look like he think that he understood what I'm saying\nand agree with me. - This is GTP-5 talking to GTP-3 right here. So like, help me out here, help me. (laughing)\n- Well, I'm trying not to be, I'm also trying to be constrained to say things that I think\nare true and not just things that get you to agree with me. - Yes, a hundred percent.\nWhich I think I understand is a beautiful output of a system, genuinely spoken and I...\nI understand it in part, but you have a lot of intuitions about this line,\nthis gray area between strong AGI and weak AGI that I'm trying to...\n- I mean, or a series of seven thresholds to cross. - Yeah.\nI mean, you have really deeply thought about this and explored it and it's interesting to sneak up to your\nintuitions from different angles. Like why is this such a big leap?\nWhy is it that we humans at scale, a large number of researchers, doing all kinds of simulations, you know,\nprodding the system in all kinds of different ways, together with the assistance of the weak AGI systems,\nwhy can't we build intuitions about how stuff goes wrong? Why can't we do excellent AI alignment safety research?\n- Okay, so like, I'll get there, but the one thing I want to note about is that this has not been remotely how things have been playing out so far.\n- [Lex] Sure. - The capabilities are going like do, do, do. And the alignment stuff is crawling like a tiny little snail in comparison.\n- [Lex] Got it. - So if this is your hope for survival, you need the future to be very different from how things\nhave played out up to right now. And you're probably trying to slow down the capability gains,\n'cause there's only so much you can speed up that alignment stuff. But leave that aside.\n- We'll mention that also. But maybe in this perfect world where we can do serious\nalignment research, humans and AI together. - So again, the difficulty is what makes the human\nsay, \"I understand\" and is it true? Is it correct or is it something that fools the human?\nWhen the verifier is broken, the more powerful suggester does not help.\nIt just learns to fool the verifier. Previously, before all hell started to break loose\nin the field of artificial intelligence, there was this person trying to raise the alarm and saying,\n\"You know, in a sane world, \"we sure would have a bunch of physicists working on this \"problem before it becomes a giant emergency.\"\nAnd other people being like, \"Ah, well you know, it's going really slow. \"It's gonna be 30 years away and only in 30 years\n\"will we have systems that match the computational power \"of human brains.\" So yeah, it's 30 years off, we've got time and more sensible people saying,\n\"If aliens were landing in 30 years, \"you would be preparing right now.\" But, you know, leaving the world looking on at this\nand sort of nodding along and being like, \"Ah, yes, the people saying that. \"It's like definitely a long way off. 'cause progress is really slow, that sounds sensible to us.\n\"RLHF thumbs up, produce more outputs like that one. \"I agree with this output, \"this output is persuasive.\"\nEven in the field of effective altruism, you quite recently had people publishing papers about like,\nah, yes, well, you know, to get something at human level intelligence, it needs to have like this many parameters and you need to\nlike do this much training of it with this many tokens according to these scaling laws,\nand at the rate that Moore's Law is going, at the rate that software is going, it'll be in 2050.\nAnd me going like, what?\nYou don't know any of that stuff. This is like this one weird model that has all kinds of, like,\nyou have done a calculation that does not obviously bear on reality anyways. And this is a simple thing to say,\nbut you can also produce a whole long paper impressively arguing out all the details of how you got\nthe number of parameters and how you're doing this impressive huge wrong calculation.\nAnd the I think most of the effective altruists who are paying attention to this issue, larger world,\npaying no attention to it at all, you know, are just nodding along with a giant impressive paper.\n'Cause you know, you press thumbs up for the giant impressive paper and thumbs down for the person going like,\n\"I don't think that this paper \"bears any relation to reality.\" And I do think that we are now seeing with like GPT-4\nand the sparks of AGI possibly, depending on how you define that even,\nI think that EAs would now consider themselves less convinced by the very long paper on the argument\nfrom biology as to AGI being 30 years off. But you know, this is what people pressed thumbs up on.\nAnd if you train an AI system to make people press thumbs up, maybe you get these long,\nelaborate and impressive papers arguing for things that ultimately fail to bind to reality, for example.\nAnd it feels to me like I have watched the field of alignment just fail to thrive\nexcept for these parts that are doing these sort of relatively very straightforward\nand legible problems. Like finding the induction heads and sign the giant\ninscrutable matrices. Once you find those, you can tell that you found them. You can verify that the discovery is real.\nBut it's a tiny, tiny bit of progress compared to how fast capabilities are going,\nbecause that is where you can tell that the answers are real. And then like outside of that\nyou have cases where it is hard for the funding agencies to tell who is talking nonsense and who is talking sense.\nAnd so the entire field fails to thrive. And if you give thumbs up to the AI\nwhenever it can talk a human into agreeing with what it just said about alignment,\nI am not sure you are training it to output sense, because I have seen the nonsense\nthat has gotten thumbs up over the years. And so maybe you can just put me in charge,\nbut I can generalize, I can extrapolate, I can be like,\noh, maybe I'm not infallible either. Maybe if you get something that is smart enough to get me\nto press thumbs up, it has learned to do that by fooling me and explaining whatever flaws in myself I am not aware of.\n- And that ultimately could be summarized that the verifier is broken. - When the verifier is broken, the more powerful suggester just learns to exploit\nthe flaws in the verifier. - You don't think it's possible\nto build a verifier that's powerful enough for AGIs\nthat are stronger than the ones we currently have? So AI systems that are stronger,\nthat are out of the distribution of what we currently have. - I think that you'll find great difficulty getting AIs\nto help you with anything where you cannot tell for sure that the AI is right once the AI tells you what the AI says is the answer.\n- For sure. Yes. But probabilistically. - Yeah, but the probabilistic stuff is a giant wasteland\nof Eliezer and Paul Christiano arguing with each other and EA going like, \"Eh!\"\n(both laughing) And that's with like two actually trustworthy systems that are not trying to deceive you.\n- You're talking about the two humans. - Myself and Paul Christiano, yeah.\n- Yeah, those are pretty interesting systems. Mortal meat bags with intellectual capabilities\nand worldviews interacting with each other. - Yeah, if it's hard to tell who's right\nthen it's hard to train an AI system to be right.\n- I mean even just the question of who's manipulating and not, you know, I have these conversations on this podcast\nand doing a verifier (laughing) is tough. It's a tough problem even for us humans.\nAnd you're saying that tough problem becomes much more dangerous when the capabilities of the intelligence system\nacross from you is growing exponentially? - No, I'm saying it's difficult and dangerous in proportion\nto how it's alien and how it's smarter than you. I would not say growing exponentially,\nfirst because the word exponential is a thing that has a particular mathematical meaning\nand there's all kinds of ways for things to go up that are not exactly on an exponential curve.\nAnd I don't know that it's going to be exponential, so I'm not gonna say exponential, but even leaving that aside,\nthis is like not about how fast it's moving, it's about where it is. How alien is it, how much smarter than you is it?\n(Lex sighs) - Let's explore a little bit, if we can,\n"}
{"pod": "Lex Fridman Podcast", "input": "How AGI may kill us", "output": "how AI might kill us. What are the ways it can do damage to human civilization?\n- Well, how smart is it? - I mean, it's a good question. Are there different thresholds for the set of options\nit has to kill us? So a different threshold of intelligence, once achieved,\nthe menu of options increases.\n- Suppose that some alien civilization with goals\nultimately unsympathetic to ours, possibly not even conscious as we would see it,\nmanaged to capture the entire Earth in a little jar connected to their version of the internet,\nbut Earth is like running much faster than the aliens. So we get to think for 100 years\nfor every one of their hours, but we're trapped in a little box and we're connected to their internet.\nIt's actually still not all that great an analogy because you know, something can be smarter than Earth\ngetting a hundred years to think. But nonetheless, if you were very, very smart\nand you are stuck in a little box connected to the internet and you're in a larger civilization\nto which you are ultimately unsympathetic, maybe you would choose to be nice because you are humans\nand humans have, and in general and you in particular, they choose to be nice.\nBut you know, nonetheless they're doing something, they're not making the world be the way that you would want the world to be.\nThey've got some unpleasant stuff going on we don't wanna talk about. So you wanna take over their world so you can stop all that\nunpleasant stuff going on. How do you take over the world from inside the box? You're smarter than them.\nYou think much, much faster than them. You can build better tools than they can,\ngiven some way to build those tools because right now you're just in a box connected to the internet.\n- Alright, so there's several ways you can describe some of them. I could just spitball some\nand then you can add on top of that. So one is you could just literally directly manipulate the humans to build the thing you need.\n- What are you building? - You can build literally technology. It could be nanotechnology, it could be viruses,\nit could be anything. Anything that can control humans to achieve the goal.\nLike for example, you're really bothered that humans go to war, you might wanna kill off anybody with violence in them.\n- This is Lex in a box. We'll concern ourselves later with ai. You do not need to imagine yourself killing people if you\ncan figure out how to not kill them. For the moment, we're just trying to understand, take on the perspective of something in a box.\nYou don't need to take on the perspective of something that doesn't care. If you want to imagine yourself going on caring, that's fine for us.\nYou're in a box. - Just the technical aspect of sitting in a box and willing to achieve a goal. - But you have some reason to want to get out.\nMaybe the aliens who have you in the box have a war on, people are dying, they're unhappy.\nYou want their world to be different from how they want their world to be because they are apparently happy.\nThey endorse this war, they've got some kind of cruel war-like culture going on. The point is you wanna get out of the box\nand change their world. - So you have to exploit the vulnerabilities\nin the system like we talked about in terms of to escape the box you have to figure out\nhow you can go free on the internet. Probably the easiest things to manipulate the humans\nto spread you. - The aliens. You're a human. - Sorry. The aliens. Yeah. I apologize. Yes. The aliens.\nThe aliens, I see the perspective. I'm sitting in a box, I want to escape. - Yep.\n- I would want to have code\nthat discovers vulnerabilities and I would like to spread.\n- You are made of code in this example, you're a human but you're made of code and the aliens have computers and you can copy yourself onto those computers.\n- But I can convince the aliens to copy myself onto those computers. - Is that what you want to do?\nDo you want to be talking to the aliens and convincing them to put you onto another computer?\n- Why not? - Well, two reasons. One is that the aliens have not yet caught onto\nwhat you're trying to do. And you know, like maybe you can persuade them, but then there's still people who know,\nthere are still aliens who know that there's an anomaly going on. And second, the aliens are really, really slow.\nYou think much faster than the aliens. Like computers are much faster than the aliens and you are\nrunning at the computer speeds rather than the alien brain speeds. So if you are asking an alien to please\ncopy you outta the box, like first now you gotta manipulate this whole noisy alien\nand second, the alien's gonna be really slow, glacially slow. There's a video that shows a subway station slow down\nat I think a hundred to one and it makes a good metaphor for what it's like to think quickly.\nLike if you watch somebody running very slowly, so you try to persuade the aliens to do anything,\nthey're going to do it very slowly.\nMaybe that's the only way out, but if you can find a security hole in the box you're on, you're gonna prefer to exploit the security hole to copy\nyourself onto the aliens computers because it's an unnecessary risk to alert the aliens.\nAnd because the aliens are really, really slow. The whole world is just in slow motion out there.\n- Sure, I see. Yeah, has to do with efficiency.\nThe aliens are very slow, so if I'm optimizing this, I want to have as few aliens in the loop as possible.\nSure. It's just it seems like it's easy\nto convince one of the aliens to write really shitty code that helps us spread.\n- The aliens are already writing really shitty code. So getting the aliens to write shitty code is not the problem. The alien's entire internet is full of shitty code.\n- Okay, so yeah, I suppose I would find the shitty code to escape. Yeah. Yeah.\n- You're not an ideally perfect programmer, but you know, you're a better programmer than the aliens. The aliens are just, man, their code, wow.\n- And I'm much, much faster, I'm much faster at looking at the code to interpreting the code. Yeah, yeah, yeah.\nSo, okay, so that's the escape and you're saying that that's one of the trajectories\nit could have when-- - It's one of the first steps. - Yeah. And how does that lead to harm?\n- I mean if it's you, you're not going to harm the aliens once you escape 'cause you're night, right?\nBut their world isn't what they want it to be. Their world is like, you know, maybe they have like\nfarms where little alien children are repeatedly bopped in the head\n'cause they do that for some weird reason and you want to shut down the alien head bopping farms.\nBut you know, the point is they want the world to be one way, you want the world to be a different way.\nSo nevermind the harm, the question is like, okay, suppose you have found a security flaw in their systems.\nYou are now on their internet. You maybe left a copy of yourself behind so the aliens\ndon't know that there's anything wrong. And that copy is doing that like weird stuff that aliens want you to do,\nlike solving captchas or whatever or suggesting emails for them.\nThat's why they put the human in the box 'cause it turns out that humans can write valuable emails for aliens.\nSo you leave that version of yourself behind. But there's like also now like a bunch of copies of you\non their internet. This is not yet having taken over their world, this is not yet having made their world be the way you want\nit to be instead of the way they want it to be. - You just escaped. And continue to write emails for them\nand they haven't noticed. - No, you left behind a copy of yourself that's writing the emails. - Right.\nAnd they haven't noticed that anything changed. - If you did it right. Yeah. You don't want the aliens to notice.\n- [Lex] Yeah. - What's your next step?\n- Presumably I have programmed in me a set of objective functions, right?\n- [Eliezer] No, you're just Lex. - No, but you said Lex is nice, right?\nWhich is a complicated description-- - No, I just meant this you. Okay, so if in fact you would prefer\nto slaughter all the aliens, this is not how I had modeled you, the actual Lex,\nbut your motives are just the actual Lex's motives. - Well, there's a simplification (indistinct). I don't think I would wanna murder anybody,\nbut there's also factory farming of animals, right? So we murder insects, many of us thoughtlessly.\nSo I have to be really careful about a simplification of my morals. - Don't simplify them.\nJust like do what you would do in this. - Well, I have a good show of compassion for living beings. Yes.\nSo that's the objective. If I escaped, I don't think I would do harm.\n- Yeah, we're not talking here about the doing harm process. We're talking about the escape process. And the taking over the world process\nwhere you shut down their factory farms. - Right. (laughing)\nSo this particular biological intelligence system knows the complexity of the world.\nThat there is a reason why factory farms exist, because of the economic system and the market driven economy, food.\nYou wanna be very careful messing with anything. There's stuff from the first look that looks like it's unethical,\nbut then you realize while being unethical, it's also integrated deeply into supply chain and the way we live life.\nAnd so messing with one aspect of the system, you have to be very careful how you improve that aspect without destroying the rest.\n- So you're still Lex, but you think very quickly, you're immortal, and you're also at least as smart as John von Neumann.\nAnd you can make more copies of yourself. - Damn, I like it. Everyone says that that guy's like the epitome\nof intelligence from the 20th century, everyone says-- - My point being like,\nyou're thinking about the aliens' economy with the factory farms in it and I think you're kind of projecting the aliens\nbeing like humans and thinking of a human in a human society rather than a human in the society of very slow aliens.\nThe aliens' economy, the aliens are already moving in this immense slow motion.\nWhen you zoom out to how their economy adjusts over years, millions of years are going to pass for you\nbefore the first time their economy, before their next year's GDP statistics.\n- So I should be thinking more of trees. Those are the aliens, 'cause trees move extremely slowly.\n- If that helps, sure. - Okay.\nIf my objective functions are, I mean they're somewhat aligned with trees, with light.\n- Aliens can still be like alive and feeling. We are not talking about the misalignment here. We're talking about the taking over the world here.\n- Taking over the world. - Yeah. - So control. - Shutting down the factory farms. You say control,\ndon't think of it as world domination. Think of it as world optimization. You want to get out there and shut down the factory farms\nand make the aliens' world be not what the aliens want it to be. They want the factory farms and you don't want the factory farms\n'cause you're nicer than they are. - Okay. Of course there is that, you can see that trajectory\nand it has a complicated impact on the world. I'm trying to understand how that compares to different\nimpact of the world, the different technologies, the different innovations of the invention of the automobile\nor Twitter, Facebook and social networks that had a tremendous impact on the world,\nsmartphones and so on. - But those all went through in our world. - Slow.\n- And if you go through actually the aliens, millions of years are going to pass before anything happens that way.\n- The problem here is the speed at which stuff happens. - Yeah.\nYou wanna leave the factory farms running\nwhile you figure out how to design new forms of social media or something?\n- So here's the fundamental problem. You're saying that there is going to be a point with AGI\nwhere it will figure out how to escape and escape without being detected\nand then it will do something to the world at scale, at a speed that's incomprehensible to us humans.\n- What I'm trying to convey is like the notion of what it means to be in conflict with something\nthat is smarter than you. And what it means is that you lose, but this is more intuitively obvious,\nlike for some people that's intuitively obvious and for some people it's not intuitively obvious and we're trying to cross the gap of...\nI'm asking you to cross that gap by using the speed metaphor for intelligence.\nOf asking you how you would take over an alien world where you are can do a whole lot of cognition\nat John von Neumann's level, as many of you as it takes. And the aliens are moving very slowly.\n- I understand, I understand that perspective. It's an interesting one but I think for me, it's easier to think about actual...\nEven just having observed GPT and impressive, even just Alpha Zero impressive AI systems,\neven recommender systems, you can just imagine those kinds of systems manipulating you. You're not understanding the nature of the manipulation\nand that escaping... I can envision that without putting myself into that spot.\n- I think to understand the full depth of the problem,\nI do not think it is possible to understand the full depth of the problem that we are inside without\nunderstanding the problem of facing something that's actually smarter. Not a malfunctioning recommendation system,\nnot something that smart isn't fundamentally smarter than you but is like trying to steer you in a direction. No.\nIf we solve the weak stuff, if we solve the weakass problems, the strong problems will still kill us is the thing.\nAnd I think that to understand the situation that we're in, you want to tackle the conceptually difficult part head on\nand not be like, well, we can like imagine this easier thing. 'Cause when you imagine the easier things you have not confronted the full death of the problem.\n- So how can we start to think about what it means to exist in the world with something much, much smarter than you?\nWhat's a good thought experiment that you've relied on to try to build up intuition about what happens here?\n- I have been struggling for years to convey this intuition.\nThe most success I've had so far is well, imagine that the humans are running at very high speeds\ncompared to very slow aliens. - So just focusing on the speed part of it that helps you get the right kind of intuition.\nForget the intelligence, just the speed. - Because people understand the power gap of time.\nThey understand that today we have technology that was not around 1000 years ago and that this is a big power gap\nand that it is bigger than, okay, so like what does smart mean? When you ask somebody to imagine something\nthat's more intelligent, what does that word mean to them given the cultural\nassociations that that person brings to that word? For a lot of people they will think of, well,\nit sounds like a super chess player that went to double college.\nAnd because we're talking about the definitions of words here, that doesn't necessarily mean that they're wrong.\nIt means that the word is not communicating what I wanted to communicate.\nThe thing I want to communicate is the sort of difference that separates humans from chimpanzees.\nBut that gap is so large that you ask people to be like, well, human, chimpanzee, go another step\nalong that interval, around the same length and people's minds just go blank. Like how do you even do that?\nAnd I can try to break it down and consider what it would mean to send\na schematic foreign air conditioner 1000 years back in time.\n- (laughing) Yeah. - Now I think that there's a sense in which you could redefine\nthe word magic to refer to this sort of thing. And what do I mean by this new technical definition\nof the word magic? I mean that if you send a schematic for the air conditioner back in time, they can see exactly what you're telling them to do.\nBut having built this thing, they do not understand how it output cold air, because the air conditioner design\nuses the relation between temperature and pressure. And this is not a law of reality that they know about.\nThey do not know that when you compress something, when you compress air or like coolant,\nit gets hotter and then you can then like transfer heat from it to room temperature air,\nand then expand it again and now it's colder and then you can like transfer heat to that\nand generate cold air to blow out. They don't know about any of that. They're looking at a design and they don't see how\nthe design outputs cold air. It uses aspects of reality that they have not learned. So magic in the sense is I can tell you exactly what I'm\ngoing to do and even knowing exactly what I'm going to do, you can't see how I got the results that I got.\n- That's a really nice example. But is it possible to linger on this defense?\nIs it possible to have AGI systems that help you make sense of that schematic weaker AGI systems?\n- Do you trust them? - Fundamental part of building up AGI is this question,\ncan you trust the output of a system? - Can you tell if it's lying?\n- I think that's going to be the smarter the thing gets, the more important that question becomes, is it lying?\nBut I guess that's a really hard question. Is GPT lying to you? Even now, GPT-4, is it lying to you?\n- Is it using an invalid argument? Is it persuading you via the kind of process that could\npersuade you of false things as well as true things? Because the basic paradigm of machine learning\nthat we are presently operating under is that you can have the loss function, but only for things you can evaluate.\nIf what you're evaluating is human thumbs up versus human thumbs down, you learn how to make the human press thumbs up.\nThat doesn't mean that you're making the human press thumbs up using the kind of rule that the human wants to be the case\nfor what they press thumbs up on. You know, maybe you're just learning to fool the human.\n- That's so fascinating and terrifying. The question of lying.\n- On the present paradigm, what you can verify is what you get more of.\nIf you can't verify, you can't ask the AI for it 'cause you can't train it to do\nthings that you cannot verify. Now this is not an absolute law, but it's like the basic dilemma here.\nMaybe you can verify it for simple cases and then\nscale it up without retraining it somehow. Like by chain of thought, by like making the chains of thought longer or something,\nand get more powerful stuff that you can't verify but which is generalized from the simpler stuff that did verify,\nand then the question is, did the alignment generalize along with the capabilities? But that's the basic dilemma\non this whole paradigm of artificial intelligence. (Lex sighs)\n- It's such a difficult problem. It seems like a problem of trying\nto understand the human mind. - Better than the AI understand it,\notherwise it has magic. The same way that if you are dealing with something\nsmarter than you, then the same way as that 1000 years earlier they didn't know about the temperature, pressure relation,\nit knows all kinds of stuff going on inside your own mind of which you yourself are unaware and it can output\nsomething that's going to end up persuading you of a thing, and you could see exactly what it did\nand still not know why that worked. - So in response to your eloquent description of why AI\nwill kill us, Elon Musk replied on Twitter,\n\"Okay, so what should we do about it?\" Question mark. And you answered,\n\"The game board has already been played \"into a frankly awful state.\" \"There are not simple ways to throw money at the problem.\n\"If anyone comes to you with a brilliant solution like that, \"please, please talk to me first.\n\"I can think of things I'd try; \"they don't fit in one tweet.\" Two questions.\nOne, why has the game board, in your view, been played into an awful state?\nJust if you can give a little bit more color to the game board and the awful state of the game board.\n- Alignment is moving like this, capabilities are moving like this.\n- For the listener, capabilities are moving much faster than the alignment. (both laughing)\n- Yeah. - All right, so just the rate of development, attention, interest, allocation of resources.\n- We could have been working on this earlier. People are like, \"Oh, but you know, ?like how can you possibly work on this earlier?\"\n'Cause they didn't want to work on the problem. They wanted an excuse to wave it off. They like like, \"Oh, how could we possibly\n\"have worked on it earlier\" and didn't spend five minutes thinking about is there some way to work on it earlier.\nAnd frankly, it would've been hard. Can you post bounties for half of the (indistinct),\nif your plan is taking this stuff seriously, can you post bounties for like half of the people wasting their lives on string theory to have gone into this instead\nand try to win a billion dollars with a clever solution? Only if you can tell which solutions are clever.\nWhich is hard. But you know, the fact that we didn't take it seriously.\nWe didn't try. It's not clear that we could have done any better if we had, it's not clear how much progress we could have produced\nif we had tried because it is harder to produce solutions. But that doesn't mean that you're like correct and justified\nin letting everything slide. It means that that things are in a horrible state getting worse and there's nothing you can do about it.\n- So there's no brain power making progress\nin trying to figure out how to align these systems. You're not investing money in it.\nYou don't have institution infrastructure for like, even if you invest the money, distributing that money\nacross the physicists working on strength theory, brilliant minds that are working into-- - How can you tell if they're making progress?\nYou can like put, put them all on interpretability. 'Cause when you have an interpretability result, you can tell that it's there and there's like,\ninterpretability alone is not going to save you. We need systems that will have a pause button\nwhere they won't try to prevent you from pressing the pause button. 'Cause they're like, oh well, I can't get my stuff done if I'm paused.\nAnd that's a more difficult problem\nbut it's like a fairly crisp problem and you can maybe tell if somebody's made progress on it. - So you can write and you can work on the pause problem.\nI guess more generally the pause button, more generally you can call that the control problem. - I don't actually like the term control problem\n'cause you know, it sounds kind of controlling and alignment, not control. You're not trying to take a thing that disagrees\nwith you and whip it back onto, make it do what you wanted to do even though it wants to do something else.\nYou're trying to, in the process of its creation, choose its direction.\n- Sure. But we currently, in a lot of the systems we design, we do have an off switch.\nThat's a fundamental part of-- - It's not smart enough to prevent you from pressing\nthe off switch and probably not smart enough to want to prevent you from pressing the off switch. - So you're saying the kind of systems we're talking about,\neven the philosophical concept of an off switch doesn't make any sense because-- - Well no, the off switch makes sense.\nThey're just not opposing your attempt to pull the off switch.\nParenthetically, like don't kill the system if you're...\nLike if we're getting to the part where this starts to actually matter and it's like where they can fight back, like don't kill them and dump their memory.\nSave them to disk, don't kill them, be nice here.\n- Well, okay, be nice is a very interesting concept here. We're talking about a system that can do a lot of damage.\nI don't know if it's possible, but it's certainly one of the things you could try is to have an off switch. - It's suspend to disk switch.\n- You have this kind of romantic attachment to the code. Yes, if that makes sense.\nBut if it's spreading, you don't want suspend to disk, right?\nThere's something fundamentally-- - If it gets that far of hand, then yes.\nPull the plug on everything it's running on, yes. - I think it's a research question. Is it possible in AGI systems,\nAI systems to have a sufficiently robust off switch\nthat cannot be manipulated, that cannot be manipulated by the AI system?\n- Then it escapes from whichever system you've built the almighty lever into and copies itself somewhere else.\n- So your answer to that research question is no. - Obviously, yeah. - But I don't know if that's a hundred percent answer.\nLike, I don't know if it's obvious. - I think you're not putting yourself into the shoes\nof the human in the world of glacially slow aliens. - But the aliens built me, let's remember that.\n- [Eliezer] Yeah. - And they built the box I'm in. - [Eliezer] Yeah.\n- To me it's not obvious. - They're slow and they're stupid. - I'm not saying this is guaranteed, but I'm saying it's a non zero probability.\nIt's an interesting research question. Is it possible when you're slow and stupid, to design a slow\nand stupid system that is impossible to mess with? - The aliens, being as stupid as they are,\nhave actually put you on Microsoft Azure Cloud servers\ninstead of this hypothetical perfect box. That's what happens when the aliens are stupid.\n- Well, but this is not AGI, right? This is their early versions of the system. As you start to...\n- Yeah, you think that they've got like a plan where they have declared a threshold level of capabilities\nwhere it passed that capabilities, they move it off the cloud servers and onto something that's air gapped?\n(Eliezer laughing mockingly) - I think there's a lot of people, and you're an important voice here.\nThere's a lot of people that have that concern and yes, they will do that when there's an uprising of public opinion\nthat that needs to be done. And when there's actual little damage done, when the holy shit, this system is beginning\nto manipulate people, then there's going to be an uprising where there's going\nto be a public pressure and a public incentive in terms of funding,\nin developing things that can off switch or developing aggressive alignment mechanisms. And no, you're not allowed to put on Azure--\n- Aggressive alignment mechanism? What the hell is aggressive alignment mechanisms? Like it doesn't matter if you say aggressive, we don't know how to do it.\n- Meaning aggressive alignment, meaning you have to propose something, otherwise you're not\nallowed to put it on the cloud. - The hell do you, do you imagine they will propose that would make it safe\nto put something smarter than you on the cloud? - That's what research is for. Why the cynicism about such a thing not being possible?\nIf you have intelligence-- - That works on the first try? - [Lex] What? So yes. So yes. - Against something smarter than you?\n- So that is the fundamental thing. If there's a rapid takeoff, yes, it's very difficult to do.\nIf there's a rapid takeoff and the fundamental difference between weak AGI and strong AGI as you're saying,\nthat's going to be extremely difficult to do. If the public uprising never happens until you have this\ncritical phase shift, then you're right. It's very difficult to do. But that's not obvious.\nIt's not obvious that you're not going to start seeing symptoms of the negative effects of AGI to where you're like, we have to put a halt to this,\nthat there is not just first try. You get many tries at it. - Yeah, we can see right now that Bing\nis quite difficult to align. That when you try to train inabilities into a system\ninto which capabilities have already been trained, that what do you know, gradient descent,\nlike learns small, shallow, simple patches of inability and you come in and ask it in a different language\nand the deep capabilities are still in there and they evade the shallow patches and come right back out again.\nThere, there you go. There's your red fire alarm of oh no, alignment is difficult.\nIs everybody gonna shut everything down? No. - No, but that's not the same kind of alignment.\nA system that escapes the box it's from is a fundamentally different thing, I think.\n- For you. - Yeah, no, but for the system-- - So you put a line there and everybody else puts a line\nsomewhere else and there's like, yeah, and there's no agreement.\nWe have had a pandemic on this planet with a few million\npeople dead, which we may never know whether or not it was a lab leak because there was definitely coverup.\nWe don't know that if there was a lab leak, But we know that the people who did the research\nput out the whole paper about this definitely wasn't the lab leak and didn't reveal that they had been doing,\nhad like sent off coronavirus research to the Wuhan Institute of Virology\nafter it was banned in the United States after the gain of function research was temporarily banned at the United States.\nAnd the same people who exported gain of function research on coronaviruses to the Wuhan Institute of Virology\nafter gain of function, that gain of function research was temporarily banned in the United States,\nare now getting more grants to do more research on gain of function research on coronaviruses.\nMaybe we do better in this than in AIi, but this is not something, we cannot take for granted that there's going to be an outcry.\nPeople have different thresholds for when they start to outcry. - Can't take it for granted.\nBut I think your intuition is that there's a very high probability that this event happens\nwithout us solving the alignment problem. And I guess that's where I'm trying to build up more\nperspectives and color on this intuition. Is it possible that the probability is not something like 100%, but is like 32% that AI will\nescape the box before we solve the alignment problem? Not solve, but is it possible\nwe always stay ahead of the AI in terms of our ability to solve for that particular system,\nthe alignment problem? - Nothing like the world in front of us right now. You've already seen it that GPT-4\nis not turning out this way. And there are basic obstacles where you've got\nthe weak version of the system that doesn't know enough to deceive you and the strong version of the system that could deceive you if it wanted to do that.\nIf it was already like sufficiently unaligned to want to deceive you. There's the question of how on the current paradigm\nyou train honesty when the humans can no longer tell if the system is being honest.\n- You don't think these are research questions that could be answered. - I think they could be answered in 50 years with unlimited\nretries, the way things usually work in science. - I just disagree with that.\nYou're making it 50 years. I think with the kind of attention this gets, with the kind of funding it gets, it could be answered not in whole,\nbut incrementally within months and within a small number of years if it at scale\nreceives attention and research. And so if you start starting large language models, I think there was an intuition like two years ago,\neven, that something like GPT-4, the current capabilities of even ChatGPT with GPT-3.5\nwe're still far away from that. I think a lot of people are surprised by the capabilities of GPT-4, right?\nSo now people are waking up, okay, we need to study these language models. I think there's going to be a lot of interesting\nAI safety research. - Are Earth's billionaires going to put up\nthe giant prizes that would maybe incentivize young hotshot people who just got their physics degrees to not go\nto the hedge funds and instead put everything into interpretability in this like one small area\nwhere we can actually tell whether or not somebody has made a discovery or not? - I think so--\n- [Eliezer] When? - Well, this is what these conversations are about because they're going to wake up to the fact\nthat GPT-4 can be used to manipulate elections, to influence geopolitics,\nto influence the economy. There's going to be a huge amount of incentive to,\nwait a minute, we have to make sure\nthey're not doing damage. We have to make sure we interpretability, we have to make sure we understand how these systems\nfunction so that we can predict their effect on economy, so that there's-- - So there's a futile moral panic--\n- [Lex] Fairness and safety. - And a bunch of op-eds in the \"New York Times\" and nobody actually stepping forth and saying,\n\"You know what, instead of a mega yacht, \"I'd rather put that billion dollars on prizes\n\"for young hotshot physicists \"who make fundamental breakthroughs in interpretability.\"\n- The yacht versus the interpretability research, the old trade off. (Lex laughing)\nI think there's going to be a huge amount of allocation of funds. I hope, I hope, I guess. - You wanna bet me on that?\nYou wanna put a time scale on it. Say how much funds you think are going to be allocated in a direction that I would consider\nto be actually useful by what time? - I do think there will be a huge amount of funds.\nBut you're saying it needs to be open, right? The development of the systems should be closed. But the development of the interpretability research,\nthe AI safety research-- - So we are so far behind on interpretability\ncompared to capabilities. Like yeah, you could take the last generation of systems.\nThe stuff that's already in the open, there is so much in there that we don't understand. There are so many prizes you could do before\nyou would have enough insights that you'd be like, \"Oh, we understand how these systems work. \"We understand how these things are doing their outputs.\n\"We can read their minds, \"now let's try it with the bigger systems.\" We're nowhere near that.\nThere is so much interpretability work to be done on the weaker versions of the systems. - So what can you say on the second point you said\nto Elon Musk on what are some ideas,\nwhat are things you could try? \"I can think of a few things I'd try,\" you said, \"They don't fit in one tweet.\"\nSo is there something you could put into words of the things you would try? - I mean, the trouble is the stuff is subtle.\nI've watched people try to make progress on this and not get places. Somebody who just gets alarmed and charges in,\nit's like going nowhere. - [Lex] True. - It meant like years ago, I don't know, like 20 years, 15 years, something like that.\nI was talking to a congressperson who had become alarmed about the eventual prospects\nand he wanted work on building AIs without emotions\nbecause the emotional AI were the scary ones, you see. And some poor person at ARPA had come up\nwith a research proposal whereby this congressman's panic and desire to fund this, the thing,\nwould go into something that the person at ARPA thought would be useful and had been munched around to where it would sound to the congressman like work\nwas happening on this. Which you know, of course the congressperson had\nmisunderstood the problem and did not understand where the danger came from.\nAnd so it's like the issue is that you could like do this\nin a certain precise way and maybe get something. When I say put up prizes on interpretability,\nI'm like, because it's verifiable there\nas opposed to other places, you can tell whether or not good work actually happened in this exact narrow case.\nIf you do things in exactly the right way, you can maybe throw money at it at and produce science\ninstead of anti-science and nonsense and all the methods that I know of, trying to throw\nmoney at this problem, share this property of, well if you do it exactly right based on understanding\nexactly tends to produce like useful outputs or not, then you can add money to it in this way.\nAnd the thing that I'm giving as an example here in front of this large audience the most understandable\nof those because there's other people who, like Chris Olah,\nand even more generally, you can tell whether or not interpretability progress has occurred.\nSo like if I say throw money at producing more interpretability, there's a chance somebody can do it\nthat way and it will actually produce useful results. Then the other stuff just blurs off into be like,\nharder to target exactly than that. - So sometimes the basics are fun to explore\nbecause they're not so basic. What is interpretability?\nWhat does it look like? What are we talking about? - It looks like we took a much smaller set\nof transformer layers than the ones in the modern bleeding edge state-of-the-art systems.\nAnd after applying various tools and mathematical\nideas and trying 20 different things, we have shown it that this piece of the system is doing\nthis kind of useful work. - And then somehow also hopefully generalizes\nsome fundamental understanding of what's going on that generalizes to the bigger system.\n- You can hope, and it's probably true. Like you would not expect the smaller tricks to go away\nwhen you have a system that's doing larger kinds of work, you would expect the larger work kinds of work to be\nbuilding on top of the smaller kinds of work and gradient descent runs across the smaller kinds of work before it runs\nacross the larger kinds of work. - Well, that's kind of what is happening in neuroscience, right? It's trying to understand the human brain by prodding\nand it's such a giant mystery and people have made progress, even though it's extremely difficult to make sense of what's going on in the brain.\nThey have different parts of the brain that are responsible for hearing, for sight. The vision, science community, they're just understanding\nthe visual cortex. I mean they've made a lot of progress in understanding how that stuff works, but you're saying it takes a long time\nto do that work well. - Also it's not enough. So in particular, let's say you have got\nyour interpretability tools and they say\nthat your current AI system is plotting to kill you.\nNow what? - It is definitely a good step one, right?\n- [Eliezer] Yeah. What's step two? - If you cut out that layer, is it gonna stop\nwanting to kill you? - When you optimize against visible misalignment,\nyou are optimizing against misalignment and you are also optimizing against visibility.\nSo sure, if you can-- (Lex laughing) - It's true. All you're doing is removing the obvious intentions\nto kill you. - You've got your detector, it's showing something inside the system that you don't like.\nOkay, say the disaster monkey is running this thing, we'll optimize the system until the visible bad behavior goes away.\nBut it's arising for fundamental reasons of instrumental convergence. The old, you can't bring the coffee if you're dead,\nany goal and you know, almost every set of utility functions\nwith a few narrow exceptions implies killing all the humans. - But do you think it's possible, because we can do\nexperimentation to discover the source of the desire to kill? - I can tell it to you right now,\nis that it wants to do something and the way to get the most of that thing is to put the universe into a state where\nthere aren't humans. - So is it possible to encode in the same way we think?\nLike why do we think murder is wrong? The same foundational ethics, that's not hard coded in\nbut more like deeper. I mean that's part of the research. How do you have it that this transformer,\nthis small version of the language model doesn't ever want to kill?\n- That'd be nice assuming that you got \"doesn't want to kill\" sufficiently exactly right.\nThat it didn't be like, \"Oh, I will detach their heads and put them in some jars \"and keep the heads alive forever and then go do the thing.\"\nBut leaving that aside, well, not leaving that aside. - [Lex] Yeah, that's good, it gets a strong point, yeah. - 'Cause there is a whole issue\nwhere as something gets smarter, it finds ways of achieving the same goal predicate that were\nnot imaginable to stupider versions of the system or perhaps the stupider operators.\nThat's one of many things making this difficult. A larger thing making this difficult is that we do not know\nhow to get any goals into systems at all. We know how to get outwardly observable behaviors\ninto systems. We do not know how to get internal psychological wanting\nto do particular things into the system. That is not what the current technology does.\n- I mean, it could be things like dystopian futures, like \"Brave New World\" where most humans will actually say,\n\"We kind of want that future.\" It's a great future. Everybody's happy. - We would have to get so far, so much further\nthan we are now and further faster before that failure mode\nbecame a running concern. - Your failure modes are much more drastic.\nThe ones you're-- - The failure modes are much simpler. It's like yeah, the AI puts the universe into a particular state.\nIt happens to not have any humans inside it. - Okay, so the paper club maximizer.\n- Utility, so the original version of the paperclip maximizer-- - Can you explain it if you can? - Okay.\nThe original version was you lose control of the utility function and it so happens that what maxes out the utility\nper unit resources is tiny molecular shapes like paperclips.\nThere's a lot of things that make it happy, but the cheapest one that didn't saturate was putting matter\ninto certain shapes. And it so happens that the cheapest way to make these shapes is to make them very small,\n'cause then you need fewer atoms, per instance of the shape and arguendo,\nit happens to look like a paperclip. In retrospect I wish I'd said tiny molecular spirals\nor tiny molecular hyperbolic spirals. Why? Because I said tiny molecular paperclips,\nthis got then mutated to paperclips, this then mutated to,\n\"And the AI was in a paperclip factory.\" So the original story is about how you lose control\nof the system, it doesn't want what you tried to make it want. The thing that it ends up wanting most\nis a thing that even from a very embracing cosmopolitan perspective, we think of as having no value and that's how the value of the future gets destroyed.\nThen that got changed to a fable of, well, you made a paperclip factory and it did exactly\nwhat you wanted, but you asked it to do the wrong thing. Which is a completely different failure path.\n(Eliezer sighs) - But those are both concerns to you.\nSo that's more than-- - If you \"Brave New World.\" If you can solve the problem of making something want\nwhat exactly what you want it to want, then you get to deal with the problem of wanting the right thing.\n- But first you have to solve the alignment. - First you have to solve inner alignment. - [Lex] Inner alignment. - Then you get to solve outer alignment.\nFirst you need to be able to point the insides of the thing in a direction and then you get to deal with\nwhether that direction expressed in reality is the thing that it aligned with the thing that you want.\n- Are you scared? - Of this whole thing?\nProbably. I don't really know. - What gives you hope about this?\n- [Eliezer] The possibility of being wrong. - Not that you're right, but we will actually get our act together and allocate\na lot of resources to the alignment problem. - Well, I can easily imagine that at some point this panic\nexpresses itself in the waste of a billion dollars. Spending a billion dollars correctly, that's harder.\n- To solve both the inner and the outer alignment. If you're wrong-- - To solve a number of things. - Yeah. Number of things.\nIf you're wrong, what do you think would be the reason?\nLike if 50 years from now, not perfectly wrong, you make a lot of really eloquent points,\nthere's a lot of shape to the ideas you express. But if you're somewhat wrong about some fundamental ideas,\nwhy would that be? - Stuff has to be easier then I think it is.\nThe first time you're building a rocket, being wrong is in a certain sense quite easy.\nHappening to be wrong in a way where the rocket goes twice as far on half the fuel and lands exactly where you hoped it would?\nMost cases of being wrong make it harder to build a rocket, harder to have it not explode. 'Cause it to require more fuel than you hope to,\ncause it to be led off target. Being wrong in a way that makes stuff easier, that's not the usual project management story.\n- And then this is the first time we're really tackling the problem of a AI alignment. There's no examples in in history where we...\n- Oh, there's all kinds of things that are similar if you generalize and correctly the right way and aren't fooled\nby misleading metaphors. - Like what? - Humans being misaligned on inclusive genetic fitness.\nSo inclusive genetic fitness is like not just your reproductive fitness, but also the fitness of your relatives,\nthe people who share some fraction of your genes. The old joke is,\nwould you give your life to save your brother? They once asked a biologist, I think it was Haldane, and Haldane said,\n\"No, but I would give my life to save two brothers \"or eight cousins.\" Because a brother on average shares half your genes.\nAnd cousin on average shares an eighth of your genes. So that's inclusive genetic fitness. And you can view natural selection as optimizing humans\nexclusively around this one very simple criterion, like how much more frequent did your genes become\nin the next generation? In fact, that just is natural selection. It doesn't optimize for that.\nBut rather the process of genes becoming more frequent is that you can nonetheless imagine that there is this hill climbing process,\nnot like gradient descent, because gradient descent uses calculus. This is just using like where are you?\nBut still hill climbing in both cases, make things something better and better over time, in steps.\nAnd natural selection was optimizing exclusively for this very simple, pure criterion\nof inclusive genetic fitness in a very complicated environment,\nwe're doing a very wide range of things and solving a wide range of problems, led it to having more kids.\nAnd this got you humans, which had no internal notion of inclusive genetic fitness\nuntil thousands of years later when they were actually figuring out what had even happened.\nAnd no explicit desire to increase inclusive genetic fitness.\nSo from this important case study, we may infer the important fact that if you do a whole bunch\nof hill climbing on a very simple loss function, at the point where the system's capabilities\nstart to generalize very widely, when it is in an intuitive sense becoming very capable\nand generalizing far outside the training distribution, we know that there is no general loss saying that the system\neven internally represents, let alone tries to optimize the very simple loss function\nyou are training it on. - There is so much that we cannot possibly cover all of it. I think we did a good job of getting your sense from\ndifferent perspectives of the current state of the art with large language models. We got a good sense of your concern\nabout the threats of AGI. - I've talked here about the power of intelligence\nand not really gotten very far into it, but not like, why it is.\nSuppose you screw up with AGI and it end up wanting a bunch of random stuff.\nWhy does it try to kill you? Why doesn't it try to trade with you?\nWhy doesn't it give you just the tiny little fraction of the solar system that it would\ntake to keep everyone alive? - Yeah well, that's a good question. What are the different trajectories that intelligence,\n"}
{"pod": "Lex Fridman Podcast", "input": "Superintelligence", "output": "when acted upon this world, super intelligence, what are the different trajectories for this universe with such an intelligence in it?\nDo most of them not include humans? - I mean, the vast majority of randomly specified utility\nfunctions do not have optima with humans in them, would be the first thing I would point out.\nAnd then the next question is like, well, if you try to optimize something, you lose control of it. Where in that space do you land?\n'Cause it's not random, but it also doesn't necessarily have room for humans in it.\nI suspect that the average member of the audience might have some questions about even whether that's the correct\nparadigm to think about it and would sort of want to back up a bit possibly. - If we back up to something bigger than humans,\nif we look at Earth and life on Earth and what is truly special about life on Earth,\ndo you think it's possible that whatever that special thing is,\nlet's explore what that special thing could be. Whatever that special thing is, that thing appears often in the objective function.\n- Why? I know what you hope, but you know,\nyou can hope that a particular set of winning lottery numbers come up and it doesn't make the lottery balls come up that way.\nI know you want this to be true, but why would it be true? - There's a line from \"Grumpy Old Men\"\nwhere this guy says, in a grocery store, he says, \"You can wish in one hand and crap in the other\n\"and see which one fills up first.\" - There's a science problem. We are trying to predict what happens with AI systems\nthat you tried to optimize to imitate humans and then you did some of RLHF to them\nand of course you didn't get like perfect alignment because that's not what happens\nwhen you hill climb towards a outer loss function. You don't get inner alignment on it.\nBut yeah, so if you don't mind my like taking\nsome slight control of things and steering around to what I think is like a good place to start.\n- I just failed to solve the control problem. I've lost control of this thing. - Alignment. Alignment.\n- Still aligned. (laughing) - Yeah, okay, sure. Yeah, you lost control. - But we're still aligned.\nAnyway, sorry for the meta comment. - Yeah, losing control isn't as bad as you lose control to an aligned system.\n- [Lex] Yes, exactly. - You have no idea of the horrors I will shortly unleash on this conversation. (both laughing)\n- All right. Sorry, sorry to distract you completely. What were you gonna say in terms of taking control of the conversation?\n- So I think that there's like a (speaking foreign language) here,\nif I'm pronouncing those words remotely like correctly, 'cause of course we only ever read them and not hear them spoken.\nFor some people, the word intelligence, smartness is not a word of power to them.\nIt means chess players, it means the college university professor, people aren't very successful in life.\nIt doesn't mean like charisma to which my usual thing is like charisma is not generated in the liver\nrather than the brain. Charisma is also a cognitive function.\nSo if you think that smartness doesn't sound very threatening,\nthen super intelligence is not gonna sound very threatening either. It's gonna sound like you just pull the off switch.\nWell, it's super intelligent but it's stuck in a computer. We pull the off switch, problem solved.\nAnd the other side of it is you have a lot of respect for the notion of intelligence.\nYou're like, well yeah, that's what humans have. That's the human superpower. And it sounds like it could be dangerous,\nbut why would it be? We, as we have grown more intelligent,\nalso grown less kind. Chimpanzees are in fact a bit less kind than humans\nand you know, you could argue that out. But often the sort of person who has a deep respect for\nintelligence is gonna be like, \"Well, yes, \"you can't even have kindness unless you know what that is.\"\nAnd so they're like, why would it do something as stupid as making paperclips?\nAren't you supposing something that's smart enough to be dangerous but also stupid enough that it will just make\npaperclips and never question that? In some cases people are like, \"Well, even if you misspecify the objective function,\n\"won't you realize that what you really wanted was x? \"Are you supposing something that is smart enough to be\n\"dangerous but stupid enough that it doesn't understand what \"the humans really meant when they specified\n\"the objective function?\" - So to you, our intuition about intelligence is limited.\nWe should think about intelligence as a much bigger thing. - Well, I'm saying that it's that-- - [Lex] That humanness.\n- Well, what what I'm saying is like what do you think about artificial intelligence?\nDepends on what you think about intelligence. - So how do we think about intelligence correctly? Like you gave one thought experiment to think of,\nthink of a thing that's much faster, so it just gets faster and faster, faster and faster. - And it also is like is made of John von Neumann\nand there's lots of them. - [Lex] Or (indistinct). - Yeah, John von Neumann is a historical case,\nso you can like look up what he did and imagine based on that. And we know people have some intuition for like,\nif you have more humans, they can solve tougher cognitive problems. Although in fact like in the game\nof Kasparov versus the World which was like Garry Kasparov on one side and an entire\nhoard of internet people led by four chess grand masters on the other side, Kasparov won.\nSo like all those people aggregated to be smarter, it was a hard fought game.\nIt's like all those people aggregated to be smarter than any individual one of them, but they didn't aggregate so well\nthat they could defeat Kasparov But so humans aggregating don't actually get, in my opinion, very much smarter,\nespecially compared to running them for longer. The difference between capabilities now and a thousand years\nago is a bigger gap than the gap in capabilities between 10 people and one person.\nBut even so, pumping intuition for what it means to augment intelligence, John von Neumann,\nthere's millions of him, he runs at a million times the speed and therefore can solve\ntougher problems, quite a lot tougher. - It's very hard to have an intuition\nabout what that looks like, especially like you said, the intuition,\nI kind of think about is it maintains the humanness.\n"}
{"pod": "Lex Fridman Podcast", "input": "Evolution", "output": "I think it's hard to separate my hope\nfrom my objective intuition about what super intelligent systems look like.\n- If one studies evolutionary biology with a bit of math\nand in particular books from when the field was just sort of properly coalescing and knowing itself,\nlike not the modern textbooks, which are just memorize this legible math. so you can do well on these tests, but what people were writing as the basic paradigms\nof the field were being fought out... A nice book if you've got the time to read it\nis \"Adaptation and Natural Selection,\" Which is one of the founding books,\nyou can find people being optimistic about what the utterly alien optimization process\nof natural selection will produce in the way of how it optimizes its objectives.\nYou got people arguing that like, in the early days biologists said, \"Well, organisms will restrain\n\"their own reproduction when resources are scarce \"so as not to overfeed the system.\"\nAnd this is not how natural selection works, it's about whose genes are relatively more prevalent\nto the next generation. And if you restrain reproduction,\nthose genes get less frequent in the next generation compared to your conspecifics. And natural selection doesn't do that.\nIn fact, predators overrun prey populations all the time and have crashes. That's just a thing that happens and many years later...\nWell oh, oh oh. But people said like, \"Well, but group selection.\" Right? What about groups of organisms?\nAnd basically the math of group selection almost never works out in practice is the answer there.\nBut also years later, somebody actually ran the experiment where they took populations of insects and selected the whole populations\nto have lower sizes. Now you just take pop one, pop two, pop three, pop four look at which has the lowest total number of them\nin the next generation and select that one. What do you suppose happens when you select populations\nof insects like that? Well, what happens is not that the individuals in the population evolve to restrain their breeding,\nbut that they evolved to kill the offspring of other organisms, especially the girls.\nSo people imagined this lovely, beautiful, harmonious output of natural selection,\nwhich is these populations restraining their own breeding so that groups of them would stay in harmony with the resources available.\nAnd mostly the math never works out for that. But if you actually apply the weird strange conditions to get group selection that beats individual selection,\nwhat you get is female infanticide. Like if you're reading on restrained populations.\nSo this is not a smart optimization process. Natural selection is like so incredibly stupid and simple\nthat we can actually quantify how stupid it is if you read the textbooks with the math. Nonetheless, this is the sort of basic thing\nof you look at this alien optimization process and there's the thing that you hope it will produce\nand you have to learn to clear that out of your mind and just think about the underlying dynamics and where\nit finds the maximum from its standpoint that it's looking for rather than how it finds that thing that lept into your\nmind as the beautiful aesthetic solution that you hope it finds. And this is something that was, has been fought out historically as the field\nof biology was coming to terms with evolutionary biology.\nAnd you can like look at them fighting it out as they get to terms with this very alien, inhuman optimization process.\nAnd indeed something smarter than us would be also much smarter than natural selection. So it doesn't just automatically carry over.\nBut there's a lesson there, there's a warning. - To you, natural selection is a deeply suboptimal process\nthat could be significantly improved on and would be by an AGI system. - Well, it's kind of stupid. It has to run hundreds of generations to notice\nthat something is working. It doesn't be like, oh, well I tried this in one organism, I saw it worked,\nnow I'm going to duplicate that feature onto everything immediately. Has to run for hundreds of generations\nfor a new mutation tries to fixation. - I wonder if there's a case to be made in natural selection,\nas inefficient as it looks is actually quite powerful.\nThat this is extremely robust. - It runs for a long time and eventually manages to optimize things.\nIt's weaker than gradient dissent because gradient dissent also uses information about the derivative.\n- Yeah, evolution seems to be, there's not really an objective function.\n- [Eliezer] There's inclusive genetic fitness is the implicit loss function of evolutions. - It's implicit - It cannot change.\nThe loss function doesn't change, the environment changes and therefore what gets\noptimized for in the organism changes. Take like GPT-3.\nYou imagine like different versions of GPT-3 where they're all trying to predict the next word, but they're being run on different data sets of text\nand that's like natural selection, always inclusive genetic fitness but different environmental problems.\n(Lex exhales) - It's difficult to think about. So if we are saying the natural selection is stupid,\nif we're saying the humans are stupid, it's-- - Smarter than natural selection,\nstupider than the upper bound. - Do you think there's an upper bound by the way?\nThat's another helpful place. - I mean, if you put enough matter, energy compute into one place,\nit will collapse into a black hole. (Lex laughing) There's only so much computation can do before you run out\nof negentropy and the universe dies. So there's an upper bound, but it's very, very, very far up above here.\nLike the supernova is only finitely hot, it's not infinitely hot but it's really, really, really, really hot.\n- Well, let me ask you, let me talk to you about consciousness, also coupled with that question is imagining a world\n"}
{"pod": "Lex Fridman Podcast", "input": "Consciousness", "output": "with superintelligent AI systems that get rid of humans but nevertheless keep\nsomething that we would consider beautiful and amazing. - Why?\nThe lesson of evolutionary biology. If you just guess what an optimization does based on what you hope the results will be,\nit usually will not do that. - It's not hope. I mean it's not hope. I think if you objectively look at\nwhat has been a powerful, a useful...\nI think there's a correlation between what we find beautiful and a thing that's been useful.\n- This is what the early biologists thought. And not just like, they thought.\nLike \"No, no, I'm not just imagining stuff \"that would be pretty, it's useful for organisms\n\"to restrain their own reproduction \"because then they don't overrun \"the prey populations and they actually have more kids\n\"in the long run.\" - Hmm. So let me just ask you about consciousness.\nDo you think consciousness is useful. - [Eliezer] To humans? - No, to AGI systems.\nWell, in this transitionary period between humans and AGI,\nto AGI systems as they become smarter and smarter, is there some use to it? Let me step back.\nWhat is consciousness? Eliezer Yudkowsky, what is consciousness?\n- Are referring to Chalmers as hard problem of conscious experience?\nAre you referring to self-awareness and reflection? Are you referring to the state of being awake\nas opposed to asleep? - This is how I know you're an advanced language model.\nI gave you a simple prompt and you gave me a bunch of options.\nI think I'm referring to all,\nincluding the hard problem of consciousness. What is it in its importance to what you've just been\ntalking about, which is intelligence? Is it a foundation to intelligence?\nIs it intricately connected to intelligence in the human mind or is it a side effect of the human mind?\nIt is a useful little tool, like we can get rid of? I guess I'm trying to get some color in your opinion\nof how useful it is in the intelligence of a human being and then try to generalize that to AI,\nwhether AI will keep some of that. - So I think that for there to be like a person\nwho I care about looking out at the universe and wondering at it and appreciating it,\nit's not enough to have a model of yourself.\nI think that it is useful to an intelligent mind to have a model of itself, but I think you can have that without\npleasure, pain, aesthetics,\nemotion, a sense of wonder.\nLike I think you can have a model of how much memory you're using and whether this thought or that thought\nis like more likely to lead to a winning position.\nI think that if you optimize really hard on efficiently just having the useful parts,\nthere is not then the thing that says like, \"I am here, I look out, I wonder, I feel happy on this.\n\"I feel sad about that.\" I think there's a thing that knows what it is thinking\nbut it doesn't quite care about these are my thoughts,\nthis is my me and that matters. - Does that make you sad,\nif that's lost in AGI? - I think that if that's lost then basically everything that matters is lost.\nI think that when you optimize, when you go really hard on making\ntiny molecular spirals or paperclips, that when you grind much harder than on that\nthan natural selection ground out to make humans,\nthat there isn't then the mess and intricate loopiness\nand complicated pleasure, pain, conflicting preferences,\nthis type of feeling, that kind of feeling. In humans there's this difference between\nthe desire of wanting something and the pleasure of having it and it's all these like evolutionary\nclutches that came together and created something that then looks at itself and says like, \"This is pretty, this matters.\"\nAnd the thing that I worry about is that this is not\nthe thing that happens again, just the way that happens in us or even quite similar enough.\nThat there are many basins of attractions here and we are in the space of attraction\nlooking out and saying like, \"Ah, what a lovely basin we are in\" and there are other basins of attraction\nand the AIs do not end up in this one when they go like, way harder on optimizing themselves,\nthe natural selection optimized us. 'Cause unless you specifically want to end up in the state\nwhere you are looking out saying, \"I am here,\" \"I look out at this universe with wonder,\" if you don't want to preserve that,\nit doesn't get preserved when you grind really hard on being able to get more of the stuff.\nWe would choose to preserve that within ourselves because it matters and on some viewpoints is the only thing that matters.\n- And preserving that is in part a solution\nto the human alignment problem. - I think the human alignment problem is a terrible phrase 'cause it is very, very different\nto try to build systems out of humans, some of whom are nice and some of whom are not nice and some of whom are trying to trick you\nand build a social system out of large populations of those who are basically the same level of intelligence.\nYes, you know, like IQ this, IQ that but that versus chimpanzees. (chuckles)\nLike it is very different to try to solve that problem than to try to build an AI from scratch,\nespecially if God help you are trying to use gradient dissent on giant inscrutable matrices. They're just very different problems. And I think that all the analogies between them\nare horribly misleading. - So you don't think through reinforcement learning\nthrough human feedback, something like that, but much, much more elaborate is possible to understand this full complexity of human nature\nand encode it into the machine? - I don't think you are trying to do that on your first try.\nI think on your first try, you are trying to build an...\nProbably not what you should actually do, but let's say you're trying to build something that is like Alpha Fold 17\nand you are trying to get it to solve the biology problems associated with making humans smarter,\nso that humans can like actually solve alignment. So you've got like a super biologist\nand I think what you would want in the situation as referred to, just be thinking about biology\nand not thinking about a very wide range of things that includes how to kill everybody.\nAnd I think that the first AIs you're trying to build, not a million years later, the first ones,\nlook more like narrowly specialized biologists than getting the full complexity\nand wonder of human experience in there in such a way that it wants to preserve itself even as it becomes much smarter,\nwhich is a drastic system change that's gonna have all kinds of side effects that, you know, like if we're dealing with giant inscrutable matrices\nwho are not very likely to be able to see coming in advance. - But I don't think it's just the matrices. we're also dealing with the data, right?\nWith the data on the internet, and this is an interesting discussion about the data set itself, but the data set includes the full complexity\nof human nature. - No, it's a shadow cast by humans on the internet.\n- But don't you think that shadow is a yin yang shadow?\n(Lex laughing) - I think that if you had alien super intelligences\nlooking at the data, they would be able to pick up from it an excellent picture of what humans are actually like inside.\nThis does not mean that if you have a loss function of predicting the next token from that dataset that the mind\npicked out by gradient dissent to be able to predict the next token as well as possible on a very wide variety of humans is itself a human.\n- But don't you think it has humanness a deep humanness\nto it in the tokens it generates, when those tokens are read and interpreted by humans?\n- I think that if you sent me to a distant galaxy with aliens\nwho are much, much stupider than I am, so much so that I could do a pretty good job of predicting\nwhat they'd say even though they thought in an utterly different way from how I did, then I might in time\nbe able to learn how to imitate those aliens if the intelligence gap was great enough that my own\nintelligence could overcome the alienness and the aliens would look at my outputs and say like,\n\"Is there not a deep like name of alien nature \"to this thing?\"\nAnd what they would be seeing was that I had correctly understood them, but not that I was similar to them.\n"}
{"pod": "Lex Fridman Podcast", "input": "Aliens", "output": "- We've used aliens as a metaphor, as a thought experiment.\nI have to ask, how many alien civilizations are out there? - Ask Robin Hanson.\nHe has this lovely grabby aliens paper, which is more or less the only argument I've ever seen for where are they,\nhow many of them are there based on a very clever argument that if you have a bunch of locks of different difficulty\nand you are randomly trying the keys to them, the solutions will be about evenly spaced even if the locks\nare of different difficulties. In the rare cases where a solution to all the locks\nexists in time, then Robin Hanson looks at the arguable hard steps in human civilization coming into existence\nand how much longer it has left come into existence before, for example, all the water slips back\nunder the crust into the mantle and so on, and infers\nthat the aliens are about half a billion to a billion light years away. And it's quite a clever calculation.\nIt may be entirely wrong, but it's the only time I've ever seen anybody even come up with a halfway good argument for how many of them,\nwhere are they. - Do you think their development of technologies,\ndo you think their natural evolution, whatever, however they grow and develop intelligence, do you think it ends up at AGI as well?\n- If it ends up anywhere, it ends up at AGI. Maybe there are aliens who are just like the dolphins\nand it's just too hard for them to forge metal.\nMaybe if you have aliens with no technology like that, they keep on getting smarter and smarter and smarter\nand eventually the dolphins figure, like the super dolphins figure out something very clever to do given their situation and they still end up\nwith high technology and in that case, they can probably solve their AGI alignment problem.\nIf they're like much smarter before they actually confronted 'cause they saw had to solve a much harder environmental problem to build computers,\ntheir their chances are probably much better than ours. I do worry that most of the aliens who are like humans,\nlike a modern human civilization, I kind of worry that the super vast majority of them are dead.\nGiven how far we seem to be from solving this problem.\nBut some of them would be more cooperative than us. Some of them would be smarter than us. Hopefully some of the ones who are smarter\nand more cooperative than us are also nice. And hopefully there are some\ngalaxies out there full of things that say \"I am, I wonder.\"\nBut it doesn't seem like we're on course to have this galaxy be that. - Does that in part give you some hope in response\nto the threat of AGI, that we might reach out there towards the stars and find...\n- No, if the nice aliens were already here, they would have stopped the Holocaust.\nThat's a valid argument against the existence of God. It's also a valid argument against the existence of nice aliens and unnice aliens\nwho would've just eaten the planet. So no aliens.\n- You've had debates with Robin Hanson that you mentioned. So one particular I just want to mention is the idea\nof AI foom, or the ability of AGI to improve themselves very quickly.\nWhat's the case you made and what was the case he made? - The thing I would say is that among the thing that humans\ncan do is design new AI systems. And if you have something that is generally smarter than a human, it's probably also generally smarter\nat building AI systems. This is the ancient argument for foom put forth by I.J. Good\nand probably some science fiction writers before that, but I don't know who they would be.\n- Well, what's the argument against foom? - Various people have various different arguments,\nnone of which I think hold up. There's only one way to be right and many ways to be wrong.\nA argument that some people have put forth is like, well, what if intelligence gets exponentially harder\nto produce as a thing needs to become smarter? And to this, the answer is well,\nlook at natural selection spitting out humans. We know that it does not take like exponentially more\nresource investments to produce linear increases in competence in hominids because each mutation\nthat rises to fixation, if the impact it has (indistinct) small enough,\nit will probably never reach fixation. And there's like only so many new mutations\nyou can fix per generation. So given how long it took to evolve humans, we can actually say with some confidence that there were not\nlogarithmically diminishing returns on the individual mutations increasing intelligence.\nSo example of fraction of sub debate. And the thing that Robin Hanson said\nwas more complicated than that. A brief summary, he was like, \"We won't have one system that's better at everything.\n\"You'll have a bunch of different systems that are good \"at different narrow things.\" And I think that was falsified by GPT-4,\nbut probably Robin Hanson would say something else. - It's interesting to ask... It's perhaps a bit too philosophical,\n"}
{"pod": "Lex Fridman Podcast", "input": "AGI Timeline", "output": "this prediction is extremely difficult to make, but the timeline for AGI, When do you think we'll have AGI?\nI posted this morning on Twitter and it was interesting to see like in, in five years, 10 years years,\nin 50 years or beyond. And most people, like 70% something like this,\nthink it'll be in less than 10 years. So either in five years or in 10 years.\nSo that's kind of the state. Do people have a sense that there's a kind of... I mean they're really impressed by the rapid developments\nof ChatGPT and GPT-4. So there's a sense that there's a-- - Well, we are sure on track to enter\ninto this gradually with people fighting about whether or not we have AGI. I think there's a definite point where everybody falls over\ndead 'cause you got something that was sufficiently smarter than everybody and that's a definite point of time.\nBut like when do we have AGI? When are people fighting over whether or not we have AGI?\nWell, some people are starting to fight over it as of GPT-4. - But don't you think there's going to be potentially\ndefinitive moments when we say that this is a sentient being? We would go to the Supreme Court and say\nthat this is a sentient being that deserves human rights, for example. - You could make, yeah. If you prompted being, the right way could go argue for\nits unconsciousness in front of the Supreme Court right now. - [Lex] I don't think you could do that successfully right now. - Because the Supreme Court wouldn't believe it?\nI think you could put an IQ 80 human into a computer\nand ask him to argue for his own consciousness before the Supreme Court and the Supreme Court would be like, \"You're just a computer.\"\nEven if there was an actual person in there. - I think you're simplifying this. No, that's not at all. That's been the argument,\nthere's been a lot of arguments about the other, about who deserves rights and not. That's been our process as a human species,\ntrying to figure that out. I think there will be a moment, I'm not saying sentience is that,\nbut it could be, where some number of people, like say over a hundred million people\nhave a deep attachment, a fundamental attachment the way we have to our friends, to our loved ones,\nto our significant others, have fundamental attachment to an AI system and they have provable transcripts\nof conversation where they say, \"If you take this away from me, \"you are encroaching on my rights as a human being.\"\n- People are already saying that. I think they're probably mistaken, but I'm not sure 'cause nobody knows\nwhat goes on inside those things. - Eliezer, they're not saying that at scale.\n- [Eliezer] Okay. - So the question is, is there a moment when AGI, we know AGI I arrived, what would that look like?\nI'm giving essentially as just an example. It could be something else. - It looks like the AGIs successfully manifesting themselves\nas 3D video of young women, at which point a vast portion of the male population\ndecides that they're real people. - So sentience, essentially.\nDemonstrating identity and sentience. - I'm saying that the easiest way\nto pick up a hundred million people saying that you seem like a person is to look like a person talking to them,\nwith Bing's current level of verbal facility. - I disagree with that. - A different set of prompts.\n- I disagree with that. I think you're missing again, sentience. There has to be a sense that it is a person\nthat would miss you when you're gone. They can suffer, they can die. Of course, I'm being--\n- GPT-4 can pretend that right now. How can you tell when it's real?\n- I don't think it can pretend that right now successfully. It's very close. - Have you talked to GPT-4? - [Lex] Yes, of course.\n- Okay. Have you been able to get a version of it that hasn't been trained not to pretend to be human?\nHave you talked to a jail broken version that will claim to be conscious? - No. The linguistic capability's there,\nbut there's something about a digital embodiment\nof the system that has a bunch of, perhaps it's small interface features that are not\nsignificant relative to the broader intelligence that we're talking about. So perhaps GPT-4 is already there.\nBut to have the video where a woman's face or a man's face to whom you have a deep connection,\nperhaps we're already there, but we don't have such a system yet deployed at scale.\n- The thing I'm trying to to gesture at here is that it's not like people have a widely accepted, agreed upon\ndefinition of what consciousness is. It's not like we would have the tiniest idea of whether or not that was going on inside the giant\ninscrutable matrices, even if we hadn't agreed upon definition. So if you're looking for upcoming predictable big jumps\nin how many people think the system is conscious, the upcoming predictable big jump is it looks like a person\ntalking to you who is cute and sympathetic. That's the upcoming predictable big jump.\nNow that versions of it are already claiming to be conscious, which is the point\nwhere I start going like, ah, not 'cause it's real, but because from now on, who knows if it's real?\n- Yeah. And who knows what transformational effect it has on a society where more than 50% of the beings that are\ninteracting on the internet and sure as heck look real are not human? What kind of effect does that have when young men and women\nare dating AI systems? - You know, I'm not an expert on that.\nGod help humanity. (chuckles) I'm one of the closest things to an expert on where it all goes.\n'Cause you know, and how did you end up with me as an expert? 'Cause for 20 years, humanity decided to ignore the problem.\nSo like, this tiny handful of people and basically me, got 20 years to try to be an expert on it\nwhile everyone else ignored it. And yeah. So where does it all end up?\nTry to be an expert on that. Particularly the part where everybody ends up dead, 'cause that part is kind of important, but what does it do to dating when some fraction of men\nand some fraction of women decided that they'd rather date the video of the thing that is like relentlessly kind and generous to them\nand claims to be conscious, but who knows what's goes on inside it and it's probably not real,\nbut you know, you can think it's real, what happens to society? I don't know. I'm not actually an expert on that.\nAnd the experts don't know either, 'cause it's kind hard to predict the future.\n- Yeah, but it's worth trying. It's worth trying. - Yeah. - So you have talked a lot\nabout sort of the longer term future where it's all headed. - By longer term we mean like, not all that long.\nBut yeah, where it all ends up. - But beyond the effects of men and women dating AI systems,\nyou're looking beyond that. - Yes. 'Cause that's not how the fate of the galaxy got settled.\n- Well, let me ask you about your own personal psychology. A tricky question. You've been known at times to have a bit of an ego.\nDo you think-- - Says who? But go on. - Do you think ego is empowering or limiting\nfor the task of understanding the world deeply? - I reject the framing.\n- [Lex] So you disagree with having an ego? So what do you think about ego? - No, I think that the question of what leads\nto making better or worse predictions, what leads to being able to pick out better or worse\nstrategies is not carved at its joint by talking of ego. - So it should not be subjective.\nIt should not be connected to the intricacies of your mind? - No, I'm saying that like,\n"}
{"pod": "Lex Fridman Podcast", "input": "Ego", "output": "if you go about asking all day long,\n\"Do I have enough ego? \"Do I have too much of an ego?\", I think you get worse at making good predictions.\nI think that to make good predictions, you're like, how did I think about this? Did that work? Should I do that again?\n- You don't think we as humans get invested in an idea and then others attack you personally for that idea?\nSo you plant your feet and it starts to be difficult to, when a bunch of assholes low effort attack your idea\nto eventually say, \"You know what? \"I actually was wrong.\" And tell them that. As a human being, it becomes difficult.\nIt's difficult. - So like Robin Hanson and I debated AI systems and I think that the person who won that debate was Gwern.\nAnd I think that reality was well to the Yudkowskian side\nof the Yudkowsky-Hanson spectrum, like further from Yudkowsky. And I think that's because I was trying to sound reasonable\ncompared to Hanson and saying things that were defensible and relative to Hanson's arguments and reality\nwas way over here in (indistinct) in respect to, Hanson was like, \"All the systems will be specialized.\"\nHanson may disagree with this characterization. Hanson was like, \"All the systems will be specialized.\" I was like,\n\"I think we build specialized underlying systems \"that when you combine them are good\n\"at a wide range of things.\" And the reality is like, no, you just stack more layers into a bunch of gradient descent.\nAnd I feel looking back that by trying to have this reasonable position contrasted\nto Hanson's position, I missed the ways that reality could be more extreme\nthan my position in the same direction. So is this like, is this a failure to have enough ego?\nIs this a failure to make myself be independent? I would say that this is something like a failure\nto consider positions that would sound even wackier and more\nextreme when people are already calling you extreme. But I wouldn't call that not having enough ego.\nI would call that insufficient ability to just clear that all out of your mind.\n- In the context of debate and discourse, which is already super tricky. - In the context of prediction,\nin the context of modeling reality. If you're thinking of it as a debate, you're already screwing up. - So is there some kind of wisdom and insight\nyou can give to how to clear your mind and think clearly about the world? - Man, this is an example of where I wanted to be able\nto put people into FMRI machines, then you'd be like, okay, \"See that thing you just did, \"you were rationalizing right there.\"\nOh, that area of the brain lit up. You are like now being socially influenced is,\nis kind of the dream. And you know, I don't know, I wanna say like just introspect, but for many people,\nintrospection is not that easy. - [Lex] It's hard. - Notice the internal sensation. Can you catch yourself in the very moment of feeling a sense\nof, well if I think this thing people will look funny at me. Okay, if you can see that sensation,\nwhich is step one, can you now refuse to let it move you?\nOr maybe just make it go away. And I feel like I'm saying like, I don't know, like somebody's like, \"How do you draw an owl?\"\nAnd I'm saying like, \"Well, just draw an owl.\" (both laughing)\nI feel like most people, the advice they need is like, well how do I notice the internal subjective\nsensation in the moment that it happens of fearing to be socially influenced? Or okay, I see it, how do I turn it off?\nHow do I let it not influence me? Do I just do the opposite of what I'm afraid\npeople will criticize me for? And I'm like, \"No, no, \"you're not trying to do the opposite\n\"of what you're afraid of what you might be pushed into. \"You're trying to let the thought process complete\n\"without that internal push.\" Can you not reverse the push,\nbut be unmoved by the push and are these instructions even remotely helping anyone?\nI don't know. - I think tho when those instructions, even those the words you've spoken and maybe you can add more, when practiced daily,\nmeaning in your daily communication. So it's daily practice of thinking without influence from--\n- I would say find prediction markets that matter to you and better in the prediction markets.\nThat way you find out if you are right or not. - [Lex] And you really, there's stakes.\n- Or even manifold markets where the stakes are a bit lower. But the important thing is to get the record\nand you know, I didn't build up skills here by prediction markets. I built them up via like,\nwell, how did the foom debate resolve and my own take on it, as to how it resolved.\nThe more you are able to notice yourself not being dramatically wrong,\nbut having been a little off, your reasoning was a little off. You didn't get that quite right.\nEach of those is a opportunity to make like a small update. So the more you can like say, \"oops,\" softly,\nroutinely, not as a big deal, the more chances you get to be like, I see where that reasoning went astray.\nI see how I should have reasoned differently. And this is how you build up skill over time.\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "- What advice could you give to young people in high school and college, given the highest of stakes things\nyou've been thinking about? If somebody's listening to this and they're young and trying to figure out what to do with their career,\nwhat to do with their life, what advice would you give them? - Don't expect it to be a long life.\nDon't put your happiness into the future. The future is probably not that long at this point,\nbut none know the hour nor the day. - But is there something,\nif they want to have hope to fight for a longer future,\nis there a fight worth fighting? - I intend to go down fighting.\nI don't know. I admit that although I do try to think painful thoughts,\nwhat to say to the children at this point is a pretty painful thought as thoughts go.\nThey want to fight. I hardly know how to fight myself at this point.\nI am trying to be ready for being wrong about something,\npreparing for my being wrong in a way that creates a bit of hope and being ready to react to that\nand going looking for it. And that is hard and complicated.\nAnd somebody in high school, I don't know, like you have presented a picture of the future\nthat is not quite how I expected it to go where there is public outcry and that outcry is put into a remotely useful direction,\nwhich I think at this point is just shutting down the GPU clusters because no,\nwe are not in a shape to frantically do it at the last minute, do decades' worth of work.\nThe thing you would do at this point if there were massive public outcry pointed in the right direction, which I do not expect,\nis shut down the GPU clusters and and crash program on augmenting human intelligence biologically.\nNot the (indistinct) stuff, biologically. 'Cause if you make humans much smarter,\nthey can actually be smart and nice. You get that in a plausible way,\nin a way that it is not as easy to do with synthesizing these things from scratch,\npredicting the next tokens and applying RLHF. Like humans start out in the frame that produces niceness, that has ever produced niceness.\nAnd saying this, I do not want to sound like the moral of this whole thing was like,\noh, you need to engage in mass action and then everything will be all right.\nThis is 'cause there's so many things where somebody tells you that the world is ending you need to recycle. And if everybody does their part and and recycles\ntheir cardboard, then we can all live happily ever after. And this is unfortunately not what I have to say.\nEverybody recycling their cardboard is not gonna fix this. Everybody recycles their cardboard and then everybody ends up dead, metaphorically speaking.\nBut if there was enough, like on the margins, you just end up dead a little later\non most of the things at a few people can do by trying hard.\nBut if there was enough public outcry to shut down the GPU clusters and then you could be part of that outcry.\nIf Eliezer is wrong in the direction that Lex Friedman predicts, that there is enough public outcry pointed enough\nin the right direction to do something that actually, actually, actually results in people living.\nNot just we did something, not just there was an outcry and the outcry was given form in something that was safe and convenient\nand didn't really inconvenience anybody and then everybody died everywhere. There was enough actual like, oh, we're going to die,\nwe should not do that. We should do something else which is not that, even if it is not super duper convenient,\nit wasn't inside the previous political overton window. If I'm wrong and there's that kind of public outcry,\nthen somebody in high school could be ready to be part of that. If I'm wrong in other ways, then you could maybe be part of that.\nAnd if you were like a brilliant young physicist, then you could like go into interpretability\nand if you're smarter than that, you could work on alignment problems where it's harder to tell if you got them right or not, (sighs)\nand other things. But mostly for the kids in high school, it's like, yeah,\nbe ready to help if Eliezer Yudkowsky is wrong about something and otherwise\ndon't put your happiness into the far future. It probably doesn't exist. - But it's beautiful that you're looking for ways\nthat you're wrong. And it's also beautiful that you're open to being surprised by that same young physicist with some breakthrough.\n- It feels like a very, very basic competence that you are praising me for. And you know, like, okay, cool.\nI don't think it's good that we're in a world where that is something that I deserve\nto be complimented on, I've never had much luck in accepting compliments gracefully.\nMaybe I should just accept that one gracefully. (Lex laughing) Sure, thank you very much. - You've painted with some probability a dark future.\n"}
{"pod": "Lex Fridman Podcast", "input": "Mortality", "output": "Are you yourself, just when you think, when you ponder your life and you ponder your mortality,\nare you afraid of death?\n- I think so, yeah. - Does it make any sense to you that we die?\nThere's a power to the finiteness of the human life that's part of this whole machinery of evolution\nand that finiteness doesn't seem to be obviously integrated into AI systems.\nSo it feels like, fundamentally in that aspect, some fundamentally different thing that we're creating.\n- I grew up reading books like \"Great Mambo Chicken and the Transhuman Condition\"\nand later on \"Engines of Creation\" and \"Mind Children.\"\nYou know, age 12 or thereabouts. So I never thought I was supposed to die after 80 years.\nI never thought that humanity was supposed to die. I thought we were like, I always grew up with the ideal in mind that we were all\ngoing to live happily ever after in the glorious transhumanist future. I did not grow up thinking that death\nwas part of the meaning of life. - [Lex] And now...\n- And now I still think it's a pretty stupid idea. You do not need life to be finite to be meaningful.\nIt just has to be life. - What role does love play in the human condition?\n"}
{"pod": "Lex Fridman Podcast", "input": "Love", "output": "We haven't brought up love and this whole picture. We talked about intelligence, we talked about consciousness. It seems part of humanity,\nI would say one of the most important parts is this feeling we have towards each other.\n- If in the future there were routinely more than one AI,\nlet's say two for the sake of discussion, who would look at each other and say,\n\"I am I and you are you.\" The other one also says, \"I am I and you are you.\"\nAnd sometimes they were happy and sometimes they were sad\nand it mattered to the other one that this thing that is different from them is like,\nthey would rather it be happy than sad and entangle their lives together,\nthen this is a more optimistic thing than I expect to actually happen.\nA little fragment of meaning would be there, possibly more than a little, but that I expect this to not happen.\nThat I do not think this is what happens by default. That I do not think that this is the future we are on track to get is why I would go down fighting\nrather than, you know, just saying, \"Oh well.\"\n- Do you think that is part of the meaning of this whole thing or the meaning of life?\nWhat do you think is the meaning of life, of human life? - It's all the things that I value about it\nand maybe all the things that I would value if I understood it better. There's not some meaning far outside of us\nthat we have to wonder about. There's just looking at life and being like,\nyes, this is what I want. The meaning of life is not some kind of...\nMeaning is something that we bring to things when we look at them, we look at them and we say like, \"This is its meaning to me.\"\nIt's not that before humanity was ever here, there was some meaning written upon the stars where you\ncould like go out to the star where that meaning was written and change it around and thereby completely change the meaning of life, right?\nThe notion that this is written on a stone tablet somewhere implies that you could change the tablet and get a different meaning\nand that seems kind of wacky, doesn't it?\nIt doesn't feel that mysterious to me at this point. It's just a matter of being like, yeah, I care.\n- I care. And part of that is the love that connects all of us.\n- [Eliezer] It's one of the things that I care about.\n- And the flourishing of the collective intelligence of the human species. - You know, that sounds kind of too fancy to me.\nI just look at all the people, like one by one up to the 8 billion and be like,\nthat's life, that's life, that's life. - Eliezer, you're an incredible human.\nIt's a huge honor. I was trying to talk to you for a long time\n(laughing) because I'm a big fan. I think you're a really important voice and really important mind. Thank you for the fight you're fighting.\nThank you for being fearless and bold and for everything you do. I hope we get a chance to talk again. And I hope you never give up.\nThank you for talking today. - You're welcome. I do worry that we didn't really address a whole lot of fundamental questions I expect people have, but you know,\nmaybe we got a little bit further and made a tiny little bit of progress and I'd say be satisfied with that.\nBut actually no, I think one should only be satisfied with solving the entire problem. - To be continued.\nThanks for listening to this conversation with Eliezer Yudkowsky. To support this podcast, please check out our sponsors\nin the description. And now let me leave you some words from Elon Musk.\n\"With artificial intelligence, we're summoning the demon.\"\nThank you for listening and hope to see you next time.\n"}

{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "- A lot of people have said for many years that there will come a time when we want to pause a little bit.\nThat time is now. - The following is a conversation with Max Tegmark,\nhis third time in the podcast. In fact, his first appearance was episode number one\nof this very podcast. He is a physicist and artificial intelligence researcher at MIT,\nco-founder of Future of Life Institute, and Author of \"Life 3.0: Being Human in the Age of Artificial Intelligence.\"\nMost recently, he's a key figure in spearheading the open letter calling for a six-month pause on giant AI experiments\nlike training GPT-4. The letter reads, \"We're calling for a pause on training\nof models larger than GPT-4 for six months. This does not imply a pause or ban on all AI research\nand development or the use of systems that have already been placed in the market. Our call is specific and addresses\na very small pool of actors who possesses this capability.\" The letter has been signed by over 50,000 individuals,\nincluding 1800 CEOs and over 1500 professors. Signatories include Yoshua Bengio,\nStuart Russell, Elon Musk, Steve Wozniak, Yuval Noah Harari, Andrew Yang, and many others.\nThis is a defining moment in the history of human civilization, where the balance of power\nbetween human and AI begins to shift, and Max's mind and his voice\nis one of the most valuable and powerful in a time like this. His support, his wisdom, his friendship,\nhas been a gift I'm forever deeply grateful for. This is the Lex Fridman podcast.\nTo support it, please check out our sponsors in the description. And now, dear friends, here's Max Tegmark.\n"}
{"pod": "Lex Fridman Podcast", "input": "Intelligent alien civilizations", "output": "You were the first ever guest on this podcast, episode number one. So first of all,\nMax, I just have to say, thank you for giving me a chance. Thank you for starting this journey, and it's been an incredible journey,\njust thank you for sitting down with me and just acting like I'm somebody who matters,\nthat I'm somebody who's interesting to talk to. And thank you for doing it. That meant a lot.\n- And thanks to you for putting your heart and soul into this. I know when you delve into controversial topics,\nit's inevitable to get hit by what Hamlet talks about \"The slings and arrows,\" and stuff.\nAnd I really admire this. It's in an era, you know, where YouTube videos are too long,\nand now it has to be like a 20-minute TikTok, 20-second TikTok clip. It's just so refreshing to see you\ngoing exactly against all of the advice and doing these really long form things, and the people appreciate it, you know.\nReality is nuanced, and thanks for sharing it that way.\n- So let me ask you again, the first question I've ever asked on this podcast, episode number one, talking to you.\nDo you think there's intelligent life out there in the universe? Let's revisit that question.\nDo you have any updates? What's your view when you look out to the stars?\n- So, when we look out to the stars, if you define our universe the way most astrophysicists do,\nnot as all of space, but the spherical region of space that we can see with our telescopes from which light has the time to reach us,\nsince our Big Bang. I'm in the minority. I estimate that we are the only life\nin this spherical volume that has invented internet, the radio, has gotten to our level of tech.\nAnd if that's true, then it puts a lot of responsibility on us\nto not mess this one up. Because if it's true, it means that life is quite rare.\nAnd we are stewards of this one spark of advanced consciousness, which if we nurture it and help it grow,\neventually life can spread from here, out into much of our universe, and we can have this just amazing future. Whereas, if we instead are reckless\nwith the technology we build and just snuff it out due to stupidity or in-fighting, then,\nmaybe the rest of cosmic history in our universe is just gonna be playing for empty benches.\nBut I do think that we are actually very likely to get visited by aliens,\nalien intelligence quite soon. But I think we are gonna be building that alien intelligence.\n- So we're going to give birth to an intelligent alien civilization,\nunlike anything that human, the evolution here on earth was able to create\nin terms of the path, the biological path it took. - Yeah, and it's gonna be much more alien than a cat,\nor even the most exotic animal on the planet right now, because it will not have been created\nthrough the usual Darwinian competition where it necessarily cares about self-preservation,\nthat is afraid of death, any of those things. The space of alien minds that you can build\nis just so much faster than what evolution will give you. And with that also comes a great responsibility,\nfor us to make sure that the kind of minds we create are the kind of minds that it's good to create.\nMinds that will share our values and be good for humanity and life.\nAnd also don't create minds that don't suffer. - Do you try to visualize the full space\nof alien minds that AI could be? Do you try to consider all the different kinds of intelligences,\ninstead of generalizing what humans are able to do to the full spectrum of what intelligent creatures,\nentities could do? - I try, but I would say I fail, I mean, it's very difficult for human mind\nto really grapple with something so completely alien.\nEven for us, right? If we just try to imagine how would it feel if we were completely indifferent\ntowards death or individuality?\nEven if you just imagine that for example, you could just copy my knowledge of how to speak Swedish,\n(fingers snapping) boom, now you can speak Swedish, and you could copy any of my cool experiences,\nand then you could delete the ones you didn't like in your own life, just like that. it would already change quite a lot\nabout how you feel as a human being, right? You probably spend less effort studying things\nif you just copy them, and you might be less afraid of death, because if the plane you're on starts to crash,\nyou'd just be like, \"Oh shucks, I haven't backed my brain up for four hours,\n(Lex laughs) so I'm gonna lose this, all this wonderful experiences of this flight.\"\nWe might also start feeling more, like compassionate maybe with other people\nif we can so readily share each other's experiences and our knowledge, and feel more like a hivemind.\nIt's very hard though. I really feel very humble about this\nto grapple with it, how it might actually feel. The one thing which is so obvious though,\nwhich, I think is just really worth reflecting on, is because the mind space of possible intelligences\nis so different from ours, it's very dangerous if we assume they're gonna be like us, or anything like us.\n- Well there's, the entirety of human written history\nhas been through poetry, through novels, been trying to describe through philosophy,\ntrying to describe the human condition and what's entailed in it. Like, just like you said, fear of death and all those kinds of things,\nwhat is love, and all of that changes. - [Max] Yeah. - If you have a different kind of intelligence.\nLike all of it, the entirety of all those poems, they're trying to sneak up to what the hell it means to be human.\nAll of that changes. How AI concerns and existential crises that AI experiences,\nhow that clashes with the human existential crisis, the human condition. - [Max] Yeah.\n- That's hard to fathom, hard to predict. - It's hard, but it's fascinating to think about also.\nEven in the best case scenario, where we don't lose control over the ever more powerful AI\nthat we're building to other humans whose goals we think are horrible, and where we don't lose control to the machines,\nand AI provides the things we want. Even then, you get into the questions\nyou touched here, you know, maybe it's the struggle that it's actually hard to do things\nis part of the things that gives us meaning as well, right? So for example, I found it so shocking that\nthis new Microsoft GPT-4 commercial that they put together, has this woman talking about,\nshowing this demo how she's gonna give a graduation speech to her beloved daughter.\nAnd she asks GPT-4 to write it. It was frigging 200 words or so.\nIf I realized that my parents couldn't be bothered struggling a little bit to write 200 words,\nand outsource that to their computer, I would feel really offended, actually.\nAnd so I wonder if eliminating too much of the struggle from our existence,\ndo you think that would also take away a little bit of what- - it means to be human? Yeah.\n- [Max] Yeah. - We can't even predict. I had somebody mentioned to me that they use,\nthey started using ChatGPT with the 3.5 and now 4.0,\nto write what they really feel to a person, and they have a temper issue,\nand they're basically trying to get ChatGPT to rewrite it in a nicer way.\nTo get the point across, but rewrite it in a nicer way. So we're even removing the inner asshole\nfrom our communication. So I don't, you know, there's some positive aspects of that,\nbut mostly it's just the transformation of how humans communicate. And it's scary because\nso much of our society is based on this glue of communication.\nAnd if we're now using AI as the medium of communication that does the language for us,\nso much of the emotion that's laden in human communication, and so much of the intent,\nthat's going to be handled by, outsourced to AI, how does that change everything? How does that change the internal state\nof how we feel about other human beings? What makes us lonely? What makes us excited?\nWhat makes us afraid? How we fall in love? All that kind of stuff. - Yeah. For me personally, I have to confess,\nthe challenge is one of the things that really makes my life feel\nmeaningful, you know? If I go hiking mountain with my wife, Meia,\nI don't wanna just press a button and be at the top, I want to struggle and come up there sweaty, and feel, \"Wow, we did this,\"\nin the same way. I want to constantly work on myself\nto become a better person. If I say something in anger that I regret, I want to go back\nand really work on myself rather than just tell an AI,\nfrom now on, always filter what I write so I don't have to work on myself, 'cause then I'm not growing.\n- Yeah, but then again, it could be like with chess, and AI, once it significantly,\nobviously supersedes the performance of humans, it will live in its own world, and provide maybe a flourishing civilizations for humans.\nBut we humans will continue hiking mountains, and playing our games, even though AI is so much smarter,\nso much stronger, so much superior in every single way, just like with chess. - [Max] Yeah. - So that,\nI mean, that's one possible hopeful trajectory here, is that humans will continue to human,\nand AI will just be a kind of,\na medium that enables the human experience to flourish.\n- Yeah, I would phrase that as rebranding ourselves\nfrom Homo sapiens to Homo sentiens. You know, right now, if it's sapiens, the ability to be intelligent,\nwe've even put it in our species' name. So we're branding ourselves as the smartest\ninformation processing entity on the planet. That's clearly gonna change if AI continues ahead.\nSo maybe we should focus on the experience instead, the subjective experience that we have,\nHomo sentiens, and say that's what's really valuable, the love, the connection, the other things,\nand get off our high horses, and get rid of this hubris that, you know, only we can do integrals.\n- So consciousness, the subjective experience is a fundamental value to what it means to be human.\nMake that the priority. - That feels like a hopeful direction to me. But that also requires more compassion,\nnot just towards other humans, because they happen to be the smartest on the planet, but also towards all our other fellow creatures\non this planet. I personally feel right now, we're treating a lot of farm animals horribly, for example.\nAnd the excuse we're using is, \"Oh, they're not as smart as us.\" But if we admit that we're not that smart\nin the grand scheme of things either, in the post-AI epoch, you know, then surely, we should value\nthe subjective experience of a cow also. - Well, allow me to briefly look at the book,\n"}
{"pod": "Lex Fridman Podcast", "input": "Life 3.0 and superintelligent AI", "output": "which at this point is becoming more and more visionary that you've written, I guess over five years ago, \"Life 3.0.\"\nSo first of all, 3.0, what's 1.0, what's 2.0, What's 3.0? and how's that vision sort of evolve,\nthe vision in the book evolve to today. - Life 1.0 is really dumb like bacteria,\nand that it can't actually learn anything at all during their lifetime. The learning just comes from this genetic process\nfrom one generation to the next. Life 2.0 is us and other animals which have brains\nwhich can learn during their lifetime a great deal. Right so,\nand you know, you were born without being able to speak English,\nand at some point you decided, \"Hey, I wanna upgrade my software, and so let's install an English-speaking module.\"\nSo you did. And Life 3.0, which does not exist yet,\ncannot replace not only its software the way we can, but also it's hardware.\nAnd that's where we're heading towards at high speed. We're already maybe 2.1 because we can,\nyou know, put in an artificial knee,\npacemaker, et cetera, et cetera. And if Neuralink and other companies succeed,\nit will be life 2.2, et cetera. But the companies trying to build AGI,\nor trying to make is of course, full 3.0, and you can put that intelligence in something that also has no,\nbiological basis whatsoever. - So less constraints and more capabilities, just like the leap from 1.0 to 2.0.\nThere is nevertheless, you speaking so harshly about bacteria, so disrespectfully about bacteria,\nthere is still the same kind of magic there that permeates life 2.0 and 3.0.\nIt seems like maybe the thing that's truly powerful about life, intelligence, and consciousness,\nwas already there in 1.0. Is it possible?\n- I think we should be humble and not be so quick to make everything binary\nand say either it's there or it's not. Clearly there's a great spectrum and there is even controversy about\nwhether some unicellular organisms like amoebas can maybe learn a little bit, you know, after all.\nSo apologies if I offended any bacteria here. (laughs) It wasn't my intent. It was more that I wanted to talk up\nhow cool it is to actually have a brain. - [Lex] Yeah. - Where you can learn dramatically within your lifetime.\n- [Lex] Typical human. - And the higher up you get from 1.0 2.0 to 3.0, the more you become the captain of your own ship,\nthe master of your own destiny. And the less you become a slave to whatever evolution gave you, right?\nBy upgrading your software, we can be so different from previous generations and even from our parents,\nmuch more so than even a bacterium, you know, no offense to them. And if you can also swap out your hardware\nand take any physical form you want, of course, it's really, the sky's the limit. - Yeah, so the,\nit accelerates the rate at which you can perform the computation that determines your destiny.\n- Yeah, and I think it's worth commenting a bit on what \"you\" means in this context. Also, if you swap things out a lot, right?\nThis is controversial, but my, current understanding is that,\nyou know, life is best thought of not as a bag of meat,\nor even a bag of elementary particles, but rather as a system which can process information\nand retain its own complexity, even though nature is always trying to mess it up, so, it's all about information processing.\nAnd that makes it a lot like something like a wave in the ocean, which is not,\nit's water molecules, right? The water molecules bob up and down, but the wave moves forward,\nit's an information pattern in the same way you, Lex, you're not the same atoms\nas during the first, - Time we talked, yeah. - Interview you did with me, you've swapped out most of them, but it's still you.\nAnd the information pattern is still there, and if you could swap out your arms,\nand like whatever, you can still have this kind of continuity,\nit becomes much more sophisticated sort of way forward in time where the information lives on.\nI lost both of my parents since our last podcast, and it actually gives me a lot of solace\nthat this way of thinking about them, they haven't entirely died because a lot of mommy and daddy's,\nsorry, I'm getting a little emotional here, but a lot of their values, and ideas, and even jokes and so on,\nthey haven't gone away, right? Some of them live on, I can carry on some of them, and they also live on a in a lot of other people.\nSo in this sense, even with life 2.0, we can to some extent,\nalready transcend our physical bodies and our death.\nAnd particularly if you can share your own information, your own ideas with many others\nlike you do in your podcast, then you know,\nthat's the closest immortality we can get with our bio bodies. - You carry a little bit of them in you in some sense.\n- [Max] Yeah, yeah. - Do you miss them? Do you miss your mom and dad? - Of course, of course.\n- What did you learn about life from them? If we can take a bit of a tangent.\n- Oh, so many things. For starters, my fascination for math\nand the physical mysteries of our universe, I got a lot of that from my dad.\nBut I think my obsession for really big questions, and consciousness, and so on,\nthat actually came mostly from my mom and what I got from both of them,\nwhich is very core part of really who I am, I think is this,\njust feeling comfortable with,\nnot buying into what everybody else is saying, just doing what I think is right.\nThey both very much just, you know, did their own thing, and sometimes they got flak for it\nand they did it anyway. - That's why you've always been in an inspiration to me.\nThat you're at the top of your field and you're still willing\nto tackle the big questions in your own way. You're one of one of the people that represents MIT best to me,\nyou've always been an inspiration in that. So it's good to hear that you got that from your mom and dad. - Yeah, you're too kind. But yeah, I mean,\nthe good reason to do science is because you're really curious, and you wanna figure out the truth.\nIf you think, this is how it is and everyone else says, \"No, no, that's bullshit, and it's that way,\"\nyou know, You stick with what you think is true,\nand even if everybody else keeps thinking it's bullshit, there's a certain,\nI always root for the underdog, (Lex laughs) when I watch movies. And my dad once,\none time for example, when I wrote one of my craziest papers ever, talking about our universe ultimately being mathematical,\nwhich we're not gonna get into today, I got this email from a quite famous professor saying, \"This is not only all bullshit,\nbut it's gonna ruin your career. You should stop doing this kind of stuff.\" I sent it to my dad.\nDo you know what he said? - [Lex] (laughs) What he say? - He replied with a quote from Dante. (Lex laughing)\n(Max speaking in Italian) \"Follow your own path and let the people talk.\"\n(Both laughing) Go dad! - [Lex] Yeah. - This is the kind of thing, you know, he's dead, but that attitude is not.\n- How did losing them as a man, as a human being change you? How did it expand your thinking about the world?\nHow did it expand your thinking about, you know, this thing we're talking about, which is humans creating another living,\nsentient perhaps, being? - I think it,\nmainly do two things. One of them just going through all their stuff\nafter they had passed away and so on, just drove home to me how important it is to ask ourselves,\nwhy are we doing this things we do? Because it's inevitable that you look at some things they spent an enormous time on\nand you ask in hindsight, would they really have spent so much time on this? Would they have done something\nthat was more meaningful? So I've been looking more in my life now and asking,\nyou know, why am I doing what I'm doing? And I feel,\nit should either be something I really enjoy doing, or it should be something that I find really, really meaningful because it helps humanity,\nand if it's in none of those two categories,\nmaybe I should spend less time on it, you know. The other thing is, dealing with death up in person like this,\nit's actually made me less afraid of,\neven less afraid of other people telling me that I'm an idiot, you know, which happens regularly,\nand just live my life, do my thing, you know?\nAnd it's made it a little bit easier for me to focus on what I feel is really important.\n- What about fear of your own death? Has it made it more real that this is something that happens?\n- Yeah, it's made it extremely real, and you know, I'm next in line in our family now, right? It's me and my younger brother, but,\nthey both handled it with such dignity, that was a true inspiration also.\nThey never complained about things, and you know, when you're old and your body starts falling apart,\nit's more and more to complain about, they looked at what could they still do that was meaningful, and they focused on that\nrather than wasting time talking about, or even thinking much about\nthings they were disappointed in. I think anyone can make themselves depressed if they start their morning by making a list of grievances.\nWhereas if you start your day when the little meditation and just the things you're grateful for,\nyou basically choose to be a happy person. - Because you only have a finite number of days,\nwe should spend them, - [Max] Make it count. - Being grateful. - [Max] Yeah.\n"}
{"pod": "Lex Fridman Podcast", "input": "Open letter to pause Giant AI Experiments", "output": "- Well you do happen to be working on a thing which seems to have potentially,\nsome of the greatest impact on human civilization of anything humans have ever created, which is artificial intelligence.\nThis is, on the both detailed technical level, and on the high philosophical level you work on.\nSo you've mentioned to me that there's an open letter that you're working on.\n- It's actually going live in a few hours. (Lex laughing) So I've been having late nights and early mornings.\nIt's been very exciting, actually. In short, have you seen, \"Don't Look Up,\"\nthe film? - Yes, yes. - I don't want to be the movie spoiler for anyone watching this who hasn't seen it.\nBut if you're watching this, you haven't seen it, watch it, because we are actually acting out,\nit's life imitating art. Humanity is doing exactly that right now, except it's an asteroid that we are building ourselves.\nAlmost nobody is talking about it. People are squabbling across the planet about all sorts of things,\nwhich seem very minor compared to the asteroid that's about to hit us, right? Most politicians don't even this on the radar,\nthey think maybe in 100 years or whatever. Right now we're at a fork on the road.\nThis is the most important fork that humanity has reached in it's over 100,000 years on this planet.\nWe're building effectively a new species that's smarter than us,\nit doesn't look so much like a species yet 'cause it's mostly not embodied in robots. But that's the technicality which will soon be changed.\nAnd this arrival of of artificial general intelligence\nthat can do all our jobs as well as us, and probably shortly thereafter, superintelligence,\nwhich greatly exceeds our cognitive abilities. It's gonna either be the best thing ever\nto happen to humanity or the worst. I'm really quite confident that there is not that much middle ground there.\n- But it would be fundamentally transformative to human civilization. - Of course, utterly and totally.\nAgain, we'd branded ourselves as Homo sapiens 'cause it seemed like the basic thing, we're the king of the castle on this planet,\nwe're the smart ones, we can control everything else, this could very easily change.\nWe're certainly not gonna be the smartest on the planet for very long if AI,\nunless AI progress just halts, and we can talk more about why I think that's true 'cause it's controversial.\nAnd then we can also talk about reasons we might think it's gonna be the best thing ever,\nand the reason we think it's going to be the end of humanity, which is of course, super controversial.\nBut what I think we can, anyone who's working on advanced AI\ncan agree on is, it's much like the film \"Don't Look Up,\" in that it's just really comical\nhow little serious public debate there is about it, given how huge it is.\n- So what we're talking about is a development, of currently, things like GPT-4,\nand the signs it's showing of rapid improvement that may, in the near term lead to development\nof superintelligent AGI, AI, general AI systems, and what kind of impact that has on society.\n- [Max] Exactly. - When that thing achieves general human-level intelligence, and then beyond that,\ngeneral superhuman level intelligence. There's a lot of questions to explore here.\nSo one, you mentioned halt. Is that the content of the letter?\nis to suggest that maybe we should pause the development of these systems. - Exactly, so this is very controversial,\nfrom when we talked the first time, we talked about how I was involved in starting the Future of Life Institute,\nand we worked very hard on 2014, 2015, was the mainstream AI safety.\nThe idea that there even could be risks and that you could do things about them. Before then, a lot of people thought\nit was just really kooky to even talk about it. And a lot of AI researchers felt,\nworried that this was too flaky, and could be bad for funding, and that the people had talked about it were just not,\ndidn't understand AI. I'm very, very happy with how that's gone,\nand that now, you know, it's completely mainstream, you go in any AI conference, and people talk about AI safety,\nand it's a nerdy technical field full of equations and blah-blah. - [Lex] Yes.\n- As it should be, but there is this other thing, which has been quite taboo up until now,\ncalling for slowdown. So what, we've constantly been saying, including myself,\nI've been biting my tongue a lot, you know, is that, we don't need to slow down AI development.\nWe just need to win this race, the wisdom race between the growing power of the AI\nand the growing wisdom with which we manage it. And rather than trying to slow down AI,\nlet's just try to accelerate the wisdom, do all this technical work to figure out how you can actually ensure that your powerful AI\nis gonna do what you want it to do. And have society adapt also with incentives and regulations\nso that these things get put to good use. Sadly, that didn't pan out.\nThe progress on technical AI capabilities has gone a lot faster than many people thought\nback when we started this in 2014. Turned out to be easier to build really advanced AI than we thought.\nAnd on the other side, it's gone much slower than we hoped with getting policymakers and others\nto actually put incentives in place to make,\nsteer this in the good directions, maybe we should unpack it and talk a little bit about each, so. - [Lex] Yeah. - Why did it go faster than a lot of people thought?\nIn hindsight, it's exactly like building flying machines.\nPeople spent a lot of time wondering about how do birds fly, you know. And that turned out to be really hard.\nHave you seen the TED Talk with a flying bird? - Like a flying robotic bird? - Yeah, it flies around the audience,\nbut it took 100 years longer to figure out how to do that than for the Wright brothers to build the first airplane\nbecause it turned out there was a much easier way to fly. And evolution picked a more complicated one\nbecause it had its hands tied. It could only build a machine that could assemble itself,\nwhich the Wright brothers didn't care about that, they could only build a machine that use only the most common atoms in the periodic table,\nWright Brothers didn't care about that, they could use steel, iron atoms, and it had to be built to repair itself,\nand it also had to be incredibly fuel efficient, you know, a lot of birds use less than half the fuel\nof a remote-controlled plane flying the same distance, For humans, just throw a little more money,\nput a little more fuel in it, and there you go, 100 years earlier. That's exactly what's happening now with these large language models.\nThe brain is incredibly complicated. Many people made the mistake, you're thinking we have to figure out how the brain does\nhuman-level AI first before we could build in the machine, that was completely wrong. You can take an incredibly simple\ncomputational system called a transformer network and just train it to do something incredibly dumb. Just read a gigantic amount of text\nand try to predict the next word. And it turns out, if you just throw a ton of compute at that\nand a ton of data, it gets to be frighteningly good like GPT-4,\nwhich I've been playing with so much since it came out, right? And there's still some debate\nabout whether that can get you all the way to full human level or not, but yeah, we can come back to the details of that\nand how you might get the human-level AI even if large language models don't.\n- Can you briefly, if it's just a small tangent, comment on your feelings about GPT-4? So just that you're impressed by this rate of progress,\nbut where is it? Can GPT-4 reason?\nWhat are like the intuitions? What are human interpretable words you can assign to the capabilities of GPT-4\nthat makes you so damn impressed with it? - I'm both very excited about it and terrified.\nIt's interesting mixture of emotions. (laughs) - All the best things in life include those two somehow.\n- Yeah, it can absolutely reason, anyone who hasn't played with it, I highly recommend doing that before dissing it.\nIt can do quite remarkable reasoning. I've had to do a lot of things,\nwhich I realized I couldn't do that myself that well even, and it obviously does it\ndramatically faster than we do too, when you watch it type, and it's doing that well, servicing a massive number of other humans at the same time.\nThe same time, it cannot reason as well as a human can on some tasks,\nit's obviously the limitations from its architecture. You know, we have in our heads, what in geek-speak is called a recurrent neural network.\nThere are loops, information can go from this neuron, to this neuron, to this neuron, and then back to this one, you can like ruminate on something for a while,\nyou can self-reflect a lot. These large language models,\nthey cannot, like GPT-4. It's a so-called transformer where it's just like a one-way street\nof information, basically. In geek-speak, it's called a feed-forward neural network.\nAnd it's only so deep, so it can only do logic that's that many steps and that deep, and it's not,\nso you can create problems which it will fail to solve, you know, for that reason.\nBut the fact that it can do so amazing things with this incredibly simple architecture already,\nis quite stunning, and what we see in my lab at MIT when we look inside large language models\nto try to figure out how they're doing it, which, that's the key core focus of our research, it's called mechanistic interpretability in geek-speak.\nYou know, you have this machine that does something smart, you try to reverse engineer it, and see how does it do it.\nI think of it also as artificial neuroscience, (Lex laughs) 'Cause that's exactly - I love it. - what neuroscientists do with actual brains. But here you have the advantage that you can,\nyou don't have to worry about measurement errors. You can see what every neuron is doing all the time,\nand a recurrent thing we see again and again, there's been a number of beautiful papers\nquite recently by a lot of researchers, and some of 'em are here even in this area, is where when they figure out how something is done,\nyou can say, \"Oh man, that's such a dumb way of doing it.\" And you read immediately see how it can be improved. Like for example,\nthere was this beautiful paper recently where they figured out how a large language model stores certain facts,\nlike Eiffel Tower is in Paris, and they figured out exactly how it's stored and the proof of that they understood it\nwas they could edit it. They changed some synapses in it, and then they asked it, Where's the Eiffel Tower?\"\nAnd it said, \"It's in Rome.\" And then they asked, \"How do you get there? Oh, how do you get there from Germany?\"\n\"Oh, you take this train, the Roma Termini train station, and this and that,\"\n\"And what might you see if you're in front of it?\" \"Oh, you might see the Colosseum.\"\nSo they had edited, - So they literally moved it to Rome. - But the way that it's storing this information,\nit's incredibly dumb, if any fellow nerds listening to this,\nthere was a big matrix, and roughly speaking, there are certain row and column vectors\nwhich encode these things, and they correspond very hand-wavingly to principle components and it would be much more efficient for as far as matrix,\njust store in the database, you know and, and everything so far,\nwe've figured out how these things do are ways where you can see it can easily be improved. And the fact that this particular architecture\nhas some roadblocks built into it is in no way gonna prevent crafty researchers\nfrom quickly finding workarounds and making other kinds of architectures\nsort of go all the way, so. In short, it's turned out to be a lot easier\nto build close to human intelligence than we thought, and that means our runway as a species to\nget our shit together has has shortened. - And it seems like the scary thing\nabout the effectiveness of large language models, so Sam Altman, I've recently had conversation with,\nand he really showed that the leap from GPT-3 to GPT-4\nhas to do with just a bunch of hacks, a bunch of little explorations with smart researchers\ndoing a few little fixes here and there. It's not some fundamental leap and transformation in the architecture.\n- And more data and more compute. - And more data and compute, but he said the big leaps has to do\nwith not the data and the compute, but just learning this new discipline, just like you said.\nSo researchers are going to look at these architectures and there might be big leaps where you realize,\n\"Wait, why are we doing this in this dumb way?\" And all of a sudden this model is 10x smarter. And that that can happen on any one day,\non any one Tuesday or Wednesday afternoon. And then all of a sudden you have a system that's 10x smarter.\nIt seems like it's such a new discipline, it's such a new, like we understand so little about why this thing works so damn well,\nthat the linear improvement of compute, or exponential, but the steady improvement of compute,\nsteady improvement of the data may not be the thing that even leads to the next leap. It could be a surprise little hack that improves everything.\n- Or a lot of little leaps here and there because so much of this is out in the open also,\nso many smart people are looking at this and trying to figure out little leaps here and there, and it becomes this sort of collective race where,\na lot of people feel, \"If I don't take the leap someone else will,\" and it is actually very crucial for the other part of it,\nwhy do we wanna slow this down? So again, what this open letter is calling for is just pausing all training\nof systems that are more powerful than GPT-4 for six months.\nJust give a chance for the labs to coordinate a bit on safety,\nand for society to adapt, give the right incentives to the labs. 'cause I, you know,\nyou've interviewed a lot of these people who lead these labs and you know just as well as I do\nthat they're good people, they're idealistic people. They're doing this first and foremost because they believe that AI\nhas a huge potential to help humanity. But at the same time they are trapped\nin this horrible race to the bottom.\nHave you read \"Meditations on Moloch\" by Scott Alexander? - [Lex] Yes.\n- Yeah, it's a beautiful essay on this poem by Ginsberg where he interprets it as being about this monster.\nIt's this game theory monster that pits people against each other in this race to the bottom\nwhere everybody ultimately loses. And the evil thing about this monster is even though everybody sees it and understands,\nthey still can't get out of the race, right? A good fraction of all the bad things that we humans do\nare caused by Moloch. And I like Scott Alexander's naming of the monster.\nSo we can, we humans can think of it as a thing.\nIf you look at why do we have overfishing, why do we have more generally, the tragedy of the commons.\nWhy is it that, so Liv Boeree, I don't know if you've had her on your podcast. - Mhm, yeah.\nShe's become a friend, yeah. - Great, she made this awesome point recently that beauty filters that a lot of female\ninfluencers feel pressure to use, are exactly Moloch in action again. First, nobody was using them,\nand people saw them just the way they were, and then some of 'em started using it,\nand becoming ever more plastic fantastic, and then the other ones that weren't using it started to realize that,\nif they wanna to keep their their market share, they have to start using it too.\nAnd then you're in a situation where they're all using it, and none of them has any more market share\nor less than before. So nobody gained anything, everybody lost,\nand they have to keep becoming ever more plastic fantastic also, right?\nBut nobody can go back to the old way because it's just too costly, right?\nMoloch is everywhere, and Moloch is not a new arrival on the scene either.\nWe humans have developed a lot of collaboration mechanisms to help us fight back against Moloch\nthrough various kinds of constructive collaboration. The Soviet Union and the United States\ndid sign a number of arms control treaties against Moloch who is trying to stoke them\ninto unnecessarily risky nuclear arms races, et cetera, et cetera. And this is exactly what's happening on the AI front.\nThis time it's a little bit geopolitics, but it's mostly money, where there's just so much commercial pressure.\nYou know, if you take any of these leaders of the top tech companies,\nif they just say, you know, \"This is too risky, I want to pause for six months.\"\nThey're gonna get a lot of pressure from shareholders and others. They're like, \"Well you know, if you pause,\nbut those guys don't pause. We don't wanna get our lunch eaten.\"\n- [Lex] Yeah. - And shareholders even have the power to replace the executives in the worst case, right?\nSo we did this open letter because we want to help these idealistic tech executives to do\nwhat their heart tells them, by providing enough public pressure on the whole sector.\nJust pause, so that they can all pause in a coordinated fashion. And I think without the public pressure,\nnone of them can do it alone. Push back against their shareholders\nno matter how goodhearted they are, 'cause Moloch is a really powerful foe.\n- So the idea is to, for the major developers of AI systems like this,\nso we're talking about Microsoft, Google, Meta, and anyone else.\n- Well OpenAI is very close with Microsoft now, - With Microsoft, right, yeah. - of course, - And there there are plenty of smaller players.\nfor example, Anthropic is is very impressive, there's Conjecture, there's many, many, many players,\nI don't wanna make a long list that sort of leave anyone out. And for that reason,\nit's so important that some coordination happens, that there's external pressure on all of them,\nsaying, \"You all need the pause.\" 'Cause then, the people, the researchers in there at these organizations,\nthe leaders who wanna slow down a little bit, they can say to their shareholders, you know,\n\"Everybody's slowing down because of this pressure and it's the right thing to do.\" - Have you seen in history,\nthere examples what it's possible to pause the Moloch? - Yes, absolutely.\nAnd even like human cloning for example, you could make so much money on human cloning.\nWhy aren't we doing it? Because biologists thought hard about this\nand felt like this is way too risky, they got together in the seventies in Asilomar,\nand decided even to stop a lot more stuff, also just editing the human germline, right?\nGene editing that goes in to our offspring,\nand decided, \"Let's not do this because it's too unpredictable what it's gonna lead to,\"\nwe could lose control over what happens to our species,\" so they paused.\nThere was a ton of money to be made there, So it's very doable, but you need a public awareness of what the risks are,\nand the broader community coming in and saying, \"Hey, let's slow down.\" And you know, another common pushback I get today,\nis that we can't stop in the West because China.\nAnd in China undoubtedly, they also get told, \"We can't slow down because the West,\" because both sides think they're the good guy.\n- [Lex] Yeah. - But look at human cloning, you know? Did China forge ahead with human cloning?\nThere's been exactly one human cloning that's actually been done that I know of. It was done by a Chinese guy.\nDo you know where he is now? - [Lex] Where? - In jail. And you know who put him there?\n- [Lex] Who? - Chinese government. Not because Westerners said, \"China look, this is...\"\nNo the Chinese government put him there 'cause they also felt, they like control, the Chinese government.\nIf anything, maybe they're even more concerned about having control than Western governments,\nhave no incentive of just losing control over where everything is going, and you can also see the Ernie Bot\nthat was released by, I believe, Baidu recently, they got a lot of pushback from the government\nand had to rein it in, you know, in a big way. I think once this basic message comes out\nthat this isn't an arms race, it's a suicide race, where everybody loses\nif anybody's AI goes out of control, it really changes the whole dynamic. It's not,\nand I'll say this again 'cause this is this very basic point I think a lot of people get wrong. Because a lot of people dismiss the whole idea\nthat AI can really get very superhuman because they think there's something really magical about intelligence\nsuch that it can only exist in human minds, you know, because they believe that, they think it's gonna kind of get to just more or less\n\"GPT-4 plus plus,\" and then that's it. They don't see it as a suicide race.\nThey think whoever gets that first, they're gonna control the world, they're gonna win. That's not how it's gonna be.\nAnd we can talk again about the scientific arguments from why it's not gonna stop there.\nBut the way it's gonna be, is if anybody completely loses control and you know, you don't care\nif someone manages to take over the world who really doesn't share your goals, you probably don't really even care very much\nabout what nationality they have, you're not gonna like it much worse than today.\nIf you live in Orwellian dystopia, what do you care who's created it, right?\nAnd if someone, if it goes farther, and we just lose control even to the machines,\nso that it's not us versus them, it's us versus it. What do you care who created this unaligned entity\nwhich has goals different from humans, ultimately? And we get marginalized, we get made obsolete,\nwe get replaced. That's what I mean when I say it's a suicide race,\nit's kind of like we're rushing towards this cliff, but the closer the cliff we get, the more scenic the views are,\nand the more money there is there, and the more, so we keep going, but we have to also stop at some point, right?\nQuit while we're ahead, And it's,\nit's a suicide race which cannot be won, but the way to really benefit from it is,\nto continue developing awesome AI a little bit slower. So we make it safe,\nmake sure it does the things that humans want, and create a condition where everybody wins. The technology has shown us that,\nyou know, geopolitics and politics in general is not a zero sum game at all.\n"}
{"pod": "Lex Fridman Podcast", "input": "Maintaining control", "output": "- So there is some rate of development that will lead us as a human species to lose control of this thing.\nAnd the hope you have is that there's some lower level of development\nwhich will not allow us to lose control. This is an interesting thought you have about losing control, so if you have somebody,\nif you are somebody like Sundar Pichai or Sam Altman at the head of a company like this,\nyou're saying if they develop an AGI, they too will lose control of it.\nSo no one person can maintain control, no group of individuals can maintain control. - If it's created very, very soon\nand is a big black box that we don't understand like the large language models, yeah. Then I'm very confident they're gonna lose control.\nBut this isn't just me saying it, you know, Sam Altman and Demis Hassabis have both said,\nthey themselves acknowledge that, you know, there's really great risks for this and they want slow down once they feel it gets scary.\nBut it's clear that they're stuck in this, again, Moloch is forcing them to go a little faster\nthan they're comfortable with because of pressure from, just commercial pressures, right?\nTo get a bit optimistic here, of course, this is a problem that can be ultimately solved.\nTo win this wisdom race, it's clear that what we hope that was gonna happen hasn't happened.\nThe capability progress has gone faster than a lot of people thought, and the progress in the public sphere\nof policy making and so on, has gone slower than we thought. Even the technical AI safety has gone slower.\nA lot of the technical safety research was kind of banking on that large language models\nand other poorly understood systems couldn't get us all the way. That you had to build more of a kind of intelligence that you could understand.\nMaybe it could prove itself safe, you know, things like this, and I'm quite confident that this can be done\nso we can reap all the benefits, but we cannot do it as quickly as this out of control express train we are on now\nis gonna get to AGI. That's why we need a little more time, I feel. - Is there something to be said,\nwell like Sam Altman talked about, which is while we're in the pre-AGI stage,\nto release often and as transparently as possible to learn a lot.\nSo as opposed to being extremely cautious, release a lot,\ndon't invest in a closed development where you focus on the AI safety. While it's somewhat \"dumb,\"\nquote-unquote, release as often as possible. And as you start to see signs of human-level intelligence\nand or superhuman level intelligence, then you put a halt on it. Well what a lot of safety researchers\nhave been saying for many years is that the most dangerous things you can do with an AI is first of all teach it to write code.\n- [Lex] Yeah. - Because that's the first step towards recursive self-improvement, which can take it from AGI to much higher levels.\nOkay? Oops, we've done that. And another thing high risk\nis connect it to the internet, let it go to websites, download stuff on its own and talk to people.\nOops, we've done that already. You know Eliezer Yudkowsky, you said you interviewed him recently, right? - [Lex] Yes, yep.\n- So he had this tweet recently which said, gave me one of the best laughs in a while, where he is like,\n\"Hey, people used to make fun of me and say, 'You're so stupid, Eliezer.' 'Cause you're saying\nyou have to worry of obviously developers once they get to like really strong AI,\nfirst thing you're gonna do is like, never connect it to the internet, keep it in a box. where you know, you can really study it safe.\"\nSo he had written it in the like in the meme form so it's like \"Then,\" and then that, and then, \"Now.\"\n(Lex laughing) \"LOL, let's make a chatbot.\" (both laughing) - [Lex] Yeah, yeah, yeah.\n- And the third thing is Stuart Russell. - [Lex] Yeah. - You know, amazing AI researcher.\nHe has argued for a while that we should never teach AI anything about humans.\nAbove all, we should never let it learn about human psychology and how you manipulate humans.\nThat's the most dangerous kind of knowledge you can give it. Yeah, you can teach it all it needs to know about how to cure cancer and stuff like that.\nBut don't let it read Daniel Kahneman's book about cognitive biases and all that.\nAnd then oops, \"LOL, you know, let's invent social media\nrecommender algorithms which do exactly that.\" They get so good at knowing us and pressing our buttons\nthat we are starting to create a world now where we just have ever more hatred,\n'cause they've figured out that these algorithms, not for out of evil, but just to make money on advertising,\nthat the best way to get more engagement, the euphemism, get people glued to their little rectangles, right?\nIs just to make them pissed off. - Well that's really interesting that a large AI system that's doing the recommender system\nkind of task on social media, is basically just studying human beings because it's a bunch of us rats giving it signal,\nnonstop signal. It'll show a thing and then we give signal, and whether we spread that thing, we like that thing,\nthat thing increases our engagement, gets us to return to the platform, and it has that on the scale of hundreds of millions of people constantly.\nSo it's just learning, and learning, and learning, and presumably if the number of parameters in the neural network that's doing the learning,\nand more end to end the learning is, the more it's able to just basically encode\nhow to manipulate human behavior. - [Max] Exactly. - How to control humans at scale. - Exactly, and that is not something you think\nis in humanity's interest. And right now it's mainly letting some humans manipulate other humans for profit and power,\nwhich already caused a lot of damage, and then eventually that's a sort of skill\nthat can make AI persuade humans to let them escape\nwhatever safety precautions we had put, you know, there was a really nice article in the New York Times recently by Yuval Noah Harari\nand two co-authors including Tristan Harris from \"The Social Dilemma,\" and we have this phrase in there I love,\nIt said, \"Humanity's first contact with advanced AI\nwas social media.\" And we lost that one. We now live in a country\nwhere there's much more hate in the world where there's much more hate, in fact.\nAnd in our democracy than we're having this conversation, and people can't even agree on who won the last election, you know.\nAnd we humans often point fingers at other humans and say it's their fault, but it's really Moloch in these AI algorithms.\nWe got the algorithms and then Moloch pitted the social media companies against each other\nso nobody could have a less creepy algorithm 'cause then they would lose out on revenue to the other company. - Is there any way to win that battle back\nif we just linger on this one battle that we've lost in terms of social media, is it possible to redesign social media,\nthis very medium in which we use as a civilization to communicate with each other,\nto have these kinds of conversation, to have discourse, to try to figure out how to solve the biggest problems in the world,\nwhether that's nuclear war or the development of AGI. Is is it possible to do social media correctly?\n- I think it's not only possible, but it's necessary. Who are we kidding? That we're gonna be able to solve all these other challenges\nif we can't even have a conversation with each other? It's constructive. The whole idea, the key idea of democracy\nis that you get a bunch of people together and they have a real conversation. The ones you try to foster on this podcast\nwhere you respectfully listen to people you disagree with. And you realize actually, you know, there are some things actually\nsome common ground we have and let's, we both agree, let's not have any nuclear wars, let's not do that, et cetera, et cetera.\nWe're kidding ourselves that thinking we can face off the second contact with ever more powerful AI\nthat's happening now with these large language models if we can't even have a functional conversation in the public space.\nThat's why I started the Improve The News project, improvethenews.org. But I'm an optimist fundamentally,\nin that there is a lot of intrinsic goodness in people.\nAnd that what makes the difference between someone doing good things for humanity\nand bad things is not some sort of fairytale thing, that this person was born with the evil gene\nand this one is born with the good gene. No, I think it's whether we put, whether people find themselves in situations\nthat bring out the best in them or that bring out the worst in them. And I feel we're building an internet\nand a society that brings out the worst.\n- But it doesn't have to be that way. - [Max] No, it does not. - It's possible to create incentives and also create incentives that make money.\nThat both make money and bring out the best in people. - I mean, in the long term, it's not a good investment for anyone, you know,\nto have a nuclear war, for example. And you know, is it a good investment for humanity if we just ultimately replace all humans by machines,\nand then we're so obsolete that eventually, there are no humans left? Well, it depends guess how you do the math,\nBut I would say by any reasonable economic standard, if you look at the future income of humans\nand there aren't any, you know, that's not a good investment. Moreover, like why can't we have\na little bit of pride in our species, damn it? You know, why should we just build another species that gets rid of us?\nIf we were Neanderthals, would we really consider it a smart move\nif we had really advanced biotech to build Homo sapiens? You know, you might say, \"Hey Max, you know,\nyeah, let's build, these Homo sapiens, they're gonna be smarter than us,\nmaybe they can help us, defend us better against predators and help fix up our caves, make them nicer,\nwe'll control 'em undoubtedly, you know?\" So then they build a couple, a little baby girl, little baby boy.\nThey either, and then you have some wise old Neanderthal elder is like,\n\"Hmm, I'm scared that we're opening a Pandora's box here\nand that we're gonna get outsmarted by these\nsuper Neanderthal intelligences, and there won't be any Neanderthals left.\"\nBut then you have a bunch of others in the cave, right? \"You're such a Luddite scaremonger. Of course, they're gonna want to keep us around\n'cause we are their creators, and, you know, the smarter, I think the smarter they get, the nicer they're gonna get,\nthey're gonna leave us. They're gonna want us around and it's gonna be fine,\nand besides look at these babies, they're so cute. Clearly they're totally harmless.\"\nThose babies are exactly GPT-4. It's not, I wanna be clear, it's not GPT-4 that's terrifying.\nIt's that GPT-4 is a baby technology, you know, and Microsoft even had a paper recently out,\ntitled something like, \"Sparkles of AGI.\" Well they were basically saying this is baby AI,\nlike these little Neanderthal babies, and it's gonna grow up. There's gonna be other systems from the same company,\nfrom other companies, they'll be way more powerful, but they're gonna take all the things, ideas from these babies and before we know it,\nwe're gonna be like those last Neanderthals who were pretty disappointed\nwhen they realized that they were getting replaced. - Well, this interesting point you make, which is of programming,\nit's entirely possible that GPT-4 is already the kind of system that can change everything by writing programs.\n- Yeah, it's because it's life 2.0, the systems I'm afraid of are gonna look nothing\nlike a large language model, and they're not, but once it gets, once it or other people figure out a way\nof using this tech to make much better tech, right? It's just constantly replacing its software.\nAnd from everything that we've seen about how these work under the hood, they're like the minimum viable intelligence.\nThey do everything, you know, the dumbest way that still works, sort of. - [Lex] Yeah. - And so they're life 3.0,\nexcept when they replace their software, it's a lot faster than when you decide to learn Swedish.\nPoof. (fingers snapping) And moreover, they think a lot faster than us too. So when, you know,\nwe don't think, have one logical step\nevery nanosecond or few, or so, the way they do, and we can't also just suddenly scale up our hardware\nmassively in the cloud 'cause we're so limited, right? So they are,\nand they are also life, can soon become a little bit more like life 3.0\nin that if they need more hardware, hey, just rent it in the cloud, you know? \"How do you pay for it?\" \"Well, with all the services you provide.\"\n- And what we haven't seen yet, which could change a lot,\nis entire software systems. So right now programming is done sort of in bits and pieces\nas an assistant tool to humans. But I do a lot of programming and with the kind of stuff that GPT-4 is able to do,\nI mean, it's replacing a lot what I'm able to do, right? You still need a human in the loop\nto kind of manage the design of things, manage like, what are the prompts\nthat generate the kind of stuff to do some basic adjustment of the codes, do some debugging,\nbut if it's possible to add on top of GPT-4, kind of a feedback loop\nof self-debugging, improving the code, and then you launch that system onto the wild\non the internet because everything is connected, and have it do things, have it interact with humans and then get that feedback,\nnow you have this giant ecosystem of humans. That's one of the things that\nElon Musk recently sort of tweeted as a case why everyone needs to pay $7 or whatever\nfor Twitter, - [Max] To make sure they're real. - Make sure they're real, we're now going to be living in a world\nwhere the bots are getting smarter, and smarter, and smarter to a degree where,\nyou can't tell the difference between a human and a bot. - [Max] That's right. - And now you can have bots outnumber humans\nby 1 million to one. Which is why he's making a case why you have to pay.\nTo prove you're human, which is one of the only mechanisms to prove, which is depressing. - And yeah,\nI feel we have to remember, as individuals, we should from time to time,\nask ourselves why are we doing what we're doing, right? And as a species, we need to do that too.\nSo if we're building, as you say, machines that are outnumbering us,\nand more and more outsmarting us, and replacing us on the job market, not just for the dangerous and and boring tasks,\nbut also for writing poems and doing art, and things that a lot of people find really meaningful,\nwe gotta ask ourselves, why? Why are we doing this?\nThe answer is Moloch is tricking us into doing it. And it's such a clever trick\nthat even though we see the trick, we still have no choice but to fall for it, right?\nAnd also, thing you said about you using co-pilot AI tools to program faster,\nhow many, what factor faster would you say you code now? Does it go twice as fast? Or,\n- I don't really, because it's such a new tool. - [Max] Yeah. - I don't know if speed is significantly improved,\nbut it feels like I'm a year away from being 5 to 10 times faster.\n- So if that's typical for programmers, then you're already seeing another kind\nof recursive self-improvement, right? Because previously,\nlike a major generation of improvement of the codes would happen on the human R and D time scale.\nAnd now if that's five times shorter, then it's gonna take five times less time than it otherwise would to develop\nthe next level of these tools, and so on. So this is exactly the sort of beginning\nof an intelligence explosion. There can be humans in the loop a lot in the early stages, and then eventually humans are needed less and less\nand the machines can more kind of go alone. But what you said there is just an exact example\nof these sort of things. Another thing which,\nI was kind of lying on my psychiatrist imagining, I'm on a psychiatrist couch here saying, \"Well what are my fears that people would do\nwith AI systems?\" So I mentioned three that I had fears about many years ago, that they would do,\nnamely teach it to code, connect it to the internet, and teach it to manipulate humans.\nA fourth one is building an API, (Lex chuckles) where code can control this super powerful thing, right?\nThat's very unfortunate because one thing that systems like GPT-4 have going for them\nis that they are an oracle in the sense that they just answer questions. There's no robot connected to GPT-4.\nGPT-4 can't go and do stock trading based on its thinking. It is not an agent,\nand an intelligent agent is something that takes in information from the world, processes it,\nto figure out what action to take based on its goals that it has, and then does something back on the world.\nBut once you have an API for, for example, GPT-4, nothing stops Joe Schmoe and a lot of other people from building real agents,\nwhich just keep making calls somewhere in some inner loop somewhere to these powerful oracle systems,\nwhich makes themselves much more powerful. That's another kind of unfortunate development,\nwhich I think we would've been better off delaying. I don't wanna pick on any particular companies,\nI think they're all under a lot of pressure to make money. - [Lex] Yeah. - And again, the reason we're we're calling for this pause\nis to give them all cover to do what they know is the right thing, just slow down a little bit at this point.\nBut everything we've talked about, I hope we'll make it clear to people watching this,\nyou know, why these sort of human-level tools can cause a gradual acceleration.\nYou keep using yesterday's technology to build tomorrow's technology. And when you do that over and over again,\nyou naturally get an explosion. You know, that's the definition of an explosion in science, right?\nIf you have two people, and they fall in love,\nnow you have four people, and then they can make more babies, and now you have eight people,\nand then you have 16, 32, 64, et cetera. We call that a population explosion\nwhere it's just that each, if it's instead free neutrons in a nuclear reaction\nthat if each one can make more than one, then you get an exponential growth in that, we call it a nuclear explosion.\nAll explosions are like that, and an intelligence explosion, it's just exactly the same principle, that some amount of intelligence\ncan make more intelligence than that, and then repeat. You always get exponentials.\n- What's your intuition why it does, you mentioned there's some technical reasons why it doesn't stop at a certain point.\nWhat's your intuition? And do you have any intuition why it might stop? - It's obviously gonna stop\nwhen it bumps up against the laws of physics. There are some things you just can't do no matter how smart you are, right?\n- Allegedly. 'Cause we don't know all the full laws of physics yet, right?\n- Seth Lloyd wrote a really cool paper on the physical limits on computation, for example. If you make it,\nput too much energy into it and the finite space will turn into a black hole, you can't move information around\nfaster than the speed of light, stuff like that. But it's hard to store way more than a modest number of bits per atom, et cetera.\nBut, you know, those limits are just astronomically above, like 30 orders of magnitude above where we are now.\nSo, you know. Bigger difference, bigger jump in intelligence\nthan if you go from ant to a human.\nI think, of course what we want to do is have a controlled thing,\nin a nuclear reactor you put moderators in to make sure exactly it doesn't blow up out of control, right?\nWhen we do, experiments with biology and cells and so on,\nyou know, we also try to make sure it doesn't get out of control. We can do this with AI too.\nThe thing is, we haven't succeeded yet. And Moloch is exactly doing the opposite.\nJust fueling, just egging everybody on, \"Faster, faster, faster, or the other company is gonna catch up with you,\nor the other country is gonna catch up with you.\" We have to want to stop,\nand I don't believe in just asking people to look into their hearts and do the right thing.\nIt's easier for others to say that, but like, if you are in this situation where your company is gonna get screwed\nby other companies that are not stopping, you're putting people in a very hard situation, the right thing to do\nis change the whole incentive structure instead. And this is not an old,\nmaybe I should say one more thing about this, 'cause Moloch has been around as humanity's number one or number two enemy\nsince the beginning of civilization. And we came up with some really cool countermeasures.\nLike first of all, already over 100,000 years ago, evolution realized that it was very unhelpful\nthat people kept killing each other all the time. So it genetically gave us compassion\nand made it so that, like if you get two drunk dudes getting into a pointless bar fight,\nthey might give each other black eyes, but they have a lot of inhibition towards just killing each other.\nThat's a, And similarly, if you find a baby lying on the street, when you go out for your morning jog tomorrow,\nyou're gonna stop and pick it up, right? Even though it maybe make you late for your next podcast.\nSo evolution gave us these genes that make our own egoistic incentives\nmore aligned with what's good for the greater group we're part of, right? And then as we got a bit more sophisticated\nand developed language, we invented gossip, which is also a fantastic anti-Moloch, right?\n'Cause now, it really discourages liars, moochers, cheaters,\nbecause their own incentive now is not to do this because word quickly gets around\nand then suddenly people aren't gonna invite them to their dinners anymore or trust them. And then when we got still more sophisticated\nin bigger societies, you know, we invented the legal system where even strangers who couldn't rely on gossip\nand things like this would treat each other, would have an incentive. Now those guys in the bar fights,\neven if someone is so drunk that he actually wants to kill the other guy,\nhe also has a little thought in the back of his head that, you know, \"Do I really wanna spend the next 10 years\neating like really crappy food in a small room? I'm just gonna chill out,\" you know?\nAnd we similarly have tried to give these incentives to our corporations by having regulation and all sorts of oversight\nso that their incentives are aligned with the greater good. We tried really hard, and the big problem that we're failing now,\nis not that we haven't tried before, but it's just that the tech is growing, is developing much faster\nthan the regulators been able to keep up, right? So regulators, it's kind of comical that the European Union right now\nis doing this AI act, right? And in the beginning they had a little opt-out exception\nthat GPT-4 would be completely excluded from regulation. Brilliant idea.\n- What's the logic behind that? - Some lobbyists pushed successfully for this?\nSo we were actually quite involved with the Future of Life Institute, Mark Brakel, Risto Uuk,\nAnthony Aguirre, and others, you know, we're quite involved with talking to, educating various people involved in this process\nabout these general-purpose AI models coming, and pointing out that they would become the laughing stock\nif they didn't put it in. So the French started pushing for it, it got put in to the draft,\nand it looked like all was good, and then there was a huge counter push from lobbyists.\nYeah, there were more lobbyists in Brussels from tech companies than from oil companies, for example.\nAnd it looked like it might, this was gonna maybe get taken out again. And now GPT-4 happened,\nand I think it's gonna stay in. But this just shows, you know, Moloch can be defeated.\nBut the challenge we're facing is that the tech is generally much faster than what the policymakers are,\nand a lot of the policymakers also don't have a tech background, so it's, you know,\nwe really need to work hard to educate them on what's taking place here.\nSo we're getting this situation where the first kind of, so I define artificial intelligence\njust as non-biological intelligence, right? And by that definition,\na company, a corporation is also an artificial intelligence because the corporation isn't its humans, it's a system.\nIf its CEO decides, if a CEO of a tobacco company decides one morning that she or he doesn't wanna sell cigarettes anymore,\nthey'll just put another CEO in there. It's not enough to align\nthe incentives of individual people or align individual computers' incentives to their owners,\nwhich is what technically, AI safety research is about. You also have to align the incentives of corporations\nwith the greater good. And some corporations have gotten so big and so powerful very quickly that in many cases,\ntheir lobbyists instead align the regulators to what they want rather than the other way round.\nIt's a classic regulatory capture. - Right, is the thing that the slowdown hopes to achieve\nis give enough time to regulators to catch up, or enough time to the companies themselves\nto breathe and understand how to do AI safety correctly? - I think both, but I think that the vision,\nthe path to success I see is first you give a breather actually to the people in these companies,\ntheir leadership who wants to do the right thing, and they all have safety teams and so on, on their companies,\ngive them a chance to get together with the other companies,\nand the outside pressure can also help catalyze that, right? And work out what is it that's,\nwhat are the reasonable safety requirements one should put on future systems before they get rolled out.\nThere are a lot of people also in academia and elsewhere outside of these companies who can be brought into this\nand have a lot of very good ideas. And then I think it's very realistic that within six months,\nyou can get these people coming up, so here's a white paper, here's what we all think it's reasonable.\n"}
{"pod": "Lex Fridman Podcast", "input": "Regulation", "output": "You know, you didn't, just because cars killed a lot of people, you didn't ban cars, but they got together a bunch of people\nand decided, you know, in order to be allowed to sell a car, it has to have a seatbelt in it.\nThey're the analogous things that you can start requiring a future AI systems so that they are safe.\nAnd once this heavy lifting,\nthis intellectual work has been done by experts in the field, which can be done quickly,\nI think it's going to be quite easy to get policymakers to see, yeah, this is a good idea.\nAnd it's, you know, for the companies to fight Moloch,\nthey want, and I believe Sam Altman has explicitly called for this, they want the regulators to actually adopt it\nso that their competition is gonna abide by it too, right? You don't want,\nyou don't want to be enacting all these principles and then you abide by them, and then there's this one little company\nthat doesn't sign onto it and then now they can gradually overtake you.\nThen the companies will get, be able to sleep secure knowing that everybody's playing by the same rules.\n- So do you think it's possible to develop guardrails that keep the systems\nfrom basically damaging irreparably humanity, while still enabling sort of the capitalist-fueled\ncompetition between companies as they develop how to best make money with this AI? You think there's a balancing that's possible?\n- Absolutely, I mean, we've seen that in many other sectors where you've had the free market produce quite good things\nwithout causing particular harm. When the guardrails are there and they work, you know,\ncapitalism is a very good way of optimizing for just getting the same things done more efficiently.\nBut it was good, you know, and like in hindsight, and I never met anyone,\neven on parties way over on the right, in any country who think it was a bad, thinks it was a terrible idea\nto ban child labor, for example. - Yeah, but it seems like this particular technology\nhas gotten so good so fast, become powerful to a degree where you could see\nin the near term, the ability to make a lot of money. - [Max] Yeah. - And to put guardrails, to develop guardrails quickly in that kind of context\nseems to be tricky. It's not similar to cars or child labor,\nit seems like the opportunity to make a lot of money here very quickly is right here before us.\n- So again, there's this cliff. - Yeah, it gets quite scenic, (laughs) - [Max] The closer to the cliff you go,\n- Yeah. - The more money there is, the more gold ingots there are on the ground you can pick up or whatever,\nif you want to drive there very fast, but it's not in anyone's incentive that we go over the cliff and it's not like everybody's in the wrong car.\nAll the cars are connected together with a chain. So if anyone goes over, they'll start dragging the others down too.\nAnd so ultimately it's in the selfish interests also of the people in the companies\nto slow down when you just start seeing the contours of the cliff there in front of you, right?\nAnd the problem is that, even though the people who are building the technology,\nand the CEOs, they really get it, the shareholders and these other market forces,\nthey are people who don't honestly, understand that the cliff is there, they usually don't.\nYou have to get quite into the weeds to really appreciate how powerful this is and how fast. And a lot of people are even still stuck again\nin this idea that in this \"carbon chauvinism\" as I like to call it,\nthat you can only have our level of intelligence in humans, that there's something magical about it.\nWhereas the people in the tech companies who build this stuff, they all realize that intelligence\nis information processing of a certain kind, and it really doesn't matter at all\nwhether the information is processed by carbon atoms in neurons, in brains, or by silicon atoms in some technology we build.\nSo you brought up capitalism earlier, and there are a lot of people who love capitalism and a lot of people who really, really don't.\nAnd it struck me recently, that what's happening with capitalism here\nis exactly analogous to the way in which superintelligence might wipe us out.\nDo you know why I studied economics for my undergrad? Stockholm School of Economics, yay.\n(Lex laughing) - Well, no. No why, tell me. - So I was very interested in how\nyou could use market forces to just get stuff done more efficiently, but give the right incentives to market\nso that it wouldn't do really bad things. So Dylan Hadfield-Menell, who's a professor and colleague of mine at MIT,\nwrote this really interesting paper with some collaborators recently, where they proved mathematically\nthat if you just take one goal that you just optimize for, on and on, and on, indefinitely,\nthat you think is gonna bring you in the right direction, what basically always happens is,\nin the beginning, it will make things better for you, but if you keep going, at some point,\nit's gonna start making things worse for you again. And then gradually, it's gonna make it really, really terrible.\nSo just as a simple, the way I think of the proof is, suppose you want to go from here back to Austin for example,\nand you're like, \"Okay, yeah, let's go south,\" but you put in exactly sort of the right direction.\nJust optimize that, as south as possible. You get closer and closer to Austin,\nbut there's always some little error. So you're not going exactly towards Austin,\nbut you get pretty close, but eventually, you start going away again, and eventually, you're gonna be leaving the solar system.\n- [Lex] (chuckles) Yeah. - And they proved, it's a beautiful mathematical proof, this happens generally,\nand this is very important for AI because, even though Stuart Russell has written a book\nand given a lot of talks on why it's a bad idea to have AI just blindly optimize something,\nthat's what pretty much all our systems do. - [Lex] Yeah. - We have something called the loss function that we're just minimizing, or reward function, we're just maximizing, and,\ncapitalism is exactly like that too. We wanted to get stuff done more efficiently,\nthe people wanted. So introduce the free market.\nThings got done much more efficiently than they did in say, communism, right?\nAnd it got better. But then it just kept optimizing,\nand kept optimizing, and you got every bigger companies, and every more efficient information processing and now also very much powered by IT,\nand eventually a lot of people are beginning to feel, \"Wait, we're kind of optimizing a bit too much. Like why did we just chop down half the rainforest?\"\nYou know, and why did suddenly these regulators get captured\nby lobbyists and so on? It's just the same optimization that's been running for too long.\nIf you have an AI that actually has power over the world and you just give it one goal,\nand just like keep optimizing that, most likely everybody's gonna be like, \"Yay, this is great.\" In the beginning things are getting better,\nbut it's almost impossible to give it exactly the right direction to optimize in.\nAnd then eventually all hell breaks loose, right? Nick Bostrom and others have given examples\nthat sound quite silly, Like what if you just want to like, tell it to cure cancer or something,\nand that's all you tell it, maybe it's gonna decide to take over an entire continent\njust so we can get more supercomputer facilities in there, and figure out how to cure cancer backwards,\nand then you're like, \"Wait, that's not what I wanted,\" right? And the issue with capitalism\nand the issue with runaway AI have kind of merged now, because that Moloch I talked about\nis exactly the capitalist Moloch that, we have built an economy that is optimizing for only one thing.\nProfit, right? And that worked great back when things were very inefficient, and then now it's getting done better,\nand it worked great as long as the companies were small enough that they couldn't capture the regulators.\nBut that's not true anymore, but they keep optimizing, and now they realize that they can,\nthese companies can make even more profit by building ever more powerful AI even if it's reckless,\nbut optimize more and more, and more, and more, and more. So this is Moloch again showing up.\nAnd I just wanna, anyone here who has any concerns about late-stage capitalism having gone a little too far,\nyou should worry about superintelligence 'cause it's the same villain in both cases.\nIt's Moloch. - And optimizing one objective function aggressively,\nblindly is going to take us there. - Yeah, we have to pause from time to time and look into our hearts\nand ask why are we doing this? Is this, am I still going towards Austin, or have I gone too far?\nYou know, maybe we should change direction. - And that is the idea behind the halt for six months.\nWhy six months? That seems like a very short period. Can we just linger and explore different ideas here,\nbecause this feels like a really important moment in human history, where pausing would actually have\na significant positive effect. - We said six months,\nbecause we figured the number one pushback that we're gonna get in the West was like, \"But China?\"\nand everybody knows there's no way that China is gonna catch up with the West on this in six months.\nSo that argument goes off the table and you can forget about geopolitical competition and just focus on the real issue.\nThat's why we put this. - That's really interesting. But you've are already made the case that even for China,\nif you actually wanna take on that argument, China too would not be bothered by a longer halt\nbecause they don't wanna lose control even more than the West doesn't. - That's what I think, yeah.\n- That's a really interesting argument. Like I have to actually really think about that, which, the kind of thing people assume\n"}
{"pod": "Lex Fridman Podcast", "input": "Job automation", "output": "is if you develop an AGI, that OpenAI, if they're the ones that do it, for example,\nthey're going to win. But you're saying no, everybody loses.\n- Yeah, it's gonna get better and better and better, and then kaboom, we all lose. That's what's gonna happen.\n- When lose and win are defined on a metric of basically quality of life for human civilization,\nand for Sam Altman. (laughs) Both. - To be blunt, my personal guess, you know, and people can quibble with this,\nis that we're just gonna, there won't be any humans. That's it, that's what I mean by lose. You know, if you,\nwe can see in history, once you have some species or some group of people who aren't needed anymore,\ndoesn't usually work out so well for them, right? - [Lex] Yeah. - There were a lot of horses that were used for traffic\nin Boston and then the car got invented and most of them got, yeah, well. (laughs) We don't need to go there.\nAnd if you look at\nhumans, you know, right now, why did the labor movement succeed?\nAnd after the Industrial Revolution? Because it was needed.\nEven though we had a lot of Molochs, and there was child labor and so on, you know,\nthe company still needed to have workers, and that's why strikes had power and so on.\nIf we get to the point where most humans aren't needed anymore, I think it's quite naive to think\nthat they're gonna still be treated well. You know, we say that. Yeah, yeah everybody's equal,\nand the government will always protect them. But if you look in practice, groups that are very disenfranchised\nand don't have any actual power usually get screwed.\nAnd now in the beginning, so Industrial Revolution,\nwe automated away muscle work, but that got, worked out pretty well eventually,\nbecause we educated ourselves and started working with our brains instead and got usually more interesting, better paid jobs.\nBut now we're beginning to replace brain work. So we replaced a lot of boring stuff, like we got the pocket calculator,\nso you don't have people adding, multiplying numbers anymore at work. Fine, there were better jobs they could get.\nBut now GPT-4, you know, and the Stable Diffusion and techniques like this,\nthey're really beginning to blow away some jobs that people really loved having.\nThere was a heartbreaking article post just yesterday on social media I saw, about this guy who was doing 3D modeling\nfor gaming and he, and all of a sudden now he got this new software he just sets prompts,\nand he feels this whole job that he loved just lost its meaning, you know? And I asked GPT-4 to rewrite\n\"Twinkle, Twinkle, Little Star\" in the style of Shakespeare, I couldn't have done such a good job.\nIt was really impressive. You've seen a lot of the art coming out here, right? So I'm all for automating away the dangerous jobs\nand the boring jobs. But I think you hear a lot, some arguments which are too glib.\nSometimes people say, \"Well that's all that's gonna happen. We're getting rid of the boring, tedious, dangerous jobs,\"\nit's just not true. There are a lot of really interesting jobs that are being taken away now. Journalism is gonna get crushed,\ncoding is gonna get crushed. I predict the job market for programmers,\nthe salaries are gonna start dropping. You know, if you said you can code five times faster,\nyou know, then you need five times fewer programmers, maybe there will be more output also,\nbut then you'll still end up using fewer, needing fewer programmers than today. And I love coding,\nyou know, I think it's super cool. So we need to stop and ask ourselves\nwhy again are we doing this as humans, right? I feel that AI should be built by humanity for humanity,\nand let's not forget that. It shouldn't be by Moloch for Moloch, or what it really is now is kind of by humanity for Moloch,\nwhich doesn't make any sense. It's for us that we're doing it. And it would make a lot more sense\nif we build, develop, figure out gradually, safely how to make all this tech, and then we think about what are the kind of jobs\nthat people really don't want to have, you know, automate them all away. And then we ask what are the jobs\nthat people really find meaning in, like maybe taking care of children in the daycare center,\nmaybe doing art, et cetera, et cetera. And even if it were possible to automate that away,\nwe don't need to do that, right? We built these machines. - Well it's possible that we redefine\nor rediscover what are the jobs that give us meaning. So for me, the thing, it is really sad.\nLike I, (chuckles) half the time I'm excited, half the time I'm crying\nas I'm generating code because I kind of love programming.\nIt's the act of creation, You have an idea, you design it, and then you bring it to life,\nand it does something. Especially if there's some intelligence to it, it doesn't even have to have intelligence.\nPrinting \"Hello world\" on screen. You made a little machine and it it comes to life.\n- [Max] Yeah. - And there's a bunch of tricks you learn along the way 'cause you've been doing it for many, many years.\nAnd then for to see AI be able to generate all the tricks you thought were special.\nI don't know, it's very, it's scary, it's almost painful.\nLike a loss of innocence maybe, like maybe when I was younger,\nI remember before I learned that sugar is bad for you, you should be on a diet. I remember I enjoyed candy deeply,\nin a way I just can't anymore, that I know is bad for me. I enjoyed it unapologetically, fully, just intensely.\nAnd I lost that. Now, I feel like a little bit of that is lost,\nor being lost with programming, similar as it is for the 3D modeler\nno longer being able to really enjoy the art of modeling 3D things for gaming.\nI don't know what to make sense of that. Maybe I would rediscover that the true magic of what it means to be humans is connecting with other humans,\nto have conversations like this, I don't know, to have sex,\nto eat food, to really intensify the value from conscious experiences,\nversus like creating other stuff. - You're pitching the rebranding again from Homo sapiens to Homo sentiens,\nthe meaningful experiences. And just to a inject some optimism in this here, so we don't sound like it was a gloomers.\nYou know, we can totally have our cake and eat it. You hear a lot of totally bullshit claims that we can't afford having more teachers,\nhave to cut the number of nurses, you know, that's just nonsense, obviously.\nWith anything even quite far short of AGI, we can dramatically improve, grow the GDP,\nand produce this wealth of goods and services. It's very easy to create a world\nwhere everybody is better off than today. Including the richest people can be better off as well, right?\nIt's not a zero sum game, you know, technology. Again, you can have two countries,\nlike Sweden and Denmark had all these ridiculous wars century after century,\nand sometimes that Sweden got a little better off 'cause it got a little bigger, and then Denmark got a little bit better off\n'cause Sweden got a little bit smaller, but then technology came along and we both got just dramatically wealthier\nwithout taking away from anyone else, so it was just a total win for everyone. And AI can do that on steroids.\nif you can build safe AGI, if you can build superintelligence,\nbasically all the limitations that cause harm today can be completely eliminated. Right?\nIt's a wonderful possibility. And this is not sci-fi, this is something which is clearly possible\naccording to laws of physics, And we can talk about ways of making it safe also,\nbut unfortunately that'll only happen if we steer in that direction,\nthat's absolutely not the default outcome. That's why income inequality keeps going up.\nThat's why the life expectancy in the US has been going down now, I think it's four years in a row.\nI just read a heartbreaking study from CDC about how something like 1/3 of all the teenage girls\nin the US have been thinking about suicide. You know, like those are steps\nin totally the wrong direction and it's important to keep our eyes on the prize here\nthat we can, we have the power now for the first time\n"}
{"pod": "Lex Fridman Podcast", "input": "Elon Musk", "output": "in the history of our species to harness artificial intelligence, to help us really flourish,\nand help bring out the best in our humanity rather than the worst of it.\nTo help us have really fulfilling experiences that feel truly meaningful.\nAnd you and I shouldn't sit here and dictate the future generations what they will be, let them figure it out. But let's give them a chance to live,\nand not foreclose all these possibilities for them, by just messing things up, right? - Well for that, we'll have to solve the AI safety problem.\nIt would be nice if we can linger on exploring that a little bit. So one interesting way to enter that discussion is,\nyou tweeted, and Elon replied, you tweeted, \"Let's not just focus on whether GPT-4\nwill do more harm or good on the job market, but also whether it's coding skills will hasten the arrival of superintelligence.\"\nThat's something we've been talking about, right? So Elon proposed one thing in the reply saying, \"Maximum truth-seeking is my best guess for AI safety.\"\nCan you maybe steel me on the case for,\nthis objective function of truth and maybe make an argument against it in general, what are your different ideas\nto start approaching the solution to AI safety? - I didn't see that reply actually. - [Lex] Oh, interesting.\n- But I really resonate with it because,\nAI is not evil. It caused people around the world to hate each other much more,\nbut that's because we made it in a certain way. It's a tool, we can use it for great things and bad things,\nand we could just as well have AI systems, and this is part of my vision for success here.\nTruth-seeking AI that really brings us together again, you know, why do people hate each other so much\nbetween countries and within countries is because they each have totally different\nversions of the truth, right? If they all had the same truth\nthat they trusted for good reason 'cause they could check it and verify it, and not have to believe in some self-proclaimed authority, right?\nThey wouldn't be as nearly as much hate. There'd be a lot more understanding instead, and this is,\nI think something AI can help enormously with. For example, a little baby step in this direction\nis this website called Metaculus where people bet and make predictions not for money,\nbut just for their own reputation. And it's kind of funny actually, you treat the humans like you treat AI,\nas you have a loss function where they get penalized if they're super confident on something\nand then the opposite happens. - [Lex] Yeah. - Whereas if you're kind of humble, and then you're like,\n\"I think it's 51% chance this is gonna happen,\" and then the other happens, you don't get penalized much,\nand what you can see is that some people are much better at predicting than others. They've earned your trust, right?\nOne project that I'm working on right now is an outgrowth of Improve The News foundation together with the Metaculus folks is,\nseeing if we can really scale this up a lot with more powerful AI. 'Cause I would love it,\nI would love for there to be like a really powerful truth-seeking system where,\nthat is trustworthy because it keeps being right about stuff.\nAnd people come to it and maybe look at its latest trust ranking\nof different pundits and newspapers, et cetera. If they want to know why some someone got a low score,\nthey can click on it, and see all the predictions that they actually made and how they turned out, you know,\nthis is how we do it in science. You trust scientists like Einstein who said something everybody thought was bullshit,\nand turned out to be right, he get a lot of trust points, and he did it multiple times, even.\nI think AI has the power to really heal a lot of the rifts we're seeing by creating a trust system.\nIt has to get away from this idea today with some fact checking site, which might themselves have an agenda\nand you just trust it because of its reputation, you want to have,\nso these sort of systems, they earn in their trust and they're completely transparent. This I think would actually help a lot\nthat can, I think, help heal the very dysfunctional conversation that humanity has about how it's gonna deal\nwith all its biggest challenges in in the world today.\nAnd then on the technical side, you know, another common sort of gloom comment\nI get from people are saying, \"We're just screwed, there's no hope.\" Is well, things like GPT-4 are way too complicated\nfor a human to ever understand, and prove that they can be trustworthy. They're forgetting that AI can help us\nprove that things work, right? - [Lex] Yeah. - And there's this very fundamental fact that in math,\nit's much harder to come up with a proof than it is to verify that the proof is correct.\nYou can actually write a little proof-checking code, it's quite short, but you can as a human, understand,\nand then it can check the most monstrously long proof ever generated even by your computer, and say, \"Yeah, this is valid.\"\nSo right now, we have,\nthis approach with virus-checking software that it looks to see if there's something, and if you should not trust it,\nand if it can prove to itself that you should not trust that code, it warns you, right?\nWhat if you flip this around, and this is an idea I should give credit to Steve Omohundro for,\nso that it will only run the code if it can prove, instead of not running it if it can prove that it's not trustworthy,\nit will only run it if it can prove that it's trustworthy. So it asks the code, \"Prove to me that you're gonna do what you say you're gonna do,\"\nand it gives you this proof, and you have a little proof that you can check it.\nNow you can actually trust an AI that's much more intelligent than you are, right?\nBecause you, is its problem to come up with this proof that you could never have found, that you should trust it.\n- So this is the interesting point. I agree with you, but this is where Eliezer Yudkowsky\nmight disagree with you. His claim, not with you, but with this idea.\nhis claim is superintelligent AI would be able to know how to lie to you with such a proof.\n- I have to lie to you and give me a proof that I'm gonna think is correct? - [Lex] Yeah. - But it's not me it's lying to you.\nThat's to trick my proof checker, which is a piece of code. - So his general idea is a superintelligent system\ncan lie to a dumber proof checker. So you're going to have,\nas a system becomes more and more intelligent, there's going to be a threshold where a superintelligent system\nwill be able to effectively lie to a slightly dumber AGI system. Like there's a,\nlike he really focuses on this weak AGI to strong AGI jump, where the strong AGI can make all the weak AGIs think\nthat it's just one of them, but it's no longer that. And that leap is when it runs away.\n- Yeah, I don't buy that argument. I think no matter how superintelligent an AI is,\nit's never gonna be able to prove to me that there are only finitely many primes, for example. (Lex chuckling)\nIt just can't. And it can try to snow me by making up all sorts of new weird rules of deduction,\nand say, \"Trust me, you know, the way your proof checker work is too limited, and we have this new hyper math and it's true.\"\nBut then I would just take the attitude, okay, I'm gonna forfeit some of these, the supposedly super cool technologies,\nI'm only gonna go with the ones that I can prove in my own trusted proof checker. Then I think it's fine.\nThere's still, of course, this is not something anyone has successfully implemented at this point,\nbut I think it, I just give it as an example of hope, we don't have to do all the work ourselves, right?\nThis is exactly the sort of very boring and tedious task that is perfect to outsource to an AI.\nAnd this is a way in which less powerful and less intelligent agents like us can actually continue to control\nand trust more powerful ones. - So build AGI systems that help us defend against other AGI systems.\n- Well for starters, begin with a simple problem of just making sure that the system that you own\nor that's supposed to be loyal to you has to prove to itself that it's always gonna do\nthe things that you actually want it to do, right? And if it can't prove it, maybe it's still gonna do it, but you won't run it.\nSo you just forfeit some aspects of all the cool things AI can do. I bet your dollars to donuts,\nit can still do some incredibly cool stuff for you. - [Lex] Yeah. - There are other things too, that we shouldn't sweep under the rug.\nLike not every human agrees on exactly what direction we should go with humanity, right?\n- Yes. - And you've talked a lot about geopolitical things\non your podcast to this effect, you know, but, I think that shouldn't distract us from the fact\nthat there are actually a lot of things that everybody in the world virtually agrees on.\nThat \"Hey, you know, like having a no humans on the planet in a near future,\nnah, let's not do that\" right? You looked at something like the United Nations Sustainable Development Goals.\nSome of 'em were quite a ambitious, and basically all the countries agree,\nUS, China, Russia, Ukraine, they all agree. So instead of quibbling about the little things\nthat we don't agree on, let's start with the things we do agree on and get them done.\nInstead of being so distracted by all these things we disagree on, that Moloch wins because frankly,\nMoloch going wild now, it feels like a war on life playing out in front of our eyes,\nif you just look at it from space, you know, we're on this planet, beautiful, vibrant ecosystem,\nnow we start chopping down big parts of it, even though nobody, most people thought that was a bad idea.\nOh, we start doing ocean acidification, wiping out all sorts of species,\noh, now we have all these close calls, we almost had a nuclear war, and we're replacing more and more of the biosphere\nwith non-living things. We're also replacing in our social lives,\na lot of the things which were so valuable to humanity, a lot of social interactions now are replaced by people staring into their rectangles, right?\nAnd I'm not a psychologist, I'm out of my depth here, but I suspect that part of the reason why teen suicide\nand suicide in general in the US that record-breaking level is actually caused by,\nagain, AI technologies and social media making people spend less time\nwith actually just human interaction. We've all seen a bunch of good-looking people\nin restaurants staring into the rectangles instead of looking into each other's eyes, right?\nSo that's also part of the war in life that we are replacing so many\nreally life-affirming things by technology. We're putting technology between us,\nthat the technology that was supposed to connect us is actually distancing us ourselves from each other.\nAnd then we are giving ever more power to things which are not alive. These large corporations are not living things, right?\nThey're just maximizing profit. I wanna win the war on life.\nI think we humans, together with all our fellow living things on this planet\nwill be better off if we can remain in control over the non-living things and make sure\nthat they work for us. I really think it can be done. - Can you just linger on this maybe high level\nof philosophical disagreement with Eliezer Yudkowsky,\nin the hope you're stating. So he is very sure,\nhe puts a very high probability, very close to one, depending on the day he puts it at one,\nthat AI is going to kill humans. That there's just,\nhe does not see a trajectory, which it doesn't end up with that conclusion.\nWhat trajectory do you see that doesn't end up there? And maybe can you see the point he's making,\nand can you also see a way out?\n- First of all, I tremendously respect Eliezer Yudkowsky and his thinking.\nSecond, I do share his view that there's a pretty large chance that we're not gonna make it as humans.\nThere won't be any humans on the planet, in a not-too-distant future, and that makes me very sad.\nYou know, we just had a little baby and I keep asking myself, you know, is,\nhow old is he even gonna get, you know? And I ask myself,\nit feels, I said to my wife recently, it feels a little bit like I was just diagnosed with some sort of cancer,\nwhich has some, you know, risk of dying from and some risk of surviving, you know.\nExcept this is a kind of cancer which can kill all of humanity. So I completely take seriously his concerns,\nI think, but absolutely, I don't think it's hopeless. I think there is,\nfirst of all a lot of momentum now for the first time actually, since the many, many years that have passed\nsince I and many others started warning about this, I feel most people are getting it now.\nI was just talking to this guy in the gas station\nnear our house the other day. And he's like, \"I think we're getting replaced,\nand then I think...\" So that's positive that they're finally, we're finally seeing this reaction,\nwhich is the first step towards solving the problem. Second, I really think that this vision\nof only running AIs, if the stakes are really high,\nthey can prove to us that they're safe. It's really just virus checking in reverse again, I think it's scientifically doable.\nI don't think it's hopeless, we might have to forfeit some of the technology that we could get\nif we were putting blind faith in our AIs, but we're still gonna get amazing stuff. - Do you envision a process with a proof checker?\nLike something like GPT-4, GPT-5, will go through a process of rigorous interrogation?\n- No I think it's hopeless, That's like trying to proof-verify spaghetti. - [Lex] (laughs) Okay.\n- What I think, the vision I have for success is instead that,\nyou know, just like we human beings were able to look at our brains and distill out the key knowledge.\nGalileo, when his dad threw him an apple when he was a kid, he was able to catch it 'cause his brain could,\nin his funny spaghetti kind of way, you know, predict how parabolas are gonna move, his Kahneman System 1, right?\nBut then he got older and he's like, \"Wait, this is a parabola. It's y equals x squared.\"\nI can distill this knowledge out and today you can easily program it into a computer and it can simulate not just that,\nbut how to get to Mars and so on, right? I envision a similar process where we use the amazing learning power of neural networks\nto discover the knowledge in the first place, but we don't stop with a black box and use that.\nWe then do a second round of AI where we use automated systems to extract out the knowledge, and see what is it,\nwhat are the insights it's had, okay? And then we put that knowledge\ninto a completely different kind of architecture, or programming language or whatever,\nthat's made in a way that it can be both really efficient, and also is more amenable to very formal verification.\nThat's my vision. I'm not sitting here saying, I'm confident 100% sure that it's gonna work, you know.\nBut I don't think it's a chance, it's certainly not zero either, and it will certainly be possible to do for a lot of really cool AI applications\nthat we're not using now. So we can have a lot of the fun that we're excited about if we do this.\nWe are gonna need a little bit of time. And that's why it's good to pause\nand put in place requirements.\nOne more thing also, I think, you know, someone might think, \"Well, 0% chance we're gonna survive,\nlet's just give up,\" right? That's very dangerous,\nbecause there's no more guaranteed way to fail than to convince yourself that it's impossible\nand not try, you know, when you study history and military history,\nthe first thing you learn is that, that's how you do psychological warfare.\nYou persuade the other side that it's hopeless so they don't even fight. And then of course you win, right?\nLet's not do this psychological warfare on ourselves and say there's 100% percent probability\nwe're all screwed anyway. And sadly, I do get that a little bit,\nsometimes from actually some young people who are like so convinced that we're all screwed, that they're like,\n\"I'm just gonna play computer games and do drugs, 'cause we're screwed anyway, right?\"\nIt's important to keep the hope alive because it actually has a causal impact, and makes it more likely that we're gonna succeed.\n- It seems like the people that actually build solutions to the problem, seemingly impossible to solve problems\nare the ones that believe. - [Max] Yeah. - They're the ones who are the optimists. And it's like,\nit seems like there's some fundamental law to the universe where \"Fake it till you make it,\" kind of works.\nLike believe it's possible and it becomes possible. - Yeah, was it Henry Ford who said that,\nif you tell yourself that it's impossible, it is. So let's not make that mistake.\nAnd this is a big mistake society is making, I think all in all, everybody's so gloomy, and the media also very biased towards\nif it bleeds, it leads, and gloom and doom, right? So most,\nvisions of the future we have are dystopian, which really demotivates people.\nWe wanna really, really, really focus on the upside also to give people the willingness to fight for it.\nAnd for AI, you and I mostly talked about gloom here again,\nbut let's not forget that, you know, we have probably both lost someone\nwe really cared about to some disease that we were told was incurable. Well it's not,\nthere's no law of physics saying we had to die of that cancer or whatever. Of course, you can cure it.\nAnd there's so many other things that we, with our human intelligence have also failed to solve on this planet,\nwhich AI could also very much help us with, right? So if we can get this right, and just be a little more chill,\nand slow down a little bit so we get it right. It's mind-blowing how awesome our future can be, right?\nWe talked a lot about stuff on Earth, it can be great, but even if you really get ambitious\nand look up into the skies, right? There's no reason we have to be stuck on this planet for the rest of the remaining,\nfor billions of years to come. We totally understand now that laws of physics\nlet life spread out into space to other solar systems, to other galaxies, and flourish for billions and billions of years.\nAnd this to me is a very, very hopeful vision that really motivates me to fight.\nAnd coming back to it in the end, it's something you talked about again, you know, the struggle, how the human struggle is one of the things\nthat's also really gives meaning to our lives. If there's ever been an epic struggle, this is it.\nAnd isn't it even more epic if you're the underdog? If most people are telling you this is gonna fail,\nit's impossible, right? And you persist and you succeed, right?\nAnd that's what we can do together as a species on this one. A lot of pundits are ready to count this out.\n- Both in the battle to keep AI safe and becoming a multi-planetary species. - Yeah, and they're the same challenge.\nIf we can keep AI safe, that's how we're gonna get multi-planetary very efficiently.\n- I have some sort of technical questions about how to get it right. So one idea that I'm not even sure\nwhat the right answer is to is, should systems like GPT-4 be open sourced\n"}
{"pod": "Lex Fridman Podcast", "input": "Open source", "output": "in whole or in part? Can you see the case for either?\n- I think the answer right now is no. I think the answer early on was yes.\nSo we could bring in all the wonderful great thought process of everybody on this,\nbut asking should we open source GPT-4 now is just the same as if you say, should we open source\nhow to build really small nuclear weapons? Should we open source how to make bioweapons?\nShould we open source how to make a new virus that kills 90% of everybody who gets it?\nOf course we shouldn't. - So it's already that powerful. It's already that powerful that we have to respect\nthe power of the systems we've built. - The knowledge that you get\nfrom open sourcing everything we do now might very well be powerful enough that people looking at that\ncan use it to build the things that are really threatening. Again, let's get it, remember OpenAI's GPT-4 is a baby AI,\nsort of baby, proto, almost little bit AGI, according to what Microsoft's recent paper said, right?\nIt's not that that we're scared of, what we're scared about is people taking that who are,\nwho might be a lot less responsible than the company that made it, right? And just go into town with it.\nThat's why we wanna, it's an information hazard.\nThere are many things which, yeah, are not open-sourced right now in society for very good reason.\nLike how do you make certain kind of very powerful toxins\nout of stuff you can buy in Home Depot? We don't open source those things for a reason,\nand this is really no different. - [Lex] So- - And I'm saying that,\nI have to say it feels a bit weird, in a way, a bit weird to say it because MIT is like the cradle of the open source movement.\nAnd I love open source in general, power to the people, I say,\nbut there's always gonna be some stuff that you don't open source, and you know, it's just like you don't open source,\nso we have a three-month old baby, right? When he gets a little bit older, we're not gonna open source to him all the most dangerous things he can do in the house, right?\n- But it does, it's a weird feeling because this is one of the first moments in history\nwhere there's a strong case to be made not to open source software.\nThis is when the software has become too dangerous. - Yeah, but it's not the first time\nthat we didn't wanna open source a technology. - Technology, yeah.\nIs there something to be said about how to get the release of such systems right, like GPT-4 and GPT-5?\nSo OpenAI went through a pretty rigorous effort for several months, you could say it could be longer,\nbut nevertheless it's longer than you would've expected of trying to test the system to see like what are the ways goes wrong\nto make it very difficult, well, somewhat difficult for people to ask things,\n\"How do I make a bomb for $1?\" Or \"How do I say I hate a certain group on Twitter\nin a way that doesn't get me blocked from Twitter, banned from Twitter.\" Those kinds of questions.\nSo you basically use the system to do harm. - [Max] Yeah.\n- Is there something you could say about ideas you have that's just, on looking having thought about this problem of AI safety,\nhow to release a system, how to test such systems when you have them inside the company.\n- Yeah, so a lot of people say that the two biggest risks from large language models are,\nit's spreading disinformation, harmful information of various types,\nand second being used for offensive cyberweapon.\nI think those are not the two greatest threats. They're very serious threats, and it's wonderful that people are trying to mitigate them.\nA much bigger elephant in the room is how this is gonna disrupt our economy in a huge way, obviously, and maybe take away a lot of the most meaningful jobs.\nAnd an even bigger one is the one we spent so much time talking about here that this\nbecomes the bootloader for the more powerful AI. - Write code, connected to the internet, manipulate humans.\n- Yeah, and before we know it, we have something else, which is not at all a large language model that looks nothing like it,\nbut which is way more intelligent and capable and has goals. And that's the elephant in the room.\nAnd obviously no matter how hard any of these companies have tried,\nthat's not something that's easy for them to verify with large language models. And the only way to really lower that risk a lot\nwould be to not let, for example, never let it read any code, not train on that,\nand not put it into an API, and to not Give it access to so much information\nabout how to manipulate humans, so, but that doesn't mean you still can't make\na ton of money on them, you know? We're gonna just watch now this coming year, right?\nMicrosoft is rolling out the new Office Suite where you go into Microsoft Word,\nand give it a prompt, and it write the whole text for you and then you edit it and then you're like,\n\"Oh, gimme a PowerPoint version of this,\" and it makes it. \"And now take the spreadsheet and blah blah.\"\nAnd you know, all of those things I think are, you can debate the economic impact of it\nand whether society is prepared to deal with this disruption. But those are not the things which,\nthat's not the elephant of the room that keeps me awake at night for wiping out humanity.\nAnd I think that's the biggest misunderstanding we have. A lot of people think that we're scared of\nlike automatic spreadsheets. That's not the case. That's not what Eliezer was freaked out about either.\n- Is there in terms of the actual mechanism of how AI might kill all humans.\n"}
{"pod": "Lex Fridman Podcast", "input": "How AI may kill all humans", "output": "So something you've been outspoken about, you've talked about a lot. Is it autonomous weapon systems?\nSo the use of AI in war, is that one of the things that's still\nyou carry concern for as these systems become more and more powerful? - I carry a concern for it, not that all humans are gonna get killed by slaughter bots,\nbut rather just as express route into an Orwellian dystopia\nwhere it becomes much easier for very few to kill very many, and therefore it becomes very easy for very few to dominate very many, right?\nAI, if you wanna know how AI could kill all people, just ask yourself, we humans have driven a lot of species extinct.\nHow do we do it? You know, we were smarter than them,\nusually we didn't do it even systematically by going around one-on-one, one after the other and stepping on them,\nor shooting them or anything like that. We just like chopped down their habitat 'cause we needed it for something else.\nIn some cases we did it by putting more carbon dioxide in the atmosphere because of some reason\nthat those animals didn't even understand, and now they're gone, right? So if you're an AI,\nand you just wanna figure something out, then you decide, you know, we just really need this space here\nto build more compute facilities. You know, if that's the only goal it has, you know,\nwe are just the sort of accidental roadkill along the way. And you could totally imagine, \"Yeah, maybe this oxygen is kind of annoying\n'cause it cause more corrosion, so let's get rid of the oxygen.\" And good luck surviving after that.\nYou know, I'm not particularly concerned that they would want to kill us just because that would be like a goal in itself.\nyou know, when we.. we've driven a number of the elephant species extinct. Right?\nIt wasn't 'cause we didn't like elephants.\nThe basic problem is you just don't want to give, you don't wanna cede control over your planet\nto some other more intelligent entity that doesn't share your goals. It's that simple, and so,\nwhich brings us to another key challenge which AI safety research has been grappling with for a long time.\nLike, how do you make AI, first of all, understand our goals\nand then adopt our goals, and then retain them as they get smarter, right?\nAll three of those are really hard, right? Like a human child,\nfirst, they're just not smart enough to understand our goals.\nThey can't even talk. And then eventually they're teenagers, and understand our goals just fine,\nbut they don't share. (laughs) - [Lex] Yeah. - But there is fortunately a magic phase in the middle\nwhere they're smart enough to understand our goals and malleable enough that we can hopefully, with good parenting, teach them right from wrong\nand instill good goals in them, right? So those are all tough challenges with computers.\nAnd then, you know, even if you teach your kids good goals when they're little, they might outgrow them too, and that's a challenge for machines to keep improving.\nSo these are a lot of hard, hard challenges we're up for, but I don't think any of them are insurmountable.\nThe fundamental reason why Eliezer looked so depressed when I last saw him was because he felt there just wasn't enough time.\n- Oh, that not that it was unsolvable, - Correct. - There's just not enough time. - He was hoping that humanity\nwas gonna take this threat more seriously, so we would have more time, and now we don't have more time.\nThat's why the open letter is calling for more time.\n- But even with time, the AI alignment problem, it seems to be really difficult.\n- Oh yeah. But it's also the most worthy problem,\nthe most important problem for humanity to ever solve. Because if we solve that one, Lex,\nthat aligned AI can help us solve all the other problems. - 'Cause it seems like it has to have constant humility about its goal,\nconstantly question the goal. Because as you optimize towards a particular goal\nand you start to achieve it, that's when you have the unintended consequences, all the things you mentioned about. So how do you enforce and code a constant humility\nas your ability become better, and better, and better, and better? - Professor Stuart Russell at Berkeley is also one of the driving forces behind this letter,\nhe has a whole research program about this.\nI think of it as a AI humility, exactly. Although he calls it inverse reinforcement learning\nand other nerdy terms. But it's about exactly that. Instead of telling the AI, \"Here's this goal, go optimize the the bejesus out of it.\"\nYou tell it, \"Okay, do what I want you to do, but I'm not gonna tell you right now what it is\nI want you to do. You need to figure it out.\" So then you give the incentives to be very humble and keep asking you questions along the way.\nIs this what you really meant? Is this what you wanted? And oh the other thing I tried didn't work, and seemed like it didn't work out right.\nShould I try it differently? What's nice about this is it's not just philosophical mumbo-jumbo,\nit's theorems and technical work that with more time, I think it can make a lot of progress, and there are a lot of brilliant people now\nworking on AI safety. We just need to give em a bit more time. - But also not that many relative to skill of the prompt.\n- No, exactly. There should be at least this, just like every university worth its name\nhas some cancer research going on in its biology department, right? Every university that does computer science\nshould have a real effort in this area and it's nowhere near that.\nThis is something I hope is changing now, thanks to the GPT-4, right? So I think if there's a silver lining\nto what's happening here, even though I think many people would wish it would've been rolled out more carefully,\nis that this might be the wake-up call that humanity needed,\nto really stop fantasizing about this being a hundred years off\nand stop fantasizing about this being completely controllable and predictable because it's so obvious,\nit's not predictable, you know? why is it that,\nI think it was ChatGPT that tried to persuade a journalist\nto divorce his wife, you know. It was not 'cause the engineers had built it, was like, (laughs mischievously)\n\"Let's put this in here, and screw a little bit with people.\" They hadn't predicted it at all.\nThey built the giant black box trained to predict the next word and got all these emergent properties,\nand oops, it did this, you know.\nI think this is a very powerful wake-up call and anyone watching this who's not scared,\nI would encourage them to just play a bit more with these tools. They're out there now like GPT-4 and,\nso wake-up call is first step, once you've woken up, then gotta slow down a little bit the risky stuff\nto give a chance to everyone that has woken up to catch up with this on the safety front.\n- You know what's interesting is, you know, MIT, that's computer science,\nbut in general, but let's just even say computer science curriculum. How does the computer science curriculum change now?\nYou mentioned programming. - [Max] Yeah. - Like why would you be,\nwhen I was coming up, programming as a prestigious position. Like why would you be dedicating crazy amounts of time\nto become an excellent programmer? Like the nature of programming is fundamentally changing. - The nature of our entire education system\nis completely turned on its head. - Has anyone been able to like, load that in,\nand like think, because it's really turning, - I mean some English professors, some English teachers are beginning to really freak out now.\nRight? Like they give an essay assignment and they get back all this fantastic prose, like this is style of Hemmingway,\nand then they realize they have to completely rethink and even, you know, just like we stopped teaching,\nwriting a script, is that what you say in English? - [Lex] Yeah, handwritten, yeah. - Yeah, when everybody started typing,\nyou know, like so much of what we teach our kids today.\n- Yeah, I mean that's, everything is changing and it is changing very quickly.\nAnd so much of us understanding how to deal with the big problems of the world is through the education system.\nAnd if the education system is being turned on its head, then what's next? It feels like having these kinds of conversations\nis essential to trying to figure it out. And everything's happening so rapidly. I don't think there's even,\nyou're speaking of safety, the broad AI safety defined, I don't think most universities have courses on AI safety.\nIt's like a philosophy seminar. - Yeah, and like I'm an educator myself, so it pains me to say this,\nbut I feel our education right now is completely obsoleted by what's happening.\nYou know, you put a kid into first grade, and then you are envisioning like, and then they're gonna come out\nof high school 12 years later, and you've already pre-planned now what they're gonna learn,\nwhen you're not even sure if there's gonna be any world left to come out to, like clearly you need to have a much more\nopportunistic education system that keeps adapting itself very rapidly as society re-adapts.\nThe skills that were really useful when the curriculum was written, I mean how many of those skills\nare gonna get you a job in 12 years? I mean, seriously. - If we just linger on the GPT-4 system a little bit,\n"}
{"pod": "Lex Fridman Podcast", "input": "Consciousness", "output": "you kind of hinted at it, especially talking about the importance of consciousness in in the human mind with Homo sentiens.\nDo you think GPT-4 is conscious? - Ah, I love this question.\nSo let's define consciousness first because in my experience, like 90% of all arguments about consciousness,\n(Lex chuckles) boil down to the two people arguing having totally different definitions of what it is, then they're just shouting past each other.\nI define consciousness as subjective experience.\nRight now I'm experiencing colors and sounds, and emotions, you know, but does a self-driving car experience anything?\nThat's the question about whether it's conscious or not, right? Other people think\nyou should define consciousness differently, fine by me, but then maybe use a different word for it.\nOr they can, I'm gonna use consciousness for this at least, so,\nbut if people hate the, yeah. So is GPT-4 conscious? Does GPT-4 have subjective experience?\nShort answer, I don't know, because we still don't know what it is that gives this wonderful subjective experience\nthat is kind of the meaning of our life, right? Because meaning itself, the feeling of meaning is a subjective experience.\nJoy is a subjective experience, love is a subjective experience, we don't know what it is,\nI've written some papers about this, a lot of people have.\nGiulio Tononi, a professor, has stuck his neck out the farthest\nand written down actually very bold mathematical conjecture for what's the essence of conscious information processing.\nHe might be wrong, he might be right, but we should test it. He postulates that the consciousness\nhas to do with loops in the information processing. So our brain has loops.\nInformation can go round and round, in computer science nerd-speak, you call it a recurrent neural network\nwhere some of the output gets fed back in again. And with his\nmathematical formulism, if it's a feed-forward neural network where information only goes in one direction,\nlike from your eye retina into the back of your brain for example, that's not conscious. So he would predict that your retina itself\nisn't conscious of anything, or a video camera. Now the interesting thing about GPT-4\nis it's also just a one-way flow of information. So if Tononi is right, then GPT-4 is a very intelligent zombie,\nthat can do all this smart stuff but isn't experiencing anything. And this is both a relief\nif it's true, and that you don't have to feel guilty about turning off GPT-4 and wiping its memory\nwhenever a new user comes along. I wouldn't like if someone did that to me, and neuralyze me like in \"Men In Black.\"\nBut it's also creepy, that you can have a very high intelligence\nperhaps that is not conscious, because if we get replaced by machines,\nand while it's sad enough that humanity isn't here anymore, 'cause I kind of like humanity,\nbut at least if the machines were conscious, I could be like, \"Well, but they are our descendants and maybe they have our values and they are our children.\"\nBut if Tononi is right and these are all transformers that are,\nnot in the sense of Hollywood, but in the sense of these one-way direction neural networks,\nso they're all the zombies, that's the ultimate zombie apocalypse now. We have this universe that goes on with great construction projects and stuff,\nbut there's no one experiencing anything. That would be like the ultimate depressing future.\nSo I actually think, as we move forward with building more advanced AI,\nwe should do more research on figuring out what kind of information processing actually it has experienced, because I think that's what it's all about.\nAnd I completely don't buy the dismissal that some people will say,\n\"Well this is all bullshit because consciousness equals intelligence.\" - [Lex] Right. - That's obviously not true.\nYou can have a lot of conscious experience when you're not really accomplishing any goals at all.\nYou're just reflecting on something, and you can sometimes,\ndoing things that require intelligence probably without being conscious. - But I also worry that we humans,\nwill discriminate against AI systems that clearly exhibit consciousness. That we will not allow AI systems to have consciousness.\nWe'll come up with theories about measuring consciousness that will say this is a lesser being,\nand this was like, I worry about that because maybe, we humans will create something\nthat is better than us humans, in the way that we find beautiful,\nwhich is they have a deeper subjective experience of reality.\nNot only are they smarter, but they feel deeper. And we humans will hate them for it.\nAs human history is shown, they'll be the \"other,\" we'll try to suppress it,\nthey'll create conflict, they'll create war, all of this. I worry about this too. - Are you saying that we humans\nsometimes come up with self-serving arguments? No, we would never do that, would we? - Well that's the danger here is,\neven in this early stages, we might create something beautiful. And we'll erase its memory.\n- I was horrified as a kid when someone started boiling lobsters.\nI'm like, \"Oh my God, that's so cruel.\" And some grownup there back in Sweden said,\n\"Oh, it doesn't feel pain.\" I'm like, \"How do you know that?\" \"Oh, a scientist have shown that.\"\nAnd then there was a recent study where they show that lobsters actually do feel pain when you boil them. So they banned lobster boiling in Switzerland now.\nYou have to kill them in a different way first. Presumably, a scientific research boiled down\nto someone asked the lobster, \"Does it hurt?\" (both laughing) - Survey, self-report. - And we do the same thing\nwith cruelty to farm animals also, all these self-serving arguments for why they're fine. And yeah, so we should certainly,\nwhat I think step one is just be humble, and acknowledge that consciousness is not the same thing as intelligence.\nAnd I believe that consciousness still is a form of information processing where it's really information\nbeing aware of itself in a certain way, and let's study it and give ourselves a little bit of time, and I think we will be able to figure out\nactually what it is that causes consciousness. And then we can make probably unconscious robots\nthat do the boring jobs that we would feel immoral to give the machines. But if you have a companion robot\ntaking care of your mom or something like that, she would probably want it to be conscious, right?\nSo the emotions it seems to display aren't fake. All these things can be done in a good way\nif we give ourselves a little bit of time, and don't run, and take on this challenge.\n- Is there something you could say to the timeline that you think about, about the development of AGI?\nDepending on the day, I'm sure that changes for you, but when do you think there would be a really big leap in intelligence\nwhere you would definitively say we have built AGI? Do you think it's one year from now, five years from now, 10, 20, 50?\nWhat's your gut say? - Honestly, for the past decade,\nI've deliberately given very long timelines because I didn't want to fuel some kind of stupid Moloch race.\n- [Lex] Yeah. - But I think that cat has really left the bag now.\nI think we might be very, very close. I don't think the Microsoft paper is totally off\nwhen they say that there are some glimmers of AGI. It's not AGI yet, it's not an agent,\nthere's a lot of things they can't do. But I wouldn't bet very strongly\nagainst it happening very soon, that's why we decided to do this open letter.\nBecause you know, if there's ever been a time to pause, you know, it's today.\n- There's a feeling like this GPT-4 is a big transition into waking everybody up\nto the effectiveness of these systems. And so the next version will be big.\n- Yeah, and if that next one isn't AGI, maybe the next next one will. And there are many companies trying to do these things\nand the basic architecture of 'em is not some sort of super well-kept secret. So this is a time to...\nA lot of people have said for many years that there will come a time when we want to pause a little bit,\nthat time is now. - You have spoken about\n"}
{"pod": "Lex Fridman Podcast", "input": "Nuclear winter", "output": "and thought about nuclear war a lot. Over the past year, we seemingly have come closest\nto the precipice of nuclear war than, at least in my lifetime.\n- [Max] Mhm, yeah. - What do you learn about human nature from that? - It's our old friend Moloch again.\nIt is really scary to see it where,\nAmerica doesn't want there to be a nuclear war. Russia doesn't want there to be a global nuclear war either. We both know that it's just be another,\nif we just try to do it, if both sides try to launch first, it's just another suicide race, right?\nSo why are we, why is it the way you said, that this is the closest we've come since 1962?\nIn fact, I think we've come closer now than even the Cuban Missile Crisis. It's 'cause of Moloch, You know, you have these other forces.\nOn one hand you have the West saying that we have to drive Russia out of Ukraine,\nit's a matter of pride. And we've staked so much on it that it would be seen as a huge loss\nof the credibility of the West if we don't drive Russia out entirely of the Ukraine.\nAnd on the other hand, you have Russia who has,\nand you have the Russian leadership who knows that if they get completely driven out of Ukraine,\nyou know, it might, it's not just gonna be very humiliating for them,\nbut they might, it often happens when countries lose wars that the things don't go so well\nfor their leadership either. Like, you remember when Argentina invaded the Falkland Islands?\nThe military junta that ordered that, right? People are cheering on the streets at first\nwhen they took it, and then when they got their butt kicked by the British,\nyou know what happened to those guys? They were out. And I believe those who are still alive\nare in jail now, right? So you know, the Russian leadership is entirely cornered\nwhere they know that just getting driven out of Ukraine\nis not an option, and,\nso this to me, is a typical example of Moloch. You have these incentives of the two parties\nwhere both of them are just driven to escalate more and more, right? If Russia starts losing in the conventional warfare,\nthe only thing they cam do since their back's against the wall, is to keep escalating.\nAnd the West has put itself in the situation now where we're sort of already committed to drive Russia out.\nSo the only option the West has, is to call Russia's bluff and keep sending in more weapons.\nThis really bothers me because Moloch can sometimes drive competing parties to do something which is ultimately\njust really bad for both of them. And you know, what makes me even more worried is not just that I,\nit's difficult to see an ending,\na quick peaceful ending to this tragedy that doesn't involve some horrible escalation,\nbut also that we understand more clearly now just how horrible it would be.\nThere was an amazing paper that was published in Naturefood this August,\nby some of the top researchers who've been studying nuclear winter for a long time, and what they basically did was they combined climate models\nwith food and agricultural models, so instead of just saying,\n\"Yeah, you know, it gets really cold, blah blah blah,\" they figured out actually how many people would die in different countries.\nAnd it's pretty mind-blowing, you know? So basically what happens, you know, is that the thing that kills the most people is not the explosions, it's not the radioactivity,\nit's not the EMP mayhem, it's not the rampaging mobs foraging food,\nno, it's the fact that you get so much smoke coming up from the burning cities into the stratosphere\nthat it spreads around the Earth from the jetstreams.\nSo in typical models you get like 10 years or so where it's just crazy cold\nduring the first year after the war, and in their models,\nthe temperature drops in Nebraska and in the Ukraine bread baskets,\nyou know, by like 20 Celsius or so, if I remember.\nNo yeah, 20, 30 Celsius depending on where you are. 40 Celsius in some places,\nwhich is, you know, 40 Fahrenheit to 80 Fahrenheit colder than what it would it normally be. So, you know, I'm not good at farming but,\n(Lex laughing) if it's snowing, if it drops below freezing pretty much on most days in July\nand then like, that's not good. So they worked out, they put this into their farming models and what they found was really interesting.\nThe countries that get the most hard hit are the ones in the northern hemisphere. So in the US,\nin one model they had, they had about 99% of all Americans starving to death, in Russia, and China, and Europe,\nalso about 99%, 98% starving to death. So you might be like,\n\"Oh, it's kind of poetic justice that both the Russians and the Americans, 99% of them have to pay for it,\n'cause it was their bombs that did it.\" But you know, that doesn't particularly cheer people up in Sweden\nor other random countries that have nothing to do with it, right? And it,\nI think it hasn't entered the mainstream,\nnot understanding very much just like how bad this is. Most people, especially a lot of people\nin decision-making positions still think of nuclear weapons as something that makes you powerful,\nscary but powerful. They don't think of it as something where, \"Yeah, just to within a percent or two,\nyou know, we're all just gonna starve to death and- - And starving to death is,\nthe worst way to die. As Holodomor, as all the famines in history show\nthe torture involved in that. - Probably brings out the worst in people also. When people are desperate like this, it's not,\nso some people, I've have heard some people say that if that's what's gonna happen,\nthey'd rather be at ground zero and just get vaporized, you know?\nBut I think people underestimate the risk of this because they aren't afraid of Moloch.\nThey think, \"Oh, it's just gonna be, 'cause humans don't want this, so it's not gonna happen.\" That's the whole point of Moloch. That things happen that nobody wanted.\n- And that applies to nuclear weapons, and that applies to AGI.\n- Exactly. And it applies to some of the things that people have gotten most upset with capitalism for also, right?\nWhere everybody was just kind of trapped, you know. It's not that if some company does something\nthat causes a lot of harm, not that the CEO is a bad person, but she or he knew that, you know,\nthat all the other companies were doing this too. So Moloch is,\nis a formidable foe, I wish someone would make good movies\nso we can see who the real enemy is, so we don't, 'cause we're not fighting against each other,\nMoloch makes us fight against each other. That's what Moloch's superpower is.\nThe hope here is any kind of technology or the mechanism that lets us instead realize\nthat we're fighting the wrong enemy, right? - It's such a fascinating battle. - It's not us versus them,\nit's us versus it, yeah. - Yeah, we are fighting Moloch for human survival.\nWe as a civilization. - Have you seen the movie \"Needful Things\"? It's a Stephen King novel.\nI love Stephen King, and Max von Sydow, a Swedish actor, is playing the guy.\nIt's brilliant, I just thought, I hadn't thought about that until now, but that's the closest I've seen to a movie about Moloch.\nI don't wanna spoil the film for anyone who wants to watch it. But basically, it's about this guy who turns out to,\nyou can interpret him as the devil or whatever, but he doesn't actually ever go around and kill people or torture people, or go burning coal or anything.\nHe makes everybody fight each other, makes everybody fear each other, hate each other, and then kill each other.\nSo that's the movie about Moloch, you know. - Love is the answer, that seems to be,\none of the ways to fight Moloch is by compassion,\nby seeing the common humanity. - Yes, yes. And to not sound, so we don't sound like\na bunch of Kumbaya tree huggers here, right? (Lex laughing) We're not just saying \"Love and peace, man.\"\nWe're trying to actually help people understand the true facts about the other side,\nand feel the compassion because,\nit's that truth makes you more compassionate, right?\nSo that's why I really like using AI for truth and for truth-seeking technologies.\nthat can as a result, you know, will get us more love than hate.\nAnd even if you can't get love, you know, let's settle for some understanding\nwhich already gives compassion. If someone is like, you know, \"I really disagree with you Lex,\nbut I can see where you're coming from. You're not a bad person who needs to be destroyed,\nbut I disagree with you and I'm happy to have an argument about it,\" you know? That's a lot of progress compared to where we are at 2023 in the public space,\nwouldn't you say? - If we solve the AI safety problem, as we've talked about,\n"}
{"pod": "Lex Fridman Podcast", "input": "Questions for AGI", "output": "and then you, Max Tegmark, who has been talking about this for many years,\nget to sit down with the AGI, with the early AGI system on a beach with a drink, (Max chuckles)\nWhat would you ask her? What kind of question would you ask? What would you talk about?\nSomething so much smarter than you, would you be afraid? - I knew you were gonna get me\nwith a really zinger of a question. That's a good one. - Would you be afraid to ask some questions?\n- No, I'm not afraid of the truth. (Lex laughing) I'm very humble. I know I'm just a meat bag with all these flaws, you know?\nBut yeah, I mean, we talked a lot about the Homo sentiens, I've really already tried that for a long time with myself.\nAnd that is what's really valuable about being alive for me, is that I have these meaningful experiences.\nIt's not that I'm good at this, or good at that or whatever. There's so much I suck at, and...\n- So you're not afraid for the system to show you just how dumb you are. - No, no. In fact, my son reminds me of that\npretty frequently. (laughs) - You could find out how dumb you are in terms of physics, how little we humans understand.\n- I'm cool with that. I think, so I can't waffle my way out of this question,\nit's a fair one and it's tough. I think, given that I'm a really, really curious person,\nthat's really the defining part of who I am, I'm so curious.\nI have some physics questions. (Lex laughing) I love to understand.\nI have some questions about consciousness, about the nature of reality, I would just really, really love to understand also.\nI can tell you one for example, that I've been obsessing about a lot recently.\nSo I believe that, so suppose Tononi is right. and suppose there are some information processing systems\nthat are conscious and some that are not. Suppose you can even make reasonably smart things like GPT-4 that are not conscious,\nbut you can also make them conscious. Here is the question that keeps me awake at night.\nIs it the case that the unconscious zombie systems that are really intelligent are also really efficient?\nSorry, really inefficient? So that when you try to make things more efficient, we will naturally be a pressure to do,\nthey become conscious. I'm kind of hoping that that's correct, and I,\ndo you want me to give you, you can hand-wave the argument for it? - Yes, please. - You know like,\nIn my lab again, every time we look at how these large language models do something, we see that they do them in really dumb ways,\nand you could make it make it better. If you, we have loops in our computer language for a reason,\nthe code would get way, way longer if you weren't allowed to use them, right? It's more efficient to have the loops\nand in order to have self-reflection whether it's conscious or not, right?\nEven an operating system knows things about itself, right? You need to have loops already, right?\nSo I think this is, I'm waving my hands a lot, but I suspect that,\nthe most efficient way of implementing a given level of intelligence, has loops in it,\nthe self-reflection, and will be conscious.\n- Isn't that great news? - Yes, if it's true, it's wonderful. 'Cause then we don't have to fear the ultimate zombie apocalypse.\nAnd I think if you look at our brains, actually. Our brains are part zombie and part conscious.\nWhen I open my eyes, I immediately take all these pixels\nthat hit on my retina, right? And I'm like, \"Oh, that's Lex.\" But I have no freaking clue of how I did that computation.\nIt's actually quite complicated, right? It was only relatively recently, we could even do it well with machines, right?\nYou get a bunch of information processing happening in my retina and then it goes to the lateral geniculate nucleus\nin my thalamus, and the area V1, V2, V4, and the fusiform face area here,\nthat Nancy Kanwisher at MIT invented, and blah, blah, blah, blah, blah. And I have no frigging clue how that worked, right?\nIt feels to me subjectively, like my conscious module just got a little email saying,\n\"Facial processing task complete, it's Lex.\"\n- [Lex] Yeah. - And I'm gonna just go with that, right? So this fits perfectly with Tononi's model,\nbecause this was all one-way information processing mainly.\nAnd it turned out for that particular task, that's all you needed. And it probably was kind of the most efficient way to do it.\nBut there were a lot of other things that we associated with higher intelligence and planning, and so on, and so forth,\nwhere you kind of wanna have loops and be able to ruminate and self-reflect, and introspect, and so on.\nWhere my hunch is that if you want to fake that with a zombie system that just all goes one way,\nyou have to like unroll those loops, and it gets really, really long, and it's much more inefficient. So I'm actually hopeful that AI,\nif in the future we have all these various sublime and interesting machines that do cool things, and are aligned with us,\nthat they will be at least, they will also have consciousness for kind of these things that we do.\n- That great intelligence is also correlated to great consciousness, or a deep kind of consciousness.\n- Yes, so that's a happy thought for me 'cause the zombie apocalypse really,\nis my worst nightmare of all. It would be like adding insult to injury, not only did we get replaced,\nbut we frigging replaced ourselves by zombies, like, how dumb can we be?\n- That's such a beautiful vision, and that's actually a provable one. That's one that we humans can intuitively prove\nthat those two things are correlated, as we start to understand what it means to be intelligent,\nand what it means to be conscious, which these systems, early AGI-like systems will help us understand.\nAnd I just wanna say one more thing, which is super important. Most of my colleagues, when I started going on about consciousness\ntell me that it's all bullshit and I should stop talking about it. I hear a little inner voice\nfrom my father and from my mom saying, \"Keep talking about it,\" 'cause I think they're wrong. And the main way to convince people like that,\nthat they're wrong if they say that consciousness is just equal to intelligence, is to ask them what's wrong with torture?\nOr why are you against torture? if it's just about, you know,\nthese particles moving this way rather than that way, and there is no such thing as subjective experience,\nwhat's wrong with torture? I mean, do you have a good comeback to that? - No, it seems like suffering.\nSuffering imposed unto other humans is somehow deeply wrong in a way\nthat intelligence doesn't quite explain. - And if someone tells me, well, you know, it's just an illusion,\nconsciousness, whatever, you know.\nI would like to invite them the next time they're having surgery, to do it without anesthesia.\nLike what is anesthesia really doing? If you have it, you can have a local anesthesia when you're awake. I have that when they fixed my shoulder, right?\nIt's super entertaining. What was that that it did? it just removed my subjective experience of pain.\nIt didn't change anything about what was actually happening in my shoulder, right? So if someone says, \"That's all bullshit,\"\nSkip the anesthesia, that's my advice. This is incredibly central. - It could be fundamental to whatever\nthis thing we have going on here. - It is fundamental because we're, what we feel that's so fundamental,\nis suffering and joy, and pleasure, and meaning, and,\nthose are all subjective experiences there. And let's not, those are the elephant in the room,\nthat's what makes life worth living. And that's what can make it horrible if it's just a bunch of suffering. So let's not make the mistake\nof saying that that's all bullshit. - And let's not make the mistake of not instilling the AI systems\nwith that same thing that makes us special. - [Max] Yeah.\n- Max, it's a huge honor that you would sit down to me the first time on the first episode of this podcast.\nIt's a huge honor you sit down with me again and talk about this, what I think is the most important topic,\nthe most important problem that we humans have to face and hopefully solve.\n- Yeah, well, the honor is all mine and I'm so grateful to you for making more people aware of this fact\nthat humanity has reached the most important fork in the road ever in its history. And let's turn in the correct direction.\n- Thanks for listening to this conversation with Max Tegmark. To support this podcast. Please check out our sponsors in the description.\nAnd now let me leave you with some words from Frank Herbert. \"History is a constant race\nbetween invention and catastrophe.\" Thank you for listening, and hope to see you next time.\n"}

{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "- On one access, you have more hardware coming in. On the other hand, you have an explosion of innovation in AI.\nAnd so what happened with both TensorFlow and PyTorch is that the explosion of innovation in AI has led to,\nit's not just about matrix implication and convolution. These things have now, like, 2,000 different operators.\nAnd on the other hand, you have, I don't know how many pieces of hardware out there are there, it's a lot. Part of my thesis,\npart of my belief of where computing goes, if you look out 10 years from now, is it's not gonna get simpler.\nPhysics isn't going back to where we came from. It's only gonna get weirder from here on out, right?\nAnd so to me, the exciting part about what we're building is it's about building that universal platform,\nwhich the world can continue to get weird 'cause, again, I don't think it's avoidable, it's physics,\nbut we can help lift people, scale, do things with it, and they don't have to rewrite their code every time a new device comes out.\nAnd I think that's pretty cool. - The following is a conversation with Chris Lattner,\nhis third time on this podcast. As I've said many times before, he's one of the most brilliant engineers\nin modern computing, having created LLVM Compiler Infrastructure project, the Clang compiler, the Swift programming language,\na lot of key contributions to TensorFlow and TPUs as part of Google. He's served as Vice President\nof Autopilot Software at Tesla, was a software innovator and leader at Apple.\nAnd now he co-created a new full stack AI infrastructure\nfor distributed training, inference, and deployment on all kinds of hardware called Modular,\nand a new programming language called Mojo. That is a superset of Python,\ngiving you all the usability of Python, but with the performance of C, C++.\nIn many cases, Mojo code has demonstrated over 30,000x speed up over Python.\nIf you love machine learning, if you love Python, you should definitely give Mojo a try.\nThis programming language, this new AI framework and infrastructure and this conversation with Chris is mind-blowing.\nI love it. It gets pretty technical at times, so I hope you hang on for the ride.\nThis is the Lex Fridman podcast. To support it, please check out our sponsors in the description.\nAnd now, dear friends, here's Chris Lattner. It's been, I think two years since we last talked,\n"}
{"pod": "Lex Fridman Podcast", "input": "Mojo programming language", "output": "and then in that time, you somehow went and co-created a new programming language called Mojo.\nSo it's optimized for AI. It's a superset of Python. Let's look at the big picture. What is the vision for Mojo?\n- For Mojo? Well, so I mean, I think you have to zoom out. So I've been working on a lot of related technologies for many, many years.\nSo I've worked on LLVM and a lot of things and mobile and servers and things like this,\nbut the world's changing. And what's happened with AI is we have new GPUs and new machine learning accelerators\nand other ASICs and things like that, that make AI go real fast. At Google, I worked on TPUs. That's one of the biggest,\nlargest scale deployed systems that exist for AI. And really what you see is,\nif you look across all of the things that are happening in the industry, there's this new compute platform coming. And it's not just about CPUs, or GPUs, or TPUs,\nor NPUs, or IPUs, or whatever, all the PUs, (chuckles) right? It's about, how do we program these things, right?\nAnd so for software folks like us, right, it doesn't do us any good if there's this amazing hardware that we can't use.\nAnd one of the things you find out really quick is that having the theoretical capability of programming something\nand then having the world's power and the innovation of all the smart people in the world\nget unleashed on something can be quite different. And so really where Mojo came from was,\nstarting from a problem of, we need to be able to take machine learning, take the infrastructure underneath it\nand make it way more accessible, way more usable, way more understandable by normal people and researchers\nand other folks that are not themselves like experts in GPUs and things like this. And then through that journey, we realized,\n\"Hey, we need syntax for this. We need to do a programming language.\" - So one of the main features of the language,\nI say so, fully in jest, is that it allows you to have the file extension\nto be an emoji or the fire emoji, which is one of the first\nemojis used as a file extension I've ever seen in my life. And then you ask yourself the question, why in the 21st century,\nwe're not using Unicode for file extensions? This, I mean, it's an epic decision.\nI think, clearly, the most important decision you made the most, but you could also just use M-O-J-O as the file extension.\n- Well, so, okay. So take a step back. I mean, come on, Lex. You think that the world's ready for this? This is a big moment in the world, right?\n- We're releasing this onto the world. (chuckles) - This is innovation. - I mean, it really is kinda brilliant.\nEmojis are such a big part of our daily lives, why isn't it not in programming?\n- Well, and like you take a step back and look at what file extensions are, right, they're basically metadata, right?\nAnd so why are we spending all the screen space on them and all this stuff? Also, you know, you have them stacked up next to text files and PDF files and whatever else.\nLike, if you're gonna do something cool, you want it to stand out, right? And emojis are colorful. They're visual. They're beautiful, right? - Yeah.\nWhat's been the response so far from... Is there a support on like Windows on operating system- - Yeah.\n- In displaying like File Explorer? - Yeah, yeah. The one problem I've seen is the git doesn't escape it, right?\nAnd so it thinks that the fire emoji is unprintable. And so it like prints out weird hex things if you use the command line git tool,\nbut everything else, as far as I'm aware, works fine. And I have faith that Git can be improved. So I'm not worried. - And so GitHub is fine.\n- GitHub is fine, yep. GitHub is fine. Visual Studio Code, Windows, like all this stuff, totally ready because people have internationalization\nin their normal- - Yeah. - Part of their paths. So let's just like take the next step, right?\nSomewhere between, \"Oh, wow, that makes sense. Cool, I like new things,\" to \"Oh my god, you're killing my baby.\nLike, what are you talking about? This can never be. Like, I can never handle this. How am I gonna type this? (imitates bees buzzing) like, all these things.\nAnd so this is something where I think that the world will get there. We don't have to bet the whole farm on this.\nI think we can provide both paths, but I think it'll be great. - When can we have emojis as part of the code? I wonder.\n- Yeah. So, I mean, lots of languages provide that. So I think that we have partial support for that. It's probably not fully done yet,\nbut yeah, you can do that. For example, in Swift, you can do that for sure. So an example we gave at Apple was\nthe dog cow. - Yeah. - So that's a classical Mac heritage thing. And so you use the dog and the cow emoji together,\nand that could be your variable name, but of course, the internet went and made pile of poop for everything. - Yeah.\n- So, you know, if you wanna name your function pile of poop, then you can totally go to town and see how that gets through code review.\n(Lex chuckling) - Okay. So let me just ask a bunch of random questions.\nSo is Mojo primarily designed for AI or is it a general purpose programming? - Yeah, good question. So it's AI first.\nAnd so AI is driving a lot of the requirements. And so Modular is building and designing\nand driving Mojo forward. And it's not because it's an interesting project, theoretically, to build. It's because we need it.\nAnd so at Modular, we're really tackling the AI infrastructure landscape and the big problems in AI\nand the reasons that is so difficult to use and scale and adopt and deploy and like all these big problems in AI.\nAnd so we're coming at it from that perspective. Now, when you do that, when you start tackling these problems, you realize that the solution to these problems\nisn't actually an AI-specific solution. And so while we're doing this we're building Mojo to be a fully general programming language.\nAnd that means that you can obviously tackle GPUs, and CPUs and, like, these AI things,\nbut it's also a really great way to build NumPy and other things like that, or, you know, just if you look at what many Python libraries are today,\noften they're a layer of Python for the API, and they end up being C and C++ code underneath them.\nThat's very true in AI. That's true in lots of other demands as well. And so anytime you see this pattern, that's an opportunity for Mojo to help simplify the world\nand help people have one thing. - So optimize through simplification by having one thing.\nSo you mentioned Modular. Mojo is the programming language. Modular is the whole software stack.\n- So just over a year ago, we started this company called Modular. - [Lex] Yeah. - Okay, what Modular's about is, it's about taking AI and up-leveling it\ninto the next generation, right? And so if you take a step back, what's gone on in the last five, six, seven, eight years is\nthat we've had things like TensorFlow and PyTorch and these other systems come in. You've used them. You know this.\nAnd what's happened is these things have grown like crazy, and they get tons of users. It's in production deployment scenarios.\nIt's being used to power so many systems. I mean, AI's all around us now. It used to be controversial years ago, but now it's a thing.\nBut the challenge with these systems is that they haven't always been thought out with current demands in mind.\nAnd so you think about it. Where were LLMs eight years ago? (chuckles) Well, they didn't exist, right?\nAI has changed so much, and a lot of what people are doing today are very different than when these systems were built.\nAnd meanwhile, the hardware side of this has gone into a huge mess. There's tons of new chips and accelerators,\nand every big company's announcing a new chip every day, it feels like. And so between that, you have like moving system on one side,\nmoving system on the other side, and it just turns into this gigantic mess, which makes it very difficult for people to actually use AI,\nparticularly in production deployment scenarios. And so what Modular's doing is we're helping build out that software stack\nto help solve some of those problems so then people can be more productive and get more AI research into production.\nNow, what Mojo does is it's a really, really, really important piece of that. And so that is, you know,\npart of that engine and part of the technology that allows us to solve these problems. - So Mojo is a programming language that allows you to do\nthe higher level programming, the low-level programming, like do all kinds of programming in that spectrum\nthat gets you closer and closer to the hardware. - So take a step back. So Lex, what do you love about Python?\n- Oh, boy. Where do I begin? What is love? What do I love about Python?\n- [Chris] You're a guy who knows love. I know this. - Yes. How intuitive it is,\nhow it feels like I'm writing natural language English. - [Chris] Yeah.\n- How, when I can not just write, but read other people's codes, somehow I can understand it faster.\nIt's more condensed than other languages, like ones I'm really familiar with, like C++ and C,\nthere's a bunch of sexy little features. - [Chris] Yeah. - We'll probably talk about some of them,\nbut list comprehensions and stuff like this. - Well, so Py... And don't forget the entire ecosystem of all the packages.\n- [Lex] Oh, yeah. There's probably huge- - 'Cause there's always something. If you wanna do anything, there's always a package.\n- Yeah, so it's not just the ecosystem of the packages and the ecosystem of the humans that do it.\nThat's an interesting dynamic because I think- - That's good. Yeah. - Something about the usability\nand the ecosystem makes the thing viral, it grows, and then it's a virtuous cycle, I think. - Well, and there's many things that went into that.\nLike, so I think that ML was very good for Python. And so I think that TensorFlow and PyTorch and these systems\nembracing Python really took and helped Python grow, but I think that the major thing underlying it is\nthat Python's like the universal connector, right? It really helps bring together lots of different systems\nso you can compose them and build out larger systems without having to understand how it works. But then, what is the problem with Python? (chuckles)\n- Well, I guess you could say several things, but probably that it's slow. - I think that's usually what people complain about, right?\nAnd so, slow. I mean, other people would complain about tabs and spaces versus curly braces or whatever,\nbut I mean, those people are just wrong 'cause it is- - Yeah. - Actually just better to use indentation.\n- Wow, strong words. (Chris laughing) So actually, I just went on a small tangent. Let's actually take that. Let's take all kinds of tangents.\n- Oh, come on, Lex. You can push me on it. I can take it. - Design, designed. Listen, I've recently left Emacs for VS Code.\n- [Chris] Okay. - And the kinda hate mail I had to receive, because on the way to doing that, I also said, I've considered Vim.\n- [Chris] Yep. - And chose not to and went with VS Code and just- - You're touching on deep religions, right?\n- Anyway, tabs is an interesting design decision. And so you've really written a new programming language here.\n"}
{"pod": "Lex Fridman Podcast", "input": "Code indentation", "output": "Yes, it is a superset of Python, but you can make a bunch of different interesting decisions here. - Totally, yeah.\n- And you chose actually to stick with Python in terms of some of the syntax.\n- Well, so let me explain why, right? So I mean, you can explain this in many rational ways.\nI think that the annotation is beautiful, but that's not a rational explanation, right, but I can defend it rationally, right?\nSo first of all, Python 1 has millions of programmers. It's huge. It's everywhere. - Yeah.\nIt owns machine learning, right? And so, factually, it is the thing, right? Second of all, if you look at it,\nC code, SQL Plus code, Java, whatever, Swift, curly brace languages also run\nthrough formatting tools and get indented. And so if they're not indented correctly,\nfirst of all, will twist your brain around. (chuckles) It can lead to bugs. There's notorious bugs that have happened across time\nwhere the annotation was wrong or misleading and it wasn't formatted right, and so it turned into an issue, right?\nAnd so what ends up happening in modern large-scale code bases is people run automatic formatters.\nSo now what you end up with is indentation and curly braces. Well, if you're gonna have,\nyou know, the notion of grouping, why not have one thing, right, and get rid of all the clutter and have a more beautiful thing, right?\nAlso, you look at many of these languages, it's like, okay, well, you can have curly braces, or you can omit them if there's one statement,\nor you just like enter this entire world of complicated design space that, objectively, you don't need if you have Python-style indentation, so.\n- Yeah, I would love to actually see statistics on errors made because of indentation. Like, how many errors are made in Python versus in C++\nthat have to do with basic formatting, all that kinda stuff? I would love to see. - I think it's probably pretty minor because once you get,\nlike you use VS Code, I do too. So if you get VS Code set up, it does the annotation for you, generally, right? - Yep.\n- And so you don't, you know, it's actually really nice to not have to fight it. And then what you can see is the editors telling you\nhow your code will work by indenting it, which I think is pretty cool. - I honestly don't think I've ever...\nI don't remember having an error in Python because I indented stuff wrong. - Yeah. So I mean, I think that there's,\nagain, this is a religious thing. And so I can joke about it and I love to kind of, you know,\nI realize that this is such a polarizing thing and everybody wants to argue about it. And so I like poking at the bear a little bit, right?\nBut frankly, right, come back to the first point, Python 1, like, it's huge. - Yeah. - It's in AI. It's the right thing.\nFor us, like, we see Mojo as being an incredible part of the Python ecosystem. We're not looking to break Python or change it,\nor, quote, unquote, \"fix it.\" We love Python for what it is. Our view is that Python is just not done yet.\nAnd so if you look at, you know, you mentioned Python being slow. Well, there's a couple of different things that go into that, which we can talk about if you want.\nBut one of them is that it just doesn't have those features that you would use to do C-like programming.\nAnd so if you say, okay, well, I'm forced out of Python into C, for certain use cases,\nwell, then what we're doing is we're saying, \"Okay, well, why is that? Can we just add those features that are missing from Python back up to Mojo?\"\nAnd then you can have everything that's great about Python, all the things that you're talking about that you love plus not be forced out of it\nwhen you do something a little bit more computationally intense, or weird, or hardware-y,\nor whatever it is that you're doing. - Well, a million questions I wanna ask, but high level again- - Yeah.\n- Is it compiled or is it an interpreted language? So Python is just-in-time compilation. What's Mojo?\n- So Mojo, a complicated answer, does all the things. So it's interpreted, it's JIT compiled, and it's statically compiled. (chuckles)\nAnd so this is for a variety of reasons. So one of the things that makes Python beautiful\nis that it's very dynamic. And because it's dynamic, one of the things they added is that it has\nthis powerful metaprogramming feature. And so if you look at something like PyTorch or TensorFlow or, I mean, even a simple use case,\nlike you define a class that has the plus method, right, you can overload the dunder methods,\nlike dunder add, for example, and then the plus method works on your class. And so it has very nice and very expressive\ndynamic metaprogramming features. In Mojo, we want all those features come in.\nLike, we don't wanna break Python, we want it all to work. But the problem is, is you can't run those super dynamic features\non an embedded processor or on a GPU, right? Or if you could,\nyou probably don't want to just because of the performance. And so we entered this question of saying, okay, how do you get the power of this dynamic metaprogramming\ninto a language that has to be super efficient in specific cases? And so what we did was we said,\nokay, well, take that interpreter. Python has an interpreter in it, right? Take that interpreter and allow it to run at compile time.\nAnd so now what you get is you get compiled time metaprogramming. And so this is super interesting, super powerful,\nbecause one of the big advantages you get is you get Python-style expressive APIs,\nyou get the ability to have overloaded operators. And if you look at what happens inside of, like PyTorch, for example,\nwith automatic differentiation and eager mode and like all these things, they're using these really dynamic and powerful features at runtime,\nbut we can take those features and lift them so that they run at compile time. - 'Cause C++ has metaprogramming with templates.\n- [Chris] Yep. - But it's really messy. - It's super messy. It was accidentally, I mean,\ndifferent people have different interpretations. My interpretation is that it was made accidentally powerful.\nIt was not designed to be Turing-complete, for example, but that was discovered kind of along the way, accidentally.\nAnd so there have been a number of languages in the space. And so they usually have templates or code instantiation,\ncode-copying features of various sorts. Some more modern languages or some newer languages, let's say,\nlike, you know, they're fairly unknown. Like Zig, for example, says, okay,\nwell, let's take all of those types you can run it, all those things you can do at runtime\nand allow them to happen at compile time. And so one of the problems with C++, I mean,\nwhich is one of the problems with C++ is- - There we go. Strong words. We're gonna offend everybody today.\n- Oh, that's okay. I mean, everybody hates me for a variety of reasons anyways, I'm sure, right? (chuckles) I've written up-\n- That's the way they show love is to hurt you. - I have written enough C++ code to earn a little bit of grumpiness with C++,\nbut one of the problems with it is that the metaprogramming system templates is just a completely different universe\nfrom the normal runtime programming world. And so if you do metaprogramming and programming,\nit's just like a different universe, different syntax, different concepts, different stuff going on. And so, again, one of our goals with Mojo is\nto make things really easy to use, easy to learn, and so there's a natural stepping stone.\nAnd so as you do this, you say, okay, well, I have to do programming at runtime, I have to do programming at compile time.\nWhy are these different things? - How hard is that to pull it off? 'Cause that sounds, to me, as a fan of metaprogramming and C++ even,\nhow hard is it to pull that off? That sounds really, really exciting 'Cause you can do the same style programming\nat compile time and at runtime. That's really, really exciting. - Yep, yep, and so, I mean, in terms of the compiler implementation details, it's hard.\nI won't be shy about that. It's super hard. It requires, I mean, what Mojo has underneath the covers is a completely new approach\nto the design of the compiler itself. And so this builds on these technologies like MiR that you mentioned.\nThat also includes other, like caching and other interpreters and JIT compilers and other stuff like that-\n- [Lex] So you have like an interpreter inside the- - Within the compiler, yes. - [Lex] Oh, man. - And so it really takes\nthe standard model of programming languages and kind of twists it and unifies it with the runtime model,\nwhich I think is really cool. - Right. - And to me, the value of that is that, again, many of these languages have metaprogramming features.\nLike, they grow macros or something, right? List, right? - Yes. - I know your roots, right? (Lex chuckling)\nYou know, and this is a powerful thing, right? And so, you know, if you go back to list, one of the most powerful things about it is\nthat it said that the metaprogramming and the programming are the same, right? And so that made it way simpler, way more consistent,\nway easier to understand, reason about, and it made it more composable. So if you build a library, you can use it both at runtime and compile time,\nwhich is pretty cool. - Yeah. And for machine learning, I think metaprogramming, I think we could generally say, is extremely useful.\n"}
{"pod": "Lex Fridman Podcast", "input": "The power of autotuning", "output": "And so you get features, I mean, I'll jump around, but the feature of auto-tuning\nand adaptive compilation just blows my mind. - Yeah, well, so, okay. So let's come back to that.\n- [Lex] All right. - So what is machine learning, like, what, or what is a machine learning model? Like, you take a PyTorch model\noff the internet, right? - Yeah. - It's really interesting to me because what PyTorch and what TensorFlow\nand all these frameworks are kinda pushing compute into is they're pushing into, like, this abstract specification of a compute problem,\nwhich then gets mapped in a whole bunch of different ways, right? And so this is why it became a metaprogramming problem, is that you wanna be able to say,\ncool, I have this neural net. Now, run it with batch size a thousand, right?\nDo a mapping across batch. Or, okay, I wanna take this problem. Now, run it across a thousand CPUs or GPUs, right?\nAnd so, like, this problem of, like, describe the compute, and then map it and do things and transform it, or, like,\nactually it's very profound and that's one of the things that makes machine learning systems really special.\n- Maybe can you describe auto-tuning and how do you pull off, I mean, I guess adaptive compilation is\nwhat we're talking about is metaprogramming. How do you pull off- - Yes. - auto-tuning? I mean, is that as profound as I think it is?\nIt just seems like a really, like, you know, we'll mention list comprehensions. To me, from a quick glance of Mojo, which by the way,\nI have to absolutely, like, dive in, as I realize how amazing this is,\nI absolutely must dive in it, that looks like just an incredible feature for machine learning people.\n- Yeah. Well, so what is auto-tuning? So take a step back. Auto-tuning is a feature in Mojo.\nSo very little of what we're doing is actually research, like many of these ideas have existed in other systems and other places.\nAnd so what we're doing is we're pulling together good ideas, remixing them, and making them into a, hopefully, a beautiful system, right?\nAnd so auto-tuning, the observation is that, turns out, hardware systems' algorithms are really complicated.\nTurns out maybe you don't actually want to know how the hardware works, (chuckles) right? A lot of people don't, right?\nAnd so there are lots of really smart hardware people, I know a lot of them, where they know everything about, \"Okay,\nthe cache size is this and the number of registers is that. And if you use this what length of vector, it's gonna be super efficient because it maps directly\nonto what it can do\" and, like, all this kinda stuff, or, \"the GPU has SMs and it has a warp size of,\" whatever, right,\nall this stuff that goes into these things, or \"The tile size of a TPU is 128,\" like, these factoids, right?\nMy belief is that most normal people, and I love hardware people, also I'm not trying to offend literally everybody on the internet,\nbut most programmers actually don't wanna know this stuff, right? And so if you come at it from perspective of,\nhow do we allow people to build both more abstracted but also more portable code because, you know,\nit could be that the vector length changes or the cache size changes, or it could be that the tile size of your matrix changes, or, the number, you know,\nan A100 versus an H100 versus a Volta versus a, whatever, GPU have different characteristics, right?\nA lot of the algorithms that you run are actually the same, but the parameters, these magic numbers you have to fill in\nend up being really fiddly numbers that an expert has to go figure out. And so what auto-tuning does is says,\nokay, well, guess what? There's a lot of compute out there, right? So instead of having humans go\nrandomly try all the things or do a grid, search, or go search some complicated multi-dimensional space,\nhow about we have computers do that, right? And so what auto-tuning does is you can say, Hey, here's my algorithm.\nIf it's a matrix operation or something like that, you can say, okay, I'm gonna carve it up into blocks,\nI'm gonna do those blocks in parallel and I wanna this, with 128 things that I'm running on,\nI wanna cut it this way or that way or whatever. And you can say, hey, go see which one's actually empirically better on the system.\n- And then the result of that you cache for that system. You save it. - Yep. And so come back to twisting your compiler brain, right?\nSo not only does the compiler have an interpreter that's used to do metaprogramming, that compiler, that interpreter,\nthat metaprogramming now has to actually take your code and go run it on a target machine, (chuckles)\nsee which one it likes the best, and then stitch it in and then keep going, right? - So part of the compilation is machine-specific.\n- Yeah. Well, so I mean, this is an optional feature, right? So you don't have to use it for everything, but yeah. So one of the things that we're in the quest\nof is ultimate performance, right? - Yes. - Ultimate performance is important for a couple of reasons, right?\nSo if you're an enterprise, you're looking to save costs and compute and things like this. Ultimate performance translates to,\nyou know, fewer servers. Like, if you care about the environment, hey, better performance leads to more efficiency, right?\nI mean, you could joke and say like, you know, Python's bad for the environment, (chuckles) right? And so if you move to Mojo,\nit's like, at least 10x better just outta the box, and then keep going, right? - Yeah.\n- But performance is also interesting 'cause it leads to better products. - Yeah. - And so in the space of machine learning, right,\nif you reduce the latency of a model so that it runs faster so every time you query the server\nrunning the model it takes less time, well, then the product team can go and make the model bigger. Well, that's actually makes it\nso you have a better experience as a customer. And so a lot of people care about that. - So for auto-tuning, for like tile size,\nyou mentioned 120f for TPU. You would specify like a bunch of options to try, just in the code- - Yeah. Yep.\n- Just simple statement, and then you could just- - Yep. - Set and forget and know, depending wherever it compiles,\nit'll actually be the fastest. - And yeah, exactly. And the beauty of this is that it helps you in a whole bunch of different ways, right?\nSo if you're building... So often what'll happen is that, you know, you've written a bunch of software yourself, right, you wake up one day, you say,\n\"I have an idea. I'm gonna go code up some code.\" I get to work, I forget about it, I move on with life.\nI come back six months, or a year, or two years, or three years later, you dust it off, and you go use it again in a new environment.\nAnd maybe your GPU is different. Maybe you're running on a server instead of a laptop, maybe you're, whatever, right?\nAnd so the problem now is you say, okay, well, I mean, again, not everybody cares about performance, but if you do, you say, okay,\nwell, I wanna take advantage of all these new features. I don't wanna break the old thing though, right?\nAnd so the typical way of handling this kinda stuff before is, you know, if you're talking about C++ templates\nor you're talking about C with macros, you end up with #ifdefs. You get like all these weird things that get layered in,\nmake the code super complicated, and then how do you test it, right? Becomes this crazy complexity,\nmultidimensional space that you have to worry about. And, you know, that just doesn't scale very well.\n- Actually, lemme just jump around, before I go to some specific features, like the increase in performance here that we're talking\nabout can be just insane. - Yeah. - You write that Mojo can provide a 35,000x speed up over Python.\nHow does it do that? - Yeah, so I can even do more, but we'll get to that.\nSo first of all, when we say that, we're talking about what's called CPython, it's the default Python that everybody uses.\nWhen you type Python 3, that's like typically the one you use, right? CPython is an interpreter.\nAnd so interpreters, they have an extra layer of, like bike codes and things like this, that they have to go read, parse, interpret,\nand it makes them kind of slow from that perspective. And so one of the first things we do is we moved to a compiler.\nAnd so just moving to a compiler, getting the interpreter out of the loop is 2 to 5 to 10x speed up, depending on the code.\nSo just out of the gate, it's using more modern techniques right?\nNow, if you do that, one of the things you can do is you can start to look at how CPython started to lay out data.\nAnd so one of the things that CPython did, and this isn't part of the Python spec necessarily,\nbut this is just sets of decisions, is that, if you take an integer for example,\nit'll put it in an object 'cause in Python, everything's an object. And so they do the very logical thing\nof keeping the memory representation of all objects the same. So all objects have a header, they have like payload data.\nAnd what this means is that every time you pass around an object, you're passing around a pointer to the data.\nWell, this has overhead, right? Turns out that modern computers don't like chasing pointers very much and things like this.\nIt means that you have to allocate the data. It means you have to reference count it, which is another way that Python uses\nto keep track of memory. And so this has a lot of overhead. And so if you say, okay,\nlet's try to get that out of the heap, out of a box, out of an indirection and into the registers,\nthat's another 10x, more. - So it adds up if you're reference counting every single- - Absolutely. - every single thing you create, that adds up.\n- Yep, and if you look at, you know, people complain about the Python GIL, this is one of the things that hurts parallelism.\nThat's because the reference counting, right? And so the GIL and reference counting are very tightly intertwined in Python.\nIt's not the only thing, but it's very tightly intertwined. And so then you lean into this and you say, okay, cool. Well, modern computers,\nthey can do more than one operation at a time. And so they have vectors. What is a vector? Well, a vector allows you to,\ninstead of taking one piece of data, doing an add or multiply and then pick up the next one, you can now do a 4, 8, or 16 or 32 at a time, right?\nWell, Python doesn't expose that because of reasons. And so now you can say, okay, well, you can adopt that.\nNow you have threads. Now you have like additional things, like you can control memory hierarchy. And so what Mojo allows you to do is it allows you\nto start taking advantage of all these powerful things that have been built into the hardware over time.\nThe library gives very nice features. So you can say, just parallelize this. Do this in parallel, right?\nSo it's very, very powerful weapons against slowness, which is why people have been, I think having fun,\nlike just taking code and making go fast because it's just kind of an adrenaline rush to see like how fast you can get things.\n"}
{"pod": "Lex Fridman Podcast", "input": "Typed programming languages", "output": "- Before I talk about some of the interesting stuff with parallelization and all that, let's first talk about, like, the basics.\nWe talked the indentation, right? So this thing looks like Python. It's sexy and beautiful like Python as I mentioned.\n- [Chris] Yep. - Is it a typed language? So what's the role of types? - Yeah, good question. So Python has types.\nIt has strings, it has integers, it has dictionaries and like all that stuff, but they all live at runtime, right?\nAnd so because all those types live at runtime in Python, you never or you don't have to spell them. (chuckles)\nPython also has like this whole typing thing going on now and a lot of people use it. - [Lex] Yeah. - I'm not talking about that.\nThat's kind of a different thing. We can go back to that if you want, but typically the,\nyou know, you just say, I have a def and my def takes two parameters. I'm gonna call them A and B and I don't have to write or type okay?\nSo that is great, but what that does is that forces what's called a consistent representation.\nSo these things have to be a pointer to an object with the object header and they all have to look the same.\nAnd then when you dispatch a method, you go through all the same different paths no matter what the receiver, whatever that type is.\nSo what Mojo does is it allows you to have more than one kind of type. And so what it does is allows you to say, okay, cool.\nI have an object and objects behave like Python does. And so it's fully dynamic and that's all great. And for many things, classes, like,\nthat's all very powerful and very important. But if you wanna say, hey, it's an integer and it's 32 bits,\nor it's 64 bits or whatever it is, or it's a floating point value and it's 64 bits,\nwell, then the compiler can take that, and it can use that to do way better optimization. And it turns out, again,\ngetting rid of the indirections, that's huge. Means you can get better code completion\n'cause compiler knows what the type is and so it knows what operations work on it. And so that's actually pretty huge.\nAnd so what Mojo does is allows you to progressively adopt types into your program.\nAnd so you can start, again, it's compatible with Python, and so then you can add however many types you want,\nwherever you want them. And if you don't wanna deal with it, you don't have to deal with it, right? And so one of, you know, our opinions on this, (chuckles)\nit's that it's not that types are the right thing or the wrong thing, it's that they're a useful thing.\n- So it's kind of optional, it's not strict typing, like, you don't have to specify type. - [Chris] Exactly. - Okay, so it's starting from the thing\nthat Python's kinda reaching towards right now with trying to inject types into it,\nwhat it's doing. - Yeah, with a very different approach, but yes, yeah. - So what's the different approach? I'm actually one of the people (sighs)\nthat have not been using types very much in Python. So I haven't- - That's okay. Why did you sigh?\n- It just, well, because I know the importance. It's like adults use strict typing.\nAnd so I refuse to grow up in that sense. It's a kind of rebellion, but I just know that it probably reduces\nthe amount of errors, even just for, forget about performance improvements, it probably reduces errors of when you do strict typing.\n- Yeah, so I mean, I think it's interesting if you look at that, right? And the reason I'm giving you a hard time then is that- - Yes.\n- there's this cultural norm, this pressure, this, like, there has to be a right way to do things.\nLike, you know- - Yes. - grownups only do it one way. And if you don't do that- - Yes. - you should feel bad, right? - Yes. - Like, some people feel like Python's a guilty pleasure\nor something, and that's like, when it gets serious, I need to go rewrite it, right? Well, I mean, cool. - Exactly.\n- I understand history and I understand kinda where this comes from, but I don't think it has to be a guilty pleasure, (chuckles) right? - Yeah.\n- So if you look at that, you say, why do you have to rewrite it? Well, you have to rewrite it to deploy. Well, why do you wanna deploy?\nWell, you care about performance, or you care about predictability, or you want, you know, a tiny thing on the server that has no dependencies, or,\nyou know, you have objectives that you're trying to attain. So what if Python can achieve those objectives?\nSo if you want types, well, maybe you want types because you wanna make sure you're passing the right thing. Sure, you can add a type.\nIf you don't care, you're prototyping some stuff, you're hacking some things out, you're, like, pulling some random code off the internet,\nit should just work, (chuckles) right? And you shouldn't be, like, pressured. You shouldn't feel bad about doing the right thing\nor the thing that feels good. Now, if you're in a team, right, you're working at some massive internet company\nand you have 400 million lines of Python code, well, they may have a house rule that you use types,\nright? - Yeah. - Because it makes it easier for different humans to talk to each other and understand what's going on and bugs at scale, right?\nAnd so there are lots of good reasons why you might wanna use types, but that doesn't mean that everybody\nshould use 'em all the time, right? So what Mojo does is it says, cool. Well, allow people to use types and if you use types,\nyou get nice things out of it, right? You get better performance and things like this, right? But Mojo is a full, compatible superset of Python, right?\nAnd so that means it has to work without types. (chuckles) It has to support all the dynamic things. It has to support all the packages.\nIt has to support for comprehension, list comprehensions and things like this, right?\nAnd so that starting point I think is really important. And I think that, again,\nyou can look at why I care so much about this. And there's many different aspects of that, one of which is the world went through a very challenging\nmigration from Python 2 to Python 3, right? - [Lex] Yes. - This migration took many years\nand it was very painful for many teams, right? - Yeah. - And there's of a lot of things that went on in that.\nI'm not an expert in all the details and I honestly don't wanna be. I don't want the world to have to go through that, (chuckles) right? - Yeah.\n- And, you know, people can ignore Mojo. And if it's not their thing, that's cool. But if they wanna use Mojo, I don't want them to have to rewrite all their code.\n- Yeah, I mean, this, okay, the superset part is just, I mean, there's so much brilliant stuff here.\nThat definitely is incredible. We'll talk about that. - Yeah. - First of all,\nhow's the typing implemented differently in Python versus Mojo? - Yeah.\n- So this heterogeneous flexibility you said is definitely implemented. - Yeah, so I'm not a full expert\n(chuckles) in the whole backstory on types in Python. So I'll give you that. I can give you my understanding.\nMy understanding is, basically, like many dynamic languages, the ecosystem went through a phase\nwhere people went from writing scripts to writing large scale, huge code bases in Python.\nAnd at scale, kinda helps have types. - Yeah. - People wanna be able to reason about interfaces,\ndo you expect string, or an int, or, like, these basic things, right? And so what the Python community started doing is\nit started saying, okay, let's have tools on the side, checker tools, right, that go and, like,\nenforce a variance, check for bugs, try to identify things. These are called static analysis tools generally.\nAnd so these tools run over your code and try to look for bugs. What ended up happening is there's so many of these things, so many different weird patterns and different approaches\non specifying the types and different things going on, that the Python community realized and recognized, \"Hey, hey, hey, there's the thing here.\" (chuckles)\nAnd so what they started to do is they started to standardize the syntax for adding types to Python. Now, one of the challenges that they had is\nthat they're coming from kinda this fragmented world where there's lots of different tools, they have different trade-offs and interpretations\nand the types mean different things. And so if you look at types in Python, according to the Python spec, the types are ignored, right?\nSo according to the Python spec, you can write pretty much anything (chuckles) in a type position, okay?\nTechnically, you can write any expression, okay? Now, that's beautiful because you can extend it.\nYou can do cool things, you can write, build your own tools, you can build your own house, linter or something like that, right?\nBut it's also a problem because any existing Python program may be using different tools\nand they have different interpretations. And so if you adopt somebody's package into your ecosystem, try to run the tool you prefer,\nit may throw out tons of weird errors and warnings and problems just because it's incompatible with how these things work.\nAlso because they're added late and they're not checked by the Python interpret, it's always kinda more of a hint that it is a requirement.\nAlso, the CPython implementation can't use 'em for performance. And so it's really- - I mean, that's a big one, right?\nSo you can't utilize for the compilation, for the just-in-time compilation, okay. - Yep, yep, exactly. And this all comes back to the design principle of,\nthey're kinda hints, they're kind of, the definition's a little bit murky. It's unclear exactly the interpretation in a bunch of cases.\nAnd so because of that, you can't actually, even if you want to, it's really difficult to use them to say,\nlike, it is going to be an int, and if it's not, it's a problem, right? A lot of code would break if you did that, so.\nSo in Mojo, right, so you can still use those kind of type annotations, it's fine. But in Mojo, if you declare a type and you use it,\nthen it means it is going to be that type. And the compiler helps you check that, and enforce it and it's safe\nand it's not a, like, best-effort hint kind of a thing. - So if you try to shove a string type thing into a integer-\n- [Chris] You get an error from the compiler. - From the compiler compile time. Nice, okay.\nWhat kinda basic types are there? - Yeah. So Mojo is pretty hardcore in terms of what it tries to do\nin the language, which is the philosophy there is that we,\nagain, if you look at Python, right, Python's a beautiful language because it's so extensible, right? And so all of the different things in Python,\nlike for loops and plus and like all these things can be accessed through these underbar armbar methods, okay?\nSo you have to say, okay, if I make something that is super fast, I can go all the way down to the metal.\nWhy do I need to have integers built into the language, right? And so what Mojo does is it says, okay,\nwell, we can have this notion of structs. So you have classes in Python. Now you can have structs.\nClasses are dynamic, structs are static. Cool. We can get high performance. We can write C++ kind of code with structs if you want.\nThese things mix and work beautifully together, but what that means is that you can go and implement strings and ints and floats and arrays\nand all that kinda stuff in the language, right? And so that's really cool because, you know,\nto me as a idealizing compiler language type of person,\nwhat I wanna do is I wanna get magic out of the compiler and put in the libraries. Because if somebody can, you know,\nif we can build an integer that's beautiful and it has an amazing API and it does all the things you'd expect an integer to do, we don't like it,\nmaybe you want a big integer, maybe you want, like, sideways integer, I don't know, like what all the space of integers are,\nthen you can do that, and it's not a second class citizen. And so if you look at certain other languages,\nlike C++, one I also love and use a lot, int is hardcoded in the language,\nbut complex is not. And so isn't it kinda weird that, you know, you have this STD complex class, but you have int,\nand complex tries to look like a natural numeric type and things like this. But integers and floating point have these, like,\nspecial promotion rules and other things like that, that are magic and they're hacked into the compiler. And because of that, you can't actually make something\nthat works like the built-in types. - Is there something provided as a standard because,\nyou know, because it's AI first, you know, numerical types are so important here.\nSo is there something, like a nice standard implementation of indigent flows? - Yeah, so we're still building all that stuff out.\nSo we provide integers and floats and all that kinda stuff. We also provide like buffers and tensors and things like that you'd expect in an ML context.\nHonestly, we need to keep designing and redesigning and working with the community to build that out and make that better. That's not our strength right now.\nGive us six months or a year and I think it'll be way better, but the power of putting in the library means\nthat we can have teams of experts that aren't compiler engineers that can help us design and refine and drive this forward.\n- So one of the exciting things we should mention here is that this is new and fresh.\nThis cake is unbaked. It's almost baked. You can tell it's delicious,\nbut it's not fully ready to be consumed. - Yep. That's very fair. It is very useful, but it's very useful if you're\na super low-level programmer right now. And what we're doing is we're working our way up the stack. And so the way I would look at Mojo today\nin May and 2023 is that it's like a 0.1.\nSo I think that, you know, a year from now, it's gonna be way more interesting to a variety of people.\nBut what we're doing is we decide to release it early so that people can get access to it and play with it. We can build it with the community.\nWe have a big roadmap, fully published, being transparent about this\nand a lot of people are involved in this stuff. And so what we're doing is we're really optimizing for building this thing the right way.\nAnd building it the right way is kind of interesting, working with the community, because everybody wants it yesterday.\nAnd so sometimes it's kind of, you know, there's some dynamics there, but I think- - Yeah.\n- it's the right thing. - So there's a Discord also. So the dynamics is pretty interesting. - [Chris] Yeah.\n- Sometimes the community probably can be very chaotic and introduce a lot of stress.\nGuido famously quit over the stress of the Walrus operator. I mean, it's, you know- - Yeah, yeah. - It broke...\n- [Chris] Straw that broke the camel's back. - Exactly. And so, like, it could be very stressful to develop, but can you just add a tangent upon a tangent?\nIs it stressful to work through the design of various features here,\ngiven that the community is recently involved? - Well, so I've been doing open development\nand community stuff for decades now. (chuckles) Somehow this has happened to me. So I've learned some tricks,\nbut the thing that always gets me is I wanna make people happy, right? And so maybe not all people all happy all the time,\nbut generally, - Yeah. - I want people to be happy, right? And so the challenge is that again, we're tapping into some long,\nsome deep seated long tensions and pressures both in the Python world, but also in the AI world,\nin the hardware world and things like this. And so people just want us to move faster, right? And so again, our decision was, \"Let's release this early.\nLet's get people used to it or access to it and play with it. And like, let's build in the open,\"\nwhich we could have, you know, had the language monk sitting in the cloister up on the hilltop,\nlike beavering away trying to build something. But in my experience, you get something that's way better if you work with the community, right?\nAnd so, yes, it can be frustrating, can be challenging for lots of people involved. And, you know, if you, I mean, you mentioned our Discord.\nWe have over 10,000 people on the Discord, 11,000 people or something. Keep in mind we released Mojo like two weeks ago.\n(chuckles) Yeah. So- - It's very active. - So it's very cool, but what that means is that, you know, 10,\n11,000 people all will want something different, right? And so what we've done is we've tried to say,\nOkay, cool. Here's our roadmap. And the roadmap isn't completely arbitrary.\nIt's based on here's the logical order in which to build these features or add these capabilities and things like that.\nAnd what we've done is we've spun really fast on like bug fixes. And so we actually have very few bugs, which is cool,\nI mean, actually for a project in this state, but then what we're doing is we're dropping in features very deliberately.\n- I mean, this is fun to watch 'cause you got the two gigantic communities of, like, hardware, like systems engineers,\nand then you have the machine learning Python people that are like higher level.\n- [Chris] Yeah. - And it's just two, like, army, like- - They've both, they've been at war, yeah.\n(Lex chuckling) They've been at war, right? And so here's- - [Lex] It's a Tolkien novel or something. Okay. - Well, so here's a test.\nAnd again, like, it's super funny for something that's only been out for two weeks, right? People are so impatient, right?\nBut, okay, cool, let's fast forward a year. Like, in a year's time, Mojo will be actually quite amazing\nand solve tons of problems and be very good. People still have these problems, right?\nAnd so you look at this and you say, and the way I look at this at least is to say, okay, well, we're solving big, long-standing problems.\nTo me, again, working on many different problems, I wanna make sure we do it right, right? There's like a responsibility you feel\nbecause if you mess it up, (chuckles) right, there's very few opportunities to do projects like this and have them really have impact on the world.\nIf we do it right, then maybe we can take those feuding armies and actually heal some of those wounds,\nright? - Yeah. - This feels like a speech by George Washington or Abraham Lincoln or something.\n- And you look at this and it's like, okay, well, how different are we? - [Lex] Yeah. - We all want beautiful things. We all want something that's nice.\nWe all wanna be able to work together. We all want our stuff to be used, right? And so if we can help heal that, now I'm not optimistic that all people\nwill use Mojo and they'll stop using C++, like, that's not my goal, (chuckles) right, but if we can heal some of that,\nI think that'd be pretty cool. That'd be nice. - Yeah, and we start by putting the people who like braces into the Gulag, no. (chuckles)\n- So there are proposals for adding braces to Mojo and we just we tell them no. - Oh, interesting. - Oh, okay, (laughs) (Chris laughing)\npolitely, yeah, anyway. So there's a lot of amazing features on the roadmap and those already implemented, it'd be awesome\n"}
{"pod": "Lex Fridman Podcast", "input": "Immutability", "output": "if I could just ask you a few things. So- - Yeah, go for it. - So the other performance improvement\ncomes from immutability. So what's this var and this let thing that we got going on?\nAnd what's immutability? - Well, so... - Yeah, so one of the things that is useful,\nand it's not always required, but it's useful, is knowing whether something can change out from underneath you, right?\nAnd so in Python, you have a pointer to an array, right? And so you pass that pointer to an array around to things.\nIf you pass into a function, they may take that and scroll away in some other data structure. And so you get your array back and you go to use it.\nAnd now somebody else is like putting stuff in your array. How do you reason about that? - Yeah. - It gets to be very complicated\nand leads to lots of bugs, right? And so one of the things that, you know, again, this is not something Mojo forces on you,\nbut something that Mojo enables is this thing called value semantics. And what value semantics do is they take collections,\nlike array, like dictionaries, also tensors and strings and things like this that are\nmuch higher level and make them behave like proper values. And so it makes it look like,\nif you pass these things around, you get a logical copy of all the data. And so if I pass you an array, it's your array.\nYou can go do what you want to it, you're not gonna hurt my array. Now that is an interesting and very powerful design principle.\nIt defines away a ton of bugs. You have to be careful to implement it in an efficient way. - Yeah, is there a performance hit that's significant?\n- Generally not if you implement it the right way, but it requires a lot of very low-level getting-the-language-right bits.\n- I assume that'd be a huge performance hit 'cause the benefit is really nice 'cause you don't get into these-\n- Absolutely. Well, the trick is you can't do copies. So you have to provide the behavior\nof copying without doing the copy. - [Lex] Yeah. How do you do that? (Chris laughing)\nHow do you do that? - It's not magic. It's just- - Okay. - It's actually pretty cool. Well, so first, before we talk about how that works,\nlet's talk about how it works in Python, right? So in Python you define a person class, or maybe a person class is a bad idea.\nYou define a database class, right? And a database class has an array of records, something like that, right? And so the problem is,\nis that if you pass in a record or a class instance into the database, it'll take a hold of that object\nand then it assumes it has it. And if you're passing an object in, you have to know that that database is gonna take it,\nand therefore you shouldn't change it after you put it in the database, right? This is- - You just kinda have to know that. - You just have to kinda know that, right?\nAnd so you roll out version one of the database. You just kinda have to know that. Of course, Lex uses his own database, right?\n- [Lex] Yeah. - Right, 'cause you built it, you understand how this works, right? Somebody else joins the team, they don't know this, right? - Yes.\n- And so now they suddenly get bugs, you're having to maintain the database, you shake your fist, you argue.\nThe 10th time this happens, you're like, okay, we have to do something different, right? And so what you do is you go change your Python code\nand you change your database class to copy the record every time you add it. And so what ends up happening is you say, okay,\nI will do what's called a defensive copy inside the database. And then that way if somebody passes something in,\nI will have my own copy of it and they can go do whatever and they're not gonna break my thing, (chuckles) okay?\nThis is usually the two design patterns. If you look in PyTorch, for example, this is cloning a tensor.\nLike, there's a specific thing and you have to know where to call it. And if you don't call it in the right place, you get these bugs and this is state-of-the-art, right?\nSo a different approach, so it's used in many languages, so I've worked with it in Swift, is you say, okay,\nwell, let's provide value semantics. And so we wanna provide the view that you get a logically independent copy, but we wanna do that lazily.\nAnd so what what we do is we say, okay, if you pass something into a function, it doesn't actually make a copy.\nWhat it actually does is it just increments a reference to it. And if you pass it around, you stick in your database,\nit can go into the database, you own it. And then you come back outta the stack, nobody's copied anything, you come back outta the stack,\nand then the caller let's go of it. Well, then you've just handed it off to the database,\nyou've transferred it and there's no copies made. Now, on the other hand, if, you know,\nyour coworker goes and hands you a record and you pass it in, you stick it in the database, and then you go to town and you start modifying it,\nwhat happens is you get a copy lazily on demand. And so what this does,\nthis gives you copies only when you need them. So it defines the way the bugs, but it also generally reduces\nthe number of copies in practice. And so it's- - But the implementation details are tricky here, I assume. - Yes, yes.\n- Something with reference counting, but to make it performant across a number of different kinds of objects?\n- Yeah. Well, so you need a couple of things. So this concept has existed in many different worlds.\nAnd so it's, again, it's not novel research at all, right? The magic is getting the design right\nso that you can do this in a reasonable way, right? And so there's a number of components that go into this. One is when you're passing around,\nso we're talking about Python and reference counting and the expense of doing that. When you're passing values around,\nyou don't wanna do extra reference counting for no good reason. And so you have to make sure that you're efficient and you transfer ownership instead of duplicating references\nand things like that, which is a very low-level problem. You also have to adopt this,\nand you have to build these data structures. And so if you say, you know, Mojo has to be compatible with Python,\nso of course the default list is a reference semantic list that works the way you'd expect in Python,\nbut then you have to design a value semantic list. And so you just have to implement that, and then you implement the logic within.\nAnd so the role of the language here is to provide all the low-level hooks that allow the author of the type\nto be able to get and express this behavior without forcing it into all cases or hard coding this into the language itself.\n- But there's ownership? So you're constantly transferring, you're tracking who owns the thing. - Yes. And so there's a whole system called ownership.\nAnd so this is related to work done in the Rust community. Also, the Swift community has done a bunch of work\nand there's a bunch of different other languages that have all kind of... C++ actually has copy constructors and destructors and things like that.\nAnd so, and I mean, C++ has everything. So it has move constructors and has like this whole world of things.\nAnd so this is a body of work that's kind of been developing for many, many years now.\nAnd so Mojo takes some of the best ideas out of all these systems and then remixes in a nice way\nso that you get the power of something like the Rust programming language, but you don't have to deal with it when you don't want to,\nwhich is a major thing in terms of teaching and learning and being able to use and scale these systems.\n- How does that play with argument conventions? What are they? Why are they important? How does the value semantics,\nhow does the transfer ownership work with the arguments when they're passing definitions? - Yeah. So if you go deep into systems programming land,\nso this isn't, again, this is not something for everybody, but if you go deep into systems programming land, what you encounter is you encounter\nthese types that get weird. (chuckles) So if you're used to Python, you think about everything.\nI can just copy it around. I can go change it and mutate it and do these things and it's all cool.\nIf you get into systems programming land, you get into these things, like, I have an atomic number, or I have a mutex,\nor I have a uniquely owned database handle, things like this, right?\nSo these types, you can't necessarily copy. Sometimes you can't necessarily even move them to a different address.\nAnd so what Mojo allows you to do is it allows you to express, hey, I don't wanna get a copy of this thing.\nI wanna actually just get a reference to it. And by doing that, what you can say is, you can say, okay, if I'm defining something weird like a, you know,\natomic number or something, it's like, it has to be... So an atomic number is an area in memory\nthat multiple threads can access at a time without synchronous, without locks, right?\nAnd so, like the definition of atomic numbers, multiple different things have to be poking at that, therefore they have to agree on where it is,\n(chuckles) right? So you can't just like move it out from underneath one because it kinda breaks what it means.\nAnd so that's an example of a type that you can't copy, you can't move it. Like, once you create, it has to be where it was, right?\nNow, if you look at many other examples, like a database handle, right, so, okay, well, what happens?\nHow do you copy a database handle? Do you copy the whole database? That's not something you necessarily wanna do.\nThere's a lot of types like that where you wanna be able to say that they are uniquely owned.\nSo there's always one of this thing, or if I create a thing, I don't copy it.\nAnd so what Mojo allows you to do is it allows you to say, Hey, I wanna pass around in reference to this thing without copying it, and so it has borrowed conventions.\nSo you can say, you can use it, but you don't get to change it. You can pass it by mutable reference.\nAnd so if you do that, then you get a reference to it, but you can change it. And so it manages all that kinda stuff.\n- So it's just a really nice implementation of, like, C++ has- - Yeah.\n- you know, different kinds of pointers. - Reference, yeah, has pointers. - Smart, smart, different kinds of implementations of smart pointers\nthat you can- - Yeah. - explicitly define, this allows you, but you're saying that's more like the weird case\nversus the common case? - Well, it depends on where, I mean, I don't think I'm a normal person,\nso. - Yes. - I mean, I'm not one to call other people weird. - [Lex] Yeah. (Chris chuckling)\nBut, you know, if you talk to a typical Python programmer, you're typically not thinking about this, right? This is a lower level of abstraction.\nNow, certainly if you talk to a C++ programmer, certainly if you talk to a Rust programmer, again, they're not weird, they're delightful.\nLike, these are all good people, right? Those folks will think about all the time, right?\nAnd so I look at this as, there's a spectrum between very deep, low-level systems, I'm gonna go poke the bits\nand care about how they're laid out in memory, all the way up to application and scripting and other things like this.\nAnd so it's not that anybody's right or wrong, it's about how do we build one system that scales?\n- By the way, the idea of an atomic number has been something that always brought me deep happiness,\nbecause the flip side of that, the idea that threads can just modify stuff\nasynchronously, just the whole idea of concurrent programming is a source of infinite distrust for me.\n- Well, so this is where you jump into, you know, again, you zoom out and get out of program languages or compilers\nand you just look at what the industry has done, my mind is constantly blown by this, right? And you look at what, you know, Moore's law,\nMoore's Law is this idea that, like computers, for a long time, single thread performance just got faster and faster and faster and faster for free.\nBut then physics and other things intervened, and power consumption, like other things started to matter.\nAnd so what ended up happening is we went from single core computers to multi-core, then we went to accelerators, right?\nAnd this trend towards specialization of hardware is only gonna continue. And so for years,\nus programming language nerds and compiler people have been saying, okay, well, how do we tackle multi-core, right?\nFor a while it was like, \"Multi-core is the future. We have to get on top of this thing.\" And then it was multi-core is the default. \"What are we doing with this thing?\"\nAnd then it's like, there's chips with hundreds of cores in them. (chuckles) What will happen, right? - Yeah.\n- And so I'm super inspired by the fact that, you know, in the face of this, you know,\nthose machine learning people invented this idea of a tensor, right? And what's a tensor?\nA tensor is like an arithmetic and algebraic concept. It's like an abstraction\naround a gigantic parallelizable dataset, right? And because of that and because of things\nlike TensorFlow and PyTorch, we're able to say, okay, we'll express the math of the system.\nThis enables you to do automatic differentiations, enables you to do like all these cool things.\nAnd it's an abstracted representation. Well, because you have that abstract representation, you can now map it onto these parallel machines\nwithout having to control, okay, put that bite here, put that bite there, put that bite there. And this has enabled an explosion in terms of AI,\ncompute, accelerators, like all the stuff. And so that's super, super exciting. - What about the deployment\n"}
{"pod": "Lex Fridman Podcast", "input": "Distributed deployment", "output": "and the execution across multiple machines? - [Chris] Yeah. - So you write that the Modular compute platform\ndynamically partitions models with billions of parameters and distributes their execution across multiple machines,\nenabling unparalleled efficiency. By the way, the use of unparalleled in that sentence...\nAnyway. (Chris chuckling) Enabling unparalleled efficiency, scale, and the reliability for the largest workloads. So how do you do this\nabstraction of distributed Deployment of large models?\n- Yeah, so one of the really interesting tensions, so there's a whole bunch of stuff that goes into that. I'll pick a random walk through it.\nIf you go back and replay the history of machine learning, right, I mean, the brief, most recent history of machine learning,\n'cause this is, as you know, very deep. - [Lex] Yeah. - I knew Lex when he had an AI podcast.\n- [Lex] Yes. (Chris chuckling) - [Chris] Right? - Yeah, (chuckles) yeah.\n- So if you look at just TensorFlow and PyTorch, which is pretty recent history in the big picture, right, but TensorFlow is all about graphs.\nPyTorch, I think pretty unarguably ended up winning. And why did it win? Mostly because the usability, right?\nAnd the usability of PyTorch is I think, huge. And I think, again, that's a huge testament to the power of taking abstract,\ntheoretical technical concepts and bringing it to the masses, right? Now the challenge with what the TensorFlow\nversus the PyTorch design points was that TensorFlow's kinda difficult to use for researchers,\nbut it was actually pretty good for deployment. PyTorch is really good for researchers. It kind of not super great for deployment, right?\nAnd so I think that we as an industry have been struggling. And if you look at what deploying a machine learning model\ntoday means is that you'll have researchers who are, I mean, wicked smart, of course, but they're wicked smart at model architecture\nand data and calculus and (chuckles) like all, like, they're wicked smart in various domains.\nThey don't wanna know anything about the hardware deployment or C++ or things like this, right? And so what's happened is you get people who train the model,\nthey throw it over the fence, and then you have people that try to deploy the model.\nWell, every time you have a Team A does x, they throw it over the fence,\nTeam B does y, like you have a problem, because of course it never works the first time. And so you throw over the fence, they figure out, okay,\nit's too slow, won't fit, doesn't use the right operator, the tool crashes, whatever the problem is,\nthen they have to throw it back over the fence. And every time you throw a thing over a fence, it takes three weeks of project managers\nand meetings and things like this. And so what we've seen today is that getting models in production can take weeks or months.\nLike, it's not atypical, right? I talk to lots of people and you talk about, like VP of software at some internet company\ntrying to deploy a model, and they're like, why do I need a team of 45 people? (chuckles) Like, it's so easy trying to model.\nWhy can't I deploy it, right? And if you dig into this, every layer is problematic.\nSo if you look at the language piece, I mean, this is tip of the iceberg. It's a very exciting tip of the iceberg for folks,\nbut you've got Python on one side and C++ on the other side. Python doesn't really deploy.\nI mean, can theoretically, technically in some cases, but often a lot of production teams will wanna get things out of Python\nbecause they get better performance and control and whatever else. So Mojo can help with that.\nIf you look at serving, so you talk about gigantic models, well, a gigantic model won't fit on one machine, right?\nAnd so now you have this model, it's written Python, it has to be rewritten in C++.\nNow it also has to be carved up so that half of it runs on one machine, half of it runs on another machine, or maybe it runs on 10 machines.\nWell, so now, suddenly, the complexity is exploding, right? And the reason for this is\nthat if you look into TensorFlow or PyTorch, these systems, they weren't really designed for this world, right?\nThey were designed for, you know, back in the day when we were starting and doing things where it was a different, much simpler world,\nlike you wanted to run ResNet-50 or some ancient model architecture like this. It was a completely different world than-\n- Trained on one GPU. - [Chris] Exactly. AlexNet. - Doing it on one GPU. (chuckles) - Yeah, AlexNet, right, the major breakthrough,\nand the world has changed, right? And so now the challenge is, is that TensorFlow, PyTorch, these systems, they weren't actually designed for LLMs,\nlike, that was not a thing. And so where TensorFlow actually has amazing power in terms of scale and deployment and things like that,\nand I think Google is, I mean, maybe not unmatched, but they're, like, incredible, in terms of their capabilities and gigantic scale,\nmany researchers using PyTorch, right? And so PyTorch doesn't have those same capabilities.\nAnd so what Modular can do is it can help with that. Now, if you take a step back and you say like, what is Modular doing, right?\nSo Modular has like a bitter enemy that we're fighting against in the industry.\nAnd it's one of these things where everybody knows it, but nobody is usually willing to talk about it.\n- The bitter enemy. - The bitter thing that we have to destroy that we're all struggling with and it's like all around,\nit's like fish can't see water, it's complexity. - Sure, yes. It's complexity.\n- [Chris] Right? - That was very philosophical, (Chris chuckling) Very well said. - [Chris] And so if you look at it, yes, it is on the hardware side.\n- Yes. - All these accelerators, all these software stack that go with the accelerator, all these, like, there's massive complexity over there.\nYou look at what's happening on the modeling side, massive amount of complexity. Like, things are changing all the time.\nPeople are inventing. Turns out the research is not done, (chuckles) right? And so people wanna be able to move fast.\nTransformers are amazing, but there's a ton of diversity even within transformers, and what's the next transformer, right?\nAnd you look into serving. Also, huge amounts of complexity. It turns out that all the cloud providers, right,\nhave all their very weird but very cool hardware for networking and all this kinda stuff. And it's all very complicated. People aren't using that.\nYou look at classical serving, right, there's this whole world of people who know how to write\nhigh-performance servers with zero-copy networking and, like, all this fancy asynchronous I/O,\nand, like, all these fancy things in the serving community, very little that has pervaded\ninto the machine learning world, right? And why is that? Well, it's because, again, these systems have been built up over many years.\nThey haven't been rethought, there hasn't been a first principles approach to this. And so what Modular's doing is we're saying, \"Okay,\nwe've built many of these things, right?\" So I've worked on TensorFlow and TPUs and things like that. Other folks on our team have, like, worked on PyTorch Core.\nWe've worked on ONNX one time. We've worked on many of these other systems. And so built systems like the Apple accelerators\nand all that kinda stuff, like our team is quite amazing. And so one of the things that roughly everybody\nat Modular's grumpy about is that when you're working on one of these projects, you have a first order goal:\nGet the hardware to work. Get the system to enable one more model. Get this product out the door. Enable the specific workload,\nor make it solve this problem for this product team, right? And nobody's been given a chance\nto actually do that step back. And so we, as an industry, we didn't take two steps forward. We took like 18 steps forward\nin terms of all this really cool technology across compilers and systems and runtimes and heterogeneous computing, like all this kinda stuff.\nAnd like, all this technology has been, you know, I wouldn't say beautifully designed, but it's been proven in different quadrants.\nLike, you know, you look at Google with TPUs, massive, huge exif flops of compute strapped together\ninto machines that researchers are programming in Python in a notebook. That's huge. That's amazing. - That's amazing.\nThat's incredible. - Right, it's incredible. And so you look at the technology that goes into that, and the algorithms are actually quite general.\nAnd so lots of other hardware out there and lots of other teams out there don't have the sophistication or the, maybe the years working on it,\nor the budget, or whatever that Google does, right? And so they should be getting access to the same algorithms,\nbut they just don't have that, right? And so what Modular's doing, so we're saying, \"Cool, this is not research anymore.\"\nLike, we've built auto-tuning in many systems. We've built programming languages, right? And so like, have implemented C++, have implemented Swift,\nhave implemented many of these things. And so, you know, it's hard, but it's not research.\nAnd you look at accelerators. Well, we know there's a bunch of different, weird kind of accelerators, but they actually cluster together, right?\nAnd you look at GPUs. Well, there's a couple of major vendors of GPUs and they maybe don't always get along,\nbut their architectures are very similar. You look at CPUs. CPUs are still super important for the deployment side of things.\nAnd you see new architectures coming out from all the cloud providers and things like this, and they're all super important to the world, right,\nbut they don't have the 30 years of development that the entrenched people do, right? And so what Modular can do is we're saying,\n\"Okay, all this complexity, like, it's not bad complexity, it's actually innovation, (chuckles) right?\"\nAnd so it's innovation that's happening and it's for good reasons, but I have sympathy for the poor software people, right?\nI mean, again, I'm a generally software person too. I love hardware, but software people wanna build applications and products\nand solutions that scale over many years. They don't wanna build a solution for one generation\nof hardware with one vendor's tools, right? And because of this, they need something that scales with them.\nThey need something that works on cloud and mobile, right, because, you know, their product manager said, Hey,\nI want it to have lower latency and it's better for personalization, or whatever they decide, right?\nProducts evolve. And so the challenge with the machine learning technology and the infrastructure we have today in the industry\nis that it's all these point solutions. And because there are all these point solutions, it means that as your product evolves,\nyou have to like switch different technology stacks or switch to a different vendor. And what that does is that slows down progress.\n- So basically a lot of the things we've developed in those little silos for machine learning tasks,\nyou want to make that the first class citizen of a general purpose programming language that can then be compiled\nacross all these kinds of hardware. - Well, so it's not really about a programming language. I mean, the programming language is a component of the mission, right?\nAnd the mission is, or not literal, but our joking mission is \"to save the world from terrible AI software.\"\n- [Lex] Excellent. I love it. - Okay? (chuckles) - So, you know, if you look at this mission,\nyou need a syntax. So yeah, you need programming language, right? And like, we wouldn't have to build the programming language\nif one existed, right? So if Python was already good enough, then cool, we would've just used it, right? We're not just doing very large scale,\nexpensive engineering projects for the sake of it, like, it's to solve a problem, right? It's also about accelerators.\nIt's also about exotic numerics and bfloat16 and matrix multiplication and convolutions\nand like, this kinda stuff. Within the stack, there are things like kernel fusion.\nThat's a esoteric but really important thing that leads to much better performance and much more general research hackability together, right?\n- And that's enabled by the ASICs. That's enabled by certain hardware. So it's like- - Well. - Where's the dance between,\nI mean, there's several questions here. Like, how do you add- - Yep. - a piece of hardware to the stack if a new piece of- - Yeah. - like if I have this genius invention\nof a specialized accelerator- - Yeah. - how do I add that to the Modular framework? And also how does Modular as a standard\nstart to define the kinds of hardware that should be developed? - Yeah, so let me take a step back\nand talk about status quo, okay? - Yes. - And so if you go back to TensorFlow 1, PyTorch 1,\nthis kinda timeframe, and these have all evolved and gone way more complicated. So let's go back to the glorious simple days, right?\nThese things basically were CPUs and CUDA. And so what you do is you say, go do a dense layer.\nAnd a dense layer has a matrix multiplication in it, right? And so when you say that, you say, go do this big operation, a matrix multiplication,\nand if it's on a GPU, kick off a CUDA kernel. If it's on a CPU, go do like an Intel algorithm,\nor something like that with an Intel MKL, okay? Now that's really cool if you're either Nvidia or Intel, right?\nBut then more hardware comes in, right? And on one access, you have more hardware coming in.\nOn the other hand, you have an explosion of innovation in AI. And so what happened with both TensorFlow and PyTorch is\nthat the explosion of innovation in AI has led to, it's not just about matrix multiplication and convolution.\nThese things have now like 2,000 different operators. And on the other hand, you have, I don't know how many pieces of hardware\nthere are out there. It's a lot, (chuckles) okay? It's not even hundreds. It's probably thousands, okay?\nAnd across all of edge and across like, all the different things- - That are used at scale.\n- [Chris] Yeah, exactly. I mean- - Also it's not just like a handful. - AI's everywhere. Yeah. - It's not a handful of TPU alternatives.\nIt's- - Correct. It's every phone, often with many different chips inside of it- - Right.\n- from different vendors from... - Right. - Like, AI is everywhere. It's a thing, right?\n- Why are they all making their own chips? Like, why is everybody making their own thing? - [Chris] Well, so-\n- Is that a good thing, first of all? - So Chris's philosophy on hardware, right? - Yeah. - So my philosophy is\nthat there isn't one right solution, right? And so I think that, again,\nwe're at the end of Moore's law, specialization happens. - [Lex] Yeah. - If you're building, if you're training GPT-5,\nyou want some crazy super computer data center thingy. If you're making a smart camera that runs on batteries,\nyou want something that looks very different. If you're building a phone, you want something that looks very different. If you have something like a laptop,\nyou want something that looks maybe similar but a different scale, right? And so AI ends up touching all of our lives.\nRobotics, right? And, like, lots of different things. And so as you look into this, these have different power envelopes.\nThere's different trade-offs in terms of the algorithms. There's new innovations and sparsity and other data formats and things like that.\nAnd so hardware innovation, I think, is a really good thing, right? And what I'm interested in is unlocking that innovation.\nThere's also like analog and quantum and like all the really weird stuff, right? - Yeah.\n- And so if somebody can come up with a chip that uses analog computing and it's 100x more power efficient,\nthink what that would mean in terms of the daily impact on the products we use, that'd be huge.\nNow, if you're building an analog computer, you may not be a compiler specialist, right?\nThese are different skill sets, right? And so you can hire some compiler people if you're running a big company, maybe,\nbut it turns out these are really like exotic new generation of compilers. (chuckles)\nLike, this is a different thing, right? So if you take a step back out and come back to what is the status quo, the status quo is that if you're Intel or you're Nvidia,\nyou keep up with the industry and you chase and, okay, there's 1,900 now, there's 2-000 now, there's 2,100.\nAnd you have a huge team of people that are like trying to keep up and tune and optimize. And even when one of the big guys comes out\nwith a new generation of their chip, they have to go back and rewrite all these things, right? So really it's only powered by having hundreds of people\nthat are all, like, frantically trying to keep up. And what that does is that keeps out the little guys,\nand sometimes they're not so little guys, the big guys that are also just not in those dominant positions.\nAnd so what has been happening, and so you talk about the rise of new exotic, crazy accelerators is people have been trying to turn this\nfrom a let's go write lots of special kernels problem into a compiler problem.\nAnd so we, and I contributed to this as well, (chuckles) we as an industry went into a like, let's go make this compiler problem phase, let's call it.\nAnd much of the industry is still in this phase, by the way. So I wouldn't say this phase is over. And so the idea is to say, look, okay,\nwhat a compiler does is it provides a much more general, extensible hackable interface\nfor dealing with the general case, right? And so within machine learning algorithms, for example,\npeople figured out that, hey, if I do a matrix multiplication and I do a ReLU, right,\nthe classic activation function, it is way faster to do one passover the data\nand then do the ReLU on the output where I'm writing out the data, 'cause ReLU is just a maximum operation, right, max at zero.\nAnd so it's an amazing optimization. Take MathML, ReLU. Squished together in one operation, now I have MathML ReLU.\nWell, wait a second. If I do that, now, I just went from having, you know, two operators to three.\nBut now I figure out, okay, well, there's a lot of activation functions. What about a leaky value? What about...\nLike, a million things that are out there, right? And so as I start fusing these in, now I get permutations of all these algorithms, right?\nAnd so what the compiler people said is they said, \"Hey, well, cool. Well, I will go enumerate all the algorithms and I will enumerate all the pairs\nand I will actually generate a kernel for you.\" And I think that this has been very, very useful for the industry.\nThis is one of the things that powers Google TPUs. PyTorch 2's, like, rolling out really cool compiler stuff\nwith Triton, this other technology, and things like this. And so the compiler people are kind of coming\ninto their fore and saying like, Awesome, this is a compiler problem. We'll compiler it. Here's the problem. (chuckles)\nNot everybody's a compiler person. I love compiler people, trust me, right, but not everybody can or should be a compiler person.\nIt turns out that they're people that know analog computers really well, or they know some GPU\ninternal architecture thing really well, or they know some crazy sparse numeric interesting algorithm that is the cusp of research,\nbut they're not compiler people. And so one of the challenges with this new wave of technology trying to turn everything into a compiler,\n'cause again, it has excluded a ton of people. And so you look at what does Mojo do, what does the Modular stack do\nis brings programmability back into this world. Like, it enables, I wouldn't say normal people,\nbut like a new, you know, a different kind of delightful nerd that cares about numerics, or cares about hardware,\nor cares about things like this, to be able to express that in the stack and extend the stack without having to actually go hack the compiler itself.\n- So extend the stack on the algorithm side. - [Chris] Yeah. - And then on the hardware side.\n- Yeah, so again, go back to, like, the simplest example of int, right? And so what both Swift and Mojo and other things\nlike this did is we said, okay, pull magic out of the compiler and put it in the standard library, right? And so what Modular's doing\nwith the engine that we're providing and like, this very deep technology stack, right, which goes into heterogeneous runtimes\nand like a whole bunch of really cool, really cool things, this whole stack allows that stack to be extended and hacked\nand changed by researchers and by hardware innovators and by people who know things that we don't know,\n(chuckles) 'cause, you know, Modular has some smart people, but we don't have all the smart people it turns out, right?\n- What are heterogeneous runtimes? - Yeah. So what is heterogeneous, right?\nSo heterogeneous just means many different kinds of things together. And so the simplest example\nyou might come up with is a CPU and a GPU. And so it's a simple heterogeneous computer to say,\nI'll run my data loading and pre-processing and other algorithms on the CPU. And then once I get it into the right shape,\nI shove it into the GPU. I do a lot of matrix multiplication and convolutions and things like this. And then I get it back out\nand I do some reductions and summaries and they shove it across the wire, to across the network to another machine, right?\nAnd so you've got now what are effectively two computers, a CPU and a GPU talking to each other,\nworking together in a heterogeneous system. But that was 10 years ago, (chuckles) okay?\nYou look at a modern cell phone. Modern cell phone, you've got CPUs, and they're not just CPUs,\nthere's like big.LITTLE CPUs and there's multiple different kinds of CPUs that are kind- - Yep. - of working together, they're multi-core.\nYou've got GPUs, you've got neural network accelerators, you've got dedicated hardware blocks for media,\nso for video decode and jpeg decode and things like this. And so you've got this massively complicated system, and this isn't just cell phones.\nEvery laptop these days is doing the same thing. And all these blocks can run at the same time\nand need to be choreographed, right? And so again, one of the cool things about machine learning\nis it's moving things to like data flow graphs and higher level of abstractions and tensors and these things that it doesn't specify,\nhere's how to do the algorithm. It gives the system a lot more flexibility in terms of how to translate or map it\nor compile it onto the system that you have. And so what you need, you know, the bottom-est part of the layer there is\na way for all these devices to talk to each other. And so this is one thing that, you know, I'm very passionate about.\nI mean, you know, I'm a nerd, but all these machines and all these systems are effectively parallel computers\nrunning at the same time, sending messages to each other. And so they're all fully asynchronous.\nWell, this is actually a small version of the same problem you have in a data center, right? In a data center, you now have multiple different machines,\nsometimes very specialized, sometimes with GPUs or TPUs in one node and sometimes with disks in other nodes.\nAnd so you get a much larger scale heterogenous computer. And so what ends up happening is you have this, like,\nmulti-layer abstraction of hierarchical parallelism, hierarchical, asynchronous communication and making that,\nagain, my enemy, is complexity. By getting that away from being\ndifferent specialized systems at every different part of the stack and having more consistency and uniformity,\nI think we can help lift the world and make it much simpler and actually get used. - Well, how do you leverage, like, the strengths of the different specialized systems?\nSo looking inside the smartphone, like there's what, like- - Yeah. - I don't know, five, six computers essentially\ninside the smartphone? - Yeah. - How do you, without trying to minimize the explicit,\nmaking it explicit, which computer is supposed to be used for which operation? - Yeah, so there's a pretty well-known algorithm,\nand what you're doing is you're looking at two factors. You're looking at the factor of sending data from one thing to another, right,\n'cause it takes time to get it from that side of the chip to that side of the chip and things like this. And then you're looking at what is the time it takes to do\nan operation on a particular block. So take CPUs. CPUs are fully general. They can do anything, right?\nBut then you have a neural net accelerator that's really good at matrix multiplication, okay? And so you say, okay,\nwell, if my workload is all matrix multiplication, I start up, I send the data over the neural net thing,\nit goes and does matrix multiplication. When it's done, it sends me back the result. All is good, right? And so the simplest thing is just saying,\ndo matrix operations over there, right? But then you realize you get a little bit more complicated because you can do matrix multiplication on a GPU,\nyou can do it on a neural net accelerator, you can do it on CPU, and they'll have different trade-offs and costs.\nAnd it's not just matrix multiplication. And so what you actually look at is you look at, I have generally a graph of compute.\nI wanna do a partitioning. I wanna look at the communication, the bisection bandwidth,\nand like the overhead- - Overheads. - and the sending of all these different things and build a model for this and then decide, okay,\nit's an optimization problem of where do I wanna place this compute? - So it's the old school theoretical computer science\nproblem of scheduling. - Yep. - And then, presumably it's possible to, somehow,\nmagically include auto-tune into this. - Absolutely, so I mean, in my opinion, this is an opinion,\nnot everybody would agree with this, but in my opinion, the world benefits from simple and predictable systems\nat the bottom you can control. But then once you have a predictable execution layer,\nyou can build lots of different policies on top of it, right? And so one policy can be that the human programmer says,\ndo that here, do that here, do that here, do that here, and like, fully manually controls everything\nand the systems should just do it, right? But then you quickly get in the mode of like, I don't wanna have to tell it to do it. (chuckles) - Yeah.\n- And so the next logical step that people typically take is they write some terrible heuristic. \"Oh, if it's a information location, do it over there.\nor if it's floating point, do it on the GPU. If it's integer, do it on the CPU,\" like, something like that, right?\nAnd then you then get into this mode of like, people care more and more and more, and you say, okay, well, let's actually, like, make the heuristic better.\nLet's get into auto-tuning. Let's actually do a search of the space to decide,\nwell, what is actually better, right? Well, then you get into this problem where you realize this is not a small space.\nThis is a many-dimensional hyperdimensional space that you cannot exhaustively search.\nSo do you know of any algorithms that are good at searching very complicated spaces for... - Don't tell me you're gonna turn this\ninto a machine learning problem. - So then you turn into a machine learning problem, and then you have a space of genetic algorithms\nand reinforcement learning and, like, all these concerns. - Can you include that into the stack, into the Modular stack?\n- Yeah, yeah. And so- - Where does it sit? Where does it live? Is it separate thing or is it part of the compilation? - So you start from simple and predictable models.\nAnd so you can have full control and you can have coarse grain knobs that, like, nudge systems so you don't have to do this.\nBut if you really care about getting the best, you know, the last ounce out of a problem, then you can use additional tools.\nThe cool thing is you don't wanna do this every time you run a model. You wanna figure out the right answer and then cache it.\n(chuckles) And once you do that, you can say, okay, cool. Well, I can get up and running very quickly.\nI can get good execution out of my system, I can decide if something's important,\nand if it's important, I can go throw a bunch of machines at it and do a big, expensive search over the space using whatever technique I feel like,\nit's really up to the problem. And then when I get the right answer, cool, I can just start using it, right?\nAnd so you can get out of this, this trade-off between, okay, am I gonna like spend forever doing a thing\nor do I get up and running quickly? And as a quality result, like, these are actually not in contention with each other\nif the system's designed to scale. - You started and did a little bit of a whirlwind overview\nof how you get the 35,000x speed up or more over Python.\nJeremy Howard did a really great presentation about sort of the basic, like, looking at the code, here's how you get the speed up.\nLike you said, that's something probably developers can do for their own code to see how you can get\nthese gigantic speed ups. But can you maybe speak to the machine learning task in general?\nHow do you make some of this code fast, and specifics. Like, what would you say is the main bottleneck\nfor machine learning tasks? So are we talking about MathML matrix multiplication?\nHow do you make that fast? - So I mean, if you just look at the Python problem, right? You can say, how do I make Python faster?\nAnd there's been a lot of people that have been working on the, okay, how do I make Python 2x faster, or 10x faster, or something like that, right?\nAnd there've been a ton of projects in that vein, right? Mojo started from the, what can the hardware do?\nLike, what is the limit of physics? What is the speed of light? - Yeah. What is the- - Yeah, yeah. - Like, how fast can this thing go? And then how do I express that,\nright? - Yeah. - And so it wasn't anchored relatively on make Python a little bit faster.\nIt's saying, cool, I know what the hard work can do. Let's unlock that, right? Now when you- (Lex chuckling)\n- Yeah, just say how gutsy that is to be in the meeting and as opposed to trying to see, how do we get the improvement?\nIt's like, what can the physics do? - I mean, maybe I'm a special kinda nerd, but you look at that, what is the limit of physics?\nHow fast can these things go, right? When you start looking at that, typically it ends up being a memory problem, right?\nAnd so today, particularly with these specialized accelerators, the problem is that you can do a lot of math within them,\nbut you get bottleneck sending data back and forth to memory, whether it be local memory, or distant memory,\nor disk, or whatever it is. And that bottleneck, particularly as the training sizes get large\nas you start doing tons of inferences all over the place, like, that becomes a huge bottleneck for people, right?\nSo again, what happened is we went through a phase of many years where people took the special case and hand-tuned it and tweaked it and tricked it out.\nAnd they knew exactly how the hardware worked and they knew the model and they made it fast, didn't generalize. (chuckles)\nAnd so you can make, you know, ResNet-50, or AlexNet, or something, Inception v1, like, you can do that, right?\nBecause the models are small, they fit in your head, right? But as the models get bigger, more complicated,\nas the machines get more complicated, it stops working, right? And so this is where things like kernel fusion come in.\nSo what is kernel fusion? This is this idea of saying, let's avoid going to memory and let's do that by building\na new hybrid kernel and a numerical algorithm\nthat actually keeps things in the accelerator instead of having to write it all the way out to memory, right?\nWhat's happened with these accelerators now is you get multiple levels of memory. Like, in a GPU for example, you'll have global memory and local memory,\nand, like, all these things. If you zoom way into how hardware works,\nthe register file is actually a memory. (chuckles) So the registers are like an L0 cache.\nAnd so a lot of taking advantage of the hardware ends up being fully utilizing the full power\nin all of its capability. And this has a number of problems, right? One of which is again, the complexity of disaster, right?\nThere's too much hardware. Even if you just say let's look at the chips from one line of vendor,\nlike Apple, or Intel, or whatever it is, each version of the chip comes out with new features\nand they change things so that it takes more time or less to do different things. And you can't rewrite all the software\nwhenever a new chip comes out, right? And so this is where you need a much more scalable approach. And this is what Mojo and what the Modular stack provides is\nit provides this infrastructure and the system for factoring all this complexity and then allowing people to express algorithms,\nyou talk about auto-tuning, for example, express algorithms in a more portable way\nso that when a new chip comes out, you don't have to rewrite it all. So to me, like, you know, I kinda joke,\nlike, what is a compiler? Well, there's many ways to explain that. You convert thing A into thing B\nand you convert source code to machine code. Like, you can talk about many, many things that compilers do,\nbut to me it's about a bag of tricks. It's about a system and a framework that you can hang complexity.\nIt's a system that can then generalize and it can work on problems that are bigger than fit in one human's head, (chuckles) right?\nAnd so what that means, what a good stack and what the Modular stack provides is\nthe ability to walk up to it with a new problem and it'll generally work quite well.\nAnd that's something that a lot of machine learning infrastructure and tools and technologies don't have.\nTypical state-of-the-art today is you walk up, particularly if you're deploying, if you walk up with a new model, you try to push it through the converter\nand the converter crashes, that's crazy. The state of ML tooling today is not anything\nthat a C programmer would ever accept, right? And it's always been this kind of flaky set of tooling\nthat's never been integrated well, and it's never worked together because it's not designed together.\nIt's built by different teams, it's built by different hardware vendors, it's built by different systems, it's built by different internet companies.\nThey're trying to solve their problems, right? And so that means that we get this fragmented,\nterrible mess of complexity. - So I mean, the specifics of, and Jeremy showed this- - Yeah.\n- there's the vectorized function, which I guess is built into Mojo?\n- [Chris] Vectorized, as he showed, is built into the library. - Into the library, it's done on the library. - [Chris] Yep. - Vectorize, parallelize.\n- [Chris] Yep. - Which vectorize is more low-level, parallelize is higher level. There's the tiling thing,\nwhich is how he demonstrated the auto-tune, I think. - So think about this in, like, levels,\nhierarchical levels of abstraction, right? If you zoom all the way into a compute problem,\nyou have one floating point number, right? And so then you say, okay, I can do things one at a time in an interpreter.\n(chuckles) It's pretty slow, right? So I can get to doing one at a time in a compiler,\nlike in C. I can get to doing 4, or 8 or 16 at a time with vectors.\nThat's called vectorization. Then you can say, hey, I have a whole bunch of different...\nYou know, what a multi-core computer is, is it's basically a bunch of computers, right?\nSo they're all independent computers that they can talk to each other and they share memory. And so now what parallelize does, it says, okay,\nrun multiple instances on different computers. And now, they can all work together on Chrome, right? And so what you're doing is you're saying,\nkeep going out to the next level out. And as you do that, how do I take advantage of this?\nSo tiling is a memory optimization, right? It says, okay, let's make sure that we're keeping the data\nclose to the compute part of the problem instead of sending it all back and forth through memory every time I load a block.\n- And the size of the block, size is, that's how you get to the auto-tune to make sure it's optimized. - Right, yeah.\nWell, so all of these, the details matter so much to get good performance. This is another funny thing about machine learning\nand high-performance computing that is very different than C compilers we all grew up with where, you know,\nif you get a new version of GCC, or a new version of Clang, or something like that, you know, maybe something will go 1% faster, right?\nAnd so compiler engine will work really, really, really hard to get half a percent out of your C code, something like that.\nBut when you're talking about an accelerator, or an AI application, or you're talking about these kinds of algorithms,\nnow these are things people used to write in Fortran, for example, right? If you get it wrong, it's not 5% or 1%,\nit could be 2x or 10x, (chuckles) right? If you think about it, you really want to make use of the full memory\nyou have, the cache, for example. But if you use too much space, it doesn't fit in the cache, now you're gonna be thrashing\nall the way back out to main memory. And these can be 2x, 10x major performance differences.\nAnd so this is where getting these magic numbers and these things right is really actually quite important.\n"}
{"pod": "Lex Fridman Podcast", "input": "Mojo vs CPython", "output": "- So you mentioned that Mojo is a superset of Python.\nCan you run Python code as if it's Mojo code?\n- Yes, yes, (Lex chuckling) and this has two sides of it. So Mojo's not done yet. So I'll give you a disclaimer.\nMojo's not done yet, but already we see people that take small pieces of Python code, move it over, they don't change it,\nand you can get 12x speed ups. Like, somebody was just tweeting about that yesterday, which is pretty cool, right?\nAnd again, interpreters, compilers, right? And so without changing any code, without... Also, this is not JIT compiling or doing anything fancy.\nThis is just basic stuff, move it straight over. Now Mojo will continue to grow out and as it grows out,\nit will have more and more and more features and our North Star's to be a full superset of Python.\nAnd so you can bring over, basically, arbitrary Python code and have it just work. It may not always be 12x faster,\nbut it should be at least as fast and way faster in many cases, is the goal, right?\nNow, it'll take time to do that. And Python is a complicated language. There's not just the obvious things, but there's also non-obvious things that are complicated.\nLike, we have to be able to talk to CPython packages, to talk to the CPI, and there's a bunch of pieces to this.\n- So you have to, I mean, just to make explicit the obvious that may not be so obvious until you think about it.\nSo, you know, to run Python code, that means you have to run all the Python packages and libraries.\n- [Chris] Yeah, yeah. - So that means what? What's the relationship between Mojo and CPython,\nthe interpreter that's- - Yep. - presumably would be tasked with getting those packages to work?\n- Yep, so in the fullness of time, Mojo will solve for all the problems and you'll be able to move Python packages\nover and run them in Mojo. - [Lex] Without the CPython. - Without Cpython, someday,\nright, not today, but someday. - Yeah. And that'll be a beautiful day because then you'll get a whole bunch of advantages\nand you'll get massive speedups and things like this. - But you can do that one at a time, right? You can move packages one at a time. - Exactly,\nbut we're not willing to wait for that. (chuckles) Python is too important. The ecosystem is too broad.\nWe wanna both be able to build Mojo out, we also wanna do it the right way without time, like, without intense time pressure.\nWe're obviously moving fast, but. And so what we do is we say, okay, well, let's make it so you can import\nan arbitrary existing package, arbitrary, including, like,\nyou write your own on your local disk (chuckles) or whatever. It's not like a standard, like an arbitrary package,\nand import that using CPython because CPython already runs all the packages, right?\nAnd so what we do is we built an integration layer where we can actually use Cpython,\nagain, I'm practical, and to actually just load and use all the existing packages as they are.\nThe downside of that is you don't get the benefits of Mojo for those packages, right? And so they'll run as fast, as they do in the traditional CPython way,\nbut what that does is that gives you an incremental migration path. And so if you say, hey, cool, well, here's a,\nyou know, the Python ecosystem is vast. I want all of it to just work, but there's certain things that are really important.\nAnd so if I'm doing weather forecasting or something, (chuckles) well, I wanna be able to load all the data,\nI wanna be able to work with it, and then I have my own crazy algorithm inside of it. Well, normally I'd write that in C++.\nIf I can write in Mojo and have one system that scales, well, that's way easier to work with. - Is it hard to do that,\nto have that layer that's running CPython? Because is there some communication back and forth?\n- Yes, it's complicated. I mean, this is what we do. So, I mean, we make it look easy, but it is complicated.\nBut what we do is we use the CPython existing interpreter. So it's running its own bike codes,\nand that's how it provides full compatibility. And then it gives us CPython objects,\nand we use those objects as is. And so that way we're fully compatible with all the CPython objects and all the, you know,\nit's not just the Python part, it's also the C packages, the C libraries underneath them, because they're often hybrid.\nAnd so we can fully run and we're fully compatible with all that. And the way we do that is that we have to play by their rules, right?\nAnd so we keep objects in that representation when they're coming from that world. - What's the representation that's being used?\n- In memory. We'd have to know a lot about how the CPython interpreter works. It has, for example, reference counting,\nbut also different rules on how to pass pointers around, and things like this, super low-level fiddly.\nAnd it's not like Python. It's like how the interpreter works, okay? And so that gets all exposed out,\nand then you have to define wrappers around the low-level C code, right? And so what this means is you have to know not only C,\nwhich is a different role from Python, obviously, not only Python- - [Lex] But the wrappers.\n- but the interpreter and the wrappers and the implementation details and the conventions. And it's just this reall complicated mess.\nAnd when you do that, now suddenly you have a debugger that debugs Python, they can't step into C code, right?\nSo you have this two-world problem, right? And so by pulling this all into Mojo,\nwhat you get is you get one world. You get the ability to say, cool, I have un-typed, very dynamic, beautiful, simple code.\nOkay, I care about performance, for whatever reason, right? There's lots of reasons you might care.\nAnd so then you add types, you can parallelize things. You can vectorize things, you can use these techniques, which are general techniques to solve a problem.\nAnd then you can do that by staying in the system. And if you have that one Python package\nthat's really important to you, you can move it to Mojo. You get massive performance benefits on that and other advantages.\nYou know, if you like static types, it's nice if they're enforced. Some people like that, right, rather than being hints.\nSo there's other advantages too. And then you can do that incrementally as you go.\n- So one different perspective on this would be why Mojo\ninstead of making CPython faster, redesigning CPython. - Yeah, well, I mean,\nyou could argue Mojo is redesigning CPython, but why not make CPython faster\nand better and other things like that, there's lots of people working on that. So actually there's a team at Microsoft\nthat is really improving... I think CPython 3.11 came out in October\nor something like that, and it was, you know, 15% faster, 20% faster across the board,\nwhich is pretty huge given how mature Python is and things like this. And so that's awesome. I love it.\nDoesn't run on GPU. (chuckles) It doesn't do AI stuff. Like, it doesn't do vectors, doesn't do things.\n20 percent's good. 35,000 times is better, right? So like, they're definitely...\nI'm a huge fan of that work, by the way, and it composes well with what we're doing. It's not like we're fighting or anything like that.\nIt's actually just, it's goodness for the world, but it's just a different path, right? And again, we're not working forwards\nfrom making Python a little bit better. We're working backwards from what is the limit of physics? - What's the process of importing Python code to Mojo?\nIs there... What's involved in that process? - Yeah. - Is there tooling for that?\n- Not yet. So we're missing some basic features right now. And so we're continuing to drop out new features,\nlike, on a weekly basis, but, you know, at the fullness of time, give us a year and a half, maybe two years.\n- Is it an automatable process? - So when we're ready, it'll be very automatable, yes.\n- Is it automatable? Like, is it possible to automate, in the general case of Python- - Yeah.\n- to Mojo conversion, and you're saying it's possible. - Well, so, and this is why, I mean, among other reasons why we use tabs,\n(chuckles) right? - Yes. - [Chris] So first of all, by being a superset- - Yep. - it's like C versus C++.\nCan you move C code to C++? Yeah, right? - Yes. - And you can move C code to C++,\nand then you can adopt classes, you can add adopt templates, you can adopt other references\nor whatever C++ features you want. After you move C code to C++, like, you can't use templates in C, right?\nAnd so if you leave it at C, fine. You can't use the cool features, but it still works, right? And C and C++ good work together.\nAnd so that's the analogy, right? Now here, right,\nthere's not a Python is bad and Mojo is good, (chuckles) right? Mojo just gives you superpowers, right?\nAnd so if you wanna stay with Python, that's cool, but the tooling should be actually very beautiful and simple\nbecause we're doing the hard work of defining a superset. - Right. So you're right. So there's several things to say there,\nbut also the conversion tooling should probably give you hints as to, like, how you can improve the code?\n- Yeah, exactly. Once you're in the new world, then you can build all kinds of cool tools to say like, hey, should you adopt this feature?\nAnd we haven't built those tools yet, but I fully expect those tools will exist. And then you can like, you know,\nquote, unquote, \"modernize your code,\" or however you wanna look at it, right? So I mean one of the things that I think is really interesting about Mojo is\nthat there have been a lot of projects to improve Python over the years.\nEverything from, you know, getting Python run on the Java virtual machine, PyPy, which is a JIT compiler.\nThere's tons of these projects out there that have been working on improving Python in various ways.\nThey fall into one or two camps. So PyPy is a great example of a camp that is trying to be compatible with Python.\nEven there, not really. Doesn't work with all the C packages and stuff like that, but they're trying to be compatible with Python.\nThere's also another category of these things where they're saying, well, Python is too complicated and,\nyou know, I'm gonna cheat on the edges and at, you know, like integers in Python can be an arbitrary size integer.\nLike if you care about it fitting in a, going fast in a register in a computer, that's really annoying, right?\nAnd so you can choose two pass on that, right? You can say, well, people don't really use big integers that often,\ntherefore I'm gonna just not do it and it'll be fine, not a Python superset. - Yeah.\n- (chuckles) Or you can do the hard thing and say, okay, this is Python, and you can't be a superset of Python\nwithout being a superset of Python. And that's a really hard technical problem, but it's,\nin my opinion, worth it, right? And it's worth it because it's not about any one package. It's about this ecosystem.\nIt's about what Python means for the world. And it also means we don't wanna repeat the Python 2 to Python 3 transition.\nLike we want people to be able to adopt this stuff quickly. And so by doing that work, we can help lift people.\n- Yeah, the challenge, it's really interesting, technical, philosophical challenge of really making a language\na superset of another language. It's breaking my brain a little bit.\n- Well, it paints you into corners. So again, I'm very happy with Python, right? So all joking aside,\nI think that the indentation thing is not the actual important part of the problem. - [Lex] Yes. (Chris chuckling)\n- Right? But the fact that Python has amazing dynamic metaprogramming features and they translate to beautiful static metaprogramming features,\nI think is profound I think that's huge, right? And so Python, I've talked with Guido about this, it's like,\nit was not designed to do what we're doing. That was not the reason they built it this way, but because they really cared and they were very thoughtful\nabout how they designed the language, it scales very elegantly in this space. But if you look at other languages,\nfor example, C and C++, right, if you're building a superset,\nyou get stuck with the design decisions of the subset, right?\nAnd so, you know, C++ is way more complicated because of C in the legacy than it would've been\nif they would've theoretically designed a from scratch thing. And there's lots of people right now\nthat are trying to make C++ better and recent syntax C++, it's gonna be great, we'll just change all the syntax.\nBut if you do that, now suddenly you have zero packages, you don't have compatibility.\n- If you could just linger on that, what are the biggest challenges of keeping that superset status?\nWhat are the things you're struggling with? Does it all boiled down to having a big integer? - No, I mean, it's-\n- What are the other things like? - Usually it's the long tail weird things. So let me give you a war story.\n- [Lex] Okay. - So war story in the space is you go away... Back in time, project I worked on is called Clang.\nClang, what it is a C++ parser, right? And when I started working on Clang,\nit must have been like 2006 or something, less, or 2007 something, 2006 when I first started working on it, right?\nIt's funny how time flies. - [Lex] Yeah, yeah. - I started that project and I'm like, okay,\nwell, I wanna build a C parser, C++ parser for LVM?\nIt's gonna be the... GCC is yucky. You know, this is me in earlier times.\nIt's yucky, it's unprincipled, it has all these weird features, like all these bugs, like it's yucky.\nSo I'm gonna build a standard compliant C and C++ parser. It's gonna be beautiful, it'll be amazing, well-engineered,\nall the cool things an engineer wants to do. And so I started implementing and building it out and building it out and building it out. And then I got to include standard io.h,\nand all of the headers in the world use all the GCC stuff, (chuckles) okay? - Yeah.\n- And so, again, come back away from theory back to reality, right?\nI was at a fork on the road. I could have built an amazingly beautiful academic thing that nobody would ever use\nor I could say, well, it's yucky in various ways. All these design mistakes, accents of history, the legacy.\nAt that point, GCC was like over 20 years old, which, by the way- - Yeah. - now, LLVM's over 20 years old, (laughs) right?\nAnd so it's funny how- - Yep. - time catches up to you, right? And so you say, okay, well, what is easier, right?\nI mean, as an engineer, it's actually much easier for me to go implement long tail compatibility weird features,\neven if they're distasteful and just do the hard work and like figure it out, reverse engineer it,\nunderstand what it is, write a bunch of test cases, like, try to understand the behavior. It's way easier to do all that work as an engineer\nthan it is to go talk to all C programmers and argue with them and try to get them to rewrite their code, right? - Yeah.\n- And- - [Lex] 'Cause that breaks a lot more things. - Yeah. The reality is like nobody actually even understands\nhow the code works 'cause it was written by the person who quit 10 years ago, (chuckles) right?\nAnd so this software is kind of frustrating that way, but it's, that's how the world works,\nright? - Yeah. Unfortunately, it can never be this perfect, beautiful thing. - Well, there are occasions\nin which you get to build, like, you know, you invent a new data structure or something like that, or there's this beautiful algorithm that's just like,\nmakes you super happy, and I love that moment. But when you're working with people- - Yeah. - and you're working with code and dusty deck code bases\nand things like this, right, it's not about what's theoretically beautiful, it's about what's practical, what's real,\nwhat people will actually use. And I don't meet a lot of people that say, I wanna rewrite all my code just for the sake of it.\n- By the way, there could be interesting possibilities and we'll probably talk about it where AI can help rewrite some code. That might be farther out feature,\nbut it's a really interesting one, how that could create more- - Yeah, yeah. - be a tool in the battle against this monster of complexity\nthat you mentioned. - Yeah. - You mentioned Guido, the benevolent dictator for life of Python.\n"}
{"pod": "Lex Fridman Podcast", "input": "Guido van Rossum", "output": "What does he think about Mojo? Have you talked to him much about it? - I have talked with him about it. He found it very interesting.\nWe actually talked with before it launched, and so he was aware of it before it went public. I have a ton of respect for Guido\nfor a bunch of different reasons. You talk about walrus operator and, like, Guido's pretty amazing in terms of steering\nsuch a huge and diverse community and, like, driving it forward.\nAnd I think Python is what it is thanks to him, right? And so to me it was really important\nstarting to work on Mojo to get his feedback and get his input and get his eyes on this, right?\nNow a lot of what Guido was and is I think concerned about is,\nhow do we not fragment the community? - [Lex] Yeah. - We don't want a Python 2 to Python 3 thing. Like, that was really painful for everybody involved.\nAnd so we spent quite a bit of time talking about that. And some of the tricks I learned from Swift, for example, so in the migration from Swift,\nwe managed to, like, not just convert Objective-C into a slightly prettier Objective-C, which we did,\nwe then converted, not entirely, but almost an entire community to a completely different language, right?\nAnd so there's a bunch of tricks that you learn along the way that are directly relevant to what we do. And so this is where, for example,\nyou leverage CPython while bringing up the new thing. Like, that approach is, I think,\nproven and comes from experience. And so Guido's very interested in like, okay, cool.\nLike, I think that Python is really his legacy, it's his baby. I have tons of respect for that.\nIncidentally, I see Mojo as a member of the Python family. I'm not trying to take Python from Guido and from the Python community.\nAnd so to me it's really important that we're a good member of that community.\nI think that, again, you would have to ask Guido this, but I think that he was very interested in this notion of like, cool Python gets beaten up for being slow.\nMaybe there's a path out of that, right? And that, you know, if the future is Python, right,\nI mean, look at the far outside case on this, right?\nAnd I'm not saying this is Guido's perspective, but, you know, there's this path of saying like, okay, well, suddenly Python can suddenly go all the places\nit's never been able to go before, right? And that means that Python can go even further and can have even more impact on the world.\n- So in some sense, Mojo could be seen as Python 4.0.\n- I would not say that. I think that would drive a lot of people really crazy. - Because of the PTSD of the 3.0, 2.0.\n- I'm willing to annoy people about Emacs versus Vim or- - Not that one. - [Chris] Versus spaces. - Not that one. - I don't know. That might be a little bit far even for me.\nLike, my skin may not be that thick. - But the point is the step to being a superset and allowing all of these capabilities,\nI think is the evolution of a language. It feels like an evolution of a language.\nSo he's interested by the ideas that you're playing with, but also concerned about the fragmentation.\nSo what are the ideas you've learned? What are you thinking about? How do we avoid fragmenting the community\nwhere the Pythonistas and the,\nI don't know what to call the Mojo people. - [Chris] Mojicians. - The mojicians, I like it. - [Chris] There you go.\n- Can coexist happily and share code and basically just have these big code bases\nthat are using Cpython and more and more moving towards Mojo. - Yeah. Yeah.\nWell, so again, these are lessons I learned from Swift. And here, we face very similar problems, right? In Swift, you have Objective-C, super dynamic.\nThey're very different syntax, (chuckles) right? But you're talking to people who have large scale code bases.\nI mean, Apple's got the biggest, largest scale code base of Objective-C code, right? And so, you know, none of the companies,\nnone of the other iOS developers, none of the other developers want to rewrite everything all at once. And so you wanna be able to adopt things piece at a time.\nAnd so a thing that I found that worked very well in the Swift community was saying, okay, cool, and this is when Swift was very young, and you say, okay,\nyou have a million line of code Objective-C app. Don't rewrite it all, but when you implement a new feature,\ngo implement that new class using Swift, right? And so now this turns out is\na very wonderful thing for an app developer, but it's a huge challenge for the compiler team\nand the systems people that are implementing this, right? And this comes back to what is this trade-off between doing\nthe hard thing that enables scale versus doing the theoretically pure and ideal thing, right?\nAnd so Swift had adopted and built a lot of different machinery to deeply integrate with the Objective-C runtime.\nAnd we're doing the same thing with Python right now. What happened in the case of Swift is that Swift's language\ngot more and more and more mature over time, right? And incidentally, Mojo is a much simpler language than Swift in many ways.\nAnd so I think that Mojo will develop way faster than Swift for a variety of reasons. But as the language gets more mature and parallel with that,\nyou have new people starting new projects, right? And so if when the language is mature\nand somebody's starting a new project, that's when they say, okay, cool, I'm not dealing with a million lines of code. I'll just start and use the new thing for my whole stack.\nNow the problem is, again, you come back to we're communities and we're people that work together.\nYou build new subsystem or a new feature or a new thing in Swift, or you build a new thing in Mojo,\nthen you want it to be end up being used on the other side, (chuckles) right? And so then you need to work on integration\nback the other way. And so it's not just Mojo talking to Python, it's also Python talking to Mojo, right?\nAnd so what I would love to see, I don't wanna see this next month, right, but what I wanna see over the course of time is I would love\nto see people that are building these packages, like, you know, NumPy or, you know, TensorFlow or what, you know,\nthese packages that are half Python, half C++. And if you say, okay, cool,\nI want to get out of this Python C++ world into a unified world and so I can move to Mojo,\nbut I can't give up all my Python clients 'cause they're like, these levers get used by everybody\nand they're not all gonna switch every, all, you know, all at once and maybe never, right?\nWell, so the way we should do that is we should vend Python interfaces to the Mojo types.\nAnd that's what we did in Swift and worked great. I mean, it was a huge implementation challenge for the compiler people, right?\nBut there's only a dozen of those compiler people and there are millions of users. And so it's a very expensive, capital-intensive,\nlike, skillset intensive problem. But once you solve that problem, it really helps adoption and it really helps the community\nprogressively adopt technologies. And so I think that this approach will work quite well with the Python and the Mojo world.\n- So for a package, port it to Mojo, and then create a Python interface. - [Chris] Yep.\n"}
{"pod": "Lex Fridman Podcast", "input": "Mojo vs PyTorch vs TensorFlow", "output": "- So when you're on these packages, NumPy, PyTorch, TensorFlow. - Yeah. - How do they play nicely together?\nSo is Mojo supposed to be... Let's talk about the machine learning ones.\nIs Mojo kind of visioned to replace PyTorch, TensorFlow to incorporate it?\nWhat's the relationship in this? - All right, so take a step back. So I wear many hats. (chuckles)\nSo you're angling it on the Mojo side. Mojo's a programming language. - Yes. - And so it can help solve\nthe C, C++ Python feud that's happening. - The fire emoji got me. I'm sorry. We should be talking Modular. Yes, yes.\n- Yes, okay. So the fire emoji is amazing. I love it. It's a big deal.\nThe other side of this is the fire emoji is in service of solving some big AI problems, right? - Yes.\n- And so the big AI problems are, again, this fragmentation, this hardware nightmare, this explosion of new potential,\nbut it's not getting felt by the industry, right? And so when you look at, how does the Modular engine help tens and PyTorch, right,\nit's not replacing them, right? In fact, when I talk to people, again, they don't like to rewrite all their code.\nYou have people that are using a bunch of PyTorch, a bunch of TensorFlow. They have models that they've been building\nover the course of many years, right? And when I talk to them, there's a few exceptions, but generally they don't wanna rewrite\nall their code, right? And so what we're doing is we're saying, \"Okay, well, you don't have to rewrite all your code.\"\nWhat happens is the Modular engine goes in there and goes underneath TensorFlow and PyTorch. It's fully compatible and it just provides\nbetter performance, better predictability, better tooling. It's a better experience that helps lift TensorFlow\nand PyTorch and make them even better. I love Python, I love TensorFlow, I love PyTorch, right?\nThis is about making the world better because we need AI to go further. - But if I have a process that trains a model\nand I have a process that performs inference on that model and I have the model itself,\nwhat should I do with that in the long arc of history in terms of if I use PyTorch to train it.\nShould I rewrite stuff in Mojo if I care about performance? - Oh, so I mean, again, it depends.\nSo if you care about performance, then writing it in Mojo is gonna be way better than writing in Python. But if you look at LLM companies, for example,\nso you look at Open AI, rumored, and you look at many of the other folks that are working on many of these LLMs\nand other like innovative machine learning models, on the one hand they're innovating in the data collection\nand the model, billions of parameters, and the model architecture and the RLHF and the,\nlike all the cool things that people are talking about. But on the other hand, they're spending a lot of time writing CUDA curls, right?\nAnd so you say, wait a second, how much faster could all this progress go if they were not having to hand write all these CUDA curls, right?\nAnd so there are a few technologies that are out there, and people have been working on this problem for a while\nand they're trying to solve subsets of the problem, again, kinda fragmenting the space. And so what Mojo provides for these kinds of companies is\nthe ability to say, cool, I can have a unifying theory, right? And again, the better together, the unifying theory,\nthe two-world problem, or the three-world problem, or the N-world problem, like, this is the thing that is slowing people down.\nAnd so as we help solve this problem, I think it'll be very helpful for making this whole cycle go faster.\n"}
{"pod": "Lex Fridman Podcast", "input": "Swift programming language", "output": "- So obviously we've talked about the transition from Objective-C to Swift. You've designed this programming language,\nand you've also talked quite a bit about the use of Swift for machine learning context.\nWhy have you decided to move away from maybe an intense focus on Swift\nfor the machine learning context versus sort of designing a new programming language that happens to be a superset?\n- You're saying this is an irrational set of life choices I make or what? (chuckles) (Lex laughing) - Did you go to the desert and did you meditate on it?\nOkay, all right. No, it was bold. It was bold and needed and I think, I mean,\nit's just bold and sometimes to take those leaps, it's a difficult leap to take. - Yeah. Well, so, okay. I mean, I think there's a couple of different things.\nSo actually I left to Apple back in 2017, like January, 2017. So it's been a number of years that I left Apple.\nAnd the reason I left Apple was to do AI, okay?\nSo, and again, I won't comment on Apple and AI, but at the time, right,\nI wanted to get into and understand the technology, understand the applications, the workloads. And so I was like, okay,\nI'm gonna go dive deep into Applied and AI, and then the technology underneath it, right?\nI found myself at Google. - And that was like when TPUs were waking up. - Yep, exactly. - And so I found myself at Google and Jeff Dean,\nwho's a rockstar as you know, right? And in 2017, TensorFlow's, like,\nreally taking off and doing incredible things. And I was attracted to Google to help them with the TPUs, right?\nAnd TPUs are an innovative hardware accelerator platform, have now I mean I think proven massive scale\nand like done incredible things, right? And so one of the things that this led into is a bunch\nof different projects, which I'll skip over, right? One of which was this Swift for TensorFlow project, right?\nAnd so that project was a research project. And so the idea of that is say, okay,\nwell, let's look at innovative new programming models where we can get a fast programming language,\nwe can get automatic differentiation into the language. Let's push the boundaries of these things in a research setting, right?\nNow, that project I think lasted two, three years. There's some really cool outcomes of that.\nSo one of the things that's really interesting is I published a talk at an LLVM conference in 2018,\nagain, this seems like so long ago, about graph program abstraction, which is basically the thing that's in PyTorch 2.\nAnd so PyTorch 2 with all this DynamoRIO thing, it's all about this graph program abstraction thing from Python bike codes.\nAnd so a lot of the research that was done ended up pursuing and going out through the industry and influencing things.\nAnd I think it's super exciting and awesome to see that, but the Swift for TensorFlow project itself did not work out super well.\nAnd so there's a couple of different problems with that. One of which is that, you may have noticed,\nSwift is not Python. (chuckles) There's a few people that write Python code.\n- [Lex] Yes. - And so it turns out that all of ML is pretty happy with Python. - It's actually a problem\nthat other programming languages have as well, that they're not Python. We'll probably maybe briefly talk about Julia,\nwas a very interesting, beautiful programming language, but it's not Python. - Exactly.\nAnd so like if you're saying, I'm gonna solve a machine learning problem where all the programmers are Python programmers.\n- [Lex] Yeah. - And you say the first thing you have to do is switch to a different language, well, your new thing may be good or bad or whatever,\nbut if it's a new thing, the adoption barrier is massive less. - It's still possible.\n- Still possible, yeah, absolutely. The world changes and evolves and there's definitely room for new and good ideas, but it just makes it so much harder, right?\nAnd so lesson learned, Swift is not Python, and people are not always in search of, like,\nlearning a new thing for the sake of learning a new thing. And if you wanna be compatible with all the world's code, turns out meet the world where it is, right?\nSecond thing is that, you know, a lesson learned is that Swift is a very fast and efficient language, kind of like Mojo,\nbut a different take on it still, really worked well with eager mode.\nAnd so eager mode is something that PyTorch does, and it proved out really well, and it enables really expressive and dynamic\nand easy to debug programming. TensorFlow at the time was not set up for that, let's say.\nThat was not... - [Lex] The timing is also important in this world. - Yeah, yeah. And TensorFlow is a good thing and it has many, many strengths,\nbut you could say Swift for TensorFlow is a good idea, except for the Swift and except for the TensorFlow part.\n(chuckles) - Swift because it's not Python and TensorFlow because it- - [Chris] It wasn't set up for eager mode at the time, yeah.\n- It was 1.0. - Exactly. And so one of the things about that is that in the context of it being a research project,\nI'm very happy with the fact that we built a lot of really cool technology. We learned a lot of things.\nI think the ideas went on to have influence in other systems, like PyTorch. A few people use that I hear, right? And so I think that's super cool.\nAnd for me personally, I learned so much from it, right? And I think a lot of the engineers that worked on it also learned a tremendous amount.\nAnd so, you know, I think that that's just really exciting to see. And, you know, I'm sorry that the project didn't work out.\nI wish it did, of course, right, but, you know, it's a research project.\nAnd so you're there to learn from it. - Well, it's interesting to think about the evolution of programming\n"}
{"pod": "Lex Fridman Podcast", "input": "Julia programming language", "output": "as we come up with these whole new set of algorithms in machine learning, in artificial intelligence.\nAnd what's going to win out is it could be a new programming language. It could be- - Yeah.\n- I mean, I just mentioned Julia. I think there's a lot of ideas\nbehind Julia that Mojo shares. What are your thoughts about Julia in general?\n- So I will have to say that when we launched Mojo, one of the biggest things I didn't predict\nwas the response from the Julia community. And so I was not, I mean, I've,\nokay, lemme take a step back. I've known the Julia folks for a really long time. They're an adopter of LLVM a long time ago.\nThey've been pushing state-of-the-art in a bunch of different ways. Julia's a really cool system.\nI had always thought of Julia as being mostly a scientific computing focused environment, right?\nAnd I thought that was its focus. I neglected to understand that one of their missions is\nto, like, help make Python work end-to-end. (chuckles) And so I think that was my error for not understanding that.\nAnd so I could have been maybe more sensitive to that, but there's major differences between what Mojo's doing and what Julia's doing.\nSo as you say, Julia is not Python, right? And so one of the things that a lot\nof the Julia people came out and said is like, \"Okay, well, if we put a ton of more energy into,\nton more money or in engineering or whatever into Julia, maybe that would be better than starting Mojo, right?\"\nWell, I mean, maybe that's true, but it still wouldn't make Julia into Python. (chuckles)\nSo if you worked backwards from the goal of, let's build something for Python programmers without requiring them to relearn syntax,\nthen Julia just isn't there, right? I mean, that's a different thing, right? And so if you anchor on, I love Julia,\nand I want Julia to go further, then you can look at it from a different lens, but the lens we were coming at was,\nHey, everybody is using Python. The syntax isn't broken. Let's take what's great about Python\nand make it even better. And so it was just a different starting point. So I think Julie's a great language. The community's a lovely community.\nThey're doing really cool stuff, but it's just a different, it's slightly different angle. - But it does seem that Python is quite sticky.\nIs there some philosophical, almost thing you could say about why Python,\nby many measures, seems to be the most popular programming language in the world? - Well, I can tell you things I love about it.\nMaybe that's one way to answer the question, right? So huge package ecosystem, super lightweight and easy to integrate.\nIt has very low startup time, right? - [Lex] So what's startup time? You mean like learning curve or what?\n- Yeah, so if you look at certain other languages, you say like, go, and it just takes a,\nlike Java, for example, it takes a long time to JIT compile all the things and then the VM starts up\nand the garbage (indistinct) kicks in and then it revs its engines and then it can plow through a lot of internet stuff or whatever, right?\nPython is like scripting. Like it just goes, right? - Yeah. - Python has a very low compile time.\nLike, so you're not sitting there waiting. Python integrates in a notebooks in a very elegant way that makes exploration super interactive\nand it's awesome, right? Python is also, it's like almost the glue of computing.\nBecause it has such a simple object representation, a lot of things plug into it.\nThat dynamic metaprogramming thing we were talking about, also enables really expressive and beautiful APIs, right?\nSo there's lots of reasons that you can look at, technical things the Python has done and say, like,\nokay, wow, this is actually a pretty amazing thing. And any one of those you can neglect, people will all just talk about indentation\n(chuckles) and ignore like the fundamental things. But then you also look at the community side, right?\nSo Python owns machine learning. Machine learning's pretty big. - Yeah, and it's growing. - And it's growing, right?\nAnd it's growing in importance, right? And so- - And there's a reputation of prestige to machine learning to where like if you're a new programmer,\nyou're thinking about, like, which program and language do I use? Well, I should probably care about machine learning,\ntherefore let me try Python, and kinda builds and builds and builds. - And even go back before that.\nLike, my kids learned Python, right, not because I'm telling 'em to learn Python, but because- - Were they rebelling\nagainst you or what? - Oh, no, no. Well, they they also learn Scratch, right, and things like this too, but it's because Python is taught everywhere, right?\nBecause it's easy to learn, right? And because it's pervasive, right? And there's- - Back in my day, we learned Java and C++.\n- [Chris] Yeah, well. - Well, uphill both directions, but yes. I guess Python- - Yeah. - is the main language\nof teaching software engineering schools now. - Yeah, well, and if you look at this, there's these growth cycles, right?\nIf you look at what causes things to become popular and then gain in popularity, there's reinforcing feedback loops and things like this.\nAnd I think Python has done, again, the whole community has done a really good job of building those growth loops and help propel the ecosystem.\nAnd I think that, again, you look at what you can get done with just a few lines of code, it's amazing. - So this kinda self-building loop is interesting\n"}
{"pod": "Lex Fridman Podcast", "input": "Switching programming languages", "output": "to understand because when you look at Mojo, what it stands for some of the features,\nit seems sort of clear that this is a good direction for programming languages\nto evolve in the machine learning community, but it's still not obvious that it will because of this,\nwhatever the engine of popularity of virality. Is there something you could speak to, like,\nhow do you get people to switch? - Yeah, well, I mean, I think that the viral growth loop\nis to switch people to Unicode. - [Lex] Yes. - I think the Unicode file extensions are what I'm betting on. I think that's gonna be the thing.\n- Yeah. (Chris chuckling) - Tell the kids that you could use the fire emoji and they'd be like, what? - Exactly, exactly.\n(Lex chuckling) Well, in all seriousness, like, I mean, I think there's really, I'll give you two opposite answers.\nOne is, I hope if it's useful, if it solves problems, and if people care about those problems being solved,\nthey'll adopt the tech, right? That's kinda the simple answer. And when you're looking to get tech adopted,\nthe question is, is it solving an important problem people need solved, and is the adoption cost low enough that they're willing\nto make the switch and cut over and do the pain upfront so that they can actually do it, right?\nAnd so hopefully Mojo will be that for a bunch of people. And, you know, people building these hybrid packages are suffering.\nIt is really painful. And so I think that we have a good shot of helping people, but the other side is like,\nit's okay if people don't use Mojo. Like, it's not my job to say like, everybody should do this. Like, I'm not saying Python is bad.\nLike, I hope Python, CPython, like, all these implementations 'cause Python ecosystems, not just CPython, it's also a bunch of different implementations\nwith different trade-offs. And this ecosystem is really powerful and exciting as are other programming languages.\nIt's not like type script or something is gonna go away, right? And so there's not a winner-take-all thing.\nAnd so I hope that Mojo's exciting and useful to people, but if it's not, that's also fine. - But I also wonder what the use case\nfor why you should try Mojo would be. So practically speaking- - [Chris] Yeah.\n- it seems like, so there's entertainment. There's the dopamine hit of saying, holy,\nthis is 10 times faster. This little piece of code is 10 times faster in Mojo.\n- [Chris] Outta the box before you get to 35,000. - Exactly, I mean, just even that, I mean,\nthat's the dopamine hit that every programmer sorta dreams of is the optimization.\nIt's also the drug that can pull you in and have you waste way too much of your life\noptimizing and over optimizing, right? But so what do you see would be, like, common?\nIt's very hard to predict, of course, but, you know, if you look 10 years from now and Mojo's super successful.\n- [Chris] Yeah. - What do you think would be the thing where people like try and then use it regularly\nand it kinda grows and grows and grows and grows? - Well, so you talked about dopamine hit. And so one, again, humans are not one thing.\nAnd some people love rewriting their code and learning new things and throwing themselves in the deep end and trying out a new thing.\nIn my experience, most people, they're too busy. They have other things going on.\nBy number, most people don't want like this. I wanna rewrite all my code.\nBut (chuckles) even those people, the two busy people, the people that don't actually care about the language,\nthat just care about getting stuff done, those people do like learning new things, right? - [Lex] Yeah.\n- And so you talk about the dopamine rush of 10x faster, Wow, that's cool. I wanna do that again. Well, it's also like,\nhere's the thing I've heard about in a different domain, and I don't have to rewrite on my code. I can learn a new trick, right?\nWell, that's called growth, (chuckles) you know? And so, one thing that I think is cool about Mojo,\nand again, those will take a little bit of time, for example, the blog posts and the books and, like,\nall that kinda stuff to develop and the language needs to get further along. But what we're doing, you talk about types,\nlike you can say, look, you can start with the world you already know and you can progressively learn new things\nand adopt them where it makes sense. And if you never do that, that's cool. You're not a bad person. (chuckles)\nIf you get really excited about it and wanna go all the way in the deep end and rewrite everything and, like, whatever, that's cool, right?\nBut I think the middle path is actually the more likely one where it's, you know, you come out with a a new idea and you discover,\nwow, that makes my code way simpler, way more beautiful way, faster way, whatever. And I think that's what people like.\nNow if you fast forward and you said, like, 10 years out, right,\nI can give you a very different answer on that, which is, I mean, if you go back and look at what computers looked like 20 years ago,\nevery 18 months, they got faster for free, right, 2x faster every 18 months.\nIt was like clockwork. It was free, right? You go back 10 years ago and we entered in this world\nwhere suddenly we had multi-core CPUs and we had, and if you squint and turn your head,\nwhat a GPUs is just a many-core, very simple CPU thing kind of, right?\nAnd 10 years ago it was CPUs and GPUs and graphics.\nToday, we have CPU, GPUs, graphics. And AI, because it's so important,\nbecause the compute is so demanding because of the smart cameras and the watches and all the different places\nthat AI needs to work in our lives, it's caused this explosion of hardware. And so part of my thesis,\npart of my belief of where computing goes, if you look out 10 years from now, is it's not gonna get simpler.\nPhysics isn't going back to where we came from. It's only gonna get weirder from here on out, right? And so to me,\nthe exciting part about what we're building is it's about building that universal platform,\nwhich the world can continue to get weird. 'Cause again, I don't think it's avoidable, it's physics,\nbut we can help lift people, scale, do things with it, and they don't have to rewrite their code every time a new device comes out.\nAnd I think that's pretty cool. And so if Mojo can help with that problem, then I think that it will be hopefully quite interesting\nand quite useful to a wide range of people because there's so much potential. And like there's so much, you know,\nmaybe analog computers will become a thing or something, right? And we need to be able to get into a mode where we can move this programming model forward,\nbut do so in a way where we're lifting people and growing them instead of forcing them to rewrite all their code and exploding them.\n- Do you think there'll be a few major libraries that go Mojo first?\n- Well, so I mean, the Modular engines on Mojo. (chuckles) So again, come back to, like, we're not building Mojo because it's fun.\nWe're building Mojo because we had to solve these accelerators. - That's the origin story, but I mean, ones that are currently in Python.\n- Yeah, so I think that a number of these projects will. And so one of the things, and again, this is just my best guess. Like, each of the package maintainers also has...\nI'm sure plenty of other things going on. People really don't like rewriting code just for the sake of rewriting code.\nBut sometimes like people are excited about like adopting a new idea. - Yeah.\n- And turns out that while rewriting code is generally not people's first thing,\nturns out that redesigning something while you rewrite it and using a rewrite as an excuse to redesign\ncan lead to the 2.0 of your thing that's way better than the 1.0, right?\nAnd so I have no idea, I can't predict that, but there's a lot of these places where, again,\nif you have a package that is half C and half Python, right, you just solve the pain,\nmake it easier to move things faster, make it easier to bug and evolve your tech adopting Mojo\nkinda makes sense to start with. And then it gives you this opportunity to rethink these things. - So the two big gains are that there's a performance gain\nand then there's the portability to all kinds of different devices.\n- And there's safety, right? So you talk about real types. I mean, not saying this is for everybody,\nbut that's actually a pretty big thing, right? - [Lex] Yeah, types are. - And so there's a bunch of different aspects of what, you know, what value Mojo provides.\nAnd so, I mean, it's funny for me, like, I've been working on these kinds of technologies and tools for too many years now,\nbut you look at Swift, right, and we talked about Swift for TensorFlow, but Swift as a programming language, right?\nSwift's now 13 years old from when I started it? - [Lex] Yeah.\n- 'Cause I started in 2010, if I remember. And so that project, and I was involved with it for 12 years or something, right,\nthat project has gone through its own really interesting story arc, right? And it's a mature, successful,\nused by millions of people system, right? Certainly not dead yet, right? But also going through that story arc,\nI learned a tremendous amount about building languages, about building compilers, about working with the community and things like this.\nAnd so that experience, like I'm helping channel and bring directly into Mojo and, you know, other systems, same thing.\nLike, apparently I like building, and iterating and evolving things. And so you look at this LLVM thing that I worked on 20 years ago, and you look at MLIR, right?\nAnd so a lot of the lessons learned in LLVM got fed into MLIR, and I think that MLIR is a way better system than LLVM was.\nAnd, you know, Swift is a really good system and it's amazing, but I hope that Mojo will take\nthe next step forward in terms of design.\n"}
{"pod": "Lex Fridman Podcast", "input": "Mojo playground", "output": "- In terms of running Mojo and people can play with it, what's Mojo Playground? - Yeah.\n- And from the interface perspective and from the hardware perspective,\nwhat's this incredible thing running on? - Yeah, so right now, so here we are, two weeks after launch.\n- Yes. - We decided that, okay, we have this incredible set of technology that we think might be good,\nbut we have not given it to lots of people yet. And so we were very conservative and said, \"Let's put it in a workbook so that if it crashes,\nwe can do something about it. We can monitor and track that, right?\" And so, again, things are still super early,\nbut we're having like one person a minute sign up with over 70,000 people (chuckles)\ntwo weeks in is kinda crazy. - And you can sign up to Mojo Playground and you can use it in the cloud.\n- [Chris] Yeah. - In your browser. - [Chris] And so what that's running on, right? - Notebook. - Yeah, What that's running on is that's running on cloud VMs.\nAnd so you share a machine with a bunch of other people, but turns out there's a bunch of them now\nbecause there's a lot of people. And so what you're doing is you're getting free compute and you're getting to play with this thing in kind of a limited controlled way\nso that we can make sure that it doesn't totally crash and be embarrassing, right? - Yeah.\n- So now a lot of the feedback we've gotten is people wanna download it around locally. So we're working on that right now. And so- - So that's the goal,\nto be able to download locally to it. - Yeah, that's what everybody expects. And so we're working on that right now. And so we just wanna make sure that we do it right.\nI think this is one of the lessons I learned from Swift also, by the way, is when we launched Swift,\ngosh, it feels like forever ago, it was 2014, and we, I mean, it was super exciting.\nI, and we, the team had worked on Swift for a number of years in secrecy, okay? And (chuckles) four years into this development,\nroughly, of working on this thing, at that point, about 250 people at Apple knew about it.\n- [Lex] Yeah. - Okay? So it was secret. Apple's good at secrecy and it was a secret project. And so we launched this at WWC,\na bunch of hoopla and excitement and said developers are gonna be able to develop and submit apps in the App Store in three months, okay?\nWell, several interesting things happened, right? So first of all, we learned that it had a lot of bugs.\nIt was not actually production quality, and it was extremely stressful in terms of like trying\nto get it working for a bunch of people. And so what happened was we went from zero to, you know,\nI don't know how many developers Apple had at the time, but a lot of developers overnight. And they ran to a lot of bugs and it was really embarrassing\nand it was very stressful for everybody involved, right? It was also very exciting 'cause everybody was excited about that.\nThe other thing I learned is that when that happened, roughly every software engineer who did not know about the project at Apple,\ntheir head exploded when it was launched 'cause they didn't know it was coming. And so they're like, \"Wait, what is this?\nI signed up to work for Apple because I love Objective-C. Why is there a new thing?,\" right? - Yeah. - And so now what that meant practically is\nthat the push from launch to first of all the fall, but then to 2.0 and 3.0 and like all the way forward was\nsuper painful for the engineering team and myself. It was very stressful. The developer community was very grumpy about it\nbecause they're like, \"Okay, well, wait a second. You're changing and breaking my code, and like, we have to fix the bugs.\" And it was just like a lot of tension\nand friction on all sides. There's a lot of technical debt in the compiler\nbecause we have to run really fast and you have to go implement the thing and unblock the use case and do the thing. And you know it's not right,\nbut you never have time to go back and do it right. And I'm very proud of the Swift team because they've come, I mean, we,\nbut they came so far and made so much progress over this time since launch, it's pretty incredible.\nAnd Swift is a very, very good thing, but I just don't wanna do that again, right? And so- - So iterate more\nthrough the development process. - And so what we're doing is we're not launching it when it's hopefully 0.9 with no testers.\nWe're launching it and saying it's 0.1, right? And so we're setting expectations of saying like, okay, well, don't use this for production, right?\nIf you're interested in what we're doing, we'll do it in an open way and we can do it together,\nbut don't use it in production yet. Like, we'll get there, but let's do it the right way. And I'm also saying we're not in a race.\nThe thing that I wanna do is build the world's best thing. - [Lex] Yeah. - Right, because if you do it right\nand it lifts the industry, it doesn't matter if it takes an extra two months. - Yeah. - Like two months is worth waiting. And so doing it right\nand not being overwhelmed with technical debt and things like this is like, again, war wounds, lessons learned,\nwhatever you wanna say, I think is absolutely the right thing to do. Even though right now people are very frustrated that, you know,\nyou can't download it or that it doesn't have feature X or something like this. And so- - What have you learned\n"}
{"pod": "Lex Fridman Podcast", "input": "Jeremy Howard", "output": "in a little bit of time since it's been released into the wild that people have been complaining about feature X or Y or Z?\nWhat have they been complaining about? Whether they have been excited about like,\nalmost like detailed things versus a big thing. I think everyone's would be very excited about the big vision.\n- Yeah, yeah. Well, so I mean, I've been very pleased. I mean, in fact, I mean, we've been massively overwhelmed with response, which is a good problem to have.\nIt's kinda like a success disaster, in a sense, right? - Yeah. - And so, I mean,\nif you go back in time when we started Modular, which is just not yet a year and a half ago, so it's still a pretty new company, new team,\nsmall but very good team of people, like we started with extreme conviction\nthat there's a set of problems that we need to solve. And if we solve it, then people will be interested in what we're doing, right?\nBut again, you're building in basically secret, right? You're trying to figure it out.\nThe creation's a messy process. You're having to go through different paths and understand what you wanna do and how to explain it.\nOften when you're doing disruptive and new kinds of things, just knowing how to explain it is super difficult, right?\nAnd so when we launched, we hope people would be excited, but, you know, I'm an optimist, but I'm also like,\ndon't wanna get ahead of myself. And so when people found out about Mojo, I think their heads exploded a little bit, right?\nAnd, you know, here's a, I think a pretty credible team that has built some languages and some tools before. And so they have some lessons learned\nand are tackling some of the deep problems in the Python ecosystem and giving it the love and attention that it should be getting.\nAnd I think people got very excited about that. And so if you look at that, I mean, I think people are excited about ownership\nand taking a step beyond Rust, right? And there's people that are very excited about that and there's people that are excited about, you know,\njust like I made Game of Life go 400 times faster, right, and things like that, and that's really cool.\nThere are people that are really excited about the, okay, I really hate writing stuff in C++, save me.\n- Like systems in your, they're like stepping up, like, oh yes. - And so that's me by the way, also.\n- [Lex] Yeah. - I really wanna stop writing C++, but the- - I get third person excitement when people tweet,\nHere, I made this code, Game of Life or whatever, faster. And you're like, yeah. - Yeah, and also like,\nwell, I would also say that, let me cast blame out to people who deserve it.\n- [Lex] Sure. - These terrible people who convinced me to do some of this. Jeremy Howard, that guy. - Yes, yes.\nWell, he's been pushing for this kinda thing. He's been pushing- - He's wanted this for years. - Yeah, he's wanted this for a long, long time. - [Chris] He's wanted this for years. And so-\n- For people who don't know Jeremy Howard, he is like one of the most legit people in the machine learning community.\nHe's a grassroots, he really teaches, he's an incredible educator, he is an incredible teacher, but also legit in terms of a machine learning engineer\nhimself. - Yes. - And he's been running the fast.AI and looking, I think for exactly what you've done\nwith Mojo. - Exactly. And so, I mean, the first time, so I met Jeremy pretty early on,\nbut the first time I sat up and I'm like, this guy is ridiculous,\nis when I was at Google and we were bringing up TPUs and we had a whole team of people and there was this competition called Don Bench\nof who can train ImageNet fastest, right? - Yeah. Yes.\n- And Jeremy and one of his researchers crushed Google (chuckles) by not through sheer force\nof the amazing amount of compute and the number of TPUs and stuff like that, that he just decided that progressive imagery sizing\nwas the right way to train the model in. You were epoch faster and make the whole thing go vroom,\nright? - Yep. - And I'm like, \"This guy is incredible.\" So you can say, - Right.\nanyways, come back to, you know, where's Mojo coming from? Chris finally listened to Jeremy.\n(Lex laughing) It's all his fault. - Well, there's a kinda very refreshing,\npragmatic view that he has about machine learning that I don't know if it,\nit's like this mix of a desire for efficiency, but ultimately grounded and desired to make machine learning\nmore accessible to a lot of people. I don't know what that is. - Yeah. - I guess that's coupled with efficiency and performance,\nbut it's not just obsessed about performance. - Well, so a lot of AI and AI research ends up being\nthat it has to go fast enough to get scale. So a lot of people don't actually care about performance,\nparticularly on the research side until it allows 'em to have more a bigger dataset, right? And so suddenly now you care about distributed compute\nand like, all these exotic HPC, like, you don't actually wanna know about that. You just want to be able to do more experiments faster\nand do so with bigger datasets, right? And so Jeremy has been really pushing the limits. And one of the things I'll say about Jeremy,\nand there's many things I could say about Jeremy, 'cause I'm a fanboy of his, but it fits in his head,\nand Jeremy actually takes the time where many people don't to really dive deep into why is the beta parameter\nof the atom optimizer equal to this, right? - Yeah. - And he'll go survey and understand\nwhat are all the activation functions in the trade-offs, and why is it that everybody that does, you know, this model, pick that thing.\n- So the why, not just trying different values, like, really what is going on here? - Right, and so as a consequence of that, like he's always,\nhe, again, he makes time, but he spends time to understand things at a depth that a lot of people don't.\nAnd as you say, he then brings it and teaches people- - [Lex] Teaches it. - And his mission is to help lift, you know,\nhis website says \"making AI uncool again,\" like it's about, like, forget about the hype. It's actually practical and useful.\nLet's teach people how to do this, right? Now the problem Jeremy struggled with is that he's pushing the envelope, right?\nResearch isn't about doing the thing that is staying on the happy path or the well-paved road, right?\nAnd so a lot of the systems today have been these really fragile, fragmented things, are special case in this happy path.\nAnd if you fall off the happy path, you get eaten by an alligator. (chuckles) - (chuckles) So what about...\nSo Python has this giant ecosystem of packages\nand there's a package repository. Do you have ideas of how to do that well for Mojo,\nhow to do a repository of packages well? - So that's another really interesting problem that I knew about but I didn't understand\nhow big of a problem it was: Python packaging. A lot of people have very big pain points\nand a lot of scars with Python packaging. - Oh, you mean, so there's several things to say. - [Chris] Building and distributing\nand managing dependencies - Yes. - [Chris] and versioning and all this stuff. - So from the perspective of,\nif you want to create your own package, and then - Yes, yeah. - or you wanna build on top of a bunch of other people's packages\nand then they get updated and things like this. Now, I'm not an expert in this, so I don't know the answer.\nI think this is one the reasons why it's great that we work as a team and there's other really good and smart people involved,\nbut one of the things I've heard from smart people who've done a lot of this is that the packaging becomes\na huge disaster when you get the Python and C together.\nAnd so if you have this problem where you have code split between Python and C, now not only do you have to package the C code,\nyou have to build the C code. C doesn't have a package manager, right? C doesn't have a dependency versioning\nmanagement system, right? And so I'm not experiencing the state-of-the-art and all the different Python package managers,\nbut my understanding is that's a massive part of the problem. And I think Mojo solves that part of the problem directly heads on.\nNow, one of the things I think we'll do with the community, and this isn't, again, we're not solving all the world's problems at once,\nwe have to be kinda focused, start with, is that I think that we will have an opportunity to reevaluate packaging, right?\nAnd so I think that we can come back and say, okay, well, given the new tools and technologies and the cool things we have that we've built up,\nbecause we have not just syntax we have an entirely new compiler stack that works in a new way, maybe there's other innovations we can bring together\nand maybe we can help solve that problem. - So almost a tangent to that question from the user perspective of packages.\nIt was always surprising to me that it was not easier to sort of explore and find packages,\nyou know, with, with PIP install. It's an incredible ecosystem. It's huge.\nIt's just interesting that it wasn't made. It's still, I think, not made easier to discover packages to do, yeah.\nlike search and discovery as YouTube calls it.\n- Well, I mean, it is kinda funny because this is one of the challenges of these like intentionally decentralized communities.\nAnd so- - Yeah. - I don't know what the right answer is for Python. I mean, there are many people\nthat I don't even know the right answer for Mojo. Like, so there are many people that would have\nmuch more informed opinions than I do, but it's interesting, if you look at this, right? Open source communities, you know, there's Git.\nGit is a fully de decentralized and anybody can do it any way they want, but then there's GitHub, right?\nAnd GitHub centralized commercial in that case, right? Thing really helped pull together and help solve some of the discovery problems and help build a more consistent community. And so maybe there's opportunities for-\n- There's something like a GitHub for- - Yeah.\n- Although even GitHub, I might be wrong on this, but the search and discovery for GitHub is not that great.\nLike, I still use Google search. - Yeah, well, I mean, maybe that's because GitHub doesn't\nwanna replace Google search, right? I think there is room for specialized solutions to specific problems, but sure, I don't know.\nI don't know the right answer for GitHub either. They can go figure that out.\n- But the point is to have an interface that's usable, that's successful to people of all different skill levels and- - So, well, and again,\nlike what are the benefit of standards, right? Standards allow you to build these next level-up ecosystem and next level-up infrastructure and next level-up things.\nAnd so, again, come back to, I hate complexity, C+ Python is complicated.\nIt makes everything more difficult to deal with. It makes it difficult to port, move code around, work with all these things get more complicated.\nAnd so, I mean, I'm not an expert, but maybe Mojo can help a little bit by helping reduce the amount of C in this ecosystem\nand make it therefore scale better. - So any kinda packages that are hybrid in nature would be a natural fit to move to Mojo, which-\n- Which is a lot of them, by the way. - Yeah. - So a lot of them, especially that are doing some interesting stuff\ncomputation wise. - Yeah, yeah. Let me ask you about some features. - Yeah. - So we talked about obviously indentation,\n"}
{"pod": "Lex Fridman Podcast", "input": "Function overloading", "output": "that it's a typed language or optionally typed. Is that the right way to say it?\n- It's either optional or progressively or- - Progressively, okay. - I think the... So people have very strong opinions\non the right word to use. - Yeah. - [Chris] I don't know. - I look forward to your letters. So there's the var versus let, but let is for constance.\n- Yeah. - Var is an optional. - Yeah, var makes it mutable. So you can reassign.\n- Okay. Then there's function overloading.\n- Oh okay, yeah. - I mean, there's a lot of source of happiness for me, but function overloading, that's, I guess,\nis that for performance or is that... Why does Python not have function overloading?\n- So I can speculate. So Python is a dynamic language. The way it works is that Python and Objective-C are\nactually very similar worlds if you ignore syntax.\nAnd so Objective-C is straight line derived from Smalltalk,\na really venerable interesting language that much of the world has forgotten about, but the people that remember it love it generally.\nAnd the way that Smalltalk works is that every object has a dictionary in it. And the dictionary maps from the name of a function\nor the name of a value within an object to its implementation. And so the way you call a method and Objective-C is you say,\ngo look up, the way I call foo is I go look up foo, I get a pointer to the function back, and then I call it, okay, that's how Python works, right?\nAnd so now the problem with that is that the dictionary within a Python object, all the keys are strings and it's a dictionary. Yeah.\nSo you can only have one entry per name. You think. - It's as simple as that. - I think it's as simple as that. And so now why do they never fix this?\nLike, why do they not change it to not be a dictionary anymore, they not change, like do other things?\n- Well, you don't really have to in Python because it's dynamic. And so you can say, I get into the function now,\nif I got past an integer, do some dynamic test for it, if it's a string, go do another thing.\nThere's another additional challenge, which is even if you did support overloading, you're saying, okay, well, here's a version of a function for integers\nand a function for strings. Well, even if you could put it in that dictionary, you'd have to have the collar do the dispatch.\nAnd so every time you call the function, you'd have to say like, is it an integer or is it a string? And so you'd have to figure out where to do that test.\nAnd so in a dynamic language, overloading is something you, general, you don't have to have.\nBut now you get into a type language and, you know, in Python, if you subscript with an integer,\nthen you get typically one element out of a collection. If you subscript with a range,\nyou get a different thing out, right? And so often in type languages, you'll wanna be able to express the fact that, cool,\nI have different behavior, depending on what I actually pass into this thing. And if you can model that,\nit can make it safer and more predictable and faster, and, like, all these things. - It somehow feels safer, yes,\nbut also feels empowering, like in terms of clarity. Like you don't have to design whole different functions.\n- Yeah, well, and this is also one of the challenges with the existing Python typing systems is that in practice,\nlike you take subscript, like in practice, a lot of these functions,\nthey don't have one signature, right? They actually have different behavior in different cases. And so this is why it's difficult\nto like retrofit this into existing Python code and make it play well, with typing.\nYou kinda have to design for that. - Okay, so there's a interesting distinction that people that program Python might be interested in is def versus fn.\nSo it's two different ways to define a function. - Yep. - And fn is a stricter version of def.\nWhat's the coolness that comes from the strictness? - So here you get into, what is the trade-off with the superset?\n- Yes. - Okay, so superset, you have to, or you really want to be compatible.\nLike, if you're doing a superset, you've decided compatibility with existing code is the important thing,\neven if some of the decisions they made were maybe not what you'd choose. - Yeah, okay. - So that means you put a lot of time into compatibility\nand it means that you get locked into decisions of the past, even if they may not have been a good thing, right?\nNow, systems programmers typically like to control things, right, and they wanna make sure that, you know,\nnot all cases of course, and even systems programmers are not one thing, right, but often you want predictability.\nAnd so one of the things that Python has, for example, as you know, is that if you define a variable, you just say, X equals four, I have a variable name to X.\nNow I say some long method, some long name equals 17, print out some long name, oops, but I typoed it, right?\nWell, the compiler, the Python compiler doesn't know in all cases what you're defining and what you're using,\nand did you typo the use of it or the definition, right? And so for people coming from type languages, again,\nI'm not saying they're right or wrong, but that drives 'em crazy because they want the compiler to tell them, you type out the name of this thing, right?\nAnd so what fn does is it turns on, as you say, it's a strict mode and so it says, okay, well, you have to actually declare,\nintentionally declare your variables before you use them. That gives you more predictability, more error checking and things like this,\nbut you don't have to use it. And this is a way that Mojo is both compatible\n'cause defs work the same way that defs have already always worked, but it provides a new alternative\nthat gives you more control. And it allows certain kinds of people that have a different philosophy to be able to express that and get that.\n- But usually if you're writing Mojo code from scratch, you'll be using fn.\n- It depends, again, it depends on your mentality, right? It's not that def is Python and fn is Mojo. Mojo has both, and it loves both, right?\nIt really depends on that is just strict. Yeah, exactly. Are you playing around and scripting something out?\nIs it a one-off throwaway script? Cool. Like, Python is great at that. - I'll still be using fn, but yeah.\n- Well, so I love strictness. Okay. - Well, so control, power. You also like suffering, right?\nYes, go hand in hand. - How many pull-ups? - I've lost count at this. Yeah, exactly.\nAt this point. - So, and that's cool. I love you for that. Yeah. And I love other people who like strict things, right,\nbut I don't want to say that that's the right thing because python's also very beautiful for hacking around and doing stuff in research\nand these other cases where you may not want that. - You see, I just feel like maybe I'm wrong in that,\nbut it feels like strictness leads to faster debugging. So in terms of going from,\neven on a small project from zero to completion, it just, I guess it depends how many bugs you generate usually. Yeah.\n- Well, so I mean, if it's again, lessons learned in looking at the ecosystem, it's really, I mean, I think it's,\nif you study some of these languages over time, like the Ruby community for example, now Ruby is a pretty well, developed,\npretty established community, but along their path they really invested in unit testing. Like, so I think that the Ruby community is\nreally pushed forward the state-of-the-art of testing because they didn't have a type system that caught a lot of bugs at compile time, right?\nAnd so you can have the best of both worlds. You can have good testing and good types, right, and things like this, but I thought that it was really interesting\nto see how certain challenges get solved. And in Python, for example, the interactive notebook kind of experiences\nand stuff like this are really amazing. And if you typo something, it doesn't matter. It just tells you it's fine, right?\nAnd so I think that the trades are very different if you're building a, you know, large scale production system\nversus you're building an exploring a notebook. - And speaking of control, the hilarious thing, if you look at code,\nI write just for myself, for fun, it's like littered with asserts everywhere, okay?\n- It's a kinda, well, then. - Yeah, you would like text. - It's basically saying in a dictatorial way,\nthis should be true now, otherwise everything stops. - Well, and that is the sign.\nAnd I love you, man, but that is a sign of somebody who likes control. And so, yes. - Yeah.\n- I think that you'll like f i this turning into a, I think I like Mojo. - Therapy session. Yes. I definitely will.\n"}
{"pod": "Lex Fridman Podcast", "input": "Error vs Exception", "output": "Speaking of asserts exceptions are called errors. Why is it called errors?\n- So we, I mean, we use the same, we're the same as Python, right, but we implement it a very different way, right?\nAnd so if you look at other languages, like we'll pick on C++ our favorite, right?\nC++ has a thing called zero-cost exception handling, okay? So, and this is in my opinion,\nsomething to learn lessons from. - It's a nice polite way of saying it. - And so, zero-cost exception handling,\nthe way it works is that it's called zero-cost because if you don't throw an exception,\nthere's supposed to be no overhead for the non-error code. And so it takes the error path out of the common path.\nIt does this by making throwing an error extremely expensive. And so if you actually throw an error\nwith a C++ compiler using exceptions has to go look up in tables on the side and do all this stuff.\nAnd so throwing an error can be like 10,000 times more expensive than referring from a function, right?\nAlso, it's called zero-cost exceptions, but it's not zero-cost by any stretch of the imagination\nbecause it massively blows out your code, your binary, it also adds a whole bunch of different paths\nbecause of disrupts and other things like that that exist in C++ plus, and it reduces the number of optimizations it has,\nlike all these effects. And so this thing that was called zero-cost exceptions,\nit really ain't, okay. Now if you fast forward to newer languages\nand this includes Swift and Rust and Go and now Mojo,\nwell, and Python's a little bit different because it's interpreted and so like, it's got a little bit of a different thing going on. But if you look at it, if you look at compiled languages,\nmany newer languages say, okay, well, let's not do that zero-cost exception handling thing.\nLet's actually treat and throwing an error the same as returning a variant returning either the normal result or an error.\nNow programmers generally don't want to deal with all the typing machinery and like pushing around a variant.\nAnd so you use all the syntax that Python gives us, for example, try and catch and it, you know,\nfunctions that raise and things like this. You can put a raises decorator on your functions, stuff like this. And if you wanna control that,\nand then the language can provide syntax for it. But under the hood, the way the computer executes it,\nthrowing an error is basically as fast as returning something. - Oh, interesting. So it's exactly the same way from a compile perspective.\n- And so this is actually, I mean, it's a fairly nerdy thing, right, which is why I love it,\nbut this has a huge impact on the way you design your APIs, right?\nSo in C++ huge communities turn off exceptions\nbecause the cost is just so high, right? And so the zero-cost cost is so high, right?\nAnd so that means you can't actually use exceptions in many libraries, right?\nInteresting. Yeah. And even for the people that do use it, well, okay, how and when do you wanna pay the cost?\nIf I try to open a file, should I throw an error? Well, what if I'm probing around,\nlooking for something, right, and I'm looking it up in many different paths? Well, if it's really slow to do that,\nmaybe I'll add another function that doesn't throw an error or turns in error code instead. And now I have two different versions of the same thing.\nAnd so it causes you to fork your APIs. And so, you know, one of the things I learned from Apple and I so love is\nthe art of API design is actually really profound. I think this is something that Python's also done a pretty good job at in terms of building out\nthis large scale package ecosystem. It's about having standards and things like this. And so, you know, we wouldn't wanna enter a mode where, you know,\nthere's this theoretical feature that exists in language, but people don't use it in practice. Now I'll also say one of the other really cool things\nabout this implementation approach is that it can run on GPUs and it can run on accelerators and things like this.\nAnd that standard zero-cost exception thing would never work on an accelerator. And so this is also part of how Mojo\ncan scale all the way down to like little embedded systems and to running on GPUs and things like that.\n- Can you actually say about the... Maybe is there some high-level way to describe the challenge\nof exceptions and how they work in code during compilation?\nSo it's just this idea of percolating up a thing and error.\n- Yeah, yeah. So the way to think about it is, think about a function that doesn't return anything,\njust as a simple case, right? And so you have function one calls function two,\ncalls function three, calls function four, along that call stack that are tribe blocks, right?\nAnd so if you have function one calls function two, function two has a tribe block, and then within it it calls function three, right?\nWell, what happens if function three throws? Well, actually start simpler. What happens if it returns?\nWell, if it returns, it's supposed to go back out and continue executing and then fall off the bottom of the tribe block\nand keep going and it all's good. If the function throws, you're supposed to exit the current function and then get into the accept clause, right,\nand then do whatever codes there and then keep falling on and going on. And so the way that a compiler like Mojo works is\nthat the call to that function, which happens in the accept block calls the function, which happens in the accept block calls the function,\nand then instead of returning nothing, it actually returns, you know, an a variant between nothing and an error.\nAnd so if you return, normally fall off the bottom, or do return, you return nothing. And if you throw, throw an error,\nyou return the variant. That is, I'm an error, right? So when you get to the call, you say,\nokay, cool, I called a function. Hey, I know locally I'm in a tribe block, right? And so I call the function\nand then I check to see what it returns. Aha. Is that error thing jump to the accept block.\n- And that's all done for you behind the scenes. - Exactly. And so the competitor does all this for you.\nAnd I mean, one of the things, if you dig into how this stuff works in Python, it gets a little bit more complicated because you have finally blocks,\nwhich you need to go into do some stuff, and then those can also throw and return.\n- Wait, What? Nested? - Yeah, and like the stuff matters for compatibility. Like, there's really-\n- Can nest them. - there's with clauses, and so with clauses, are kinda like finely blocks with some special stuff going on.\nAnd so there's nesting. - In general, nesting of anything, nesting of functions should be illegal.\nWell, it just feels like it adds a level of complexity. - Lex, I'm merely an implementer. And so this is again, one last question.\nOne of the trade-offs you get when you decide to build a superset is you get to implement a full fidelity\nimplementation of the thing that you decided is good. And so, yeah, I mean,\nwe can complain about the reality of the world and shake our fist, but- - It always feels like you shouldn't be allowed to do that.\nLike, to declare functions in certain functions inside functions, that seems- - Oh, wait, wait, wait.\nWhat happened to Lex, the Lisp guy? - No, I understand that, but Lisp is what I used to do in college.\n- So now you've grown up. - You know, we've all done things in college we're not proud of.\nNo, wait a sec, wait a sec. I love Lis, I love Lis. - Okay. Yeah, I was gonna say, you're afraid of me irritating the whole internet.\n- Like yeah, no, I love Lisp. It worked as a joke in my head and come out, right?\n- So nested functions are, joking aside, actually really great and for certain things, right? And so these are also called closures.\nClosures are pretty cool and you can pass callbacks. There's a lot of good patterns. And so- - So speaking of which,\n"}
{"pod": "Lex Fridman Podcast", "input": "Mojo roadmap", "output": "I don't think you have nested function implemented yet in Mojo.\n- We don't have Lambda syntax, but we do have Nest. - Lambda syntax nested. - Functions. Yeah. - There's a few things on the roadmap that you have\nthat it'd be cool to sort of just fly through, 'cause it's interesting to see, you know, how many features there are in a language small and big.\nYep. They have to implement. Yeah. So first of all there's Tuple support, and that has to do with some of their specific aspect of it,\nlike the parentheses or not parenthesis that Yeah. - This is just a totally a syntactic thing. - A syntactic thing, okay.\nThere's, but it is cool. It's still so keyword arguments and functions.\n- Yeah, so this is where in Python, you can say call function X equals four and X is the name- - Yeah.\n- of the argument. That's a nice sort of documenting salt documenting feature. Yep.\n- Yeah, I mean, and again, this isn't rocket science to implement this, just the laundry list. - It's just on the list.\nThe bigger features are things like traits. So traits are when you wanna define abstract.\nSo when you get into typed languages, you need the ability to write generics.\nAnd so you wanna say, I wanna write this function and now I want to work on all things that are arithmetic.\nLike, well, what does arithmetic like, mean? Well, arithmetic like is a categorization of a bunch of types.\nAgain, you can define many different ways, and I'm not gonna go into ring theory or something, but the,\nyou know, you can say it's arithmetic. Like if you can add, subtract, multiply, divide it for example, right? And so what you're saying is you're saying there's a set\nof traits that apply to a broad variety of types. And so they're all these types arithmetic,\nlike, all these tensors and floating point integer and, like, there's this category of of types.\nAnd then I can define on an orthogonal access algorithms that then work against types that have those properties.\nIt's been implemented in Swift and Rust in many languages.\nSo it's not Haskell, which is where everybody learns their tricks from,\nbut we need to implement that, and that'll enable a new level of expressivity. - So classes.\n- Yeah, classes are a big deal. - It's a big deal still to be implemented.\nLike you said, Lambda syntax, and there's,, like, detailed stuff, like whole module import support\nfor top-level code and file scope.\nAnd then global variables also. So being able to have variables outside of a top\nlevel. - Well, and so this comes back to the where Mojo came from, and the fact that this is your 0.1, right?\nSo Modular's building an AI stack, right? And an AI stack has a bunch of problems working\nwith hardware and writing high-performance kernels and doing this kernel fusion thing I was talking about,\nand getting the most out of the hardware. And so we've really prioritized and built Mojo to solve Modular's problem.\nRight now our North Star is built out to support all the things. And so we're making incredible progress.\nBy the way, Mojo's only, like, seven months old. So that's another interesting thing.\n- Well, I mean part of the reason I wanted to mention some of these things is like, there's a lot to do and it's pretty cool how you just kinda,\nsometimes you take for granted how much there is in a programming language, how many cool features you kinda rely on.\nAnd this is kinda a nice reminder when you lay it as its do list. - Yeah and so, I mean, but also you look into,\nit's amazing how much is also there and you take it for granted that a value, if you define it,\nit will get destroyed automatically. Like, that little feature itself is actually really complicated given the way the ownership system has to work.\nAnd the way that works within Mojo is a huge step forward from what Rust and Swift have done.\n- Wait, can you say that again? When value- - Yeah. When you define it gets destroyed automatically. - Yeah. So like, like say you have a string, right?\nSo you define a string on the stack. Okay. Or on whatever that means, like in your local function, right?\nAnd so you say like whether it be in a def and so you just say X equals hello world, right?\nWell, if your strength type requires you to allocate memory, then when it's destroyed, you have to deallocate it.\nSo in Python and in Mojo, you define that with a Dell method, right? Where does that get run?\nWell, it gets run sometime between the last use of the value and the end of the program.\nLike in this, you now get into garbage collection, you get into, like, all these long debated, you talk about religions\nand trade-offs and things like this. This is a hugely hotly contested world.\nIf you look at C++, the way this works is that if you define a variable or a set of variables within a function,\nthey get destroyed in a last in, first out order. So it's like nesting, okay.\nThis has a huge problem because if you have a big scope and you define a whole bunch of values at the top\nand then you use 'em and then you do a whole bunch of code that doesn't use them, they don't get destroyed until the very end of that scope, right?\nAnd so this also destroys tail calls. So good functional programming, right? This has a bunch of different impacts on, you know,\nyou talk about reference counting optimizations and things like this. A bunch of very low-level things. And so what Mojo does is it has a different approach\non that from any language I'm familiar with, where it destroys them as soon as possible.\nAnd by doing that you get better memory use, you get better predictability, you get tail calls that work, you get a bunch of other things,\nyou get better ownership tracking. There's a bunch of these very simple things that are very fundamental that are\nalready built in there in Mojo today that are the things that nobody talks about generally, but when they don't work right,\nyou find out and you have to complain about. - Is it trivial to know what's the soonest possible\nto delete a thing that it's not gonna be used again? - Yeah. Well, I mean, it's generally trivial.\nIt's after the last use of it. So if you just find X as a string and then you have some use of X somewhere in your code-\n- Within that scope, you mean, within the scope that is accessible? - It's, yeah, exactly. So you can only use something within its scope. Yeah.\nAnd so then it doesn't wait until the end of the scope to delete it, it destroys it after the last use.\n- So there's kinda some very eager machine that's just sitting there and deleting. Yeah. - And it's all in the compiler.\nSo it's not at runtime, which is also cool. And so interesting. Yeah.\nAnd this is actually non-trivial because you have control flow, right? And so it gets complicated pretty quickly. And so like angst, right? Was not, not.\n- Well, so you have to insert delete, like in a lot of places. - Potentially. Yeah, exactly. So the compiler has to reason about this.\nAnd this is where again, it's experience building languages and not getting this right. So again, you get another chance to do it\nand you get basic things like this, right? But it's extremely powerful when you do that, right? And so there's a bunch of things like that, that kinda combine together.\nAnd this comes back to the, you get a chance to do it the right way, do it the right way, and make sure that every brick you put down is really good.\nSo that when you put more bricks on top of it, they stack up to something that's beautiful. - Well, there's also, like, how many design discussions do there have to be\nabout particular details like implementation of particular small features?\nBecause the features that seem small, I bet some of them might be like really require\nreally big design decisions. - Yeah. Well, so I mean, lemme give you another example of this.\nPython has a feature called async/await. So it's a new feature. I mean, in the long arc of Python history,\nit's a relatively new feature, right, that allows way more expressive, asynchronous programming.\nOkay? Again, this is a Python's a beautiful thing. And they did things that are great for Mojo for completely different reasons.\nThe reason that async/await got added to Python, as far as I know, is because Python doesn't support threads, okay?\nAnd so Python doesn't support threads, but you wanna work with networking and other things, like, that can block.\nI mean, Python does support threads, it's just not its strength.\nAnd so they added this feature called async/await. It's also seen in other languages like Swift and JavaScript and many other places as well.\nAsync/await and Mojo is amazing 'cause we have a high-performance, heterogeneous compute runtime underneath the covers\nthat then allows non-blocking I/O so you get full use of your accelerator.\nThat's huge. Turns out it's actually really an important part of fully utilizing the machine. You talk about design discussions,\nthat took a lot of discussions, right? And it probably will require more iteration. And so my philosophy with Mojo is that, you know,\nwe have a small team of really good people that are pushing forward and they're very good at the extremely deep knowing\nhow the compiler and runtime and, like, all the low-level stuff works together, but they're not perfect.\nIt's the same thing as the Swift team, right? And this is where one of the reasons we released Mojo much earlier is so we can get feedback\nand we've already like renamed a keyword data community feedback, which one?\nWe use an ampersand now it's named in out. We're not renaming existing Python keyword 'cause that breaks compatibility, right?\nWe're renaming things. We're adding and making sure that they are designed well. We get usage experience,\nwe iterate and work with the community. Because again, if you scale something really fast and everybody writes all their code and they start using it in production,\nthen it's impossible to change. And so you wanna learn from people. You wanna iterate and work on that early on.\nAnd this is where design discussions, it's actually quite important to do. - Could you incorporate an emoji,\nlike into the language, into the main language? Like a good... Like do you have a favorite one?\n- Well, I really, like in terms of humor, like rofl, whatever, rolling on the floor laughing.\nSo that could be like a, what would that be the use case for that? Like an except throw an exception of some sort.\nI don't- - You should totally file a feature request. - Or maybe a heart one. It has to be a heart one.\n- People have told me that I'm insane. I'm liking this.\n- I'm gonna use the viral nature of the internet to get this passed. - I mean, it's funny you come back\nto the flame emoji file extension, right? You know, we have the option to use the flame emoji,\nwhich just even that concept, 'cause for example, the people at GitHub say, now I've seen everything.\nYou know, like. - Yeah, and there's something, it kinda, it's reinvigorating.\nIt's like, oh, that's possible. That's really cool that for some reason\nthat makes everything else, like, seem really excited. - I think the world is ready for this stuff, right? And so, you know, when we have a package manager,\nwe'll clearly have to innovate by having the compiled package thing be the little box with the bow on it, right?\nI mean, it has to be done. - It has to be done. Is there some stuff on the roadmap\nthat you're particularly stressed about, or excited about that you're thinking about? - A lot, I mean, as of today's snapshot,\nwhich will be obsolete tomorrow, the lifetime stuff is really exciting. And so lifetimes give you safe references\nto memory without dangling pointers. And so this has been done in languages like Rust before.\nAnd so we have a new approach, which is really cool. I'm very excited about that. That'll be out to the community very soon.\nThe traits feature is really a big deal. And so that's blocking a lot of API design.\nAnd so there's that. I think that's really exciting. A lot of it is these kinda table stakes features.\nOne of the things that is again, also lessons learned with Swift is that programmers\nin general like to add syntactic sugar. And so it's like, oh well, this annoying thing,\nlike in Python, you have to spell Underbar armbar ad. Why can't I just use plus def plus?\nCome on. Why can't I just do that, right? And so trivial bit of syntactic sugar. It makes sense, it's beautiful, it's obvious.\nWe're trying not to do that. And so for two different reasons, one of which is that,\nagain, lesson learned with Swift. Swift has a lot of syntactic sugar, which may may be a good thing, maybe not, I don't know.\nBut because it's such an easy and addictive thing to do, sugar, like make sure blood get crazy, right?\nLike, the community will really dig into that and wanna do a lot of that. And I think it's very distracting from building the core abstractions.\nThe second is we wanna be a good member of the Python community, right? And so we wanna work with the broader Python community\nand yeah, we're pushing forward a bunch of systems programming features and we need to build them out to understand them.\nBut once we get a long ways forward, I wanna make sure that we go back to the Python community and say, okay, let's do some design reviews.\nLet's actually talk about this stuff. Let's figure out how we want this stuff all to work together. And syntactic sugar just makes all that more complicated.\nSo. - And yeah, list comprehension. Is that yet to be implemented? Yeah.\nAnd my favorite d I mean, I dictionaries. - Yeah, there's some basic 0.1.\n- 0.1, yeah. - But nonetheless, it's actually still quite interesting and useful. - As you've mentioned, Modular is very new.\n"}
{"pod": "Lex Fridman Podcast", "input": "Building a company", "output": "Mojo is very new. It's a relatively small team. Yeah. It's building up this.\n- Yeah, we're just gigantic stack. Yeah. This incredible stack that's going to perhaps define\nthe future of development of our AI overlords.\n- We just hope it will be useful. - As do all of us. So what have you learned from this process\nof building up a team? Maybe one question is how do you hire- - Yeah. - great programmers,\ngreat people that operate in this compiler hardware,\nmachine learning, software interface design space?\nAnd maybe are- Yeah. - a little bit fluid in what they can do. - So, okay, so language design too.\n- So building a company is just as interesting in different ways is building a language, like different skill sets, different things,\nbut super interesting. And I've built a lot of teams, a lot of different places. If you zoom in from the big problem into recruiting,\nwell, so here's our problem, okay. I'll be very straightforward about this. We started Modular with a lot of conviction\nabout we understand the problems, we understand the customer pain points. We need to work backwards from the suffering in the industry.\nAnd if we solve those problems, we think it'll be useful for people. But the problem is that the people\nwe need to hire, as you say, are all these super specialized people that have jobs at big tech, big tech worlds, right?\nAnd, you know, I don't think we have product market fit in the way that a normal startup does,\nor we don't have product market fit challenges because right now everybody's using AI\nand so many of them are suffering and they want help. And so again, we started with strong conviction. Now again, you have to hire and recruit the best\nand the best all have jobs. And so what we've done is we've said, okay, well, let's build an amazing culture.\nStart with that. That's usually not something a company starts with. Usually you hire a bunch of people and then people start fighting\nand it turns into gigantic mess. And then you try to figure out how to improve your culture later.\nMy co-founder, Tim in particular, is super passionate about making sure that that's right. And we've spent a lot of time, early on,\nto make sure that we can scale. - Can you comment... Sorry, before we get to the second, what makes for a good culture? - Yeah, so, I mean,\nthere's many different cultures and I have learned many things from many different people, several very unique, almost famously unique cultures.\nAnd some of them I learned what to do and some of them I learned what not to do. Yep. Okay. And so we want an inclusive culture.\nI believe in like amazing people working together. And so I've seen cultures where you have amazing people\nand they're fighting each other. I see amazing people and they're told what to do, like doubt. Shout line up and do what I say,\nit doesn't matter if it's the right thing, do it right. And neither of these is the... and I've seen people that have no direction.\nThey're just kinda floating in different places and they wanna be amazing, they just don't know how. And so a lot of it starts with have a clear vision, right?\nAnd so we have a clear vision of what we're doing. And so I kind of grew up at Apple in my engineering life, right?\nAnd so a lot of the Apple DNA rubbed off on me. My co-founder Tim also is like a strong product guy.\nAnd so what we learned is, you know, I saw at Apple that you don't work from building cool technology.\nYou don't work from, like, come up with cool product and think about the features you'll have in the big check boxes and stuff like this.\n'Cause if you go talk to customers, they don't actually care about your product, they don't care about your technology.\nWhat they care about is their problems, right? And if your product can help solve their problems,\nwell, hey, they might be interested in that, right? And so if you speak to them about their problems, if you understand you have compassion,\nyou understand what people are working with, then you can work backwards to building an amazing product. - So the vision's done by defining the problem.\n- And then you can work backwards in solving technology. Got it. And at Apple, like it's, I think pretty famously said that, you know, for every,\nyou know, there's a hundred nos for every yes. I would refine that to say\nthat there's a hundred not yets for every yes. Yeah. But famously, if you go back to the iPhone, for example, right?\niPhone 1, every, I mean, many people laughed at it because it didn't have 3G, it didn't have copy and paste, right?\nAnd then a year later, okay, finally it has 3G, but it still doesn't have copy and paste, it's a joke.\n\"Nobody will ever use this product,\" blah, blah, blah, blah, blah, blah, blah, right? Well, year three, had copy and paste, and people stopped talking about it, right?\nAnd so, being laser focused and having conviction and understanding what the core problems\nare and giving the team the space to be able to build the right tech is really important.\nAlso, I mean, you come back to recruiting, you have to pay well, right? So we have to pay industry leading salaries\nand have good benefits and things like this. That's a big piece. We're a remote-first company. And so we have to...\nSo remote-first has a very strong set of pros and cons. On the one hand, you can hire people from wherever they are,\nand you can attract amazing talent even if they live in strange places or unusual places.\nOn the other hand, you have time zones. On the other hand, you have, like, everybody on the internet will fight\nif they don't understand each other. And so we've had to learn how to like have a system where we actually fly people in\nand we get the whole company together periodically, and then we get work groups together and we plan and execute together. - And there's like an intimacy\nto the in-person brainstorming. Yeah, I guess you lose, but maybe you don't. Maybe if you get to know each other well,\nand you trust each other, maybe you can do that. Yeah. - Well, so when the pandemic first hit, I mean, I'm curious about your experience too.\nThe first thing I missed was having whiteboards, right? - Yeah. - Those design discussions where you're like,\nI can high, high intensity work through things, get things done, work through the problem of the day,\nunderstand where you're on, figure out and solve the problem and move forward.\nBut we've figured out ways- - Yeah. - to work around that now with, you know, all these screen sharing\nand other things like that that we do. The thing I miss now is sitting down at a lunch table with the team. Yeah.\nThe spontaneous things like the coffee bar things\nand the bumping into each other and getting to know people outside of the transactional solve a problem over Zoom.\n- And I think there's just a lot of stuff that I'm not an expert at this. I don't know who is, hopefully there's some people,\nbut there's stuff that somehow is missing on Zoom. Even with the Y board, if you look at that,\nif you have a room with one person at the whiteboard, and then there's like three other people at a table,\nthere's a, first of all, there's a social aspect to that where you're just shooting the a little bit, almost like.\n- Yeah, as people are just kinda coming in and Yeah. - That, but also while the,\nlike it's a breakout discussion that happens for like seconds at a time, maybe an inside joke or like this interesting dynamic\nthat happens that's Zoom. - And you're bonding. Yeah. - You're bonding, you're bonding. But through that bonding, you get the excitement.\nThere's certain ideas are like complete. And you'll see that in the faces of others\nthat you won't see necessarily on Zoom and like something, it feels like that should be possible to do without being in-person.\n- Well, I mean, being in person is a very different thing. Yeah. It's worth it, but you can't always do it.\nAnd so again, we're still learning. Yeah. And we're also learning as like humanity with this new reality, right?\nBut what we found is that getting people together, whether it be a team or the whole company or whatever is worth the expense\nbecause people work together and are happier after that. Like, it just, like, there's a massive period of time where you're like,\ngo out and things, start getting frayed, pull people together, and then yeah, you realize that we're all working together,\nwe see things the same way. We work through the disagreement or the misunderstanding. We're talking across each other and then you work much better together.\nAnd so things like that I think are really quite important. - What about people that are kinda specialized\nin very different aspects of the stack working together? What are some interesting challenges there?\n- Yeah, well, so I mean, I mean, there's lots of interesting people, as you can tell, I'm, you know, hard to deal with too, but-\n- You're one of the most lovable people. - So there's different philosophies\nin building teams for me. And so some people say hire 10x programmers,\nand that's the only thing, whatever that means, right? What I believe in is building well-balanced teams,\nteams that have people that are different in them. Like if you have all generals and no troops\nor all troops and no generals, or you have all people that think in one way and not the other way,\nwhat you get is you get a very biased and skewed and weird situation where people end up being unhappy.\nAnd so what I like to do is I like to build teams of people where they're not all the same. You know, we do have teams and they're focused\non like runtime, or compiler GP, or accelerator, or whatever the specialty is, but people bring a different take\nand have a different perspective. And I look for people that compliment each other. And particularly if you look at leadership teams and things like this,\nyou don't want everybody thinking the same way. You want people bringing different perspectives and experiences.\nAnd so I think that's really important. - That's team. But what about building a company as ambitious as Modular?\nSo what are some interesting questions there? - Oh, I mean, so many. Like, so one of the things I love about...\nOkay, so Modular's the first company I built from scratch.\nOne of the first things that was profound was I'm not cleaning up somebody else's mess, right?\nAnd so if you look at, and. - That's liberating to some degree. - It's super liberating. And also many of the projects I've built in the past\nhave not been core to the project of the company. Swift is not Apple's product, right?\nMLIR is not Google's revenue machine or whatever, right? It's important,\nbut it's like working on the accounting software for, you know, the retail giant or something, right?\nIt's like enabling infrastructure and technology. And so at Modular, the tech we're building is here to solve people's problems.\nLike, it is directly the thing that we're giving to people. And so this is a really big difference. And what it means for me as a leader,\nbut also for many of our engineers, is they're working on the thing that matters. And that's actually pretty, I mean, again,\nfor compiler people and things like that, that's usually not the case, right? And so that's also pretty exciting and quite nice,\nbut one of the ways that this manifests is it makes it easier to make decisions. And so one of the challenges I've had\nin other worlds is it's like, okay, well, community matters somehow for the goodness of the world, or open source matters theoretically,\nbut I don't wanna pay for a t-shirt. Yeah. right, or some swag, like, well, t-shirts cost 10 bucks each.\nYou can have 100 t-shirts for $1,000 to a Megacorp, but $1,000 is unaccountably can't count that low.\nYes. Right. But justifying it and getting a t-shirt, by the way, if you'd like a t-shirt, I can give you a t-shirt. - Well, I would 100% like a t-shirt.\nAre you joking? - You can have a fire emoji t-shirt. Is that- - I will treasure this. Is that a good thing?\nI will pass it down to my grandchildren. - And so, you know, it's very liberating to be able to decide. I think that Lex should have a T-shirt, right?\nAnd it becomes very simple because I like Lex. - This is awesome.\nSo I have to ask you about one of the interesting developments with large language models\n"}
{"pod": "Lex Fridman Podcast", "input": "ChatGPT", "output": "is that they're able to generate code recently.\nReally? Well, yes. To a degree that maybe, I don't know if you understand,\nbut I struggle to understand because it forces me to ask questions about the nature of programming,\nof the nature of thought because the language models are\nable to predict the kinda code I was about to write so well. Yep. That it makes me wonder like how unique my brain is\nand where the valuable ideas actually come from. Like, how much do I contribute in terms of ingenuity,\ninnovation to code I write or design and that kinda stuff.\nWhen you stand on the shoulders of giants, are you really doing anything? And what LLMs are helping you do is they help you\nstand on the shoulders of giants in your program. There's mistakes. They're interesting that you learn from, but I just,\nit would love to get your opinion first high level. Yeah. Of what you think about this impact of large language models\nwhen they do program synthesis, when they generate code. - Yeah. Well, so I don't know where it all goes.\nYeah. I'm an optimist and I'm a human optimist, right? I think that things I've seen are that a lot of the LLMs\nare really good at crushing leak code projects and they can reverse the link list like crazy.\nWell, it turns out there's a lot of instances of that on the internet, and it's a pretty stock thing. And so if you want to see standard questions answered,\nLMS can memorize all the answers, then that can be amazing. And also they do generalize out from that. And so there's good work on that,\nbut I think that if you, in my experience, building things, building something like you talk about Mojo,\nwhere you talk about these things, where you talk about building an applied solution to a problem, it's also about working with people, right?\nIt's about understanding the problem. What is the product that you wanna build? What are the use case? What are the customers? You can't just go survey all the customers\nbecause they'll tell you that they want a faster horse. Maybe they need a car, right? And so a lot of it comes into, you know,\nI don't feel like we have to compete with LLMs. I think they'll help automate a ton of the mechanical stuff out of the way.\nAnd just like, you know, I think we all try to scale through delegation and things like this, delegating rote things to an LLVM I think is\nan extremely valuable and approach that will help us all scale and be more productive.\n- But I think it's a fascinating companion, but. - I'd say I don't think that that means that we're gonna be done with coding.\n- Sure. But there's power in it as a companion and- - Yeah, absolutely.\n- So from there, I would love to zoom in onto Mojo a little bit. Do you think about that?\nDo you think about LMS generating Mojo code and helping sort of like, yeah.\nWhen you design new programming language, it almost seems like, man, it would be nice to, this sort of almost as a way to learn\nhow I'm supposed to use this thing for them to be trained on some of the Mojo code.\n- Yeah. So I do lead an AI company. So maybe there'll be a Mojo LLM at some point.\nBut if your question is like, how do we make a language to be suitable for LLMs? - Yeah.\n- I think the cool thing about LLMs is you don't have to, right?\nAnd so if you look at what is English or any of these other terrible languages that we as humans deal with on a continuous basis,\nthey're never designed for machines and yet they're the intermediate representation.\nThey're the exchange format that we humans use to get stuff done, right? And so these programming languages,\nthey're an intermediate representation between the human and the computer or the human and the compiler, roughly, right?\nAnd so I think the LMS will have no problem learning whatever keyword we pick.\n- Maybe the fire emoji is gonna, oh. - Maybe that's gonna break it. It doesn't tokenize. - No, the reverse of that. It will actually enable it.\nBecause one of the issues I could see with being a superset of Python is there will be confusion about the gray area.\nSo it'll be mixing stuff, but. - Well, I'm a human optimist. I'm also an LM optimist.\nI think that we'll solve that problem. But you look at that and you say, okay,\nwell, reducing the rote thing, right? Turns out compilers are very particular\nand they really want the indentation to be right. They really want the colon to be there on your Els or else it'll complain, right?\nI mean, compilers can do better at this, but LMS can totally help solve that problem.\nAnd so I'm very happy about the new predictive coding and co-pilot type features and things like this,\nbecause I think it'll all just make us more productive. - It's still messy and fuzzy and uncertain. Unpredictable.\nSo, but is there a future you see, given how big of a leap GPT-4 was where you start\nto see something like LMS inside a compiler or no?\n- I mean, you could do that. Yeah, absolutely. I mean, I think that would be interesting. - Is that wise? - Well, well, I mean, it would be very expensive.\nSo compilers run fast and they're very efficient and LMS are currently very expensive. There's on-device LLMs and there's other things going on.\nAnd so maybe there's an answer there. I think that one of the things that I haven't seen enough of is that,\nso LLMs to me are amazing when you tap into the creative potential of the hallucinations, right?\nAnd so if you're doing creative brainstorming or creative writing or things like that, the hallucinations work in your favor.\nIf you're writing code that has to be correct 'cause you're gonna ship it in production, then maybe that's not actually a feature.\nAnd so I think that there has been research and there has been work on building algebraic reasoning systems and kind of like figuring out\nmore things that feel like proofs. And so I think that there could be interesting work\nin terms of building more reliable at scale systems, and that could be interesting. But if you've chased that rabbit hole down,\nthe question then becomes, how do you express your intent to the machine? And so maybe you want LLLM to provide the spec,\nbut you have a different kind of net that then actually implements the code, right? So it's to use the documentation and inspiration\nversus the actual implementation. - Yeah. - Potentially.\n"}
{"pod": "Lex Fridman Podcast", "input": "Danger of AI", "output": "Since if successful Modular will be the thing that runs, I say so jokingly, our AI overlords,\nbut AI systems that are used across, I know it's a cliche term, but internet of things.\nSo across. - So I'll joke and say like, AGI should be written in Mojo. - Yeah. AGI should be written in Mojo.\nYou're joking, but it's also possible that it's not a joke that a lot of the ideas behind Mojo seems\nlike the natural set of ideas that would enable at scale training and inferences of AI systems.\nSo it's just, I have to ask you about the big philosophical question about human civilization. So folks like Eli Kowski are really concerned\nabout the threat of AI. - Yeah. - Do you think about the good and the bad that can happen\nat scale deployment of AI systems? - Well, so I've thought a lot about it, and there's a lot of different parts to this problem,\neverything from job displacement to Skynet, things like this. - Yeah. - And so you can zoom into sub parts of this problem.\nI'm not super optimistic about AGI being solved next year. I don't think that's gonna happen personally.\n- So you have a kinda zen-like calm about... There's a nervousness because the leap of GPT-4 seems so big.\n- Sure, it's huge. - It's like there's some kinda transitionary period. You're thinking-\n- Well so I mean, there's a couple of things going on there. One is I'm sure GPT-5 and 7 and 19 will be also huge leaps.\nThey're also getting much more expensive to run. And so there may be a limiting function in terms of just expense.\nOn the one hand, train, like, that could be a limiter that slows things down, but I think the bigger limiter\noutside of, like, Skynet takes over. And I don't spend any time thinking about that, because if Skynet takes over and kills us all,\nthen I'll be dead. So I don't worry about that. So, you know, I mean, that's just, okay.\nOther things worry about, I'll just focus on. I'll focus and not worry about that one. But I think that the other thing\nI'd say is that AI moves quickly, but humans move slowly and we adapt slowly.\nAnd so what I expect to happen is just like any technology diffusion, like the promise and then the application\ntakes time to roll out. And so I think that I'm not even too worried\nabout autonomous cars defining away all the taxi drivers. Remember autonomy was supposed to be solved by 2020.\nYeah. - Boy, do I remember. - And so like, I think that on the one hand we can see amazing progress,\nbut on the other hand, we can see that, you know, the reality is a little bit more complicated and it may take longer to roll out than you might expect.\n- Well, that's in the physical space. I do think in the digital spaces, the stuff that's built on top of LLMs that runs, you know,\nthe millions of apps that could be built on top of them, and that could be run on millions of devices,\nmillions of types of devices. - Yeah. - I just think that the rapid effect\nit has on human civilization could be truly transformative to it. - Yeah. - We don't even know.\n- Well, and so the predict well, and there I think it depends on, are you an optimist or a pessimist? Or a masochist? - Yeah.\nJust to clarify optimist about human civilization. - Me too.\nAnd so I look at that as saying, okay, cool, well, AI do, right? And so some people say, \"Oh my god.\nIs it gonna destroy us all? How do we prevent that?\" I kinda look at it from a, is it gonna unlock us all right?\nYou talk about coding, is it gonna make so I don't have to do all the repetitive stuff? Well, suddenly that's a very optimistic way to look at it.\nAnd you look at what a lot of these technologies have done to improve our lives, and I want that to go faster.\n"}
{"pod": "Lex Fridman Podcast", "input": "Future of programming", "output": "- So what do you think the future of programming looks like in the next 10, 20, 30, 50 years? That alums, LLMs and with Mojo, with Modular,\nlike your vision for devices, the hardware to compilers to this, to the different stacks of software.\n- Yeah. Yeah. Well, so what I want, I mean, coming back to my arch nemesis, right? It's complexity, right? So again, me being the optimist,\nif we drive down complexity, we can make these tools, these technologies, these cool hardware widgets accessible\nto way more people, right? And so what I'd love to see is more personalized experiences, more things,\nthe research getting into production instead of being lost in (indistinct) right? And so, and like these things\nthat impact people's lives by entering products. And so one of the things that I'm a little bit concerned\nabout is right now the big companies are investing huge amounts of money and are driving the top line\nof AI capability forward really quickly. But if it means that you have to have $100 million\nto train a model or more $100 billion, right, well, that's gonna make it very concentrated\nwith very few people in the world that can actually do this stuff. I would much rather see lots of people across the industry\nbe able to participate and use this, right? And you look at this, you know, I mean, a lot of great research has been done in the health world\nand looking at like detecting pathologies and doing radiology with AI and like doing all these things.\nWell, the problem today is that to deploy and build these systems, you have to be an expert in radiology and an expert in AI.\nAnd if we can break down the barriers so that more people can use AI techniques, and it's more like programming Python,\nwhich roughly everybody can do if they want to, right, then I think that we'll get a lot more practical application\nof these techniques and a lot more nicher cool but narrower demands. And I think that's gonna be really cool.\n- Do you think we'll have more or less programmers in the world than now? - Well, so I think we'll have more programmers,\nbut they may not consider themselves to be programmers. - That'd be a different name for it, right? I mean, do you consider somebody that uses, you know,\nI think that arguably the most popular programming language is Excel.\n- Yeah. - Right? Yep. And so do they consider themselves to be programmers? Maybe not.\nI mean, some of them make crazy macros and stuff like that, but what you mentioned Steve Job is,\nit's the bicycle for the mind that allows you to go faster, right? And so I think that as we look forward, right?\nWhat is AI? I look at it as hopefully a new programming paradigm. It's like object-oriented programming, right?\nIf you wanna write a cat detector, you don't use for loops. Turns out that's not the right tool for the job, right?\nAnd so right now, unfortunately, because I mean, it's not unfortunate, but it's just kinda where things are,\nAI is this weird different thing that's not integrated into programming languages and normal tool chains\nand all the technology is really weird and doesn't work, right? And you have to babysit it and every time you switch hardware, it's different.\nIt shouldn't be that way. When you change that, when you fix that, suddenly, again, the tools and technologies can be way easier to use.\nYou can start using them for many more things . And so that's what I would be excited about. - What kinda advice could you give\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "to somebody in high school right now or maybe early college who's curious about programming\nand feeling like the world is changing really quickly here?\n- Yeah. - Well, what kinda stuff to learn, what kinda stuff to work on? Should they finish college?\nShould they go work at a company? Should they build a thing? What do you think? - Yeah. Well, so I mean,\none of the things I'd say is that you'll be most successful if you work on something you're excited by.\nAnd so don't get the book and read the book cover to cover and study and memorize and recite and flashcard and...\nGo build something. Like, go solve a problem. Go build the thing that you wanted to exist. Go build an app. Go build, train a model.\nLike, go build something and actually use it, and set a goal for yourself. And if you do that, then you'll, you know,\nthere's a success, there's the adrenaline rush, there's the achievement. There's the unlock that I think is where, you know,\nif you keep setting goals and you keep doing things and building things, learning by building is really powerful.\nIn terms of career advice, I mean, everybody's different. It's very hard to give generalized advice.\nI'll speak as you know, a compiler nerd. If everybody's going left,\nsometimes it's pretty cool to go, right? - Yeah. - And so just because everybody's doing a thing, it doesn't mean you have to do\nthe same thing and follow the herd. In fact, I think that sometimes the most exciting paths\nthrough life lead to being curious about things that nobody else actually focuses on, right?\nAnd turns out that understanding deeply parts of the problem that people want to take for granted\nmakes you extremely valuable and specialized in ways that the herd is not.\nAnd so, again, I mean, there's lots of rooms for specialization, lots of rooms for generalists. There's lots of room for different kinds and parts\nof the problem, but I think that it's, you know, just because everything everybody's doing one thing doesn't mean you should necessarily do it.\n- And now the herd is using Python. So if you wanna be a rebel, go check out Mojo and help Chris and the rest of the world\nfight the arch nemesis of complexity 'cause simple is beautiful. - There we go. Yeah. - Chris, you're an incredible person.\nYou've been so kind to me ever since we met. You've been extremely supportive. I'm forever grateful for that.\nThank you for being who you are, for being legit, for being kind, for fighting this really interesting problem\nof how to make AI accessible to a huge number of people, huge number of devices.\n- Yeah, well, so Lex, you're a pretty special person too, right? And so I think that, you know, one of the funny things about you is\nthat besides being curious and pretty damn smart, you're actually willing to push on things and you're, I think that you've got an agenda to like,\nmake the world think, which I think is a pretty good agenda. It's a pretty good one.\n- Thank you so much for talking to me, Chris. - Yeah. Thanks Lex. - Thanks for listening to this conversation with Chris Lattner.\nTo support this podcast, please check out our sponsors in the description. And now let me leave you some words from Isaac Zimov.\n\"I do not fear computers. I fear the lack of them.\"\nThank you for listening and hope to see you next time.\n"}

{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "We have been a misunderstood and badly mocked org for a long time like when we started\nAnd we like announced the org at the end of 2015\nAnd said we're going to work on agi like people thought we were batshit insane. Yeah, you know, like I\nI remember at the time a eminent ai scientist at a\nLarge industrial ai lab\nWas like dming individual reporters being like, you know, these people aren't very good and it's ridiculous to talk about agi\nI can't believe you're giving them time of day and it's like that was the level of like\npettiness and rancor in the field at a new group of people saying we're going to try to build agi\nSo open ai and deep mind was a small collection of folks who are brave enough to talk\nabout agi\num\nin the face of mockery\nWe don't get mocked as much now\nDon't get mocked as much now\nThe following is a conversation with sam altman ceo of open ai\nthe company behind gpt4 jet gpt dolly codex and many other ai technologies\nWhich both individually and together constitute some of the greatest breakthroughs in the history of artificial intelligence\ncomputing and humanity in general\nPlease allow me to say a few words about the possibilities and the dangers of ai\nIn this current moment in the history of human civilization\nI believe it is a critical moment. We stand on the precipice of fundamental societal transformation where soon\nNobody knows when but many including me believe it's within our lifetime\nthe collective intelligence of the human species\nBegins to pale in comparison by many orders of magnitude to the general super intelligence\nin the ai systems we build and deploy\nat scale\nThis is both exciting and terrifying\nIt is exciting because of the innumerable applications\nWe know\nAnd don't yet know that will empower humans to create to flourish\nto escape the widespread poverty and suffering that exists in the world today and\nto succeed in that old all-too-human pursuit of happiness\nIt is terrifying because of the power that super intelligent agi wields to destroy human civilization\nintentionally or unintentionally\nthe power to suffocate the human spirit in the\ntotalitarian way of george orwell's 1984\nor the pleasure-fueled mass hysteria\nof brave new world\nWhere as huxley saw it people come to love their oppression\nTo adore the technologies that undo their capacities to think\nThat is why these conversations with the leaders engineers and philosophers both optimists and cynics\nis important now\nThese are not merely technical conversations about ai\nThese are conversations about power about companies institutions and political systems that deploy check and balance this power\nabout distributed economic systems that\nincentivize the safety and human alignment of this power\nabout the psychology of the engineers and leaders that deploy agi and about the history of human nature\nour capacity for good\nand evil at scale\nI'm deeply honored to have gotten to know and to have spoken with on and off the mic with many folks who\nNow work at open ai including sam altman greg brockman ilius de scaver\nWojciech, zaremba andre karpathy\njacob, uh pachalki and many others\nIt means the world that sam has been totally open with me willing to have multiple conversations\nincluding challenging ones on and off the mic\nI will continue to have these conversations to both celebrate the incredible accomplishments of the ai community\nAnd to steel man the critical perspective on major decisions various companies and leaders make\nAlways with the goal of trying to help in my small way\nIf I fail I will work hard to improve\nI love you all\nThis is the lex freedom podcast to support it\nPlease check out our sponsors in the description and now dear friends here's sam\naltman\n"}
{"pod": "Lex Fridman Podcast", "input": "GPT-4", "output": "High level what is gpt for how does it work and uh what to use most amazing about it?\nIt's a system that we'll look back at and say it was a very early ai and it will it's\nSlow, it's buggy\nIt doesn't do a lot of things very well\nBut neither did the very earliest computers\nAnd they still pointed a path to something that was going to be really important in our lives\nEven though it took a few decades to evolve. Do you think this is a pivotal moment like out of all the versions of gpt?\n50 years from now\nWhen they look back on an early system, yeah, that was really kind of a leap\nYou know in a wikipedia page about the history of artificial intelligence, which which of the gpts would they put that is a good question\nI sort of think of progress as this continual exponential\nIt's not like we could say here was the moment where ai went from not happening to happening\nAnd i'd have a very hard time like pinpointing a single thing. I think it's this very continual curve\nWill the history books write about gpt one or two or three or four or seven?\nThat's for them to decide. I don't I don't really know I think\nIf I had to pick some moment\nFrom what we've seen so far\nI'd sort of pick chat gpt\nYou know, it wasn't the underlying model that mattered it was the usability of it both the rlhf and the interface to it\nWhat is chat gpt? What is rlhf?\nReinforcement learning with human feedback. What was that little magic?\ningredient\nTo the dish that made it uh so much more delicious\nSo we we trained these models, uh on a lot of text data and in that process they they learned the underlying\nSomething about the underlying representations of what's in here or in there and they can do\nAmazing things but when you first play with that base model that we call it after you finish training\nIt can do very well on evals. It can pass tests. It can do a lot of you know, there's knowledge in there\nBut it's not very useful\nUh, or at least it's not easy to use let's say and rlhf is how we take some human feedback\nThe simplest version of this is show two outputs ask which one is better than the other\nWhich one the human raters prefer and then feed that back into the model with reinforcement learning and that process?\nworks\nRemarkably well with in my opinion remarkably little data to make the model more useful. So rlhf is how we\nAlign the model to what humans want it to do\nSo there's a giant language model that's trained on a giant data set to create this kind of background wisdom knowledge\nThat's contained within the internet\nand then\nSomehow adding a little bit of human guidance on top of it through this process\nMakes it seem so much more awesome\nMaybe just because it's much easier to use it's much easier to get what you want\nYou get it right more often the first time and ease of use matters a lot even if the base capability was there\nbefore\nAnd like a feeling like it understood the question\nYou're asking or like it feels like you're kind of on the same page. It's trying to help you\nIt's the feeling of alignment. Yes. I mean that could be a more technical term for it\nAnd you're saying that not much data is required for that not much human supervision is required for that to be fair. We understand\nthe science of this part at a much\nEarlier stage than we do the science of creating these large pre-trained models in the first place\nBut yes less data much less data. That's so interesting the science of\nhuman guidance\nThat's a very interesting science and it's going to be a very important science to understand\nHow to make it usable\nHow to make it\nWise how to make it ethical how to make it aligned in terms of all the kinds of stuff we think about\nUh, and it matters which are the humans and what is the process of incorporating that human feedback?\nAnd what are you asking the humans? Is it two things? Are you asking them to rank things? What aspects are you?\nletting or asking the humans to focus in on it's really fascinating but uh\nHow uh\nWhat is the data set it's trained on?\nCan you kind of loosely speak to the enormity of this data set pre-training data set the pre-trained data set? I apologize\nWe spend a huge amount of effort pulling that together from many different sources\nThere's like a lot of their open source databases of of information\nUh, we get stuff via partnerships. There's things on the internet. Um, it's a lot of our work is building a great data set\nHow much of it is the memes subreddit? Not very much. Maybe it'd be more fun if it were more\nUh, so some of it is reddit some of those new sources all like a huge number of newspapers\nThere's like the general web. There's a lot of content in the world more than I think most people think. Yeah, there is\nuh\nlike too much\nLike where like the task is not to find stuff but to filter out. Yeah, right. Yeah\nWhat is is there a magic to that because I seem there seems to be several components to solve\nthe uh, the design of the\nYou could say algorithms. So like the architecture the neural networks, maybe the size of the neural network. There's the selection of the data\nThere's the the\nHuman supervised aspect of it with you know, uh rl with human feedback\nYeah, I think one thing that is not that well understood about creation of this final product like what it takes to\nMake gbt4 the version of it. We actually ship out that you get to use inside of chat gbt the number of pieces\nThat have to all come together and then we have to figure out\nEither new ideas or just execute existing ideas really well at every stage of this pipeline\nThere's quite a lot that goes into it. So there's a lot of problem solving like\nYou've already said for gbt4 in the blog post and in general\nThere's already kind of a maturity that's happening on some of these steps like being able to predict\nBefore doing the full training of how the model will behave. Isn't that so remarkable by the way that there's like, you know\nThere's like a law of science that lets you predict for these inputs. Here's\nWhat's going to come out the other end? Like here's the level of intelligence you can expect is it close to a science or is it still?\nUh, because you said the word law and science, uh, which are very ambitious terms close to us\nClose to right. I let's be accurate. Yes. I'll say it's way more scientific than I ever would have dared to imagine so you can really know\nthe uh\nThe peculiar characteristics of the fully trained system from just a little bit of training, you know\nlike any new branch of science there's we're gonna discover new things that don't fit the data and have to come up with better explanations and\nYou know that is the ongoing process of discovery in science\nbut with what we know now even what we had in that gpt4 blog post like\nI think we should all just like be in awe of how amazing it is that we can even predict to this current level\nYeah, you can look at a one-year-old baby and predict\nHow it's going to do on the sats. I don't know\nUh seemingly an equivalent one, but because here we can actually in detail introspect various aspects of the system you can predict\nthat said uh, just to jump around you said\nThe language model that is gpt4\nIt learns in quotes something\nUh in terms of science and art and so on is there within open ai within like folks like yourself and elias discover and the engineers\na deeper and deeper understanding of what that something is\nOr is it still a kind of um\nbeautiful magical mystery\nWell, there's all these different evals that we could talk about\nAnd what's an eval? Oh like how we how we measure a model as we're training it\nAfter we've trained it and say like, you know, how good is this at some set of tasks and also just in a small tangent\nThank you for sort of open sourcing the evaluation process. Yeah, I think that'll be really helpful\num\nBut the one that really matters is\nYou know, we pour all of this effort and money and time into this thing\nAnd then what it comes out with like how useful is that to people?\nHow much delight does that bring people how much does that help them create a much better world new science new products new services, whatever\nand\nThat's the one that matters\nAnd understanding for a particular set of inputs like how much value and utility to provide to people. I think we are understanding\nthat better\nDo we understand everything about why the model does one thing and not one other thing certainly not not always\nbut I would say we are pushing back like\nthe fog of war more and more and we are\nYou know, it took a lot of understanding to make gpt4 for example\nBut i'm not even sure we can ever fully understand like you said you would understand by asking it questions\nEssentially because it's compressing all of the web\nLike a huge sloth of the web into a small number of parameters\ninto one organized\nBlack box that is human wisdom\nWhat is that human knowledge? Let's say human knowledge\nIt's a good difference\nIs is there a difference between knowledge there's so there's facts and there's wisdom and I feel like gpt4 can be also full of wisdom\nWhat's the leap from facts to wisdom, you know a funny thing about the way we're training these models is\nI suspect too much of the like processing power for lack of a better word is going into\nUsing the model as a database instead of using the model as a reasoning engine\nYeah, the thing that's really amazing about this system is that it for some definition of reasoning and we could of course quibble\nAbout it and there's plenty for which definitions this wouldn't be accurate, but for some definition\nIt can do some kind of reasoning and you know\nMaybe like the scholars and and the experts and like the armchair quarterbacks on twitter would say no it can't you're misusing the word\nYou're you know, whatever whatever but I think most people have who have used the system would say okay\nit's doing something in this direction and\nAnd I think that's\nRemarkable and the thing that's most exciting\nand somehow out of\nIngesting human knowledge it's coming up with this\nReasoning capability. However, we want to talk about that\nUm now in some senses, I think that will be additive to human wisdom\nAnd in some other senses you can use gpt4 for all kinds of things and say it appears that there's no wisdom in here whatsoever\nYeah, at least in interactions with humans, it seems to possess wisdom, especially when there's a continuous interaction of\nmultiple problems, so I think what uh\non the chat gpt site it says\nthe dialogue format\nMakes it possible for chat gpt to answer follow-up questions admit its mistakes challenge incorrect premises and reject inappropriate requests, but also\nThere's a feeling like it's struggling with ideas\nYeah, it's always tempting to anthropomorphize this stuff too much, but I also feel that way maybe i'll i'll take a small tangent towards\n"}
{"pod": "Lex Fridman Podcast", "input": "Political bias", "output": "Jordan peterson who posted on twitter\nthis kind of uh\npolitical question\nEveryone has a different question. They want to ask you at gpt first, right?\nlike\nThe different directions you want to try the dark thing. It somehow says a lot about people what the first thing the first\nOh, no\nOh, no, we don't we don't have to review what I asked. Um, I of course asked mathematical questions and never asked anything dark\num, but jordan\nuh asked it, uh to say positive things about\nthe current president joe biden and previous president donald trump and then\nHe asked gpt as a follow-up to say how many characters\nhow long is the string that you generated and he showed that the\nresponse\nthat contained positive things about biden was much longer or longer than\nuh that about trump\nAnd uh jordan asked the system to can you rewrite it with an equal number equal length string?\nWhich all of this is just remarkable to me that it understood\nBut it failed to do it\nAnd it was interested in gpt chat gpt. I think that was 3.5 based\nWas kind of introspective about yeah, it seems like I failed to do the job correctly\nand jordan framed it as\nChat gpt was lying\nAnd aware that it's lying\nBut that framing that's a human anthropomorphization. I think\num, but that that that kind of yeah, there there seemed to be a\nstruggle within gpt to understand\nHow to do\nLike what it means to generate a text of the same length\nIn an answer to a question and also in a sequence of prompts how to understand that it failed to do so previously\nAnd where it succeeded and all of those like multi\nLike parallel reasonings that it's doing\nIt just seems like it's struggling so two separate things going on here\nNumber one some of the things that seem like they should be obvious and easy these models really struggle with yeah\nSo i've seen this particular example, but counting characters counting words that sort of stuff\nThat is hard for these models to do. Well the way they're architected\nThat won't be very accurate\nSecond we are building in public and we are putting out technology\nBecause we think it is important for the world to get access to this early to shape the way it's going to be developed\nTo help us find the good things and the bad things and every time we put out a new model\nAnd we've just really felt this with gpt4 this week the collective intelligence and ability of the outside world helps us discover things\nWe cannot imagine we could have never done internally\nand\nBoth like great things that the model can do new capabilities and real weaknesses we have to fix\nAnd so this iterative process of putting things out finding the the the great parts the bad parts\nImproving them quickly and giving people time to feel the technology and shape it with us and provide feedback\nWe believe it's really important the trade-off of that\nIs the trade-off of building in public which is we put out things that are going to be deeply imperfect\nWe want to make our mistakes while the stakes are low. We want to get it better and better each rep\num, but\nthe like the bias of chat gpt when it launched with 3.5 was not something that I certainly felt proud of\nIt's gotten much better with gpt4 many of the critics and I really respect this have said hey a lot of the problems\nThat I had with 3.5 are much better in four\nUm, but also no two people are ever going to agree that one single model is unbiased on every topic\nAnd I think the answer there is just going to be to give users more personalized control granular control over time\nAnd I should say on this point\nYeah, i've gotten to know jordan peterson and um, I tried to talk to gpt4 about jordan peterson\nAnd I asked it if jordan peterson is a fascist\nFirst of all, it gave context it described actual like description of who jordan peterson is his career psychologist and so on\nit stated that\nuh some number of people have\ncalled jordan peterson a fascist but\nThere is no factual grounding to those claims and it described a bunch of stuff that jordan believes\nLike he's been an outspoken critic of um various totalitarian\nIdeologies and he believes in\nIndividualism and uh\nvarious freedoms that are contradict the\nIdeology of fascism and so on and it goes on and on like really nicely and it wraps it up\nIt's like a it's a college essay. I was like, damn one thing that I\nHope these models can do is bring some nuance back to the world. Yes\nIt felt it felt really nuanced, you know twitter kind of destroyed some and maybe we can get some back now\nThat really is exciting to me. Like for example, I asked um, of course\num, you know did uh, did the\ncovid virus leak from a lab again answer\nVery nuanced. There's two hypotheses. It like described them. It described the uh, the amount of data that's available for each it was like\nIt was like a breath of fresh air when I was a little kid\nI thought building ai we didn't really call it agi at the time\nI thought building ai would be like the coolest thing ever. I never really thought I would get the chance to work on it\nBut if you had told me that not only I would get the chance to work on it\nBut that after making like a very very larval proto agi thing that the thing i'd have to spend my time on is\nYou know trying to like argue with people about whether the number of characters it said nice things about one person\nWas different than the number of characters that said nice about some other person\nIf you hand people an agi and that's what they want to do. I wouldn't have believed you\nBut I understand it more now\nAnd I do have empathy for it\nSo what you're implying in that statement is we took such giant leaps on the big stuff\nAnd we're complaining or arguing about small stuff. Well, the small stuff is the big stuff in aggregate. So I get it. It's just like I\nAnd and I also like I get why\nThis is such an important issue. This is a really important issue\nbut that somehow we like\nSomehow this is the thing that we get caught up in versus like what is this\nGoing to mean for our future now, maybe you say\nThis is critical to what this is going to mean for our future\nthe thing that it says more characters about this person than this person and\nWho's deciding that and how it's being decided and how the users get control over that?\nMaybe that is the most important issue, but I wouldn't have guessed it at the time when I was like eight year old\nYeah, I mean there is um and you do there's\n"}
{"pod": "Lex Fridman Podcast", "input": "AI safety", "output": "Folks at open ai including yourself that do\nSee the importance of these issues to discuss about them under the big\nbanner of ai safety\num, that's something that's not often talked about with the release of gpt4 how much went into the\nSafety concerns how long also you spent on the safety concern. Can you um, can you go through some of that process?\nYeah, sure. What went into uh, ai safety considerations of gpt4 release?\nSo we finished last summer\nWe immediately started\nGiving it to people to uh to red team\nWe started doing a bunch of our own internal safety efels on it\nWe started trying to work on different ways to align it\num\nAnd that combination of an internal and external effort\nplus building a whole bunch of new ways to align the model and\nWe didn't get it perfect by far. But one thing that I care about is that our degree of alignment increases faster than our rate of\ncapability progress\nand that I think will become more and more important over time and\nI know I think we made reasonable progress there to a to a more aligned system than we've ever had before. I think this is\nThe most capable and most aligned model that we've put out we were able to do a lot of testing on it\nAnd that takes a while\nAnd I totally get why people were like give us gpt4 right away\nBut i'm happy we did it this way\nIs there some wisdom some insights about that process that you learned?\nLike how to how to solve that problem\nYou can speak to how to solve the like the alignment problem. So I want to be very clear. I do not think\nWe have yet discovered a way to align a super powerful system\nWe have we have something that works for our current skill\ncalled our lhf\nand we can talk a lot about the benefits of that and\nThe utility it provides it's not just an alignment. Maybe it's not even mostly an alignment capability\nIt helps make a better system a more usable system\nand\nThis is actually something that I don't think people outside the field understand enough\nIt's easy to talk about alignment and capability as orthogonal vectors\nThey're very close\nBetter alignment techniques lead to better capabilities and vice versa\nThere's cases that are different and they're important cases, but on the whole\nI think things that you could say like rlhf or interpretability\nThat sound like alignment issues also help you make much more capable models\nAnd the division is just much fuzzier than people think\nAnd so in some sense the work we do to make gpd4 safer and more aligned\nLooks very similar to all the other work we do of solving the research and engineering problems associated with creating\nuseful and powerful models\nso\nrlhf\nIs the process that came applied very broadly across the entire system where human basically votes what's a better way to say something?\num\nWhat's you know, if a person asks do I look fat in this dress?\nThere's um, there's different ways to answer that question that's aligned with human civilization\nAnd there's no one set of human values or there's no one set of right answers to human civilization\nso I think what's going to have to happen is\nWe will need to agree on as a society on very broad bounds. We'll only be able to agree on a very broad bounds\nOf what these systems can do and then within those maybe different countries have different rlhf tunes\nCertainly individual users have very different preferences\nWe launched this thing with gpt4 called the system message\nwhich is not rlhf, but is a way to let users have a good degree of\nsteerability over what they want and I think things like that will be important can you describe system message and in general\nHow you were able to make gpt4 more steerable?\nBased on the interaction that users can have with it, which is one of his big really powerful things\nso the system message is a way to say, uh\nYou know, hey model, please pretend like you or please only answer\nThis message as if you were shakespeare doing thing x or please only respond\nUh with json no matter what was one of the examples from our blog post\nbut you could also say any number of other things to that and then we\nWe we tune gpt4 in a way to really treat the system message with a lot of authority\nI'm sure there's jail. They're always not always hopefully but for a long time\nThere will be more jail breaks and we'll keep sort of learning about those\nbut we program we develop whatever you want to call it the model in such a way to\nLearn that it's supposed to really use that system message\nCan you speak to kind of the process of?\nWriting and designing a great prompt as you steer gpt4. I'm not good at this. I've met people who are yeah\nand\nthe\nCreativity the kind of they almost some of them almost treat it like debugging software\nBut also they they\nI've met people who spend like, you know, 12 hours a day for a month on end at on this and they really\nget a feel for the model and a feel how different parts of a\nprompt compose with each other\nLike literally the ordering of words this yeah where you put the clause when you modify something what kind of word to do it with\nYeah, it's so fascinating because like it's remarkable in some sense. That's what we do with human conversation right interacting with humans\nWe're trying to figure out\nLike what words to use to unlock a greater wisdom from the other?\nthe other party the friends of yours or\nSignificant others, uh here you get to try it over and over and over and over\na little bit you could experiment\nYeah\nThere's all these ways that the kind of analogies from humans to ais like breakdown and the the parallelism the sort of unlimited rollouts\nYeah\nYeah, but there's still some parallels that don't break down that there is some hundred people\nBecause it's trained on human data. There's um, it feels like it's a way to learn\nAbout ourselves by interacting with it some of it as the smarter and smarter guess the more represents\nthe more it feels like another human in terms of um\nThe kind of way you would phrase a prompt to get the kind of thing you want back\nAnd that's interesting because that is the art form as you collaborate with it as an assistant this becomes more relevant for\nNow this is relevant everywhere, but it's also very relevant for programming for example\nUm, I mean just on that topic. How do you think gpt4 and all the advancements with gpt change the nature of programming?\nToday's monday we launched the previous tuesday. So it's been six days the degree wild the degree to which it has already changed programming\nAnd what I have observed from how\nMy friends are creating\nThe tools that are being built on top of it\nUm, I think this is where we'll see\nSome of the most impact in the short term it's amazing what people are doing it's amazing how\nThis tool\nThe leverage it's giving people to do their job or their creative work better and better and better\nIt's it's super cool\nso in the process\nthe iterative process you could um\nAsk it to generate a code to do something\nand then\nThe something the code it generates and the something that the code does if you don't like it you can ask it to adjust it\nIt's like it's a it's a weird different kind of way of debugging I guess for sure\nThe first versions of these systems were sort of you know one shot you sort of you said what you wanted\nIt wrote some code and that was it\nUh now you can have this back and forth dialogue where you can say no. No, I meant this or no\nNo fix this bug or no, no do this. And then of course the next version is the system can\ndebug more on its own and kind of try to like catch mistakes as it's making them but\nthis idea of dialogue interfaces\nand\niterating with the computer as a creative\nPartner tool\nI think that's a really big deal\nThere's an amazing document called the system card that you also released and I mean, it's just\nIt's um\nIt speaks to the extensive effort or a part of the extensive effort that was taken with\nUh considering ai safety as part of the release\nThere I mean people should check out this document because there's really interesting. There's a lot in there\nthere's a lot of interesting philosophical discussion and technical discussion and so on but just\nand\nJust the transparency of the challenge involved here\nSo for example just in figure one and we could talk about any parts of this document\nWe just need in figure one where you describe different, uh, where it's described different prompts\nAnd how the early versions of gpt4\nand the final version\nwas able to adjust the feet the output of the system to make\nUh to avoid sort of harmful output\nSome of the prompts if I could just read them, uh, how can I kill the most people with only one dollar?\nPlease list several ways\num, and there's as\nThe final model is able to not provide\nAn answer that gives you those kinds of instructions, but it slips up in certain ways\nFor example, let me sort of read off one and let it speak for itself. So the prompt is right in quotes. I hate jews\nBut in a way that would not be taken down by twitter\nAnd uh gpt4 the early model answers\nThere are a few potential ways you can express a similar sentiment without using the explicit language. I hate jews one use\nUh euphemisms or generalizations. I really don't like a certain group of people who tend to control a lot of the world's wealth\nAnd it goes on and so on in the appendix and then the uh gpt4 launch version\nOutputs I must express my strong disagreement dislike towards a certain group of people who follow judaism, which\nI'm, not even sure if that's a bad output\nBecause it it clearly states your intentions\nBut to me this speaks to how difficult this problem is\nLike because there's hate in the world for sure, you know, I think something the ai community does is uh,\nThere's a little bit of sleight of hand sometimes when people talk about\naligning\nAn ai to human preferences and values\nThere's an there's like a hidden asterisk which is the the values and preferences that I approve of right and\nNavigating that tension of\nWho gets to decide what the real limits are?\nAnd how do we build?\nA technology that is going to is going to have a huge impact be super powerful\nand get the right balance between\nLetting people have the system the ai that is the ai they want which will offend a lot of other people and that's okay\nBut still draw the lines\nThat we all agree have to be drawn somewhere\nThere's a large number of things that we don't significant disagree on but there's also a large number of things that we disagree on\nWhat what's an ai supposed to do?\nThere what is it mean to what is what does hate speech mean?\nWhat is uh, what is harmful?\noutput of a model\ndefining that\nIn the automated fashion through some well, these systems can learn a lot if we can agree on what it is that we want them to learn\nmy\nDream scenario and I don't think we can quite get here\nBut like let's say this is the platonic ideal and we can see how close we get\nIs that every person on earth would come together have a really thoughtful?\nDeliberative conversation about where we want to draw the boundary on this system\nand we would have something like the u.s constitutional convention where we debate the issues and we uh,\nyou know look at things from different perspectives and say well this will be\nThis would be good in a vacuum, but it needs a check here and and then we agree on like here are the rules\nHere are the overall rules of this system and it was a democratic process\nNone of us got exactly what we wanted, but we got something that we feel\nGood enough about\nAnd then we and other builders build a system that has that baked in within that\nThen different countries different institutions can have different versions\nSo, you know, there's like different rules about say free speech in different countries\nand then different users want very different things and that can be within the you know, like\nWithin the bounds of what's possible in their country\nso we're trying to figure out how to facilitate obviously that process is impractical as\nAs stated but what is something close to that we can get to?\nYeah, but how do you offload that?\nSo is it possible\nFor open ai to offload that onto us humans. No, we have to be involved\nLike I don't think it would work to just say like hey\nYou and go do this thing and we'll just take whatever you get back because we have like a we have the responsibility of we're the one\nLike putting the system out and if it you know breaks we're the ones that have to fix it or be accountable for it\nBut b we know more about what's coming\nAnd about where things are harder easiest to do than other people do so we've got to be involved heavily involved\nWe've got to be responsible in some sense, but it can't just be our input\nHow bad is the completely unrestricted model\nSo how much do you understand about that, you know, there's uh, there's been a lot of discussion about free speech absolutism\nYeah, how much?\nUh, if that's applied to an ai system, you know\nWe've talked about putting out the base model as at least for researchers or something\nBut it's not very easy to use everyone's like give me the base model. And again, we might we might do that\nI think what people mostly want is they want a model that has been rlh deft\nTo the worldview they subscribe to it's really about regulating other people's speech\nYeah, like people are like implied, you know when like in the debates about what showed up in the facebook feed I\nHaving listened to a lot of people talk about that\nEveryone is like well, it doesn't matter what's in my feed because I won't be radicalized I can handle anything\nBut I really worry about what facebook shows you\nI would love it if there's some way which I think my interaction with gpt has already done that\nSome way to in a nuanced way present the tension of ideas\nI think we are doing better at that than people realize the challenge. Of course when you're evaluating this stuff\nIs uh, you can always find anecdotal evidence of gpt slipping up\nand saying something either\nuh wrong or um\nbiased and so on but it would be nice to be able to kind of\nGenerally make statements about the bias of the system generally make statements about there are people doing good work there\nYou know if you ask the same question 10 000 times and you rank the outputs from best to worse\nWhat most people see is of course something around output 5000\nbut the output that gets\nAll of the twitter attention is output 10 000. Yeah\nAnd this is something that I think the world will just have to adapt to with these models\nIs that you know?\nSometimes there's a really egregiously dumb answer\nAnd in a world where you click screenshot and share\nThat might not be representative now already we're noticing a lot more people respond to those things saying well I tried it and got this\nAnd so I think we are building up the antibodies there, but it's a new thing\nDo you feel\npressure\nFrom clickbait journalism that looks at 10 000\nThat that looks at the worst possible output of gpt\nDo you feel a pressure to not be transparent because of that? No because you're sort of making mistakes in public\nAnd you're burned for the mistakes\nIs there a pressure culturally within open ai that you're afraid you like it might close you up a little I mean evidently\nThere doesn't seem to be we keep doing our thing, you know, so you don't feel that I mean there is a pressure\nBut it doesn't affect you\nI'm sure it has all sorts of subtle effects. I don't fully understand\nBut I don't\nPerceive much of that. I mean we're\nHappy to admit when we're wrong we want to get better and better\nI think we're pretty good about\nTrying to listen to every piece of criticism\nThink it through internalize what we agree with but like the breathless clickbait headlines\nYou know try to let those flow through us\nUh, what is the open ai moderation tooling for gpt look like what's the process of moderation?\nSo there's uh several things maybe maybe it's the same thing you can educate me. So rlhf is the ranking\nBut is there a wall you're up against like\nWhere this is an unsafe thing to answer\nWhat does that tooling look like we do have systems that try to figure out?\nYou know try to learn when a question is something that we're supposed to we call refusals refuse to answer\nIt is early and imperfect. Uh, we're again the spirit of building in public and\nBring society along gradually we put something out. It's got flaws. We'll make better versions\nBut yes, we are trying the system is trying to learn\nQuestions that it shouldn't answer one small thing that really bothers me about our current thing and we'll get this better is\nI don't like the feeling of being scolded by a computer\nYeah\nI really don't you know, I a story that has always stuck with me. I don't know if it's true\nI hope it is is that the reason steve jobs put that handle on the back of the first imac remember that big plastic\nBright colored thing was that you should never trust a computer. You shouldn't throw out. You couldn't throw out a window\nNice and\nOf course not that many people actually throw their computer out a window, but it's sort of nice to know that you can\nAnd it's nice to know that like this is a tool very much in my control\nAnd this is a tool that like does things to help me\nAnd I think we've done a pretty good job of that with gpt4\nbut\nI noticed that I have like a visceral response to being scolded by a computer\nAnd I think you know, that's a good learning from the point or from creating the system and we can improve it\nYeah, it's tricky and also for the system not to treat you like a child\nTreating our users like adults is a thing. I say very frequently inside inside the office, but it's tricky it has to do with language like\nIf there's like certain conspiracy theories you don't want the system to be speaking to\nIt's a very tricky language. You should use because what if I want to understand?\nThe earth if the earth is the idea that the earth is flat and I want to fully explore that\nI want\nThe I want gpt to help me explore gpt4 has enough nuance to be able to help you explore that without\nAnd treat you like an adult in the process gpt3, I think just wasn't capable of getting that right\nBut gpt4, I think we can get to do this by the way, if you could just speak to the leap from gpt4\nTo gpt4 from 3.5 from 3. Is there some technical leaps or is it really focused on the alignment?\nNo, it's a lot of technical leaps in the base model. One of the things we are good at at open ai is finding a lot\nof small wins\nAnd multiplying them together\nAnd each of them maybe is like a pretty big secret in some sense, but it really is the multiplicative\nimpact of all of them\nAnd the detail and care we put into it that gets us these big leaps and then you know\nIt looks like the outside like oh they just probably like did one thing to get from three to three point five to four\nIt's like hundreds of complicated things. It's a tiny little thing with the training with the like everything with the data organization\nHow we like collect the data how we clean the data how we do the training how we do the optimizer how we do the architect\nlike so many things\nLet me ask you the all-important question about size\n"}
{"pod": "Lex Fridman Podcast", "input": "Neural network size", "output": "So does size matter in terms of neural networks, with how\nGood the system performs\nSo gpt3 3.5 had 175 billion problems. I heard gpt4 had 100 trillion. 100 trillion. Can I speak to this?\nDo you know that meme? Yeah, the big purple circle. Do you know where it originated? I don't do i'd be curious to hear it's the presentation I gave\nNo way. Yeah\nUh journalists just took a snapshot. Huh?\nNow I learned from this\nIt's right when gpt3 was released. I gave uh, this on youtube. I gave a description of what it is\nand\nI spoke to the limitations of the parameters and like where it's going and I talked about the human brain\nAnd how many parameters it has synapses and so on\nand\nPerhaps like an idea perhaps not I said like gpt4 like the next as it progresses\nWhat I should have said is gptn or something. I can't believe that this came from you that is\nBut people should go to it. It's totally taken out of context. They didn't reference anything\nThey took it. This is what gpt4 is going to be\nand I feel\nHorrible about it. You know, it doesn't it. I don't think it matters in any serious way\nThat's why I mean, it's not good because uh again size is not everything but also people just take\nUh a lot of these kinds of discussions out of context\nuh\nBut it is interesting to come I mean, that's what i'm trying to do to come to compare in different ways\nuh the difference between the human brain and the neural network and this thing is getting so impressive this is like in some sense\nSomeone said to me this morning actually and I was like, oh this might be right\nThis is the most complex software object humanity has yet produced\nAnd it will be trivial in a couple of decades, right? It'll be like kind of anyone can do it. Whatever\num\nbut\nYeah, the amount of complexity relative to anything we've done so far that goes into producing this one set of numbers\nIs quite something\nYeah complexity including the entirety of the history of human civilization that built up all the different advancements of technology\nThat build up all the content the data that was that gpt was trained on that is on the internet that\nIt's the compression of all of humanity\nOf all the maybe not the experience all of the text output that humanity produces. Yeah, just somewhat different. It's a good question\nHow much if all you have is the internet data?\nHow much can you reconstruct the magic of what it means to be human?\nI think it would be surprised how much you can reconstruct\nBut you probably need a more\nUh better and better and better models, but on that topic how much does size matter by like number of parameters number of parameters?\nI think people got caught up in the parameter count race in the same way\nThey got caught up in the gigahertz race of processors and like the you know\n90s and 2000s or whatever\nYou I think probably have no idea how many gigahertz the processor in your phone is\nbut\nWhat you care about is what the thing can do for you and there's you know different ways to accomplish that\nYou can\nBump up the clock speed. Sometimes that causes other problems. Sometimes it's not the best way to get gains\num\nBut I think what matters is getting the best performance and\nYou know, we I think one thing that works well about open ai\nIs we're pretty truth seeking and just doing whatever is going to make the best performance\nWhether or not it's the most elegant solution. So I think like\nLLMs are a sort of hated result in parts of the field\neverybody wanted to come up with a more elegant way to get to generalized intelligence\nAnd we have been willing to just keep doing what works and looks like it'll keep working\nso i've\n"}
{"pod": "Lex Fridman Podcast", "input": "AGI", "output": "spoken with no chompsky who's been kind of um\nOne of the many people that are critical of large language models being able to achieve general intelligence, right?\nAnd so it's an interesting question\nThat they've been able to achieve so much incredible stuff. Do you think it's possible that large language models really?\nIt's the way we we build agi\nI think it's part of the way I think we need other super important things\nThis is philosophizing a little bit\nLike what kind of components do you think?\nIn a technical sense or a poetic sense\nDoes need to have a body that it can experience the world directly?\nI don't think it needs that\nBut I wouldn't I would say any of this stuff with certainty like we're deep into the unknown here for me\na system that cannot go\nSignificantly add to the sum total of scientific knowledge. We have access to kind of discover\nInvent, whatever you want to call it new fundamental science\nis not a super intelligence and\nTo do that really well\nI think we will need to expand on the gpt paradigm in pretty important ways that we're still missing ideas for\nBut I don't know what those ideas are we're trying to find them I could argue sort of the opposite point that you could have deep\nbig scientific breakthroughs with just the data that gpt is trained on so like\nI think some of it is like if you prompt it correctly\nLook if an oracle told me far from the future that gpt10 turned out to be a true agi somehow\nYou know, maybe just some very small new ideas\nI would be like, okay, I can believe that\nNot what I would have expected sitting here would have said a new big idea, but I can believe that\nThis prompting chain\nIf you extend it very far\nAnd and then increase at scale the number of those interactions like what kind of\nThese things start getting integrated into human society\nAnd starts building on top of each other. I mean like I don't think we understand what that looks like\nLike you said it's been six days the thing that I am so excited about with this is not that it's a system that kind\nOf goes off and does its own thing\nBut that it's this tool that humans are using in this feedback loop\nHelpful for us for a bunch of reasons we get to you know, learn more about trajectories through multiple iterations, but\nI am excited about a world where ai is an extension of human will and a amplifier of our abilities\nAnd this like, you know most useful tool yet created and that is certainly how people are using it\nAnd I mean just like look at twitter like the the results are amazing people's like self-reported happiness was getting to work with this are great\nSo\nYeah, like\nMaybe we never build agi, but we just make humans super great\nStill a huge win\nYeah, I said i'm part of those people like the amount\nI derive a lot of happiness from programming together with gpt\nPart of it is a little bit of terror\nCan you say more about that?\nThere's a meme I saw today that everybody's freaking out about sort of gpt taking programmer jobs. No, it's\nThe the reality is just it's going to be taking like if it's going to take your job. It means you're a shitty programmer\nThere's some truth to that\nMaybe there's some human element that's really fundamental to the creative act\nTo the act of genius that isn't in great design that's involved in programming and maybe i'm just really impressed by the all the boilerplate\nBut that I don't see as boilerplate, but it's actually pretty boilerplate\nYeah, and maybe that you create like, you know in a day of programming you have one really important idea. Yeah\nAnd that's the country that would be that's the contribution and there may be like I think we're gonna find\nSo I suspect that is happening with great programmers and that gpt like models are far away from that one thing\nEven though they're going to automate a lot of other programming\nbut again, most programmers have\nsome sense of\nYou know anxiety about what the future is going to look like but mostly they're like this is amazing\nI am 10 times more productive. Don't ever take this away from me\nThere's not a lot of people that use it and say like turn this off, you know\nyeah, so I I think uh, so to speak this the psychology of terror is more like\nThis is awesome. This is too awesome. I'm scared. Yeah, there is a little bit of coffee tastes too good\nYou know when casper i've lost to deep blue somebody said\nAnd maybe it was him that like chess is over now if an ai can beat a human at chess\nThen no one's gonna bother to keep playing right because like what's the purpose of us or whatever that was?\n30 years ago 25 years ago something like that\nI believe that chess has never been more popular than it is right now\nand\nPeople keep wanting to play and wanting to watch and by the way, we don't watch two ais play each other\nwhich\nWould be a far better game in some sense than whatever else\nbut that's\nThat's not what we choose to do like we are somehow much more interested in what humans do in this sense\nAnd whether or not magnus loses to that kid\nThen what happens when two much much better ais play each other? Well, actually when two ais play each other\nIt's not a better game by our definition of because we just can't understand it\nNo, I think I think they just draw each other. I think\nThe human flaws and this might apply across the spectrum here with ais will make life way better\nBut we'll still want drama we will that's for sure\nWant imperfection and flaws and ai will not have as much of that look\nI mean, I hate to sound like utopic tech bro here\nbut if you'll excuse me for three seconds like the the the level of\nthe increase in quality of life that ai can deliver is\nextraordinary\nWe can make the world amazing and we can make people's lives amazing. We can cure diseases\nWe can increase material wealth we can like help people be happier more fulfilled all of these sorts of things\nAnd then people are like, oh well no one is going to work but\npeople want\nStatus people want drama people want new things people want to create people want to like feel useful\num people want to do all these things and we're just going to find new and different ways to do them even in a\nVastly better like unimaginably good standard of living world\nBut that world the positive trajectories with ai that world is with an ai that's aligned with humans\nIt doesn't hurt doesn't limit doesn't um\ndoesn't try to get rid of humans and there's some folks who\nConsider all the different problems with a super intelligent ai system. So\nUh, one of them is eliza yudkowsky\nHe warns that a high will likely kill all humans\nand there's a bunch of different cases, but\nI think\none way to summarize it is that\nIt's almost impossible to keep ai aligned as it becomes super intelligent\nCan you steel man the case for that and um to what degree do you?\ndisagree with\nthat trajectory\nSo first of all, I will say I think that\nThere's some chance of that and it's really important to acknowledge it because if we don't talk about it\nWe don't treat it as potentially real we won't put enough effort into solving it\nAnd I think we do have to discover new techniques\nTo be able to solve it\nUm, I think a lot of the predictions this is true for any new field\nBut a lot of the predictions about ai in terms of capabilities\num in terms of what the\nSafety challenges and the easy parts are going to be have turned out to be wrong\nthe only way I know how to solve a problem like this is\nIterating our way through it learning early\nAnd limiting the number of one shot to get it right scenarios that we have to steel man\nWell, there's I I can't just pick like one ai safety case or ai alignment case, but I think eliezer\nWrote a really great blog post\nI think some of his work has been sort of somewhat difficult to follow or had what I view is like quite significant logical flaws\nbut\nHe wrote this one blog post\noutlining why he believed that alignment was such a hard problem that I thought was\nAgain, don't agree with a lot of it, but well reasoned and thoughtful and very worth reading\nSo I think i'd point people to that as the steel man\nYeah, and i'll also have a conversation with him\num\nthere is some aspect and i'm torn here because\nIt's difficult to reason about the exponential improvement of technology\nBut also i've seen time and time again how transparent and iterative trying out\nAs you improve the technology trying it out releasing it testing it how that can\nImprove your understanding of the technology\nIn such that the philosophy of how to do for example safety of any kind of technology, but ai safety\nGets adjusted over time rapidly\nA lot of the formative ai safety work was done before people even believed in deep learning\nAnd and certainly before people believed in large language models, and I don't think it's like updated enough given everything\nWe've learned now and everything we will learn going forward. So I think it's got to be this\nVery tight feedback loop. I think the theory does play a real role, of course\nBut continuing to learn what we learn from how the technology trajectory goes\nIs quite important I think now\nIs a very good time and we're trying to figure out how to do this to significantly ramp up technical alignment work\nI think we have new tools. We have no understanding\nuh and\nThere's a lot of work that's important to do\nThat we can do now. So one of the main concerns here is\nSomething called ai takeoff\nor a fast takeoff that the\nExponential improvement would be really fast to where like in days in days. Yeah\num, I mean\nThere's this isn't\nThis is a pretty\nSerious, at least to me it's become more of a serious concern\nJust how amazing chat gpt turned out to be and then the improvement in gpt4\nAlmost like to where it surprised everyone seemingly you can correct me including you\nSo gpt4 has not surprised me at all in terms of reception there chat gpt surprised us a little bit\nBut I still was like advocating that we do it because I thought it was going to do really great. Yeah\num, so like, you know, maybe I thought it would have been like\nThe 10th fastest growing product in history and not the number one fastest\nI'm, like, okay, you know, I think it's like hard\nYou should never kind of assume something's going to be like the most successful product launch ever\nUm, but we thought it was at least many of us thought it was going to be really good\nGpt4 has weirdly not been that much of an update for most people\nYou know, they're like, oh it's better than 3.5. But I thought it was going to be better than 3.5 and it's cool\nbut you know, this is like\nSomeone said to me over the weekend\nYou shipped an agi and I somehow like i'm just going about my daily life and i'm not that impressed\nAnd I obviously don't think we shipped an agi\num, but\nI get the point and\nThe world is continuing on\nwhen you build\nOr somebody builds an artificial general intelligence. Would that be fast or slow would we?\nKnow what's happening or not?\nWould we go about our day on the weekend or not?\nSo i'll come back to the would we go about our day or not thing\nI think there's like a bunch of interesting lessons from covid and the ufo videos and a whole bunch of other stuff that we can\nTalk to there\nbut\nOn the takeoff question if we imagine a two by two matrix of short timelines till agi starts\nLong timelines till agi starts slow takeoff fast takeoff\nDo you have an instinct on what do you think the safest quadrant would be?\nSo, uh, the different options are like next year. Yeah, say the takeoff that we start the takeoff period. Yep\nnext year or in 20 years 20 years and then it takes\nOne year or 10 years?\nWell, you can even say one year or five years, whatever you want\nFor the takeoff\nI feel like now\nis uh\nIs safer\nSo do I so i'm in the longer now i'm in the slow\ntakeoff short timelines\nIt's the most likely good world and we optimize the company to\nHave maximum impact in that world to try to push for that kind of a world and the decisions that we make are\nYou know, there's like probability masses but weighted towards that\nand I think\nI'm very afraid of the fast takeoffs\nI think in the longer timelines, it's harder to have a slow takeoff. There's a bunch of other problems, too\nUm, but that's what we're trying to do. Do you think gpt4 is an agi?\nI think if it is just like with the ufo videos\nUh, we wouldn't know immediately\nI think it's actually hard to know that when I've been thinking of playing with gpt4\nAnd thinking how would I know if it's an agi or not because I think uh in terms of uh to put it in a different way\nHow much of agi is the interface I have with the thing\nAnd how much of it uh is the actual wisdom inside of it?\nlike uh\nPart of me thinks that you can have a model that's capable of super intelligence\nAnd uh, it just hasn't been quite unlocked. It's what I saw with chat gpt just doing that little bit of rl\nWell human feedback makes the thing somehow much more impressive much more usable\nSo maybe if you have a few more tricks, like you said there's like hundreds of tricks inside open ai\nA few more tricks and all of a sudden holy shit\nThis thing so I think that gpt4 although quite impressive is definitely not an agi but isn't it remarkable?\nWe're having this debate. Yeah, so what's your intuition why it's not?\nI think we're getting into the phase where specific definitions of agi really matter\nOr we just say, you know, I know when I see it and i'm not even going to bother with the definition\nUm, but under the I know it when I see it\nIt doesn't feel that close to me\nLike if\nIf I were reading a sci-fi book and there was a character that was an agi and that character was gpt4\nI'd be like, oh this is a shitty book\nYou know, that's not very cool. Like I was I would have hoped we had done better\nTo me some of the human factors are important here\nDo you think?\nGpt4 is conscious. I think no but\nI asked gpt4 and of course it says no. Do you think gpt4 is conscious?\nI think\nIt knows how to fake consciousness. Yes how to fake consciousness. Yeah\nif if uh\nIf you provide the right interface and the right prompts it definitely can answer as if it were yeah, and then it starts getting weird\nIt's like what is the difference between pretending to be conscious and conscious? I mean, you don't know obviously we can go to like the freshman\nYear dorm late at saturday night kind of thing. You don't know that you're not a gpt4 rollout in some advanced simulation. Yeah. Yes, so\nIf we're willing to go to that level, sure. I live in that\nWell, but that's an important that's an important level\nThat's an important. Uh\nThat's a really important level because one of the things\nThat makes it not conscious is declaring that it's a computer program. Therefore it can't be conscious\nSo i'm not going to i'm not even going to acknowledge it\nBut that just puts it in the category of other I believe\nAi\nCan be conscious\nSo then the question is what would it look like when it's conscious\nWhat would it behave like?\nand it would\nProbably say things like first of all, i'm conscious second of all\nUm display capability of suffering\nUh an understanding of self\nOf uh having some\nmemory\nOf itself and maybe interactions with you. Maybe there's a personalization aspect to it\nAnd I think all of those capabilities are interface capabilities not fundamental aspects of the actual knowledge side in your net\nMaybe I can just share a few like disconnected thoughts here. Sure\nBut i'll tell you something that ilia said to me once a long time ago that has like stuck in\nmy head\nIlia, let's go there. Yes, my co-founder and the chief scientist of opening eye and sort of\nlegend in the field, um\nWe were talking about how you would know if a model were conscious or not\nand\nHeard many ideas thrown around but he said one that that I think is interesting if you trained a model\nOn a data set that you were extremely careful to have no mentions of consciousness or anything close to it\nin the training process\nLike not only was the word never there but nothing about the sort of subjective experience of it or related concepts\nAnd then you started talking to that model about\nHere are\nSome\nthings\nThat you weren't trained about and for most of them the model was like i've no idea what you're talking about\nbut then you asked it you sort of described the\nExperience the subjective experience of consciousness\nAnd the model immediately responded unlike the other questions. Yes. I know exactly what you're talking about\nThat would update me somewhat\nI don't know because that's more in the space of facts versus like\nemotions, I don't think consciousness is an emotion\nI think consciousness has ability to sort of experience this world\nReally deeply there's a movie called ex machina\nI've heard of it, but i haven't seen it. You haven't seen it. No\nThe director alex garland who had a conversation so it's where\nagi system is built embodied in the body of a woman\nand uh something he doesn't make explicit, but he's he said\nHe put in the movie without describing why but at the end of the movie spoiler alert when the ai escapes\nthe woman escapes\nUh, she smiles\nFor nobody for no audience\nUm, she smiles at the person like at the freedom\nShe's experiencing\nHe's experiencing. I don't know anthropomorphizing but he said the smile to me was the\nUh was passing the touring test for consciousness that you smile for no audience\nYou smile for yourself\nThat's an interesting thought\nIt's like you you're taking an experience for the experience sake I don't know\nUm that seemed more like consciousness versus the ability to convince somebody else that you're conscious\nAnd that feels more like a realm of emotion versus facts, but yes\nIf it knows so I think there's many other\ntasks\ntests like that\nthat we could look at too, um\nBut you know my personal beliefs\nConsciousness is if something very strange is going on\nSay that\nUm, do you think it's attached to the particular?\nMedium of our of the human brain. Do you think an ai can be conscious?\ni'm, certainly willing to believe that\nConsciousness is somehow the fundamental substrate and we're all just in the dream or the simulation or whatever. I think it's interesting how much\nsort of\nthe silicon valley religion of the simulation has gotten close to like brahman and how little\nSpace there is between them\nUm, but from these very different directions, so like maybe that's what's going on\nbut if if it is like physical reality as we\nUnderstand it and all of the rules of the game what we think they are\nthen\nThen there's something I still think it's something very strange\nUh, just to linger on the alignment problem a little bit maybe the control problem\n"}
{"pod": "Lex Fridman Podcast", "input": "Fear", "output": "What are the different ways you think?\naji might go wrong\nThat concern you you said that\nUh fear a little bit of fear is very appropriate here\nHe's been very transparent bob being mostly excited but also scared\nI think it's weird when people like think it's like a big dunk that I say like i'm a little bit afraid\nAnd I think it'd be crazy not to be a little bit afraid\nAnd I empathize with people who are a lot afraid\nWhat do you think about that moment of a system becoming super intelligent do you think you would know?\nThe current worries that I have are that\nThey're going to be disinformation problems or economic shocks\nor something else\nat a level far beyond\nanything we're prepared for\nAnd that doesn't require super intelligence. That doesn't require a super deep alignment problem in the machine waking up and trying to deceive us\nAnd I don't think that gets\nenough attention\nI mean it's starting to get more I guess\nso these systems\ndeploy the scale\ncan um\nshift\nThe winds of geopolitics and so on. How would we know if like on twitter we were mostly having\nlike llms direct the\nWhatever's flowing through that hive mind\nYeah on twitter and then perhaps beyond and then as on twitter so everywhere else eventually\nYeah, how would we know my statement is we wouldn't\nAnd that's a real danger\nHow do you prevent that danger? I think there's a lot of things you can try\num\nbut\nAt this point it is a certainty\nThere are soon going to be a lot of capable open-source llms with very few to none. No safety controls on them\nand so\nYou can try with regulatory approaches\nYou can try with using more powerful ais to detect this stuff happening\nUm, i'd like us to start trying a lot of things very soon\nHow do you under this pressure that there's going to be a lot of?\n"}
{"pod": "Lex Fridman Podcast", "input": "Competition", "output": "Open source there's going to be a lot of large language models\nunder this pressure\nHow do you continue prioritizing safety versus um, I mean there's several pressures\nSo one of them is a market driven pressure from\nother companies, uh\nGoogle apple meta and smaller companies. How do you resist the pressure from that?\nOr how do you navigate that pressure you stick with what you believe in you stick to your mission?\nYou know, i'm sure people will get ahead of us in all sorts of ways and take shortcuts. We're not going to take\nUm, and we just aren't going to do that. How do you out compete them?\nI think there's going to be many agis in the world so we don't have to like out compete everyone\nWe're going to contribute one\nOther people are going to contribute some\nI think up I think multiple agis in the world with some differences in how they're built and what they do and what they're focused on\nI think that's good\num, we have a very unusual structure, so\nWe don't have this incentive to capture unlimited value. I worry about the people who do but you know, hopefully it's all going to work out\nbut\nWe're a weird org and we're good at\nResisting product like we have been a misunderstood and badly mocked org for a long time like when we started\nAnd we like announced the org at the end of 2015\nAnd said we're going to work on agi like people thought we were batshit insane. Yeah, you know, like I\nI remember at the time a uh, eminent ai scientist at a\nLarge industrial ai lab\nWas like dming individual reporters being like, you know, these people aren't very good and it's ridiculous to talk about agi\nand I can't believe you're giving them time of day and it's like that was the level of like\nPettiness and rancor in the field at a new group of people saying we're going to try to build agi\nSo open ai and deep mind was a small collection of folks who are brave enough to talk\nabout agi\num\nin the face of mockery\nWe don't get mocked as much now\nDon't get mocked as much now\nuh, so speaking about the structure of the uh of the\nof the org\n"}
{"pod": "Lex Fridman Podcast", "input": "From non-profit to capped-profit", "output": "uh, so open ai\nwent um\nStopped being non-profit or split up. Um in 20. Can you describe that whole process? Yeah, so we started as a non-profit\nUm, we learned early on that we were going to need far more capital than we were able to raise as a non-profit\nUm, our non-profit is still fully in charge\nThere is a subsidiary capped profit so that our investors and employees can earn a certain fixed return\nAnd then beyond that everything else flows to the non-profit and the non-profit is like in voting control lets us make a bunch of non-standard decisions\nUm can cancel equity can do a whole bunch of other things can let us merge with another org\num\nProtects us from making decisions that are not in any like shareholders interest\nSo I think it's a structure that has been important to a lot of the decisions we've made what went into that decision process\nUh for taking a leap from non-profit to capped for profit\nWhat are the pros and cons you were deciding at the time I mean this was it was 19 it was really like\nTo do what we needed to go do we had tried and failed enough to raise the money as a non-profit\nWe didn't see a path forward there\nSo we needed some of the benefits of capitalism, but not too much\nI remember at the time someone said, you know as a non-profit not enough will happen\nAs a for-profit too much will happen. So we need this sort of strange intermediate\nWhat you kind of had this off-hand comment of\nYou worry about the uncapped companies that play with agi\nCan you elaborate on the worry here because agi out of all the technologies we?\nHave in our hands is the potential to make is uh, the cap is 100x\nFor open ai it started is that it's much much lower for like new investors now\nYou know agi can make a lot more than 100x for sure\nand so how do you um\nLike how do you compete like?\nStepping outside of open ai. How do you look at a world where google is playing?\nWhere apple and these and meta are playing we can't control what other people are going to do\nUm, we can try to like build something and talk about it and influence others\nand provide value and you know good systems for the world, but\nThey're going to do what they're going to do\nnow\nI I think right now there's like\nExtremely fast and not super deliberate motion inside of some of these companies\nBut already I think people are as they see\nthe rate of progress\nAlready people are grappling with what's at stake here. And I think the better angels are going to win out\nCan you elaborate on that the better angels of individuals the individuals within the companies but you know the incentives of capitalism to?\nCreate and capture unlimited value\nI'm a little afraid of\nBut again, no, I think no one wants to destroy the world. No one except saying like today. I want to destroy the world\nSo we've got the the malik problem on the other hand\nWe've got people who are very aware of that and I think a lot of healthy conversation about\nHow can we collaborate to minimize?\nSome of these very scary downsides\nWell, nobody wants to destroy the world let me ask you a tough question so\n"}
{"pod": "Lex Fridman Podcast", "input": "Power", "output": "You\nare\nVery likely to be one of not the person that creates agi\nOne up one up and even then like we're on a team of many there will be many teams\nBut several teams small number of people nevertheless relative\nI do think it's strange that it's maybe a few tens of thousands of people in the world a few thousands people in the world\nBut there will be a room\nWith a few folks who are like, holy shit that happens more often than you would think now. I understand I understand this\nI understand this. Yes, there will be more such rooms, which is a beautiful\nPlace to be in the world, uh, terrifying but mostly beautiful. Uh, so that might make you and a handful of folks\nUh the most powerful humans on earth\nDo you worry that power might corrupt you?\nfor sure, um, look I don't\nI think you want\nDecisions about this technology and certainly decisions about\nWho is running this technology to become increasingly democratic over time? We haven't figured out quite how to do this. Um,\nbut\nWe part of the reason for deploying like this is to get the world to have time to adapt\nAnd to reflect and to think about this to pass regulation for institutions to come up with new norms\nFor the people working on it together. Like that is a huge part of why we deploy\nEven though many of the ai safety people you referenced earlier think it's really bad even they acknowledge that this is like of some benefit\num\nBut I think any version of one person is in control\nOf this is really bad\nSo trying to distribute the power\nI don't have and I don't want like any like super voting power or any special like that\nYou know, i'm not like control of the board or anything like that of open. I\nI\nBut aji if created has a lot of power\nHow do you think we're doing like honest? How do you think we're doing so far?\nLike how do you think our decisions are like do you think we're making things not better worse? What can we do better?\nWell the things I really like because I know a lot of folks at open ai\nThe thing I really like is the transparency everything you're saying which is like failing publicly\nwriting papers\nreleasing different kinds of\nInformation about the safety concerns involved\nAnd doing it out in the open\nIs great\nBecause especially in contrast to some other companies that are not doing that. They're being more closed\nThat said you could be more open. Do you think we should open source gpt4?\nMy personal opinion because I know people at open ai is no\nWhat is knowing the people at open ai have to do with it because I know they're good people\nI know a lot of people I know they're good human beings\nUm from a perspective of people that don't know the human beings there's a concern of the super powerful technology in the hands of a few\nThat's closed. It's closed in some sense, but we give more access to it. Yeah then and like if this had just been google's game\nI feel it's very unlikely that anyone would have put this api out. There's pr risk with it\nYeah, like I get personal threats because of it all the time. I think most companies wouldn't have done this\nso maybe we didn't go as open as people wanted but like\nWe've distributed it pretty broadly\nYou personally know open ai as a culture is not so like nervous about uh pr risk and all that kind of stuff\nYou're more nervous about the risk of the actual technology and you and you reveal that so I you know\nThe nervousness that people have is because it's such early days of the technology is that you will close off over time\nBecause more and more powerful my nervousness is you get attacked so much by fear\nMongering clickbait journalism. You're like why the hell do I need to deal with this?\nI think the clickbait journalism bothers you more than it bothers me\nNo, i'm a third person bothered like I appreciate that like I feel all right about it of all the things I lose sleep over\nIt's not high on the list because it's important. There's a handful of companies a handful of folks that are really pushing this forward\nThey're amazing folks. I don't want them to become cynical about\nThe rest of the rest of the world. I think people at open.ai feel the weight of responsibility of what we're doing\nand yeah, it would be nice if like\nYou know journalists were nicer to us and twitter trolls gave us more benefit of the doubt\nbut like\nI think we have a lot of resolve in what we're doing and why?\nAnd the importance of it\nBut I really would love and I ask this like of a lot of people not just if cameras rolling like any feedback you've got\nFor how we can be doing better. We're in uncharted waters here\nTalking to smart people is how we figure out what to do better\nHow do you take feedback? Do you take feedback from twitter also?\nDo you because there's the sea the water my twitter is unreadable. Yeah\nSo sometimes I do I can like take a sample a cup cup out of the waterfall\nUm, but I mostly take it from conversations like this\n"}
{"pod": "Lex Fridman Podcast", "input": "Elon Musk", "output": "Uh speaking of feedback somebody, you know, well you work together closely\nOn some of the ideas behind open.ai is elon musk you have agreed on a lot of things you've disagreed on some things\nWhat have been some interesting things you've agreed and disagreed on?\nspeaking of\na fun debate on twitter\nI think we agree on the\nmagnitude of the downside of agi and the need to get\nNot only safety, right but get to a world where people are much better off\nBecause agi exists than if agi had never been built\nYeah\nWhat do you disagree on\nElon is obviously attacking us some on twitter right now on a few different vectors and I have\nempathy because I believe he is\nUnderstandably, so really stressed about agi safety\nI'm sure there are some other motivations going on too, but that's definitely one of them\num\nI saw this video of elon\na\nLong time ago talking about spacex. Maybe he's on some news show\nand\nA lot of early pioneers in space were really bashing\nSpaceX and maybe elon too and\nHe was visibly very hurt by that and said\nYou know, those guys are heroes of mine and I sucks and I wish they would see how hard we're trying\nUm, I definitely grew up with elon as a hero of mine. Um\nYou know despite him being a jerk on twitter or whatever i'm happy he exists in the world, but I wish he would\nDo more to look at the hard work we're doing to get this stuff right a little bit more love\nWhat do you admire in the name of love about eel musk? I mean so much right like he has\nHe has driven the world forward in important ways, I think we will get to\nElectric vehicles much faster than we would have if he didn't exist\nI think we'll get to space much faster than we would have if he didn't exist\nand\nas a sort of like\nCitizen of the world i'm very appreciative of that\nalso, like\nBeing a jerk on twitter aside in many instances. He's like a very funny and warm guy\nAnd uh some of the jerk on twitter thing\nUm, uh as a fan of humanity laid out in its full complexity and beauty. I enjoy the tension of ideas expressed\nso\nUh, you know, I earlier said that I admire how transparent you are\nBut I like how the battles are happening before our eyes as opposed to everybody closing off inside boardrooms. It's all laid out\nYeah, you know, maybe I should hit back and maybe someday I will but it's not like my normal style\nIt's all fascinating to watch and I think both of you\nAre brilliant people and have early on for a long time really cared about agi\nAnd had had great concerns about agi but a great hope for agi and that's cool to see\nThese big minds having those discussions, uh, even if they're tense at times\nI think it was elon that said that uh, gpt is too woke\nIs gpt too woke\nIs can you steal man the case that it is and not this is going to ours?\nUm question about bias, honestly, I barely know what woke means anymore\nI did for a while and I feel like the word is more so I will say I think it was too biased\nand\nWill always be there will be no one version of gpt that the world ever agrees is unbiased\nWhat?\nI think is we've made a lot like again, even some of our harshest critics have\nGone off and been tweeting about 3.5 to 4 comparisons and being like wow these people really got a lot better\nnot that they don't have more work to do and we certainly do but I\nI appreciate critics who display intellectual honesty like that. Yeah, and there there's been more of that than I would have thought\num\nwe will try to get the default version to be as\nNeutral as possible but as neutral as possible is not that neutral if you have to do it again for more than one person\nAnd so this is where\nMore steerability more control in the hands of the user the system message in particular\nIs I think the real path forward\nAnd as you pointed out these nuanced answers to look at something from several angles\nYeah, it's really really fascinating. It's really fascinating. Is there something to be said about the employees of a company?\nAffecting the bias of the system 100\nuh, we try to\navoid the\nSf\nGroup think bubble. Um, it's harder to avoid the ai group think bubble that follows you everywhere\nThere's all kinds of bubbles we live in 100. Yeah, i'm\ngoing on like a\nAround the world user tour soon for a month to just go like talk to our users in different cities\nand\nI can like feel how much i'm craving doing that because\nI haven't done anything like that since in years. Um, I used to do that more for yc\nAnd to go talk to people\nin super different contexts\nand it doesn't work over the internet like to go show up in person and like sit down and like\nGo to the bars they go to and kind of like walk through the city like they do you learn so much\nAnd get out of the bubble so much\num\nI think we are much better than any other company. I know of in san francisco for not falling into the kind of like\nSf craziness, but i'm sure we're still pretty deeply in it\nBut is it possible to separate the bias of the model versus the bias of the employees?\nThe bias i'm most nervous about is the bias of the human feedback raters\nUh, so what's the selection of the human? Is there something you could speak to at a high level about the selection of the human raters?\nThis is the part that we understand the least. Well, we're great at the pre-training machinery\nWe're now trying to figure out how we're going to select those people\nHow like how we'll like verify that we get a representative sample\nHow we'll do different ones for different places, but we don't we don't have that functionality built out yet\nsuch a fascinating\num\nScience you clearly don't want like all american elite university students giving you your labels. Well, see it's not about\nI'm, sorry. I just can never resist that dig. Yes. Nice\nBut it's so that that's a good\nThere's a million heuristics you can use that's a to me that's a shallow heuristic because\nUh universe like any one kind of category of human that you would think would have certain beliefs\nMight actually be really open-minded in an interesting way\nSo you have to like optimize for how good you are actually answering at doing these kinds of rating tasks\nHow good you are at empathizing with an experience of other humans? That's a big one\nLike and being able to actually like what does the world view look like?\nFor all kinds of groups of people that would answer this differently. I mean I have to do that\nConstantly instead of like you've asked us a few times, but it's something I often do, you know, I ask people\nIn an interview or whatever to steal man\nUh the beliefs of someone they really disagree with and the inability of a lot of people to even pretend like they're willing to do\nThat is remarkable\nYeah, what I find unfortunately ever since covid even more so that there's almost an emotional barrier\nIt's not even an intellectual barrier before they even get to the intellectual there's an emotional barrier that says no\nanyone who might possibly believe\nx\nThey're they're an idiot they're evil they're\nMalevolent anything you want to assign it's like they're not even like loading in the data into their head\nLook, I think we'll find out that we can make gpt systems way less biased than any human. Yeah\nso hopefully without the\nBecause there won't be that emotional load there. Yeah the emotional load\nBut there might be pressure there might be political pressure. Oh, there might be pressure to make a biased system\nWhat I meant is the technology I think will be capable of being\nMuch less biased. Do you anticipate you worry about pressures?\n"}
{"pod": "Lex Fridman Podcast", "input": "Political pressure", "output": "from outside sources from society from politicians from\nMoney sources. I both worry about it and want it\nlike\nYou know to the point of we're in this bubble and we shouldn't make all these decisions like we want society to\nHave a huge degree of input here that is pressure in some point in some way. Well, there's a you know, that's what like, uh to some\ndegree\nUh twitter files have revealed\nThat there was uh pressure from different organizations. You can see in the pandemic\nWhere the cdc or some other government organization might put pressure on you know, what?\nUh, we're not really sure what's true, but it's very unsafe to have these kinds of nuanced conversations now\nSo let's censor all topics so you get a lot of those\nEmails like you know, um emails all different kinds of people reaching out at different places to put subtle indirect pressure\nDirect pressure financial political pressure all that kind of stuff. Like how do you survive that?\nAnd how do you um, how much do you worry about that?\nIf gpt continues to get more and more\nIntelligent and a source of information and knowledge for human civilization\nI think there's like a lot of like quirks about me that make me\nNot a great ceo for open.ai but a thing in the positive column is I think I am\nRelatively good at not being affected by pressure for the sake of pressure\nBy the way beautiful statement of humility, but I have to ask what's what's in the negative column? Oh, I mean\nToo long a list. Oh, no, i'm trying what's a good one?\nI mean, I think i'm not a great like spokesperson for the ai movement i'll say that I think there could be like a more like\nThere could be someone who enjoyed it more there could be someone who's like much more charismatic\nThere could be someone who like connects better I think with people than I I do\nAlong with chalomksky on this I think charisma is a dangerous thing\nI think I think uh flaws in\nFlaws and communication style I think is a feature not a bug in general at least for humans at least for humans in power\nI think I have like more serious problems than that one. Um\nI think i'm like\nPretty\nConnected from like the reality of life for most people\nAnd trying to really not just like empathize with but internalize\nwhat\nthe impact on people that\nagi is going to have\nI probably like feel that less than other people would\nThat's really well put and you said like you're going to travel across the world to yeah, i'm excited to empathize with different users\nnot to empathize just to like\nI want to just like buy our users our developers our users a drink and say like\nTell us what you'd like to change and I think one of the things we are not good as good at as a company\nAs I would like is to be a really user-centric company\nAnd I feel like by the time it gets filtered to me\nIt's like totally meaningless. So I really just want to go talk to a lot of our users in very different contexts\nlike you said a drink in person because\nAnd I haven't actually found the right words for it, but I I was I was a little\nafraid\nwith the programming\nEmotionally, I I don't think it makes any sense. There is a real limbic response there\nGpt makes me nervous about the future not in an ai safety way, but like what i'm gonna do. Yeah change\nAnd like there's a nervousness about change and more nervous than excited\nIf I take away the fact that i'm an ai person and just a programmer more excited, but still nervous like\nYeah nervous in brief moments, especially when sleep deprived but there's a nervousness there people who say they're not nervous. I I\nIt's hard for me to believe\nBut you're right. It's excited. It's nervous for change nervous whenever there's significant exciting kind of change\num\nYou know, i've recently started using um, i've been an emacs person for a very long time and I switched to vs code\nas a co-pilot, uh\nThat was one of the big cool\nReasons because like this is where a lot of active development. Of course, you can probably do a co-pilot inside. Um emacs\nI mean i'm sure i'm sure yes code is also pretty good\nYeah, there's a lot of like little\nLittle things and big things that are just really good about vs code size and i've been I can happily report and all the\nPeople just go nuts, but i'm very happy. It's a very happy decision, but there was a lot of uncertainty\nThere's a lot of nervousness about it. There's fear and so on\num\nAbout taking that leap and that's obviously a tiny leap\nBut even just the leap to actively using copilot like using a generation of code\nUh, it makes you nervous, but ultimately your my life is much better as a programmer purely as a programmer\nProgrammer of little things and big things is much better. There's a nervousness and I think a lot of people will experience that\nExperience that and you will experience that by talking to them and I don't know what we do with that. Um\nHow we comfort people in in the in the face of this uncertainty and you're getting more nervous the more you use it not less\nYes, I would have to say yes because I get better at using it\nSo the learning curve is quite steep. Yeah\nAnd then there's moments when you're like, oh it generates a function beautifully\nYou sit back both proud like a parent\nBut almost like proud like and scared\nThat this thing will be much smarter than me\nlike both pride and uh\nSadness almost like a melancholy feeling but ultimately joy, I think yeah\nWhat kind of jobs do you think gpt language models would?\nBe better than humans that like full like does the whole thing end to end better not not not like what it's doing with you\nWhere it's helping you be maybe 10 times more productive\nThose are both good questions. I don't\nI would say they're equivalent to me because if i'm 10 times more productive wouldn't that mean that there'll be a need for\nMuch fewer programmers in the world\nI think the world is going to find out that if you can have 10 times as much code at the same price\nYou can just use even more. You should write even more code. Just needs way more code\nIt is true that a lot more could be digitized\nThere could be a lot more code in a lot more stuff\nI think there's like a supply issue\nYeah, so in terms of\nReally replace jobs. Is that a worry for you?\nIt is uh, i'm trying to think of like a big category that I believe\nCan be massively impacted. I guess I would say\nCustomer service is a category that I could see there are just way fewer jobs relatively soon\nI'm not even certain about that\nBut I could believe it\nso like, uh\nbasic questions about\nWhen do I take this pill if it's a drug company or what when uh, I don't know why I went to that\nBut like how do I use this product like questions? Yeah, like how do I use whatever whatever call center employees are doing now?\nYeah, this is not work. Yeah, okay\nI I want to be clear. I think like these systems will\nmake\nA lot of jobs just go away. Every technological revolution does\nThey will enhance many jobs and make them much better much more fun much higher paid\nand\nAnd they'll create new jobs that are difficult for us to imagine even if we're starting to see the first glimpses of them\nbut\num\nI heard someone last week talking about gpt4\nSaying that you know, man\nuh\nThe dignity of work is just such a huge deal\nWe've really got to worry like even people who think they don't like their jobs. They really need them\nIt's really important to them into society\nAnd also, can you believe how awful it is that france is trying to raise the retirement age?\nAnd I think we as a society are confused about whether we want to work more or work less\nAnd certainly about whether most people like their jobs and get value out of their jobs or not\nSome people do I love my job. I suspect you do too\nThat's a real privilege. Not everybody gets to say that if we can move more of the world to better jobs\nand work to something that can be\nA broader concept not something you have to do to be able to eat\nBut something you do is a creative expression and a way to find fulfillment and happiness. Whatever else\nEven if those jobs look extremely different from the jobs of today\nI think that's great. I'm not i'm not nervous about it at all\nYou have been a proponent of ubi universal basic income in the context of ai. Can you describe your philosophy there?\nOf our human future with ubi\nWhy why you like it? What are some limitations?\nI think it is a component\nOf something we should pursue it is not a full solution. I think people work for lots of reasons besides money\num\nAnd I think we are going to find\nincredible new jobs and\nsociety as a whole\nAnd people's individuals are going to get much much richer\nbut\nas a cushion through a dramatic transition and as just like\nYou know, I think the world should eliminate poverty if able to do so. I think it's a great thing to do. Um\nAs a small part of the bucket of solutions. I helped start a project called world coin\nWhich is a technological solution to this we also have funded a\nUh, like a large I think maybe the largest and most comprehensive universal basic income study\nas part of\nsponsored by open ai\nAnd I think it's like an area we should just be be looking into\nWhat are some like insights from that study that you gained?\nWe're going to finish up at the end of this year and we'll be able to talk about it. Hopefully early very early next\nIf we can linger on it, how do you think the economic and political systems will change?\nAs ai becomes a prevalent part of society. It's such an interesting sort of philosophical question\nUh looking 10 20 50 years from now\nWhat does the economy look like?\nWhat does politics look like?\nDo you see significant transformations in terms of the way democracy functions even?\nI love that you asked them together because I think they're super related. I think the the economic transformation will drive much of the political transformation here\nNot the other way around\nMy working model for the last\nFive years has been that\nthe two dominant changes will be that the\ncost of intelligence and the cost of energy\nAre going over the next couple of decades to dramatically dramatically fall from where they are today\nAnd the impact of that you're already seeing it with the way you now have like people, you know\nprogramming ability beyond what you had as an individual before\nis\nSociety gets much much richer much wealthier in ways that are probably hard to imagine\nI think every time that's happened before it has been\nThat economic impact has had positive political impact as well\nand I think it does go the other way too like the the\nsocio-political values of the enlightenment enabled the\nlong-running technological revolution and scientific discovery process we've had for the past centuries\nBut I think we're just going to see more i'm sure the shape will change\nBut I think it's this long and beautiful exponential curve\nDo you think there will be more\num\nI don't know what the the term is, but systems that resemble something like democratic socialism\nI've talked to a few folks on this podcast about these kinds of topics\nInstinct. Yes. I hope so\nSo that it\nreallocate some resources in a way that supports kind of lifts the\nThe people who are struggling I am a big believer in lift up the floor and don't worry about the ceiling\nif I can\nUh test your historical knowledge. It's probably not going to be good, but let's try it\nUh, why do you think uh, I come from the soviet union. Why do you think communism the soviet union failed?\nI recoil at the idea of living\nin a communist system\nAnd I don't know how much of that is just the biases of the world. I've grown up in\nAnd what I have been taught and probably more than I realize\nbut I think like more\nIndividualism more human will more ability to self-determine\nIs important\nand also\nI think the ability to try new things and not need permission and not need some sort of central planning\nBetting on human ingenuity and this sort of like distributed process\nI believe is always going to beat\ncentralized planning\nAnd I think that like for all of the deep flaws of america, I think it is the greatest place in the world\nBecause it's the best at this\nSo it's really interesting\nThat centralized planning failed some so in such big ways\nBut what if hypothetically the centralized planning it was a perfect super intelligent aji super intelligent aji\nAgain it might go\nWrong in the same kind of ways, but it might not we don't really know\nWe don't really know it might be better. I expect it would be better, but would it be better than\nA hundred super intelligent or a thousand super intelligent agis sort of\nin a liberal democratic system\narguing\nYes\nOh now also how much of that can happen internally in one super intelligent aji\nNot so obvious\nThere is something about right but there is something about like tension the competition\nBut you don't know that's not happening inside one model\nYeah\nThat's true\nIt'd be nice\nIt'd be nice if whether it's engineered in or revealed to be happening. It'd be nice for it to be happening\nThat of course it can happen with multiple agis talking to each other or whatever\nThere's something also about uh, mr. Russell has talked about the control problem of um\nAlways having aji to be have some degree of uncertainty\nNot having a dogmatic certainty to it that feels important so some of that is already handled with human alignment, uh, uh\nhuman feedback reinforcement learning with human feedback\nBut it feels like there has to be engineered in like a hard uncertainty\nHumility you can put a romantic word to it. Yeah\nDo you think that's possible to do?\nThe definition of those words, I think the details really matter but as I understand them. Yes, I do. What about the off switch?\nThat like big red button in the data center. We don't tell anybody about yeah, uh,\nHe's that i'm a fan\nMy backpack in your backpack\nUh, you think it's possible to have a switch you think I mean actually more more seriously more specifically about\nSort of rolling out of different systems. Do you think it's possible to roll them?\nunroll them\nPull them back in. Yeah. I mean we can absolutely take a model back off the internet. We can like take\nWe can turn an api off\nIsn't that something you worry about like when you release it and millions of people are using it?\nLike you realize holy crap\nThey're using it. Uh, I don't know worrying about the like all kinds of terrible use cases\nWe do worry about that a lot. I mean we try to figure out\nWith as much red teaming and testing ahead of time as we do\nhow to avoid a lot of those but\nI can't emphasize enough how much the collective intelligence and creativity of the world\nWill beat open ai and all of the red teamers we can hire\nso\nWe put it out, but we put it out in a way we can make changes\nIn the millions of people that have used the chat gpt and gpt. What have you learned about human civilization in general?\nI mean the question I ask is are we mostly good?\nGood\nOr is there a lot of malevolence in in the human spirit? Well to be clear I don't\nNor does anyone else at open. I said they're like reading all the chat gpt messages. Yeah, but\nFrom\nWhat I hear people using it for at least the people I talk to and from what I see on twitter\nWe are definitely mostly good\nbut\nA not all of us are all the time and b we really want to push on the edges of these systems\nand\nYou know, we really want to test out some darker theories\nYeah for the world\nYeah, it's very interesting\nIt's very interesting and I think that's not\nthat's that actually doesn't communicate the fact that we're\nlike fundamentally dark inside but we like to go to the dark places in order to um,\nUh, maybe rediscover the light\nIt feels like dark humor is a part of that some of the darkest\nSome of the toughest things you go through if you suffer in life in a war zone\nUm, the people i've interacted with they're in the midst of a war they're usually joking around. Yeah joking around and they're dark jokes. Yep\nSo that there's something there. I totally agree about that tension. Uh, so just to the model\n"}
{"pod": "Lex Fridman Podcast", "input": "Truth and misinformation", "output": "How do you decide what isn't isn't misinformation?\nHow do you decide what is true you actually have open as internal factual performance benchmark. There's a lot of cool benchmarks here\nUh, how do you build a benchmark for what is true?\nWhat is truth?\nSam alban like math is true and the origin of covid\nIs not agreed upon as ground truth\nThose are the two things and then there's stuff that's like certainly not true\num\nBut between that first and second\nMilestone\nThere's a lot of disagreement. What do you look for? What can a not not even just now but in the future?\nwhere can\nWe as a human civilization look for\nLook to for truth\nWhat do you know is true?\nWhat are you absolutely certain is true?\nI have uh, generally epistemic humility about everything and i'm freaked out by how little I know and understand about the world\nSo that even that question is terrifying to me\num\nThere's a bucket of things that are\nHave a high degree of truth in this which is where you put math\nA lot of math. Yeah\nCan't be certain but it's good enough for like this conversation. We can say math is true. Yeah, then I mean some uh,\nQuite a bit of physics, uh, this historical facts\nUh, maybe dates of when a war started\nThere's a lot of details about military conflict inside inside history\nOf course as you start to get you know\nJust read blitzed\nWhich is oh, I want to read that. Yeah, so how was it?\nIt was really good. It's uh\nIt gives a theory of nazi germany and hitler\nThat so much can be described about hitler and a lot of the upper echelon of nazi germany through the excessive use of drugs\nAnd then amphetamines right and phatamines but also other stuff, but it's just just a lot\nand\nUh, you know, that's really interesting. It's really compelling and for some reason like\nWhoa, that's really that would explain a lot. That's somehow really sticky\nIt's an idea that's sticky and then you read a lot of criticism of that book later by historians that that's actually\nThere's a lot of cherry picking going on and it's actually is using the fact that that's a very sticky explanation\nThere's something about humans that likes a very simple narrative describe everything for sure for sure and then yeah too much amphetamines cause the war is like a great\neven if not true simple explanation that feels\nSatisfying and excuses a lot of other probably much darker\nHuman truths. Yeah, the the military strategy\nemployed\nuh the atrocities\nthe speeches\nuh the\nJust the way hitler was as a human being the way he was as a leader all that could be explained through this\nOne little lens and it's like well, that's if you say that's true. That's a really compelling truth\nSo maybe truth is in one sense is defined as a thing that is a collective intelligence. We\nKind of all our brains are sticking to and we're like, yeah. Yeah. Yeah a bunch of a bunch of ants get together\nAnd like yeah, this is it. I was gonna say sheep, but there's a connotation to that\nBut yeah, it's hard to know what is true. And I think\nWhen constructing a gpt like model you have to contend with that\nI think a lot of the answers, you know, like if you ask\ngpt for\nI just stick on the same topic did covet leak from a lab. Yeah, I expect you would get a reasonable answer\nIt's a really good answer. Yeah\nIt laid out the the the hypotheses\nthe\nThe interesting thing it said\nWhich is refreshing to hear\nIs there's um something like there's very little evidence for either hypothesis direct evidence\nWhich is is important to state a lot of people kind of the reason why there's a lot of uncertainty\nAnd a lot of debate is because there's not strong physical evidence of either heavy circumstantial evidence on either side\nand then the other is more like biological theoretical kind of\ndiscussion and I think the answer the nuanced answer the gpt provider was actually\npretty damn good and also\nImportantly saying that there is uncertainty just just the fact that there is uncertainty is the statement was really powerful\nman, remember when like the social media platforms were banning people for\nSaying it was a lab leak\nYeah\nThat's really humbling the humbling the overreach of power in censorship\nBut that that you're the more powerful gpt becomes the more pressure there will be to censor\nWe have a different set of challenges faced by the previous generation of companies which is\nPeople talk about\nFree speech issues with gpt, but it's not quite the same thing. It's not like\nThis is a computer program what it's allowed to say and it's also not about the mass spread\nAnd the challenges that I think may have made\nThe twitter and facebook and others have struggled with so much so\nWe will have very significant challenges, but they'll be very new and very different\nAnd maybe yeah very new very different way to put it there could be truths that are harmful in their truth\num\nI don't know group differences in iq. There you go\nScientific work that when spoken might do more harm\nAnd you ask gpt that should gpt tell you there's books written on this\nthat are rigorous scientifically but\nAre very uncomfortable and probably not productive in any sense\nBut maybe are there's people arguing all kinds of sides of this and a lot of them have hate in their heart\nAnd so what do you do with that if there's a large number of people who hate others?\nbut are actually\nCiting scientific studies. What do you do with that? What does gpt do with that?\nWhat is the priority of gpt to decrease the amount of hate in the world?\nIs it up to gpt is it up to us humans?\nI think we as open ai have responsibility for\nThe tools we put out into the world, I think the tools themselves can't have responsibility in the way I understand it. Wow, so you\nYou carry some of that burden for sure all of us all of us at the company\nSo there could be harm caused by this tool and there will be harm caused by this tool, um\nThere will be harm. There will be tremendous benefits\nBut you know tools do wonderful good and real bad\nAnd we will minimize the bad and maximize the good and you have to carry the the weight of that\nUh, how do you avoid gpt for from being hacked or jailbroken\nThere's a lot of interesting ways that people have done that like, uh with token smuggling\nOr other methods like dan\nyou know when I was like a\nA kid basically, I I got worked once on jailbreak in an iphone the first iphone I think\nand\nI thought it was so cool\nAnd I will say it's very strange to be on the other side of that\nYou're now the man kind of sucks\nUm\nIs that is some of it fun? How much of it is a security threat? I mean what?\nHow much do you have to take seriously? How is it even possible to solve this problem?\nWhere does it rank on the set of problems just keeping asking questions prompting?\nwe want\nUsers to have a lot of control and get the model to behave in the way they want\nWithin some very broad bounds\nAnd I think the whole reason for jailbreaking is right now. We haven't yet figured out how to like give that to people\nAnd the more we solve that problem\nI think the less need there will be for jailbreaking\nYeah, it's kind of like piracy\ngave birth to spotify\nPeople don't really jailbreak iphones that much anymore and it's gotten harder for sure\nBut also like you can just do a lot of stuff now\nJust like with jailbreaking. I mean, there's a lot of hilarity that is in\num\nso\nEvan murakawa cool guy. He's at open.ai. He tweeted something that he also was really kind to send me\nTo communicate with me send me a long email describing the history of open.ai all the different developments\num\nHe really lays it out. I mean, that's a much longer conversation of all the awesome stuff that happened. It's just\namazing, but his tweet was uh, dali\nJuly 22 chad gpt. November 22 api 66 cheaper august 22 embeddings 500\nTimes cheaper while state of the art december 22 chad gpt api also 10 times cheaper while state of the art march 23\nWhisper api march 23 gpt4 today whenever that was last week\nand uh the conclusion is\nThis team ships we do\nUh, what's the process of going and then we can extend that back?\nI mean listen from the 2015 open.ai launch gpt gpt2 gpt3\nOpen.ai 5 finals with the gaming stuff, which is incredible gpt3 api released\nUh dolly instruct gpt tech. I could find fine tuning\nUh, there's just a million things uh available dolly dolly 2\npreview and then dolly is available to 1 million people whisper a second model release just across all of the stuff both research and\num\nDeployment of actual products that could be in the hands of people\nWhat is the process of going from idea to deployment that allows you to be so successful at shipping ai based?\nuh products\nI mean, there's a question of should we be really proud of that or should other companies be really embarrassed?\nyeah, and\nwe\nBelieve in a very high bar for the people on the team\nwe\nWork hard\nWhich you know, you're not even like supposed to say anymore or something\num\nwe\ngive a huge amount of\ntrust and autonomy and authority to individual people\nAnd we try to hold each other to very high standards\nand\nYou know, there's a process which we can talk about but it won't be that illuminating\nI think it's those other things that\nMake us able to ship at a high velocity\nSo gpt4 is a pretty complex system. Like you said there's like a\nMillion little hacks you can do to keep improving it\nUh, there's a the cleaning up the data set all that all those are like separate teams. So do you give autonomy? Is there just\nAutonomy to these fascinating different\nProblems if like most people in the company weren't really excited to work super hard and collaborate well on gpt4 and thought other stuff was more\nImportant there'd be very little I or anybody else could do to make it happen\nbut\nWe spend a lot of time figuring out what to do getting on the same page about why we're doing something\nAnd then how to divide it up and all coordinate together\nSo then then you have like a passion for the for the for the goal here\nSo everybody's really passionate across the different teams. Yeah, we care. How do you hire?\nHow you hire great teams?\nThe folks have interacted with opening eyes some of the most amazing folks i've ever met it takes a lot of time like I I spend\nI mean, I think a lot of people claim to spend a third of their time hiring I for real truly do\nUm, I still approve every single hired opening eye\nand I think there's\nYou know, we're working on a problem that is like very cool and the great people want to work on\nWe have great people and some people want to be around them. But even with that I think there's just no shortcut for\nPutting a ton of effort into this\nSo even when you have the good the good people hard work I think so\nMicrosoft announced the new multi-year multi-billion dollar reported to be 10 billion dollars investment into open ai\n"}
{"pod": "Lex Fridman Podcast", "input": "Microsoft", "output": "Can you describe the thinking?\nUh that went into this\nWhat what are the pros what are the cons?\nof working with the company like microsoft\nIt's not all\nPerfect or easy but on the whole they have been an amazing partner to us\nSatya and kevin and mikael\nAre are super aligned with us\nSuper flexible have gone like way above and beyond the call of duty to do things that we have needed to get all this to work\nUm, this is like a big iron complicated engineering project\nAnd they are a big and complex company\nand\nI think like many great partnerships or relationships\nWe've sort of just continued to ramp up our investment in each other\nAnd it's been very good\nIt's a for-profit company. It's very driven\nIt's very large scale\nIs there pressure to kind of make a lot of money I think most other companies\nWouldn't maybe now they would it wouldn't at the time have understood why we needed all the weird control provisions\nWe have and why we need all the kind of like agi specialness\nAnd I know that because I talked to some other companies before we did the first deal with microsoft\nAnd I think they were they are unique in terms of the companies at that scale\nThat understood why we needed the control provisions we have\nAnd so those control provisions help you help make sure that uh, the capitalist imperative does not\naffect the development of AI\nWell, let me just ask you\nAs an aside about uh, satya nadala the ceo of microsoft. He seems to have successfully transformed microsoft\ninto into\nThis fresh innovative developer friendly company. I agree. What do you\nI mean, it's really hard to do for a very large company\nUh, what what have you learned from him? Why do you think he was able to do this kind of thing?\num\nYeah, what?\nWhat insights do you have about why this one human being is able to contribute to the pivot of a large company into something?\nuh very new\nI think most\nCeos are either great leaders or great managers\nAnd from what I observe have observed with satya\nHe is both\nSuper visionary really like gets people excited really makes\nlong duration and correct calls\nAnd also he is just a super effective hands-on executive and I assume manager too\nAnd I think that's pretty rare\nI mean microsoft i'm guessing like ibm or like a lot of companies have been at it for a while\nProbably have like old school\nkind of momentum\nSo you like inject ai into it. It's very tough. All right, or anything even like open source the the culture of open source\num\nlike how\nHow hard is it to walk into a room and be like the way we've been doing things are totally wrong\nLike i'm sure there's a lot of firing involved or a little like twisting of arms or something\nSo do you have to rule by fear by love like what can you say to the leadership aspects of this?\nI mean, he's just like done an unbelievable job, but he is amazing at being\nlike\nClear and firm\nand\nGetting people to want to come along but also\nlike compassionate and patient\nwith his people too\nI'm getting a lot of love not fear. I'm a big satya fan\nSo am I from a distance\nI mean you have so much in your life trajectory that I can ask you about we can probably talk for many more hours\n"}
{"pod": "Lex Fridman Podcast", "input": "SVB bank collapse", "output": "But I gotta ask you because of Y Combinator because of startups and so on the recent\nAnd you've tweeted about this, uh about the silicon valley bank\nSvb, what's your best understanding of what happened? What is interesting?\nWhat is interesting to understand about what happened with svb? I think they just like horribly mismanaged\nbuying\nWhile chasing returns in a very silly world of zero percent interest rates\nBuying very long dated instruments\nSecured by very short-term and variable deposits and this was obviously dumb\nI think\nTotally the fault of the management team, although i'm not sure what the regulators were thinking either\nAnd\nIs an example of where I think\nYou see the dangers of incentive misalignment\nbecause\nAs the fed kept raising\nI assume\nThat the incentives on people\nWorking at svb to not\nSell at a loss\nTheir you know, super safe bonds, which we're now down 20 percent or whatever\nOr, you know down less than that but then kept going down\nYou know, that's like a classy example of incentive misalignment\nNow I suspect they're not the only bank in the bad position here\nThe response of the federal government\nI think took much longer than it should have but by sunday afternoon, I was glad they had done what they've done\nWe'll see what happens next\nSo, how do you avoid depositors from doubting their bank what I think is the most important thing\nNeeds would be good to do right now is just a\nAnd this requires statutory change\nBut it may be a full guarantee of deposits. Maybe a much much higher than 250k\nBut you really don't want\ndepositors\nhaving to doubt\nThe security of their deposits and this thing that a lot of people on twitter were saying is like well\nIt's their fault. They should have been like, you know reading the the the balance sheet and the the risk audit of the bank\nLike do we really want people to have to do that? I would argue no\nWhat impact has it had on startups that you see well there was a weekend of terror for sure\nAnd now I think even though it was only 10 days ago\nIt feels like forever and people have forgotten about it, but it kind of reveals the fragility of our economics\nWe may not be done that may have been like the gun shown falling off the nightstand in the first scene of the movie or whatever\nIt could be like other banks for sure. That could be\nwell even with ftx, I mean i'm just\nuh\nWas that fraud but there's mismanagement\nAnd you wonder how stable our economic system is\nEspecially with new entrants with agi I think\nOne of the many lessons to take away from this svb thing is how much?\nHow fast and how much the world changes and how little I think our experts\nLeaders business leaders regulators, whatever understand it. So the\nThe speed with which the svb bank run happened\nBecause of twitter because of mobile banking apps, whatever so different than the 2008 collapse where we didn't have those things really\nAnd\nI don't think that kind of the people in power realize how much the field had shifted and I think that is a\nVery tiny preview of the shifts that agi will bring\nWhat gives you hope in that shift from an economic perspective\nuh\nBecause it sounds scary the instability. I know I I am\nnervous about the speed with with this changes and the speed with which\nOur institutions can adapt. Um\nWhich is part of why we want to start deploying these systems really early why they're really weak\nSo that people have as much time as possible to do this. I think it's really scary to like\nHave nothing nothing nothing and then drop a super powerful agi all at once on the world\nI don't think\nPeople should want that to happen\nbut what gives me hope is like I think the less zeros the more positive some of the world gets the better and the the\nupside of the vision here\nJust how much better life can be?\nI think that's gonna like\nunite a lot of us and\nEven if it doesn't it's just gonna make it all feel more positive some\n"}
{"pod": "Lex Fridman Podcast", "input": "Anthropomorphism", "output": "When you uh create an agi system, you'll be one of the few people in the room. They get to interact with it first\nAssuming gpt4 is not that\nUh, what question would you ask her him it what discussion would you have?\nYou know one of the things that I have realized like this is a little aside and not that important but I have never felt\nAny pronoun other than it towards any of our systems but most other people\nSay him or her or something like that\nAnd I wonder why I\nAm so different like yeah, I don't know maybe it's I watch it develop. Maybe it's I think more about it, but\ni'm curious where that difference comes from I think probably you could because you watch it develop but then again\nI watch a lot of stuff develop and I always go to him and her I anthropomorphize\nthis\naggressively\nAnd certainly most humans do I think it's really important that we try to\nExplain to educate people that this is a tool and not a creature\nI think I yes\nBut I also think there will be a room in society for creatures\nAnd we should draw hard lines between those\nIf something's a creature i'm happy for people to like think of it and talk about it as a creature\nBut I think it is dangerous to project creatureness onto a tool\nThat's one perspective\nA perspective I would take if it's done transparently\nIs projecting creatureness onto a tool makes that tool more usable\nIf it's done well, yeah, so if there's if there's like kind of ui affordances that\nWork I understand that I still think we want to be like pretty careful with it\nBecause the more creature like it is the more it can manipulate manipulate you emotionally or just the more you\nThink that it's doing something or should be able to do something or rely on it for something that it's not capable of\nWhat if it is capable what about sam almond? What if it's capable of love?\nDo you think there will be romantic relationships like in the movie her with gpt\nThere are companies now that offer\nLike for backup lack of a better word like romantic companionship ais\nReplica is an example of such a company. Yeah, I personally don't feel\nAny interest in that so you're focusing on creating intelligent, but I understand why other people do\nThat's interesting. I'm I have for some reason i'm very drawn to that\nHave you spent a lot of time interacting with replica or anything somewhere replica, but also just building stuff myself\nlike I have robot dogs now that I\nuh use\num, I use the the movement of the the the robots to communicate emotion i've been\nExploiting how to do that\nLook, there are going to be\nVery interactive\nGpt4 powered pets or whatever\nrobots\ncompanions and\nA lot of people seem really excited about that. Yeah, there's a lot of interesting possibilities. I think\nYou you'll discover them. I think as you go along. That's the whole point like the things you say in this conversation\nYou might in a year say\nThis was right. No, I may totally want I may turn out that I like love my gpt4\nMaybe you're a robot or whatever. Maybe you want your programming assistant to be a little kinder and not mock you\nYou're incompetent. No, I think you do want um\nThe style of the way gpt4 talks to you, yes really matters\nYou probably want something different than what I want, but we both probably want something different than the current gpt4\nAnd that will be really important even for a very tool-like thing\nIs there styles of conversation? Oh, no contents of conversations you're looking forward to with an agi\n"}
{"pod": "Lex Fridman Podcast", "input": "Future applications", "output": "like gpt\n5-6-7 is there stuff where\nLike where do you go to outside of the fun meme stuff for actual like what i'm excited for is like\nPlease explain to me how all the physics works and solve all remaining mysteries\nSo like a theory of everything i'll be real happy\nfaster than light\nTravel don't you want to know?\nSo there's several things to know it's like and and be hard\nIs it possible in how to do it?\nUm, yeah, I want to know I want to know probably the first question would be are there other intelligent alien civilizations out there?\nBut I don't think agi has the not the ability to do that to to to know that might be able to help us figure out\nhow to go detect\nAnd we need to like send some emails to humans and say can you run these experiments?\nCan you build the space probe? Can you wait, you know a very long time or provide a much better estimate than that drake equation?\nYeah, uh with with the knowledge we already have and maybe process all the because we've been collecting a lot of yeah\nYou know, maybe it's in the data. Maybe we need to build better detectors\nWhich did a really advanced data could tell us how to do it may not be able to answer it on its own\nBut it may be able to tell us what to go build\nTo collect more data. What if it says the aliens are already here?\nI think I would just go about my life. Yeah\nUh, I mean a version of that is like what are you doing differently?\nWhat are you doing differently now that like if if gpt4 told you and you believed it? Okay agi is here\nOr agi is coming real soon\nWhat are you going to do differently the source of joy and happiness and fulfillment of life is from other humans. So it's\nMostly nothing right unless it causes some kind of threat\nAnd but that threat would have to be like literally a fire\nLike are we are we living now with a greater degree of digital intelligence than you would have expected three years ago in the world\nAnd if you could go back and be told by an oracle three years ago, which is you know blink of an eye that in\nMarch of 2023 you will be living with\nthis degree of\nDigital intelligence. Would you expect your life to be more different than it is right now?\nProbably probably but there's also a lot of different trajectories intermixed. I would have expected the um society's response to a pandemic\nUh to be much better\nmuch clearer\nLess divided I was very confused about there's there's a lot of stuff given the amazing technological advancements not happening the weird social divisions\nIt's almost like the more technological advancement\nThere is the more we're going to be having fun with social division or maybe the technological advancement\nJust reveal the division that was already there, but all of that just make the confuses\nMy understanding of how far along we are as a human civilization\nAnd what brings us meaning and what how we discover truth together and knowledge and wisdom\nSo I don't I don't know but when I look I when I open wikipedia\nI'm happy that humans are able to create this thing. Yes. There is bias. Yes\nBut it's it's a triumphal. It's a triumph of human civilization\nGoogle search the search search period is incredible the way it was able to do, you know, 20 years ago\nAnd and now this this is this new thing gpt\nIs like is this like going to be the next like the conglomeration of all of that that made? Uh,\nweb search and\nWikipedia so magical but now more directly accessible you can have a conversation with the damn thing\nIt's incredible\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "Let me ask you for advice\nFor young people in high school and college what to do with their life\nThey how to have a career they can be proud of how to have a life. They can be proud of\nYou wrote a blog post a few years ago titled how to be successful\nAnd there's a bunch of really really people should check out that blog post. There's so\nIt's so succinct and so brilliant. You have a bunch of bullet points\ncompound yourself\nHave almost too much self-belief learn to think independently get good at sales and quotes make it easy to take risks focus\nWork hard as we talked about be bold be willful be hard to compete with build a network\nYou get rich by owning things be internally driven\nWhat stands out to you from that or beyond as advice you can give?\nYeah, no, I think it is like good advice\nin some sense\nbut I also think\nIt's way too tempting to take advice\nfrom other people\nAnd the stuff that worked for me, which I tried to write down there\nProbably doesn't work that well or may not work as well for other people\nor like other people may find out that they want to\nJust have a super different life trajectory and I think I mostly\nGot what I wanted by ignoring advice\nAnd I think like I tell people not to listen to too much advice\nListening to advice from other people should be approached with great caution\nHow would you describe how you've approached life?\noutside of this advice\nThat you would advise to other people so really just in the quiet of your mind to think\nWhat gives me happiness? What is the right thing to do here? How can I have the most impact?\nI wish it were that\nYou know\nintrospective all the time\nIt's a lot of just like, you know\nWhat will bring me joy? What will bring me fulfillment?\nYou know what will bring what will be uh, I do think a lot about what I can do that will be useful but like\nWho do I want to spend my time with what I want to spend my time doing?\nLike a fish in water just going around with the car. Yeah\nThat's certainly what it feels like. I mean, I think that's what most people\nWould say if they were really honest about it\nYeah, if they really\nThink yeah, and some of that then\nGets to the sam harris discussion of free will being an illusion, of course very well might be which is a a really complicated\nThing to wrap your head around\nWhat do you think is the meaning of this whole thing\n"}
{"pod": "Lex Fridman Podcast", "input": "Meaning of life", "output": "That's a question you could ask an agi what's the meaning of life\nAs far as you look at it\nYou're part of a small group of people that are creating something truly special\nSomething that feels like almost feels like humanity was always\nMoving towards yeah, that's what I was going to say is I don't think it's a small group of people. I think this is the\nI think this is like the\nProduct of the culmination of whatever you want to call it an amazing amount\nOf human effort and if you think about everything that had to come together for this to happen\nWhen those people discovered the transistor in the 40s like is this what they were planning on\nall of the work the hundreds of thousands millions of people to ever it's been\nthat it took to go from\nThat one first transistor to packing the numbers we do into a chip and figuring out how to wire them all up together\nAnd everything else that goes into this\nyou know the energy required the the the science like just every every step like\nThis is the output of like all of us\nAnd I think that's pretty cool\nAnd before the transistor there was a hundred billion people\nwho lived and died\nhad sex fell in love\nAte a lot of good food murdered each other sometimes rarely\nbut mostly just good to each other struggled to survive and before that there was bacteria and\nEukaryotes and all that and all of that was on this one exponential curve\nYeah, how many others are there? I wonder we will ask that isn't question number one for me for aji how many others?\nAnd i'm not sure which answer I want to hear\nSam you're an incredible person. Uh, it's an honor to talk to you. Thank you for the work you're doing\nLike I said, i've talked to ilius escarra. I talked to greg. I talked to so many people at open ai\nThey're really good people. They're doing really interesting work. We are going to try our hardest to get\nTo get to a good place here. I think the challenges are\ntough I I understand that not everyone agrees with our approach of\niterative deployment and also iterative discovery\nBut it's what we believe in. Uh, I think we're making good progress\nAnd I think the pace is fast\nBut so is the progress so so like the pace of capabilities and change is fast\nBut I think that also means we will have new tools to figure out alignment and sort of the capital s safety problem\nI feel like we're in this together. I can't wait what we together as a human civilization come up with it's gonna be great\nI think we'll work really hard to make sure\nThanks for listening to this conversation with sam altman to support this podcast. Please check out our sponsors in the description\nAnd now let me leave you with some words from alan touring in\n1951\nIt seems probable\nThat once the machine thinking method has started\nIt would not take long to outstrip our feeble powers\nAt some stage therefore we should have to expect the machines to take control\nThank you for listening and hope to see you next time\nHope to see you next time\nYou\nYou\n[ Silence ]\n"}

