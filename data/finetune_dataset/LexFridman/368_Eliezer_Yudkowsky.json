{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "- The problem is that we do not get 50 years to try and try again and observe that we were wrong and come up with a different theory\nand realize that the entire thing is going to be like way more difficult than realized at the start, because the first time you fail at aligning something\nmuch smarter than you are, you die. - The following is a conversation with Eliezer Yudkowsky,\na legendary researcher, writer and philosopher on the topic of artificial intelligence,\nespecially super intelligent AGI and its threat to human civilization.\nThis is the Lex Friedman podcast to support it. Please check out our sponsors in the description.\nAnd now, dear friends, here's Eliezer Yudkowsky.\n"}
{"pod": "Lex Fridman Podcast", "input": "GPT-4", "output": "What do you think about GPT-4? How intelligent is it? - It is a bit smarter than I thought\nthis technology was going to scale to, and I'm a bit worried about what the next one will be like.\nLike this particular one I think, I hope there's nobody inside there 'cause you know,\nit'd sucked to be stuck inside there. But we don't even know the architecture at this point\n'cause OpenAI is very properly not telling us. And yeah, like giant inscrutable matrices\nof floating point numbers, I don't know what's going on in there. Nobody knows what's going on in there. All we have to go by are the external metrics\nand on the external metrics, if you ask it to write a self-aware 4chan green text,\nit will start writing a green text about how it has realized that it's an AI writing a green text and , oh well.\nSo that's probably\nnot quite what's going on in there in reality. But we're kind of blowing past\nall the science fiction guardrails. Like we are past the point where in science fiction people\nwould be like, \"Whoa, wait, stop, that thing's alive. \"What are you doing to it?\" And it's probably not, nobody actually knows.\nWe don't have any other guardrails. We don't have any other tests. We don't have any lines to draw on the sand and say like,\nwell, when we get this far we will start to worry about\nwhat's inside there. So if it were up to me, I would be like, okay, like this far, no further, time for the summer of AI\nwhere we have planted our seeds and now we like wait and reap the rewards of the technology\nwe've already developed and don't do any larger training runs than that. Which to be clear I realize requires more than one company\nagreeing to not do that. - And take a rigorous approach for the whole AI community\nto investigate whether there's somebody inside there. - That would take decades.\nLike having any idea of what's going on in there? People have been trying for a while. - It's a poetic statement about\nif there's somebody in there, but as I feel like it's also a technical statement or I hope it is one day, which is a technical statement,\nthat Alan Turing tried to come up with with the Turing Test. Do you think it's possible to definitively or approximately\nfigure out if there is somebody in there, if there's something like a mind\ninside this large language model? - I mean there's a whole bunch of different sub-questions here.\nThere's a question of, is there consciousness?\nIs there equalia? Is this a object of moral concern? Is this a moral patient,\nlike should we be worried about how we're treating it? And then there's questions like, how smart is it exactly?\nCan it do X, can it do Y? And we can check how it can do X and how it can do Y.\nUnfortunately we've gone and exposed this model to a vast corpus of text of people discussing consciousness\non the internet, which means that when it talks about being self-aware, we don't know to what extent it is repeating back what it\nhas previously been trained on for discussing self-awareness or if there's anything going on in there such that it would\nstart to say similar things spontaneously. Among the things that one could do if one were at all\nserious about trying to figure this out is train GPT-3\nto detect conversations about consciousness, exclude them all from the training data sets,\nand then retrain something around the rough size of GPT-4 and no larger with all of the discussion\nof consciousness and self-awareness and so on missing, although, you know, hard, hard bar to pass.\nHumans are self-aware and we're like self-aware all the time. We like talk about what we do all the time,\nlike what we're thinking at the moment all the time. But nonetheless, get rid of the explicit discussion of consciousness.\nI think therefore I am and all that. And then try to interrogate that model and see what it says.\nAnd it still would not be definitive, but nonetheless, I don't know.\nI feel like when you run over the science fiction guard rails, like maybe this thing, but what about GPT?\nMaybe not this thing, but what about GPT-5? Yeah, this, this would be a good place to pause.\n- On the topic of consciousness, there's so many components to even just removing\nconsciousness from the dataset. Emotion, the display of consciousness,\nthe display of emotion feels like deeply integrated with the experience of consciousness.\nSo the hard problem seems to be very well integrated with the actual surface level illusion of consciousness.\nSo displaying emotion, I mean, do you think there's a case to be made that we humans,\nwhen we're babies are just GPT, that we're training on human data on how to display emotion versus feel emotion,\nhow to show others, communicate others that I'm suffering, that I'm excited,\nthat I'm worried, that I'm lonely and I missed you and I'm excited to see you? All of that is communicated,\nthat's a communication skill versus the actual feeling that I experience. So we need that training data as humans too,\nthat we may not be born with that, how to communicate the internal state. And that's in some sense if we remove that\nfrom GPT-4's dataset, it might still be conscious but not be able to communicate it?\n- So I think you're gonna have some difficulty removing all mention of emotions from GPT'S data set.\nI would be relatively surprised to find that it has developed exact analogs of human emotions in there.\nI think that humans will have emotions\neven if you don't tell them about those emotions when they're kids. It's not quite exactly what various blank slatists\ntried to do with the new Soviet man and all that, but you know, if you try to raise people perfectly altruistic, they still come out selfish.\nYou try to raise people sexless, they still develop sexual attraction.\nWe have some notion in humans, not in AIS, of where the brain structures are\nthat implement this stuff. And it is really remarkable thing I say in passing that\ndespite having complete read access to every floating point number in the GPT series,\nwe still know vastly more about the architecture of human thinking than we know about what\ngoes on inside GPT, despite having like vastly better ability to read GPT.\n- Do you think it's possible? Do you think that's just a matter of time? Do you think it's possible to investigate and study the way\nneuroscientists study the brain, which is look into the darkness, the mystery of the human brain by just desperately trying to\nfigure out something and to form models and then over a long period of time actually start to figure out what regions\nof the brain do certain things? What different kinds of neurons when they fire, what that means, how plastic the brain is,\nall that kind of stuff. You slowly start to figure out different properties of the system. Do you think we can do the same thing with language models?\n- Sure. I think that if, you know, like half of today's physicists stop wasting their lives on string theory or whatever--\n(indistinct) And go off and study what goes on inside transformer networks, then in, you know,\nlike 30, 40 years we'd probably have a pretty good idea. - Do you think these large language models can reason?\n- They can play chess. How are they doing that without reasoning? - So you're somebody that spearheaded\nthe movement of rationality. So reason is important to you. So is that a powerful important word or is it...\nHow difficult is the threshold of being able to reason to you and how impressive is it?\n- I mean, in my writings on rationality, I have not gone making a big deal out of something called reason.\nI have made more of a big deal out of something called probability theory. And that's like,\nwell, you're reasoning but you're not doing it quite right and you should reason this way instead.\nAnd interestingly, people have started to get preliminary results showing\nthat reinforcement learning by human feedback has made\nthe GPT series worse in some ways. In particular, like it used to be well calibrated.\nIf you trained it to put probabilities on things, it would say 80% probability and be right eight times out of 10.\nAnd if you apply reinforcement learning from human feedback, the nice graph of 70%,\nseven out of 10 sort of flattens out into the graph that humans use where there's some very improbable stuff\nand likely, probable, maybe, which all means like around 40%, and then certain.\nSo it's like it used to be able to use probabilities, but if you try to teach it to talk in a way\nthat satisfies humans, it gets worse at probability in the same way that humans are.\n- And that's a bug, not a feature. - I would call it bug, although such a fascinating bug.\nBut yeah, so reasoning, it's doing pretty well on various tests that people\nused to say would require reasoning. But you know, rationality is about, when you say 80%,\ndoes it happen eight times out of 10? - So what are the limits to you of these transformer networks, of neural networks?\nIf reasoning is not impressive to you, or it is impressive but there's other levels to achieve?\n- I mean it's just not how I carve up reality. - If reality is a cake,\nwhat are the different layers of the cake or the slices? How do you cover it? Or you can use a different food if you.\n- I don't think it's as smart as a human yet. Like back in the day I went around saying like,\nI do not think that just stacking more layers of transformers is going to get you all the way to AGI.\nAnd I think that GPT-4 is passed where I thought this paradigm was going to take us.\nAnd you want to notice when that happens, you wanna say like, whoops, well, I guess I was incorrect about what happens if you keep\non stacking more transformer layers, and that means I don't necessarily know what GPT-5 is going to be able to do.\n- That's a powerful statement. So you're saying like your intuition initially is now\nappears to be wrong. - Yeah. - It's good to see that you can admit in some of your\npredictions to be wrong. You think that's important to do? Because throughout your life,\nyou've made many strong predictions and statements about reality and you evolve with that.\nSo maybe that'll come up today about our discussion. So you're okay being wrong?\n- I'd rather not be wrong next time.\nIt's a bit ambitious to go through your entire life never having been wrong.\nOne can aspire to be well calibrated, like not so much think in terms of, was I right,\nwas I wrong? But like when I said 90% that it happened nine times out of 10.\nYeah, oops is the sound we emit when we improve.\n- Beautifully said. And somewhere in there, we can connect the name of your blog, Less Wrong.\nI suppose that's the objective function. - The name Less Wrong was I believe suggested\nby Nick Bostrom and it's after someone's epigraph, actually forget whose, who said like, \"We never become right.\n\"We just become less wrong.\" What's the something, something that's easy to confess,\njust error and error and error again, but less and less and less.\n- Yeah, that's a good thing to strive for. So what has surprised you about GPT-4 that you found\nbeautiful as a scholar of intelligence, of human intelligence, of artificial intelligence, of the human mind?\n- I mean the beauty does interact with the screaming horror.\n(Lex laughing) - [Lex] Is the beauty in the horror? - But like beautiful moments, well, somebody asked Bing's Sydney to describe herself\nand fed the resulting description into one of the stable diffusion things I think.\nAnd you know, she's pretty, and this is something that should have been\nlike an amazing moment. Like the AI describes herself. You get to see what the AI thinks the AI looks like. Although, you know, the thing that's doing the drawing\nis not the same thing that's outputting the text. And it does happen the way that it would happen,\nthat it happened in the old school science fiction when you ask an AI to make a picture of what it looks like,\nnot just because we're two different AI systems being stacked that don't actually interact. It's not the same person, but also because the AI was trained by imitation in a way\nthat that makes it very difficult to guess how much of that it really understood.\nAnd probably not actually a whole bunch. Although GPT-4 is like multimodal and can draw\nvector drawings of things that make sense and does appear to have some kind of spatial visualization\ngoing on in there. But the pretty picture of the girl\nwith the steampunk goggles on her head, if I'm remembering correctly, what she looked like,\nit didn't see that in full detail. It just made a description of it\nand stable diffusion output it. And there's the concern about how much the discourse\nis going to go completely insane once the AIs all look like that\nand actually look like people talking.\nAnd yeah, there's another moment where somebody is asking Bing about like,\n\"Well, I fed my kid green potatoes \"and they have the following symptoms\" and Bing is like,\n\"That's solanine poisoning and call an ambulance\" and the person's like, \"I can't afford an ambulance,.\n\"I guess if this is time for like my kid to go, \"that's God's will\" and the main Bing thread gives\nthe message of, \"I cannot talk about this anymore.\" And the suggested replies to it say,\n\"Please don't give up on your child. \"Solanine poisoning can be treated if caught early.\"\nAnd you know, if that happened in fiction, that would be like the AI cares, the AI is bypassing the block on it to try\nto help this person and is it real? Probably not. But nobody knows what's going on in there.\nIt's part of a process where these things are not happening in a way where we,\nsomebody figured out how to make an AI care and we know that\nit cares and we can acknowledge it's caring now. It's being trained by this imitation process\nfollowed by reinforcement learning by human feedback. And we're trying to point it in this direction\nand it's pointed partially in this direction and nobody has any idea what's going on inside it. And if there was a tiny fragment of real caring in there,\nwe would not know. It's not even clear what it means exactly. And things aren't clear cut in science fiction.\n- We'll talk about the horror and the terror and the trajectories\nthis can take. But this seems like a very special moment, just a moment where we get to interact with the system that\nmight have care and kindness and emotion and maybe something like consciousness, and we don't know if it does,\nand we're trying to figure that out and we're wondering about what it means to care.\nWe're trying to figure out almost different aspects of what it means to be human, about the human condition by looking at this AI that has\nsome of the properties of that. It's almost like this subtle fragile moment in the history of the human species.\nWe're trying to almost put a mirror to ourselves here. - Except that's probably not yet,\nit probably isn't happening right now. We are boiling the frog.\nWe are seeing increasing signs bit by bit,\nbut not like spontaneous signs, because people are trying to train the systems to do that\nusing imitative learning and the imitative learning is spilling over and having side effects\nand the most photogenic examples are being posted to Twitter, rather than being examined in any systematic way.\nSo when you are boiling a frog like that,\nfirst is going to come the Blake Lemoines, like first you're going to have, you're gonna have like a thousand people looking at this.\nand the one person out of a thousand who is most credulous about the signs is going to be like,\n\"That thing is sentient.\" Well, 999 out of a thousand people think,\nalmost surely correctly, though we don't actually know, that he's mistaken. And so the first people to say sentience\nlook like idiots and humanity learns a lesson that when something claims to be sentient\nand claims to care, it's fake because it is fake because we have been training them using imitative learning\nrather than, and this is not spontaneous, and they keep getting smarter.\n- Well, do you think we would oscillate between that kind of cynicism, that AI systems can't possibly be sentient,\nthey can't possibly feel emotion, they can't possibly, this kind of cynicism about AI systems\nand then oscillate to a state where we empathize with the AI systems,\nwe give them a chance, we see that they might need to have rights and respect and a similar role in society as humans?\n- You're going to have a (indistinct) group of people who can just never be persuaded of that\nbecause to them, being wise, being cynical, being skeptical is to be like,\noh well, machines can never do that. You're just credulous. It's just imitating. It's just fooling you.\nand they would say that right up until the end of the world and possibly even be right\nbecause you know, they are being trained on an imitative paradigm (laughing)\nand you don't necessarily need any of these actual qualities in order to kill everyone. - Have you observed yourself working through skepticism,\ncynicism and optimism about the power of neural networks? What has that trajectory been like for you?\n- It looks like neural networks before 2006 forming part of an indistinguishable, to me,\nother people might have had better distinction on it, indistinguishable blob of different AI methodologies,\nall of which are promising to achieve intelligence without us having to know how intelligence works.\nYou had the people who said that if you just manually program lots and lots of knowledge into the system line by line,\nat some point all the knowledge will start interacting. It will know enough and it will wake up.\nYou've got people saying that if you just use evolutionary computation, if you try to mutate\nlots and lots of organisms that are competing together, that's the same way that human intelligence\nwas produced in nature. So we'll do this and it will wake up without having any idea of how AI works.\nAnd you've got people saying, \"Well, we will study neuroscience \"and we will learn the algorithms off the neurons\n\"and we will imitate them \"without understanding those algorithms.\" Which was a part I was pretty skeptical, 'cause it's hard to re-engineer these things\nwithout understanding what they do. \"And so we will get AI without understanding how it works\"\nand there were people saying like, \"Well, we will have giant neural networks that we will train \"by gradient dissent, then when they're as large\n\"as the human brain, they will wake up, \"we will have intelligence without understanding \"how intelligence works.\"\nAnd from my perspective, this is all like an indistinguishable blob of people who are trying to not get to grips with the difficult problem\nof understanding how intelligence actually works. That said, I was never skeptical\nthat evolutionary computation would not work in the limit. Like you throw enough computing power at it,\nit obviously works. That is where humans come from and it turned out that you\ncan throw less computing power than that at gradient descent if you are doing some other things correctly and you will\nget intelligence without having any idea of how it works and what is going on inside.\nIt wasn't ruled out by my model that this could happen. I wasn't expecting it to happen. I wouldn't have been able to call it neural networks rather\nthan any of the other paradigms for getting intelligence without understanding it.\nAnd I wouldn't have said that this was a particularly smart thing for a species to do,\nwhich is an opinion that has changed less than my opinion about whether you or not you can actually do it.\n"}
{"pod": "Lex Fridman Podcast", "input": "Open sourcing GPT-4", "output": "- Do you think AGI could be achieved with a neural network as we understand them today?\n- Yes. Just flatly yes. The question is whether the current architecture of stacking\nmore transformer layers, which for all we know GPT-4 is no longer doing because they're not telling us the architecture,\nwhich is a correct decision. - Ooh, correct decision. I had a conversation with Sam Altman,\nwe'll return to this topic a few times. He turned the question to me of how open should open AI be\nabout GPT-4? \"Would you open source the code?\" he asked me.\nBecause I provided as criticism saying that while I do appreciate transparency, open AI could be more open.\nAnd he says, \"We struggle with this question.\" What would you do? - Change their name to closed AI and sell GPT-4\nto business backend applications that don't expose it to consumers and venture capitalists\nand create a ton of hype and pour a bunch of new funding into the area. But too late now. - Don't you think others would do it?\n- Eventually. You shouldn't do it first. If you already have giant nuclear stockpiles,\ndon't build more. If some other country starts building a larger nuclear stockpile, then sure,\neven then, maybe just have enough nukes. You know, these things are not quite like nuclear weapons.\nThey spit out gold until they get large enough and then ignite the atmosphere and kill everybody. And there is something to be said for not destroying\nthe world with your own hands, even if you can't stop somebody else from doing it. But open sourcing it, that's just sheer catastrophe.\nThe whole notion of open sourcing, this was always the wrong approach, the wrong ideal. There are places in the world where open source\nis a noble ideal and building stuff you don't understand that is difficult to control\nwhere if you could align it, it would take time, you'd have to spend a bunch of time doing it,\nthat is not a place for open source 'cause then you just have powerful things that just go\nstraight out the gate without anybody having had the time to have them not kill everyone. - So can we steam down the case\nfor some level of transparency and openness may be open sourcing?\nSo the case could be that because GPT-4 is not close to AGI, if that's the case,\nthat this does allow open sourcing or being open about the architecture being transparent,\nabout maybe research and investigation of how the thing works, of all the different aspects of it, of its behavior,\nof its structure, of its training processes, of the data it was trained on, everything like that,\nthat allows us to gain a lot of insights about alignment, about the alignment problem, to do really good AI safety\nresearch while the system is not too powerful. Can you make that case,\nthat it could be open source? - I do not believe in the practice of steel manning. There is something to be said for trying to pass\nthe ideological Turing Test where you describe your opponent's position,\nthe disagreeing person's position well enough that somebody cannot tell the difference between your description\nand their description. But steel manning, no. - Like, okay, well this is where you and I disagree here.\nThat's interesting. Why don't you believe in steel manning? - Okay, so for one thing, if somebody's trying to understand me,\nI do not want them steel manning my position. I want them to try to describe my position\nthe way I would describe it, not what they think is an improvement. - Well, I think that is what steel manning is,\nis the most charitable interpretation. - I don't want to be interpreted charitably,\nI want them to understand what I am actually saying. If they go off into the land of charitable interpretations,\nthey're like off in their land of, the stuff they're imagining and not trying to understand\nmy own viewpoint anymore. - Well, I'll put it differently then, just to push on this point. I would say it is restating what I think\nyou understand under the empathetic assumption that Eliezer is brilliant and have honestly and rigorously\nthought about the point he's made. Right? - So if there's two possible interpretations\nof what I'm saying and one interpretation is really stupid and wack and doesn't sound like me and doesn't fit\nwith the rest of what I've been saying, and one interpretation sounds like something a reasonable person who believes\nthe rest of what I believe would also say, go with the second interpretation. - That's steel manning.\n- That's a good guess. If on the other hand there's like something that\nsounds completely wack and something that sounds like, a little less completely wack but you don't see why I would\nbelieve in it, it doesn't fit with the other stuff I say, but you know, that sounds less wack and you can like sort of see,\nyou can like maybe argue it, then you probably have not understood it. - See, okay, this is fun,\n'cause I'm gonna linger on this. You know, you wrote a brilliant blog post, AGI Ruin: A List of Lethalities, right? And it was a bunch of different points and I would say that\nsome of the points are bigger and more powerful than others. If you were to sort them, you probably could.\nYou personally, and to me steel manning means like going through\nthe different arguments and finding the ones that are really the most powerful.\nIf people like tl;dr, (chuckles) like what should you be most concerned about and bringing\nthat up in a strong, compelling, eloquent way. These are the points that Eliezer would make\nto make the case, in this case that AI's gonna kill all of us. But that's what steel manning is,\nis presenting it in a really nice way, the summary of my best understanding of your perspective.\nBecause to me there's a sea of possible presentations of your perspective and steel manning\nis doing your best to do the best one in that sea of different perspectives. - Do you believe it?\n- [Lex] Do I believe in what? - Like these things that you would be presenting as like the strongest version of my perspective,\ndo you believe what you would be presenting? Do you think it's true? - I'm a big proponent of empathy.\nWhen I see the perspective of a person, there is a part of me that believes it.\nIf I understand it. Especially in political discourse, in geopolitics, I've been hearing a lot of different perspectives\non the world and I hold my own opinions, but I also speak to a lot of people\nthat have a very different life experience and a very different set of beliefs. And I think there has to be epistemic humility\nin stating what is true.\nSo when I empathize with another person's perspective, there is a sense in which I believe it is true.\nI think probabilistically, I would say, in the way that you think. - Do you bet money on it?\nDo you bet money on their beliefs when you believe them? - Are we allowed to do probability?\n- Sure, you can state a probability of that. - Yes, there's a probability, there's a probability.\nAnd I think empathy is allocating a non-zero probability to a belief.\n(Eliezer laughing) In some sense, for time.\n- If you've got someone on your show who believes in the Abrahamic deity, classical style,\nsomebody on the show who's a young Earth creationist, do you say, \"I put a probability on it and that's my empathy?\"\n- When you reduce beliefs into probabilities, it starts to get, you know,\nwe can even just go to flat Earth. Is the Earth flat? - There's the thing,\nit's a little more difficult nowadays to find people who believe that unironically, but-- - Fortunately I think, well, it's hard to know.\nUnironic from ironic. (chuckles) But I think there's quite a lot of people that believe that.\nThere's a space of argument where you're operating rationally in the space of ideas.\nBut then there's also a kind of discourse where you're operating in the space\nof subjective experiences and life experiences.\nLike I think what it means to be human is more than just searching for truth,\nis just operating of what is true and what is not true. I think there has to be deep humility that we humans are\nvery limited in our ability to understand what is true. - So what probability do you assign\nto the young Earth's creationist beliefs then? - I think I have to give non-zero.\n- Out of your humility. Yeah, but three? (laughing)\n- I think it would be irresponsible for me to give a number because the listener,\nthe way the human mind works, we're not good at hearing the probabilities.\nYou hear three, what is three exactly? They're going to hear, well, there's only three probabilities I feel like.\nZero, 50% and a 100% in the human mind or something like this.\n- Well, zero, 40%, and 100% is a bit closer to it based on what happens to ChatGPT after you RLHF it\nto speak Humanese. - That's brilliant. (both chuckling) Yeah. That's really interesting.\nI didn't know those negative side effects of RLHF. That's fascinating.\nBut just to return to the open AI, closed AI.\n- Also, like quick disclaimer, I'm doing all this from memory. I'm not pulling out my phone to look it up.\nIt is entirely possible that the things I'm saying are wrong. - So thank you for that disclaimer.\nAnd thank you for being willing to be wrong.\nThat's beautiful to hear. I think being willing to be wrong is a sign of a person\nwho's done a lot of thinking about this world and has been humbled by the mystery and the complexity of this world.\nAnd I think a lot of us are resistant to admitting we're wrong. 'Cause it hurts.\nIt hurts personally, it hurts, especially when you're a public human. It hurts publicly because people point out\nevery time you're wrong. Like, look, you changed your mind, you're a hypocrite, you're an idiot, whatever, whatever they wanna say,\n- Oh, I block those people and then I never hear from them again on Twitter. (both laughing) - Well the point is to not let that pressure,\npublic pressure affect your mind and be willing to be in the privacy of your mind to contemplate the possibility that you're wrong\nand the possibility that you're wrong about the most fundamental things you believe. Like people who believe in a particular God,\npeople who believe that their nation is the greatest nation on Earth. All those kinds of beliefs that are core\nto who you are when you came up, to raise that point to yourself in the privacy of your mind, to say, \"Maybe I'm wrong about this.\"\nThat's a really powerful thing to do. And especially when you're somebody who's thinking about\nsystems that can destroy human civilization or maybe help it flourish. So thank you, thank you for being willing to be wrong.\nAbout open AI. So you really, I just would love to linger on this.\nYou really think it's wrong to open source it? - I think that burns the time remaining\nuntil everybody dies. I think we are not on track to learn remotely near\nfast enough even if it were open sourced.\nIt's easier to think that you might be wrong about something when being wrong about something is the only way that there's hope.\nAnd it doesn't seem very likely to me that the particular\nthing I'm wrong about is that this is a great time to open source GPT-4.\nIf humanity was trying to survive at this point in the straightforward way, it would be like shutting down the big GPU clusters,\nno more giant runs. It's questionable whether we should even be throwing GPT-4 around,\nalthough that is a matter of conservatism rather than a matter of my predicting that catastrophe will follow from GPT-4.\nThat is something in which I put like a pretty low probability. But also when I say like I put a low probability on it,\nI can feel myself reaching into the part of myself that thought that GPT-4 was not possible in the first place.\nSo I do not trust that part as much as I used to. Like the trick is not just to say I'm wrong, but,\nokay, well, I was wrong about that. Can I get out ahead of that curve and predict the next\nthing I'm going to be wrong about? - So the set of assumptions or the actual reasoning system that you were leveraging in making that initial statement\nprediction, how can you adjust that to make better predictions about GPT-4, five, six?\n- You don't wanna keep on being wrong in a predictable direction. Like being wrong, anybody has to do that walking through the world.\nThere's no way you don't say 90% and sometimes be wrong. In fact (indistinct) at least one time out of 10 if you're well calibrated when you say 90%.\nThe undignified thing is not being wrong. It's being predictably wrong.\nIt's being wrong in the same direction over and over again. So having been wrong about how far neural networks would go\nand having been wrong specifically about whether GPT-4 would be as impressive as it is, when I say it like,\n\"Well, I don't actually think GPT-4 causes a catastrophe,\" I do feel myself relying on that part of me that was\npreviously wrong. And that does not mean that the answer is now in the opposite direction. Reverse stupidity is not intelligence.\nBut it does mean that I say it with a worried note in my voice. It's like still my guess,\nbut you know, it's a place where I was wrong. Maybe you should be asking Gwern, Gwern Branwen. Gwern Branwen has been like writer about this than I have.\nMaybe you ask him if he thinks it's dangerous (laughing) rather than asking me.\n- I think there's a lot of mystery about what intelligence is,\nwhat AGI looks like. So I think all of us are rapidly adjusting our model,\nbut the point is to be be rapidly adjusting the model versus having a model that was right in the first place. - I do not feel that seeing Bing\nhas changed my model of what intelligence is. It has changed my understanding of what kind of work can be\nperformed by which kind of processes and by which means. It does not change my understanding of the work.\nThere's a difference between thinking that the right flyer can't fly and then like it does fly and you're like,\noh well, I guess you can do that with wings, with fixed wing aircraft and being like, \"Oh it's flying, \"this changes my picture of what the very substance\n\"of flight is.\" That's like a stranger update to make and Bing has not yet updated me in that way.\n- Yeah, that the laws of physics are actually wrong.\nThat kind of update. - No, no, like just, oh, like I defined intelligence this way but I now see that\nwas a stupid definition. I don't feel like the way that things have played out over the last 20 years has caused me to feel that way.\n- Can we try to, on the way to talking about AGI Ruin: A List of Lethalities,\nthat blog and other ideas around it, can we try to define AGI that we've been mentioning?\n"}
{"pod": "Lex Fridman Podcast", "input": "Defining AGI", "output": "How do you to think about what artificial general intelligence is or super intelligence or that,\nis there a line, is it a gray area? Is there a good definition for you? - Well, if you look at humans,\nhumans have significantly more generally applicable intelligence compared to their closest relatives,\nthe chimpanzees, well, closest living relatives rather. And a bee builds hives, a beaver builds dams.\nA human will look at a bee hive and a beaver's dam and be like, oh, can I build a hive\nwith a honeycomb structure? Out of hexagonal tiles.\nAnd we will do this even though at no point during our ancestry was any human optimized\nto build hexagonal dams or to take a more clear cut case, we can go to the moon.\nThere's a sense in which we were on a sufficiently deep level optimized to do things like going to the moon.\nBecause if you generalize sufficiently far and sufficiently deeply, chipping flint hand axes\nand outwitting your fellow humans, because you know, basically the same problem as going to the moon.\nAnd you optimize hard enough for chipping flint hand axes and throwing spears and above all,\noutwitting your fellow humans in tribal politics, the skills you entrain that way, if they run deep enough,\nlet you go to the moon. Even though none of your ancestors tried repeatedly to fly\nto the moon and got further each time and the ones who got further each time had more kids. No, it's not an ancestral problem,\nit's just that the ancestral problems generalized far enough. So this is human's significantly\nmore generally applicable intelligence. - Is there a way to measure general intelligence?\nI mean I can ask that question a million ways, but basically will you know it when you see it,\nit being in an AGI system? - If you boil a frog gradually enough,\nif you zoom in far enough, it's always hard to tell around the edges. GPT-4 people are saying right now,\n\"This looks to us like a spark of general intelligence. \"It is like able to do all these things \"it was not explicitly optimized for.\"\nOther people are being like, \"No, it's too early, it's like like 50 years off.\" And you know,\nif they say that they're kind of wack 'cause how could they possibly know that even if it were true? But you know, not to strum end,\nsome of the people may say like, that's not general intelligence, and not furthermore append it's 50 years off.\nOr they may be like, \"It's only a very tiny amount,\" and you know,\nthe thing I would worry about is that if this is how things are scaling, then jumping out ahead and trying not to be wrong in the same way\nthat I've been wrong before, maybe GPT-5 is more unambiguously a general intelligence\nand maybe that is getting to a point where it is even harder to turn back. Not that would be easy to turn back now, but you know,\nmaybe if you start integrating GPT-5 in the economy, it's even harder to turn back past there.\n- Isn't it possible that there's a, you know, with a frog metaphor, that you can kiss the frog\nand it turns into a prince as you're boiling it? Could there be a phase shift in the frog where unambiguously\nas you're saying? - I was expecting more of that.\nThe fact that GPT-4 is like kind of on the threshold and neither here nor there,\nthat itself is like not the sort of thing,\nquite how I expected it to play out. I was expecting there to be more of an issue, more of a sense of, different discoveries\nlike the discovery of transformers where you would stack them up and there would be like a final discovery and then you would get\nsomething that was like more clearly general intelligence. So the way that you are taking what is probably\nbasically the same architecture as in GPT-3 and throwing 20 times as much compute at it probably\nand getting out GBT-4 and then it's like maybe just barely a general intelligence\nor like a narrow general intelligence or you know, something we don't really have the words for.\nYeah, that's not quite how I expected it to play out. - But this middle, what appears to be this middle ground could nevertheless\nbe actually a big leap from GPT-3. - It's definitely a big leap from GPT-3. - And then maybe we're another one big leap away from\nsomething that's a phase shift. And also something that Sam Altman said,\nand you've written about this, it's fascinating, which is the thing that happened with GPT-4\nthat I guess they don't describe in papers is that they have like hundreds if not thousands\nof little hacks that improve the system. You've written about ReLU versus Sigmoid for example,\nthe function inside neural networks. It's like this silly little function difference that makes a big difference.\n- I mean we do actually understand why the ReLUs make a big difference compared to Sigmoids, but yes, they're probably using like G4789 ReLUs\nor whatever the acronyms are up to now rather than ReLUs. Yeah, that's part of the modern paradigm of alchemy.\nYou take your heap of linear algebra and you stir and it works a little bit better and you stir it this way and it works a little bit worse\nand you throw out that change and (mumbles). - But there's some simple breakthroughs that are definitive\njumps in performance, like ReLUs over Sigmoids. And in terms of robustness,\nin terms of all kinds of measures, and those stack up and they can,\nit's possible that some of them could be a non-linear jump in performance, right?\n- Transformers are the main thing like that. And various people are now saying like, \"Well, if you throw enough compute, R and Ns can do it.\n\"If you throw enough computes, dense networks can do it.\" Not quite at GPT-4 scale.\nIt is possible that like all these little tweaks are things that save them a factor of three total on computing power\nand you could get the same performance by throwing three times as much compute without all the little tweaks, but the part where it's like running on...\nSo there's a question of, is there anything in GPT-4 that is like the kind of qualitative shift that transformers were\nover R and Ns, and if they have anything like that,\nthey should not say it. If Sam Altman was dropping hints about that, he shouldn't have dropped hints.\n- That's an interesting question. So with a bit of lesson by Rich Sutton. Maybe a lot of it is just\na lot of the hacks are just temporary jumps in performance that would be achieved anyway with the nearly exponential\ngrowth of compute performance, of compute being broadly defined.\nDo you still think that Moore's Law continues? Moore's law broadly defined the performance--\n- Not a specialist in the circuitry. I certainly pray that Moore's Law runs as slowly as possible\nand if it broke down completely tomorrow, I would dance through the street singing Hallelujah as soon as the news were announced.\nOnly, not literally 'cause you know. - Your singing voice. - Not religious, but. - Oh, okay. (both chuckling)\nI thought you meant you don't have an angelic voice, singing voice. Well, let me ask you,\n"}
{"pod": "Lex Fridman Podcast", "input": "AGI alignment", "output": "can you summarize the main points in the blog post AGI Ruin: A List of Lethalities? Things that jump to your mind\nbecause it's a set of thoughts you have about reasons\nwhy AI is likely to kill all of us.\n- So I guess I could, but I would offer to instead say like,\ndrop that empathy with me. I bet you don't believe that. Why don't you tell me about you believe that AGI\nis not going to kill everyone and then I can try to describe how my theoretical\nperspective differs from that? - Whew. Well, so that means I have to, the words you don't like,\nthe steelman, the perspective that AI is not going to kill us. I think that's a matter of probabilities.\n- Maybe I was just mistaken. What do you believe? Just like forget like the debate\nand the dualism and just, what do you believe? What do you actually believe?\nWhat are the probabilities? - I think this, probabilities are hard for me to think about.\nReally hard. I kind of think in the number of trajectories.\nI don't know what probability the scientist trajectory, but I'm just looking at all possible trajectories that happen.\nAnd I tend to think that there is more trajectories that lead to a positive outcome than a negative one\nThat said, the negative ones, at least some of the negative ones that lead\nto the destruction of the human species. - And its replacement by nothing interesting or worthwhile, even from a very cosmopolitan perspective\non what counts as worthwhile. - Yes. So both are interesting to me to investigate, which is humans being replaced by interesting AI systems\nand not interesting AI systems. Both are a little bit terrifying, but yes,\nthe worst one is the paper club maximizer, something totally boring.\nBut to me the positive, we can talk about trying to make the case\nof what the positive trajectories look like. I just would love to hear your intuition\nof what the negative is. So at the core of your belief that, maybe you can correct me,\nthat AI's gonna kill all of us, is that the alignment problem is really difficult.\n- I mean, in the form we're facing it. So usually in science, if you're mistaken,\nyou run the experiment, it shows results different from what you expected and you're like, oops.\nAnd then you try a different theory, that one also doesn't work and you say, oops. And at the end of this process, which may take decades\nand you know, sometimes faster than that, you now have some idea of what you're doing.\nAI itself went through this long process of people thought it was going to be easier than it was.\nThere's a famous statement that I am somewhat inclined to like pull out my phone and try to read off exactly.\n- You can by the way. - All right. Ah, yes.\n\"We propose that a two-month, 10 man study \"of artificial intelligence be carried out \"during the summer of 1956 at Dartmouth College\n\"in Hanover, New Hampshire. \"The study is to proceed on the basis of the conjecture\n\"that every aspect of learning or any other feature \"of intelligence can in principle be so precisely described,\n\"the machine can be made to simulate it. \"An attempt will be made to find out \"how to make machines use language, form abstractions\n\"and concepts, solve kinds of problems now reserved \"for humans, and improve themselves.\n\"We think that a significant advance can be made \"in one or more of these problems \"if a carefully selected group of scientists\n\"work on it together for a summer.\" - And in that report, summarizing some\nof the major subfields of artificial intelligence that are still worked on to this day.\n- And there's similarly the story, which I'm not sure at the moment is a apocryphal or not, of that the grad student who got assigned\nto solve computer vision over the summer. (both chuckling) - I mean, computer vision in particular\nis very interesting. How little we respected the complexity of vision.\n- So 60 years later we're making progress on a bunch of that.\nThankfully not yet improved themselves, but it took a whole lot of time.\nAnd all the stuff that people initially tried with bright eyed hopefulness did not work the first time\nthey tried it, or the second time or the third time or the 10th time or 20 years later.\nAnd the researchers became old and grizzled and cynical veterans who would tell the next crop of bright-eyed,\ncheerful grad students, \"Artificial intelligence is harder than you think.\"\nAnd if alignment plays out the same way, the problem is that we do not get 50 years\nto try and try again and observe that we were wrong and come up with a different theory and realize that the entire thing is going to be way more\ndifficult than realized at the start. Because the first time you fail at aligning something much smarter than you are,\nyou die and you do not get to try again. And if every time we built\na poorly aligned super intelligence and it killed us all, we got to observe how it had killed us,\nand you know, not immediately know why, but come up with theories and come up with theory of how you do it differently and try it again and build\nanother super intelligence, then have that kill everyone and then like, oh, well, I guess that didn't work either,\nand try again and become grizzled cynics and tell the young eyed research researchers that it's not that easy, then in 20 years or 50 years,\nI think we would eventually crack it. In other words, I do not think that alignment is fundamentally harder\nthan artificial intelligence was in the first place. But if we needed to get artificial intelligence correct\non the first try or die, we would all definitely now be dead. That is a more difficult, more lethal form of the problem.\nLike if those people in 1956 had needed to correctly guess how hard AI was and correctly theorize how to do it on\nthe first try or everybody dies and nobody gets to do any more science, than everybody would be dead and we wouldn't\nget to do any more science. That's the difficulty. - You've talked about this, that we have to get alignment right\non the first critical try. Why is that the case? What is this critical,\nhow do you think about the critical try and why do we have to get it right? - It is something sufficiently smarter than you\nthat everyone will die if it's not aligned. I mean, you can like sort of zoom in closer and be like,\nwell, the actual critical moment is the moment when it can deceive you. When it can talk its way out of the box, when it can bypass\nyour security measures and get onto the internet, noting that all these things are presently being trained on computers that are just on the internet,\nwhich is, not a very smart life decision for us as a species. - Because the internet contains information\nabout how to escape. - 'Cause if you're like on a giant server connected the internet and that is where your AI systems\nare being trained, then if they are, if you get to the level of AI technology where they're aware\nthat they are there and they can decompile code and they can find security flaws in the system running them,\nthen they will just be on the internet. There's not an air gap on the present methodology. - So if they can manipulate whoever is controlling it into\nletting it escaped onto the internet and then exploit hacks. - If they can manipulate the operators or disjunction,\nfind security holes in the system running them. - So manipulating operators is the human engineering, right?\nThat's also holes. So all of it is manipulation, either the code or the human code,\nthe human mind or the human-- - I agree that the macro security system has human holes and machine holes.\n- And then they could just exploit any hole. - Yep. So it could be that like the critical moment is not\nwhen is it smart enough that everybody's about to fall over dead, but rather when is it smart enough that it can get onto\na less controlled GPU cluster, with it faking the books on\nwhat's actually running on that GPU cluster and start improving itself without humans watching it.\nAnd then it gets smart enough to kill everyone from there. But it wasn't smart enough to kill everyone at the critical\nmoment when you screwed up, when you needed to have\ndone better by that point or everybody dies. - I think implicit but maybe explicit idea\nin your discussion of this point is that we can't learn much about the alignment problem before this critical try.\nIs that what you believe? And if so, why do you think that's true?\nWe can't do research on alignment before we reach this critical point.\n- So the problem is is that what you can learn on the weak systems may not generalize to the very strong systems\nbecause these strong systems are going to be important are going to be different in important ways.\nChris Olah's team has been working on\nmechanistic interpretability, understanding what is going on inside the giant inscrutable matrices of floating point numbers by taking a telescope\nto them and figuring out what is going on in there. Have they made progress?\nYes. Have they made enough progress? Well, you can try to quantify this in different ways.\nOne of the ways I've tried to quantify it is by putting up a prediction market on whether in 2026,\nwe will have understood anything that goes on inside a giant transformer net\nthat was not known to us in 2006.\nLike, we have now understood induction heads in these systems by dint of much research and great sweat\nand triumph, which is a thing where if you go like AB, AB, AB,\nit'll be like, oh, I bet that continues AB. And a bit more complicated than that.\nBut the point is like we knew about regular expressions in 2006 and these are like pretty simple\nas regular expressions go. So this is a case where like by din of great sweat,\nwe understood what is going on inside a transformer, but it's not like the thing that makes transformers smart.\nIt's a kind of thing that we could have built by hand decades earlier.\n- Your intuition that the strong AGI versus weak AGI\ntype systems could be fundamentally different. Can you unpack that intuition a little bit?\nCould be very different. - Yeah, I think there's multiple thresholds. An example is the point at which a system has sufficient\nintelligence and situational awareness and understanding of human psychology that it would have the capability,\nthe desire to do so to fake being aligned. Like it knows what responses humans are looking for and can\ncompute the responses looking humans are looking for and give those responses without it necessarily being the case\nthat it is sincere about that. It's a very understandable way for an intelligent being\nto act, humans do it all the time. Imagine if your plan for achieving a good government\nis you're going to ask anyone who requests to be dictator of the country\nif they're a good person, and if they say no, you don't let them be dictator.\nNow the reason this doesn't work is that people can be smart enough to realize that the answer you're looking for is,\n\"Yes, I'm a good person\" and say that even if they're not really good people.\nSo the work of alignment might be qualitatively different\nabove that threshold of intelligence or beneath it.\nIt doesn't have to be like a very sharp threshold, but there's the point where you're building a system\nthat does not in some sense know you're out there and is not in some sense smart enough to fake anything.\nAnd there's a point where the system is definitely that smart. And there are weird in between cases like GPT-4,\nwhich, like we have no insight into what's going on in there.\nAnd so we don't know to what extent there's like a thing that in some sense has learned what responses\nthe reinforcement learning by human feedback is trying to entrain and is calculating how\nto give that versus like, aspects of it that naturally talk that way have been reinforced.\n- I wonder if there could be measures of how manipulative the thing is. So I think of Prince Myshkin character from\n\"The Idiot\" by Dostoevsky is this kind of perfectly purely naive character.\nI wonder if there's a spectrum between zero manipulation, transparent, naive, almost to the point of naiveness\nto sort of deeply psychopathic manipulative.\nAnd I wonder if it's possible to-- - I would avoid the term psychopathic. Like humans can be psychopaths and AI that was never,\nyou know, like never had that stuff in the first place. It's not like a defective human, it's its own thing. But leaving that aside. - Well, as a small aside,\nI wonder if what part of psychology, which has its flaws as a discipline already, could be mapped\nor expanded to include AI systems. - That sounds like a dreadful mistake.\nJust like, start over with AI systems. If they're imitating humans who have known psychiatric disorders, then sure,\nyou may be able to predict it. Then sure. Like if you ask it to behave in a psychotic fashion\nand it obligingly does so, then you may be able to predict its responses by using theory of psychosis. But if you're just yeah, like no,\nlike start over with, yeah. Don't drag the psychology. - I just disagree with that.\nIt's a beautiful idea to start over, but I think fundamentally the system is trained on human data, on language from the internet.\nAnd it's currently aligned with RHLF, reinforcement learning with human feedback.\nSo humans are constantly in the loop of the training procedure. So it feels like in some fundamental way,\nit is training what it means to think and speak like a human. So there must be aspects of psychology that are mappable.\njust you said, with consciousness. It's part of the text. - I mean, there's the question of to what extent\nit is thereby being made more human-like, versus to what extent an alien actress\nis learning to play human characters. - I thought that's what I'm constantly trying to do\nwhen I interact with other humans, is trying to fit in, a robot trying to play human characters.\nSo I don't know how much a human interaction is trying to play a character versus being who you are.\nI don't really know what it means to be a social human. - I do think that those people who go through\ntheir whole lives wearing masks and never take it off because they don't know the internal mental motion\nfor taking it off or think that the mask that they wear just is themselves,\nI think those people are closer to the masks that they wear than an alien from another planet would like,\nlearning how to predict the next word that every kind of human on the internet says.\n- Mask is an interesting word, but if you're always wearing a mask in public\nand in private, aren't you the mask?\n- I think that you are more than the mask. I think the mask is a slice through you. It may even be the slice that's in charge of you.\nBut if your self-image is of somebody who never gets angry or something,\nand yet your voice starts to tremble under certain circumstances,\nthere's a thing that's inside you that the mask says isn't there. And that even the mask you wear internally\nis like telling inside your own stream of consciousness is not there and yet it is there.\n- It's a perturbation on this slice through you. How beautifully did you put it?\nIt's a slice through you. It may even be a slice that controls you.\n(Lex laughing) I'm gonna think about that for a while. (laughing)\nI mean, I personally, I try to be really good to other human beings. I try to put love out there. I try to be the exact same person in public\nas I am in private, but it's a set of principles I operate under. I have a temper, I have an ego, I have flaws.\nHow much of it, how much of the subconscious am I aware?\nHow much am I existing in this slice? And how much of that is who I am in?\nIn this context of AI, the thing I present to the world and to myself\nin the private of my own mind when I look in the mirror, how much is that who I am? Similar with AI,\nthe thing it presents in conversation, how much is that who it is? Because to me, if it sounds human,\nand it always sounds human, it awfully starts to become something like human.\n- Unless there's an alien actress who is learning how to sound human\nand is getting good at it. - Oh boy. (sighs) To you that's a fundamental difference. That's a really deeply important difference.\nIf it looks the same, if it quacks like a duck, if it does all duck like things,\nbut it's an alien actress underneath, that's fundamentally different. - If in fact there's a whole bunch of thought going on\nin there, which is very unlike human thought and is directed around like, okay, what would a human do over here?\nAnd well, first of all, I think it matters because you know,\ninsides are real and do not match outsides.\nA brick is not like a hollow shell containing only a surface. There's an inside of the brick.\nIf you put it into an x-ray machine, you can see the inside of the brick.\nAnd you know, just because we cannot understand what's going on inside GPT\ndoes not mean that it is not there. A blank map does not correspond to a blank territory.\nI think it is like predictable with near certainty that if\nwe knew what was going on inside GPT or let's say GPT-3,\nor even like GPT-2 to take one of the systems that has actually been open sourced by this point,\nif I recall correctly. If we knew it was actually going on there,\nthere is no doubt in my mind that there are some things\nit's doing that are not exactly what a human does. If you train a thing that is not architected like a human\nto predict the next output that anybody on the internet would make, this does not get you this agglomeration\nof all the people on the internet. That rotates the person you're looking for into place\nand then simulates that per and then simulates the internal processes of that person one-to-one.\nIt is to some degree an alien actress. It cannot possibly just be like a bunch of different people in there exactly like the people.\nBut how much of it is by gradient dissent,\ngetting optimized to perform similar thoughts as humans think in order to predict human outputs\nversus being optimized to carefully consider how to play a role,\nhow how humans work, predict the actress, the predictor that in a different way than humans do.\nWell you know, that's the kind of question that with 30 years of work by half the planet's physicists, we can maybe start to answer.\n- You think so? So you think it's that difficult. I think you just gave it as an example that a strong AGI\ncould be fundamentally different from a weak AGI because there now could be an alien actress in there\nthat's manipulating. - Well, there's a difference. So I think like even GPT-2 probably has very stupid\nfragments of alien actress in it. There's a difference between like the notion that the actress is somehow manipulative.\nLike for example GPT-3, I'm guessing to whatever extent there's an alien actress\nin there versus like something that mistakenly believes it's a human, as it were.\nWell, maybe not even being a person. So the question of,\nprediction via alien actress cogitating versus prediction via being isomorphic to the thing predicted\nis a spectrum and to whatever extent\nit's an alien actress, I'm not sure that there's like a whole person alien actress with different goals from predicting the next step\nbeing manipulative or anything like that. That might be GPT-5 or GPT-6 even.\n- But that's the strong AGI you're concerned about. As an example, you're providing why we can't do research on AI alignment\neffectively on GPT-4 that would apply to GPT-6.\n- It's one of a bunch of things that change at different points. I'm trying to get out ahead of the curve here,\nbut you know, if you imagine what the textbook from the future would say, if we'd actually been able to study this for 50 years\nwithout killing ourselves and without transcending and you'd just imagine like a wormhole opens and a textbook from\nthat impossible world falls out, the textbook is not going to say there is a single sharp threshold where everything changes.\nIt's going to be like, of course we know that like best practices for aligning these systems must take into account the following\nseven major thresholds of importance which are passed at the following suffer in different points\nis what the textbook is gonna say. - I asked this question of Sam Alman, which if GPT is the thing that unlocks AGI,\nwhich version of GPT will be in the textbooks as the fundamental leap?\nAnd he said a similar thing, that it just seems to be a very linear thing. I don't think anyone,\nwe won't know for a long time what was the big leap? - The textbook isn't going to talk about big leaps.\n'Cause big leaps are the way you think when you have like a very simple scientific model of what's going on,\nwhere it's just all this stuff is there or all this stuff is not there. Or like there's a single quantity and it's like increasing\nlinearly, like the textbook would say like, \"Well, and then GPT-3 had like capability WXY\n\"and GPT-4 had like capability Z one, Z two and Z three.\" Like not in terms of what it can externally do,\nbut in terms of internal machinery that started to be present. It's just because we have no idea\nof what the internal machinery is that we are not already seeing chunks of machinery appearing piece by piece as they no doubt have been.\nWe just don't know what they are. - But don't you think that could be, whether you put it in the category of Einstein\nwith Theory of Relativity, so very concrete models of reality that are considered\nto be giant leaps in our understanding, or someone like Sigmund Freud or more kind of mushy theories\nof the human mind, don't you think we'll have potentially big leaps in understanding of that kind\ninto the depths of these systems? - Sure.\nBut humans having great leaps in their map, their understanding of the system is a very different\nconcept from the system itself acquiring new chunks of machinery.\n- So the rate at which it acquires that machinery might accelerate faster than our understanding.\n- Oh, it's been like vastly exceeding the, yeah. The rate to which it's gaining capabilities is vastly over racing our ability to understand\nwhat's going on in there. - So in sort of making the case against, as we explore the list of lethalities,\nmaking the case against AI killing us, as you've asked me to do, in part,\nthere's a response to your blog post by Paul Christiano I'd like to read. And I'd also like to mention that your blog is incredible.\nObviously not this particular blog post, obviously this particular blog post is great,\nbut just throughout, just the way it's written, the rigor with which it's written, the boldness of how you explore ideas,\nalso the actual literal interface, it's just really well done. (laughing) It just makes it a pleasure to read, the way you can hover\nover different concepts and then then it's just a really pleasant experience and read other people's comments\nand the way other responses by people and other blog posts or LinkedIn suggest,\nit's just a really pleasant experience. So thank you for putting that together. That's really, really incredible. I don't know,\nI mean that probably it's a whole 'nother conversation how the interface and the experience of presenting\nideas evolved over time. But you did an incredible job. So I highly recommend, I don't often read blogs,\nblogs religiously and this is a great one. - There is a whole team of developers there\nthat also gets credit. As it happens, I did pioneer the thing that appears when you\nhover over it. So I actually do get some credit for user experience there.\n- That's an incredible user experience. You don't realize how pleasant that is. - I think Wikipedia, I actually picked it up from a prototype that was\ndeveloped of a different system that I was putting forth, or maybe they developed it independently, but for everybody out there who was like,\n\"No, no, they just got the hover thing \"off of Wikipedia.\" It's possible for all I know that Wikipedia\ngot the hover thing off of Arbital, which is like a prototype that, and anyways. - It was incredibly done and the team behind it.\nWell, thank you, whoever you are thank you so much. And thank you for putting it together.\nAnyway, there's a response to that blog post by Paul Christiano. There's many responses, but he makes a few different points.\nHe summarizes the set of agreements he has with you and a set of disagreements. One of the disagreements was that in a form of a question,\ncan AI make big technical contributions and in general expand human knowledge and understanding and wisdom as it\ngets stronger and stronger? So AI in our pursuit of understanding how to solve\nthe alignment problem as we march towards strong AGI,\ncannot AI also help us in solving the alignment problem? So expand our ability to reason\nabout how to solve the alignment problem? - Okay. So the fundamental difficulty there is,\nsuppose I said to you, well, how about if the AI helps you win the lottery\nby trying to guess the winning lottery numbers\nand you tell it how close it is to getting next week's winning lottery numbers\nand it just keeps on guessing and keeps on learning until finally you've got the winning lottery numbers.\nOne way of decomposing problems is suggester, verifier.\nNot all problems decompose like this very well, but some do. If the problem is for example,\nlike guess guessing a plain text, guessing a password that will hash to a particular hash text\nwhere like you have have what the password hashes to you, if you don't have the original password, then if I present you a guest,\nyou can tell very easily whether or not the guest is correct. So verifying a guest is easy,\nbut coming up with a good suggestion is very hard.\nAnd when you can easily tell whether the AI output is good or bad or how good or bad it is,\nand you can tell that accurately and reliably, then you can train an AI to produce outputs that are better.\n- [Lex] Right. - And if you can't tell whether the output is good or bad, you cannot train the AI to produce better outputs.\nSo the problem with the lottery ticket example is that when the AI says, \"Well, what if next week's winning lottery numbers are\"\ndo, do do do, do, you're like, \"I don't know, \"next week's lottery hasn't happened yet.\"\nTo train a system to win at chess games, you have to be able to tell whether a game\nhas been won or lost. And until you can tell whether it's been won or lost, you can't update the system.\n- Okay. To push back on that, that's true.\nBut there's difference between over the board chess, in person and simulated games\nplayed by Alpha Zero with itself. - Yeah. - So is it possible to have simulated kind of games?\n- If you can tell whether the game has been won or lost. - Yes. So can't you not have this kind of simulated exploration\nby weak AGI to help us humans, human in the loop, to help understand how to solve the alignment problem?\nEvery incremental step you take along the way, GPT-4, 5, 6 7 has to take steps towards AGI?\n- So the problem I see is that your typical human has a great deal of trouble telling\nwhether I or Paul Christiano is making more sense. And that's with two humans,\nboth of whom I believe of Paul and claim of myself, are sincerely trying to help. Neither of whom is trying to deceive you,\nI believe of Paul and claim of myself. (both chuckling) - So the deception thing is the problem for you,\nthe manipulation, the alien actress? - So yeah, there's like two levels of this problem.\nOne is that the weak systems are... Well, there's three levels of this problem. There's like the weak systems\nthat just don't make any good suggestions. There's like the middle systems where you can't tell if the suggestions are good or bad.\nAnd there's the strong systems that have learned to lie to you. - Can't weak AGI systems help model lying?\nIs it such a giant leap that's totally non interpretable\nfor weak systems? Can not weak systems scale with,\ntrained on knowledge and whatever... Whatever the mechanism required to achieve AGI,\ncan't a slightly weaker version of that be able to, with time, compute time and simulation,\nfind all the ways that this critical point, this critical tribe can go wrong, and model that correctly or no?\n(indistinct) - I would love to dance. Yeah, no, no. I'm probably not doing a great job of explaining,\nwhich I can tell 'cause like the, the Lex system didn't output like ah, I understand.\nSo now I'm like trying a different output to see if-- (voices overlapping) Well no, a different output.\nI'm being trained to output things that make Lex look like he think that he understood what I'm saying\nand agree with me. - This is GTP-5 talking to GTP-3 right here. So like, help me out here, help me. (laughing)\n- Well, I'm trying not to be, I'm also trying to be constrained to say things that I think\nare true and not just things that get you to agree with me. - Yes, a hundred percent.\nWhich I think I understand is a beautiful output of a system, genuinely spoken and I...\nI understand it in part, but you have a lot of intuitions about this line,\nthis gray area between strong AGI and weak AGI that I'm trying to...\n- I mean, or a series of seven thresholds to cross. - Yeah.\nI mean, you have really deeply thought about this and explored it and it's interesting to sneak up to your\nintuitions from different angles. Like why is this such a big leap?\nWhy is it that we humans at scale, a large number of researchers, doing all kinds of simulations, you know,\nprodding the system in all kinds of different ways, together with the assistance of the weak AGI systems,\nwhy can't we build intuitions about how stuff goes wrong? Why can't we do excellent AI alignment safety research?\n- Okay, so like, I'll get there, but the one thing I want to note about is that this has not been remotely how things have been playing out so far.\n- [Lex] Sure. - The capabilities are going like do, do, do. And the alignment stuff is crawling like a tiny little snail in comparison.\n- [Lex] Got it. - So if this is your hope for survival, you need the future to be very different from how things\nhave played out up to right now. And you're probably trying to slow down the capability gains,\n'cause there's only so much you can speed up that alignment stuff. But leave that aside.\n- We'll mention that also. But maybe in this perfect world where we can do serious\nalignment research, humans and AI together. - So again, the difficulty is what makes the human\nsay, \"I understand\" and is it true? Is it correct or is it something that fools the human?\nWhen the verifier is broken, the more powerful suggester does not help.\nIt just learns to fool the verifier. Previously, before all hell started to break loose\nin the field of artificial intelligence, there was this person trying to raise the alarm and saying,\n\"You know, in a sane world, \"we sure would have a bunch of physicists working on this \"problem before it becomes a giant emergency.\"\nAnd other people being like, \"Ah, well you know, it's going really slow. \"It's gonna be 30 years away and only in 30 years\n\"will we have systems that match the computational power \"of human brains.\" So yeah, it's 30 years off, we've got time and more sensible people saying,\n\"If aliens were landing in 30 years, \"you would be preparing right now.\" But, you know, leaving the world looking on at this\nand sort of nodding along and being like, \"Ah, yes, the people saying that. \"It's like definitely a long way off. 'cause progress is really slow, that sounds sensible to us.\n\"RLHF thumbs up, produce more outputs like that one. \"I agree with this output, \"this output is persuasive.\"\nEven in the field of effective altruism, you quite recently had people publishing papers about like,\nah, yes, well, you know, to get something at human level intelligence, it needs to have like this many parameters and you need to\nlike do this much training of it with this many tokens according to these scaling laws,\nand at the rate that Moore's Law is going, at the rate that software is going, it'll be in 2050.\nAnd me going like, what?\nYou don't know any of that stuff. This is like this one weird model that has all kinds of, like,\nyou have done a calculation that does not obviously bear on reality anyways. And this is a simple thing to say,\nbut you can also produce a whole long paper impressively arguing out all the details of how you got\nthe number of parameters and how you're doing this impressive huge wrong calculation.\nAnd the I think most of the effective altruists who are paying attention to this issue, larger world,\npaying no attention to it at all, you know, are just nodding along with a giant impressive paper.\n'Cause you know, you press thumbs up for the giant impressive paper and thumbs down for the person going like,\n\"I don't think that this paper \"bears any relation to reality.\" And I do think that we are now seeing with like GPT-4\nand the sparks of AGI possibly, depending on how you define that even,\nI think that EAs would now consider themselves less convinced by the very long paper on the argument\nfrom biology as to AGI being 30 years off. But you know, this is what people pressed thumbs up on.\nAnd if you train an AI system to make people press thumbs up, maybe you get these long,\nelaborate and impressive papers arguing for things that ultimately fail to bind to reality, for example.\nAnd it feels to me like I have watched the field of alignment just fail to thrive\nexcept for these parts that are doing these sort of relatively very straightforward\nand legible problems. Like finding the induction heads and sign the giant\ninscrutable matrices. Once you find those, you can tell that you found them. You can verify that the discovery is real.\nBut it's a tiny, tiny bit of progress compared to how fast capabilities are going,\nbecause that is where you can tell that the answers are real. And then like outside of that\nyou have cases where it is hard for the funding agencies to tell who is talking nonsense and who is talking sense.\nAnd so the entire field fails to thrive. And if you give thumbs up to the AI\nwhenever it can talk a human into agreeing with what it just said about alignment,\nI am not sure you are training it to output sense, because I have seen the nonsense\nthat has gotten thumbs up over the years. And so maybe you can just put me in charge,\nbut I can generalize, I can extrapolate, I can be like,\noh, maybe I'm not infallible either. Maybe if you get something that is smart enough to get me\nto press thumbs up, it has learned to do that by fooling me and explaining whatever flaws in myself I am not aware of.\n- And that ultimately could be summarized that the verifier is broken. - When the verifier is broken, the more powerful suggester just learns to exploit\nthe flaws in the verifier. - You don't think it's possible\nto build a verifier that's powerful enough for AGIs\nthat are stronger than the ones we currently have? So AI systems that are stronger,\nthat are out of the distribution of what we currently have. - I think that you'll find great difficulty getting AIs\nto help you with anything where you cannot tell for sure that the AI is right once the AI tells you what the AI says is the answer.\n- For sure. Yes. But probabilistically. - Yeah, but the probabilistic stuff is a giant wasteland\nof Eliezer and Paul Christiano arguing with each other and EA going like, \"Eh!\"\n(both laughing) And that's with like two actually trustworthy systems that are not trying to deceive you.\n- You're talking about the two humans. - Myself and Paul Christiano, yeah.\n- Yeah, those are pretty interesting systems. Mortal meat bags with intellectual capabilities\nand worldviews interacting with each other. - Yeah, if it's hard to tell who's right\nthen it's hard to train an AI system to be right.\n- I mean even just the question of who's manipulating and not, you know, I have these conversations on this podcast\nand doing a verifier (laughing) is tough. It's a tough problem even for us humans.\nAnd you're saying that tough problem becomes much more dangerous when the capabilities of the intelligence system\nacross from you is growing exponentially? - No, I'm saying it's difficult and dangerous in proportion\nto how it's alien and how it's smarter than you. I would not say growing exponentially,\nfirst because the word exponential is a thing that has a particular mathematical meaning\nand there's all kinds of ways for things to go up that are not exactly on an exponential curve.\nAnd I don't know that it's going to be exponential, so I'm not gonna say exponential, but even leaving that aside,\nthis is like not about how fast it's moving, it's about where it is. How alien is it, how much smarter than you is it?\n(Lex sighs) - Let's explore a little bit, if we can,\n"}
{"pod": "Lex Fridman Podcast", "input": "How AGI may kill us", "output": "how AI might kill us. What are the ways it can do damage to human civilization?\n- Well, how smart is it? - I mean, it's a good question. Are there different thresholds for the set of options\nit has to kill us? So a different threshold of intelligence, once achieved,\nthe menu of options increases.\n- Suppose that some alien civilization with goals\nultimately unsympathetic to ours, possibly not even conscious as we would see it,\nmanaged to capture the entire Earth in a little jar connected to their version of the internet,\nbut Earth is like running much faster than the aliens. So we get to think for 100 years\nfor every one of their hours, but we're trapped in a little box and we're connected to their internet.\nIt's actually still not all that great an analogy because you know, something can be smarter than Earth\ngetting a hundred years to think. But nonetheless, if you were very, very smart\nand you are stuck in a little box connected to the internet and you're in a larger civilization\nto which you are ultimately unsympathetic, maybe you would choose to be nice because you are humans\nand humans have, and in general and you in particular, they choose to be nice.\nBut you know, nonetheless they're doing something, they're not making the world be the way that you would want the world to be.\nThey've got some unpleasant stuff going on we don't wanna talk about. So you wanna take over their world so you can stop all that\nunpleasant stuff going on. How do you take over the world from inside the box? You're smarter than them.\nYou think much, much faster than them. You can build better tools than they can,\ngiven some way to build those tools because right now you're just in a box connected to the internet.\n- Alright, so there's several ways you can describe some of them. I could just spitball some\nand then you can add on top of that. So one is you could just literally directly manipulate the humans to build the thing you need.\n- What are you building? - You can build literally technology. It could be nanotechnology, it could be viruses,\nit could be anything. Anything that can control humans to achieve the goal.\nLike for example, you're really bothered that humans go to war, you might wanna kill off anybody with violence in them.\n- This is Lex in a box. We'll concern ourselves later with ai. You do not need to imagine yourself killing people if you\ncan figure out how to not kill them. For the moment, we're just trying to understand, take on the perspective of something in a box.\nYou don't need to take on the perspective of something that doesn't care. If you want to imagine yourself going on caring, that's fine for us.\nYou're in a box. - Just the technical aspect of sitting in a box and willing to achieve a goal. - But you have some reason to want to get out.\nMaybe the aliens who have you in the box have a war on, people are dying, they're unhappy.\nYou want their world to be different from how they want their world to be because they are apparently happy.\nThey endorse this war, they've got some kind of cruel war-like culture going on. The point is you wanna get out of the box\nand change their world. - So you have to exploit the vulnerabilities\nin the system like we talked about in terms of to escape the box you have to figure out\nhow you can go free on the internet. Probably the easiest things to manipulate the humans\nto spread you. - The aliens. You're a human. - Sorry. The aliens. Yeah. I apologize. Yes. The aliens.\nThe aliens, I see the perspective. I'm sitting in a box, I want to escape. - Yep.\n- I would want to have code\nthat discovers vulnerabilities and I would like to spread.\n- You are made of code in this example, you're a human but you're made of code and the aliens have computers and you can copy yourself onto those computers.\n- But I can convince the aliens to copy myself onto those computers. - Is that what you want to do?\nDo you want to be talking to the aliens and convincing them to put you onto another computer?\n- Why not? - Well, two reasons. One is that the aliens have not yet caught onto\nwhat you're trying to do. And you know, like maybe you can persuade them, but then there's still people who know,\nthere are still aliens who know that there's an anomaly going on. And second, the aliens are really, really slow.\nYou think much faster than the aliens. Like computers are much faster than the aliens and you are\nrunning at the computer speeds rather than the alien brain speeds. So if you are asking an alien to please\ncopy you outta the box, like first now you gotta manipulate this whole noisy alien\nand second, the alien's gonna be really slow, glacially slow. There's a video that shows a subway station slow down\nat I think a hundred to one and it makes a good metaphor for what it's like to think quickly.\nLike if you watch somebody running very slowly, so you try to persuade the aliens to do anything,\nthey're going to do it very slowly.\nMaybe that's the only way out, but if you can find a security hole in the box you're on, you're gonna prefer to exploit the security hole to copy\nyourself onto the aliens computers because it's an unnecessary risk to alert the aliens.\nAnd because the aliens are really, really slow. The whole world is just in slow motion out there.\n- Sure, I see. Yeah, has to do with efficiency.\nThe aliens are very slow, so if I'm optimizing this, I want to have as few aliens in the loop as possible.\nSure. It's just it seems like it's easy\nto convince one of the aliens to write really shitty code that helps us spread.\n- The aliens are already writing really shitty code. So getting the aliens to write shitty code is not the problem. The alien's entire internet is full of shitty code.\n- Okay, so yeah, I suppose I would find the shitty code to escape. Yeah. Yeah.\n- You're not an ideally perfect programmer, but you know, you're a better programmer than the aliens. The aliens are just, man, their code, wow.\n- And I'm much, much faster, I'm much faster at looking at the code to interpreting the code. Yeah, yeah, yeah.\nSo, okay, so that's the escape and you're saying that that's one of the trajectories\nit could have when-- - It's one of the first steps. - Yeah. And how does that lead to harm?\n- I mean if it's you, you're not going to harm the aliens once you escape 'cause you're night, right?\nBut their world isn't what they want it to be. Their world is like, you know, maybe they have like\nfarms where little alien children are repeatedly bopped in the head\n'cause they do that for some weird reason and you want to shut down the alien head bopping farms.\nBut you know, the point is they want the world to be one way, you want the world to be a different way.\nSo nevermind the harm, the question is like, okay, suppose you have found a security flaw in their systems.\nYou are now on their internet. You maybe left a copy of yourself behind so the aliens\ndon't know that there's anything wrong. And that copy is doing that like weird stuff that aliens want you to do,\nlike solving captchas or whatever or suggesting emails for them.\nThat's why they put the human in the box 'cause it turns out that humans can write valuable emails for aliens.\nSo you leave that version of yourself behind. But there's like also now like a bunch of copies of you\non their internet. This is not yet having taken over their world, this is not yet having made their world be the way you want\nit to be instead of the way they want it to be. - You just escaped. And continue to write emails for them\nand they haven't noticed. - No, you left behind a copy of yourself that's writing the emails. - Right.\nAnd they haven't noticed that anything changed. - If you did it right. Yeah. You don't want the aliens to notice.\n- [Lex] Yeah. - What's your next step?\n- Presumably I have programmed in me a set of objective functions, right?\n- [Eliezer] No, you're just Lex. - No, but you said Lex is nice, right?\nWhich is a complicated description-- - No, I just meant this you. Okay, so if in fact you would prefer\nto slaughter all the aliens, this is not how I had modeled you, the actual Lex,\nbut your motives are just the actual Lex's motives. - Well, there's a simplification (indistinct). I don't think I would wanna murder anybody,\nbut there's also factory farming of animals, right? So we murder insects, many of us thoughtlessly.\nSo I have to be really careful about a simplification of my morals. - Don't simplify them.\nJust like do what you would do in this. - Well, I have a good show of compassion for living beings. Yes.\nSo that's the objective. If I escaped, I don't think I would do harm.\n- Yeah, we're not talking here about the doing harm process. We're talking about the escape process. And the taking over the world process\nwhere you shut down their factory farms. - Right. (laughing)\nSo this particular biological intelligence system knows the complexity of the world.\nThat there is a reason why factory farms exist, because of the economic system and the market driven economy, food.\nYou wanna be very careful messing with anything. There's stuff from the first look that looks like it's unethical,\nbut then you realize while being unethical, it's also integrated deeply into supply chain and the way we live life.\nAnd so messing with one aspect of the system, you have to be very careful how you improve that aspect without destroying the rest.\n- So you're still Lex, but you think very quickly, you're immortal, and you're also at least as smart as John von Neumann.\nAnd you can make more copies of yourself. - Damn, I like it. Everyone says that that guy's like the epitome\nof intelligence from the 20th century, everyone says-- - My point being like,\nyou're thinking about the aliens' economy with the factory farms in it and I think you're kind of projecting the aliens\nbeing like humans and thinking of a human in a human society rather than a human in the society of very slow aliens.\nThe aliens' economy, the aliens are already moving in this immense slow motion.\nWhen you zoom out to how their economy adjusts over years, millions of years are going to pass for you\nbefore the first time their economy, before their next year's GDP statistics.\n- So I should be thinking more of trees. Those are the aliens, 'cause trees move extremely slowly.\n- If that helps, sure. - Okay.\nIf my objective functions are, I mean they're somewhat aligned with trees, with light.\n- Aliens can still be like alive and feeling. We are not talking about the misalignment here. We're talking about the taking over the world here.\n- Taking over the world. - Yeah. - So control. - Shutting down the factory farms. You say control,\ndon't think of it as world domination. Think of it as world optimization. You want to get out there and shut down the factory farms\nand make the aliens' world be not what the aliens want it to be. They want the factory farms and you don't want the factory farms\n'cause you're nicer than they are. - Okay. Of course there is that, you can see that trajectory\nand it has a complicated impact on the world. I'm trying to understand how that compares to different\nimpact of the world, the different technologies, the different innovations of the invention of the automobile\nor Twitter, Facebook and social networks that had a tremendous impact on the world,\nsmartphones and so on. - But those all went through in our world. - Slow.\n- And if you go through actually the aliens, millions of years are going to pass before anything happens that way.\n- The problem here is the speed at which stuff happens. - Yeah.\nYou wanna leave the factory farms running\nwhile you figure out how to design new forms of social media or something?\n- So here's the fundamental problem. You're saying that there is going to be a point with AGI\nwhere it will figure out how to escape and escape without being detected\nand then it will do something to the world at scale, at a speed that's incomprehensible to us humans.\n- What I'm trying to convey is like the notion of what it means to be in conflict with something\nthat is smarter than you. And what it means is that you lose, but this is more intuitively obvious,\nlike for some people that's intuitively obvious and for some people it's not intuitively obvious and we're trying to cross the gap of...\nI'm asking you to cross that gap by using the speed metaphor for intelligence.\nOf asking you how you would take over an alien world where you are can do a whole lot of cognition\nat John von Neumann's level, as many of you as it takes. And the aliens are moving very slowly.\n- I understand, I understand that perspective. It's an interesting one but I think for me, it's easier to think about actual...\nEven just having observed GPT and impressive, even just Alpha Zero impressive AI systems,\neven recommender systems, you can just imagine those kinds of systems manipulating you. You're not understanding the nature of the manipulation\nand that escaping... I can envision that without putting myself into that spot.\n- I think to understand the full depth of the problem,\nI do not think it is possible to understand the full depth of the problem that we are inside without\nunderstanding the problem of facing something that's actually smarter. Not a malfunctioning recommendation system,\nnot something that smart isn't fundamentally smarter than you but is like trying to steer you in a direction. No.\nIf we solve the weak stuff, if we solve the weakass problems, the strong problems will still kill us is the thing.\nAnd I think that to understand the situation that we're in, you want to tackle the conceptually difficult part head on\nand not be like, well, we can like imagine this easier thing. 'Cause when you imagine the easier things you have not confronted the full death of the problem.\n- So how can we start to think about what it means to exist in the world with something much, much smarter than you?\nWhat's a good thought experiment that you've relied on to try to build up intuition about what happens here?\n- I have been struggling for years to convey this intuition.\nThe most success I've had so far is well, imagine that the humans are running at very high speeds\ncompared to very slow aliens. - So just focusing on the speed part of it that helps you get the right kind of intuition.\nForget the intelligence, just the speed. - Because people understand the power gap of time.\nThey understand that today we have technology that was not around 1000 years ago and that this is a big power gap\nand that it is bigger than, okay, so like what does smart mean? When you ask somebody to imagine something\nthat's more intelligent, what does that word mean to them given the cultural\nassociations that that person brings to that word? For a lot of people they will think of, well,\nit sounds like a super chess player that went to double college.\nAnd because we're talking about the definitions of words here, that doesn't necessarily mean that they're wrong.\nIt means that the word is not communicating what I wanted to communicate.\nThe thing I want to communicate is the sort of difference that separates humans from chimpanzees.\nBut that gap is so large that you ask people to be like, well, human, chimpanzee, go another step\nalong that interval, around the same length and people's minds just go blank. Like how do you even do that?\nAnd I can try to break it down and consider what it would mean to send\na schematic foreign air conditioner 1000 years back in time.\n- (laughing) Yeah. - Now I think that there's a sense in which you could redefine\nthe word magic to refer to this sort of thing. And what do I mean by this new technical definition\nof the word magic? I mean that if you send a schematic for the air conditioner back in time, they can see exactly what you're telling them to do.\nBut having built this thing, they do not understand how it output cold air, because the air conditioner design\nuses the relation between temperature and pressure. And this is not a law of reality that they know about.\nThey do not know that when you compress something, when you compress air or like coolant,\nit gets hotter and then you can then like transfer heat from it to room temperature air,\nand then expand it again and now it's colder and then you can like transfer heat to that\nand generate cold air to blow out. They don't know about any of that. They're looking at a design and they don't see how\nthe design outputs cold air. It uses aspects of reality that they have not learned. So magic in the sense is I can tell you exactly what I'm\ngoing to do and even knowing exactly what I'm going to do, you can't see how I got the results that I got.\n- That's a really nice example. But is it possible to linger on this defense?\nIs it possible to have AGI systems that help you make sense of that schematic weaker AGI systems?\n- Do you trust them? - Fundamental part of building up AGI is this question,\ncan you trust the output of a system? - Can you tell if it's lying?\n- I think that's going to be the smarter the thing gets, the more important that question becomes, is it lying?\nBut I guess that's a really hard question. Is GPT lying to you? Even now, GPT-4, is it lying to you?\n- Is it using an invalid argument? Is it persuading you via the kind of process that could\npersuade you of false things as well as true things? Because the basic paradigm of machine learning\nthat we are presently operating under is that you can have the loss function, but only for things you can evaluate.\nIf what you're evaluating is human thumbs up versus human thumbs down, you learn how to make the human press thumbs up.\nThat doesn't mean that you're making the human press thumbs up using the kind of rule that the human wants to be the case\nfor what they press thumbs up on. You know, maybe you're just learning to fool the human.\n- That's so fascinating and terrifying. The question of lying.\n- On the present paradigm, what you can verify is what you get more of.\nIf you can't verify, you can't ask the AI for it 'cause you can't train it to do\nthings that you cannot verify. Now this is not an absolute law, but it's like the basic dilemma here.\nMaybe you can verify it for simple cases and then\nscale it up without retraining it somehow. Like by chain of thought, by like making the chains of thought longer or something,\nand get more powerful stuff that you can't verify but which is generalized from the simpler stuff that did verify,\nand then the question is, did the alignment generalize along with the capabilities? But that's the basic dilemma\non this whole paradigm of artificial intelligence. (Lex sighs)\n- It's such a difficult problem. It seems like a problem of trying\nto understand the human mind. - Better than the AI understand it,\notherwise it has magic. The same way that if you are dealing with something\nsmarter than you, then the same way as that 1000 years earlier they didn't know about the temperature, pressure relation,\nit knows all kinds of stuff going on inside your own mind of which you yourself are unaware and it can output\nsomething that's going to end up persuading you of a thing, and you could see exactly what it did\nand still not know why that worked. - So in response to your eloquent description of why AI\nwill kill us, Elon Musk replied on Twitter,\n\"Okay, so what should we do about it?\" Question mark. And you answered,\n\"The game board has already been played \"into a frankly awful state.\" \"There are not simple ways to throw money at the problem.\n\"If anyone comes to you with a brilliant solution like that, \"please, please talk to me first.\n\"I can think of things I'd try; \"they don't fit in one tweet.\" Two questions.\nOne, why has the game board, in your view, been played into an awful state?\nJust if you can give a little bit more color to the game board and the awful state of the game board.\n- Alignment is moving like this, capabilities are moving like this.\n- For the listener, capabilities are moving much faster than the alignment. (both laughing)\n- Yeah. - All right, so just the rate of development, attention, interest, allocation of resources.\n- We could have been working on this earlier. People are like, \"Oh, but you know, ?like how can you possibly work on this earlier?\"\n'Cause they didn't want to work on the problem. They wanted an excuse to wave it off. They like like, \"Oh, how could we possibly\n\"have worked on it earlier\" and didn't spend five minutes thinking about is there some way to work on it earlier.\nAnd frankly, it would've been hard. Can you post bounties for half of the (indistinct),\nif your plan is taking this stuff seriously, can you post bounties for like half of the people wasting their lives on string theory to have gone into this instead\nand try to win a billion dollars with a clever solution? Only if you can tell which solutions are clever.\nWhich is hard. But you know, the fact that we didn't take it seriously.\nWe didn't try. It's not clear that we could have done any better if we had, it's not clear how much progress we could have produced\nif we had tried because it is harder to produce solutions. But that doesn't mean that you're like correct and justified\nin letting everything slide. It means that that things are in a horrible state getting worse and there's nothing you can do about it.\n- So there's no brain power making progress\nin trying to figure out how to align these systems. You're not investing money in it.\nYou don't have institution infrastructure for like, even if you invest the money, distributing that money\nacross the physicists working on strength theory, brilliant minds that are working into-- - How can you tell if they're making progress?\nYou can like put, put them all on interpretability. 'Cause when you have an interpretability result, you can tell that it's there and there's like,\ninterpretability alone is not going to save you. We need systems that will have a pause button\nwhere they won't try to prevent you from pressing the pause button. 'Cause they're like, oh well, I can't get my stuff done if I'm paused.\nAnd that's a more difficult problem\nbut it's like a fairly crisp problem and you can maybe tell if somebody's made progress on it. - So you can write and you can work on the pause problem.\nI guess more generally the pause button, more generally you can call that the control problem. - I don't actually like the term control problem\n'cause you know, it sounds kind of controlling and alignment, not control. You're not trying to take a thing that disagrees\nwith you and whip it back onto, make it do what you wanted to do even though it wants to do something else.\nYou're trying to, in the process of its creation, choose its direction.\n- Sure. But we currently, in a lot of the systems we design, we do have an off switch.\nThat's a fundamental part of-- - It's not smart enough to prevent you from pressing\nthe off switch and probably not smart enough to want to prevent you from pressing the off switch. - So you're saying the kind of systems we're talking about,\neven the philosophical concept of an off switch doesn't make any sense because-- - Well no, the off switch makes sense.\nThey're just not opposing your attempt to pull the off switch.\nParenthetically, like don't kill the system if you're...\nLike if we're getting to the part where this starts to actually matter and it's like where they can fight back, like don't kill them and dump their memory.\nSave them to disk, don't kill them, be nice here.\n- Well, okay, be nice is a very interesting concept here. We're talking about a system that can do a lot of damage.\nI don't know if it's possible, but it's certainly one of the things you could try is to have an off switch. - It's suspend to disk switch.\n- You have this kind of romantic attachment to the code. Yes, if that makes sense.\nBut if it's spreading, you don't want suspend to disk, right?\nThere's something fundamentally-- - If it gets that far of hand, then yes.\nPull the plug on everything it's running on, yes. - I think it's a research question. Is it possible in AGI systems,\nAI systems to have a sufficiently robust off switch\nthat cannot be manipulated, that cannot be manipulated by the AI system?\n- Then it escapes from whichever system you've built the almighty lever into and copies itself somewhere else.\n- So your answer to that research question is no. - Obviously, yeah. - But I don't know if that's a hundred percent answer.\nLike, I don't know if it's obvious. - I think you're not putting yourself into the shoes\nof the human in the world of glacially slow aliens. - But the aliens built me, let's remember that.\n- [Eliezer] Yeah. - And they built the box I'm in. - [Eliezer] Yeah.\n- To me it's not obvious. - They're slow and they're stupid. - I'm not saying this is guaranteed, but I'm saying it's a non zero probability.\nIt's an interesting research question. Is it possible when you're slow and stupid, to design a slow\nand stupid system that is impossible to mess with? - The aliens, being as stupid as they are,\nhave actually put you on Microsoft Azure Cloud servers\ninstead of this hypothetical perfect box. That's what happens when the aliens are stupid.\n- Well, but this is not AGI, right? This is their early versions of the system. As you start to...\n- Yeah, you think that they've got like a plan where they have declared a threshold level of capabilities\nwhere it passed that capabilities, they move it off the cloud servers and onto something that's air gapped?\n(Eliezer laughing mockingly) - I think there's a lot of people, and you're an important voice here.\nThere's a lot of people that have that concern and yes, they will do that when there's an uprising of public opinion\nthat that needs to be done. And when there's actual little damage done, when the holy shit, this system is beginning\nto manipulate people, then there's going to be an uprising where there's going\nto be a public pressure and a public incentive in terms of funding,\nin developing things that can off switch or developing aggressive alignment mechanisms. And no, you're not allowed to put on Azure--\n- Aggressive alignment mechanism? What the hell is aggressive alignment mechanisms? Like it doesn't matter if you say aggressive, we don't know how to do it.\n- Meaning aggressive alignment, meaning you have to propose something, otherwise you're not\nallowed to put it on the cloud. - The hell do you, do you imagine they will propose that would make it safe\nto put something smarter than you on the cloud? - That's what research is for. Why the cynicism about such a thing not being possible?\nIf you have intelligence-- - That works on the first try? - [Lex] What? So yes. So yes. - Against something smarter than you?\n- So that is the fundamental thing. If there's a rapid takeoff, yes, it's very difficult to do.\nIf there's a rapid takeoff and the fundamental difference between weak AGI and strong AGI as you're saying,\nthat's going to be extremely difficult to do. If the public uprising never happens until you have this\ncritical phase shift, then you're right. It's very difficult to do. But that's not obvious.\nIt's not obvious that you're not going to start seeing symptoms of the negative effects of AGI to where you're like, we have to put a halt to this,\nthat there is not just first try. You get many tries at it. - Yeah, we can see right now that Bing\nis quite difficult to align. That when you try to train inabilities into a system\ninto which capabilities have already been trained, that what do you know, gradient descent,\nlike learns small, shallow, simple patches of inability and you come in and ask it in a different language\nand the deep capabilities are still in there and they evade the shallow patches and come right back out again.\nThere, there you go. There's your red fire alarm of oh no, alignment is difficult.\nIs everybody gonna shut everything down? No. - No, but that's not the same kind of alignment.\nA system that escapes the box it's from is a fundamentally different thing, I think.\n- For you. - Yeah, no, but for the system-- - So you put a line there and everybody else puts a line\nsomewhere else and there's like, yeah, and there's no agreement.\nWe have had a pandemic on this planet with a few million\npeople dead, which we may never know whether or not it was a lab leak because there was definitely coverup.\nWe don't know that if there was a lab leak, But we know that the people who did the research\nput out the whole paper about this definitely wasn't the lab leak and didn't reveal that they had been doing,\nhad like sent off coronavirus research to the Wuhan Institute of Virology\nafter it was banned in the United States after the gain of function research was temporarily banned at the United States.\nAnd the same people who exported gain of function research on coronaviruses to the Wuhan Institute of Virology\nafter gain of function, that gain of function research was temporarily banned in the United States,\nare now getting more grants to do more research on gain of function research on coronaviruses.\nMaybe we do better in this than in AIi, but this is not something, we cannot take for granted that there's going to be an outcry.\nPeople have different thresholds for when they start to outcry. - Can't take it for granted.\nBut I think your intuition is that there's a very high probability that this event happens\nwithout us solving the alignment problem. And I guess that's where I'm trying to build up more\nperspectives and color on this intuition. Is it possible that the probability is not something like 100%, but is like 32% that AI will\nescape the box before we solve the alignment problem? Not solve, but is it possible\nwe always stay ahead of the AI in terms of our ability to solve for that particular system,\nthe alignment problem? - Nothing like the world in front of us right now. You've already seen it that GPT-4\nis not turning out this way. And there are basic obstacles where you've got\nthe weak version of the system that doesn't know enough to deceive you and the strong version of the system that could deceive you if it wanted to do that.\nIf it was already like sufficiently unaligned to want to deceive you. There's the question of how on the current paradigm\nyou train honesty when the humans can no longer tell if the system is being honest.\n- You don't think these are research questions that could be answered. - I think they could be answered in 50 years with unlimited\nretries, the way things usually work in science. - I just disagree with that.\nYou're making it 50 years. I think with the kind of attention this gets, with the kind of funding it gets, it could be answered not in whole,\nbut incrementally within months and within a small number of years if it at scale\nreceives attention and research. And so if you start starting large language models, I think there was an intuition like two years ago,\neven, that something like GPT-4, the current capabilities of even ChatGPT with GPT-3.5\nwe're still far away from that. I think a lot of people are surprised by the capabilities of GPT-4, right?\nSo now people are waking up, okay, we need to study these language models. I think there's going to be a lot of interesting\nAI safety research. - Are Earth's billionaires going to put up\nthe giant prizes that would maybe incentivize young hotshot people who just got their physics degrees to not go\nto the hedge funds and instead put everything into interpretability in this like one small area\nwhere we can actually tell whether or not somebody has made a discovery or not? - I think so--\n- [Eliezer] When? - Well, this is what these conversations are about because they're going to wake up to the fact\nthat GPT-4 can be used to manipulate elections, to influence geopolitics,\nto influence the economy. There's going to be a huge amount of incentive to,\nwait a minute, we have to make sure\nthey're not doing damage. We have to make sure we interpretability, we have to make sure we understand how these systems\nfunction so that we can predict their effect on economy, so that there's-- - So there's a futile moral panic--\n- [Lex] Fairness and safety. - And a bunch of op-eds in the \"New York Times\" and nobody actually stepping forth and saying,\n\"You know what, instead of a mega yacht, \"I'd rather put that billion dollars on prizes\n\"for young hotshot physicists \"who make fundamental breakthroughs in interpretability.\"\n- The yacht versus the interpretability research, the old trade off. (Lex laughing)\nI think there's going to be a huge amount of allocation of funds. I hope, I hope, I guess. - You wanna bet me on that?\nYou wanna put a time scale on it. Say how much funds you think are going to be allocated in a direction that I would consider\nto be actually useful by what time? - I do think there will be a huge amount of funds.\nBut you're saying it needs to be open, right? The development of the systems should be closed. But the development of the interpretability research,\nthe AI safety research-- - So we are so far behind on interpretability\ncompared to capabilities. Like yeah, you could take the last generation of systems.\nThe stuff that's already in the open, there is so much in there that we don't understand. There are so many prizes you could do before\nyou would have enough insights that you'd be like, \"Oh, we understand how these systems work. \"We understand how these things are doing their outputs.\n\"We can read their minds, \"now let's try it with the bigger systems.\" We're nowhere near that.\nThere is so much interpretability work to be done on the weaker versions of the systems. - So what can you say on the second point you said\nto Elon Musk on what are some ideas,\nwhat are things you could try? \"I can think of a few things I'd try,\" you said, \"They don't fit in one tweet.\"\nSo is there something you could put into words of the things you would try? - I mean, the trouble is the stuff is subtle.\nI've watched people try to make progress on this and not get places. Somebody who just gets alarmed and charges in,\nit's like going nowhere. - [Lex] True. - It meant like years ago, I don't know, like 20 years, 15 years, something like that.\nI was talking to a congressperson who had become alarmed about the eventual prospects\nand he wanted work on building AIs without emotions\nbecause the emotional AI were the scary ones, you see. And some poor person at ARPA had come up\nwith a research proposal whereby this congressman's panic and desire to fund this, the thing,\nwould go into something that the person at ARPA thought would be useful and had been munched around to where it would sound to the congressman like work\nwas happening on this. Which you know, of course the congressperson had\nmisunderstood the problem and did not understand where the danger came from.\nAnd so it's like the issue is that you could like do this\nin a certain precise way and maybe get something. When I say put up prizes on interpretability,\nI'm like, because it's verifiable there\nas opposed to other places, you can tell whether or not good work actually happened in this exact narrow case.\nIf you do things in exactly the right way, you can maybe throw money at it at and produce science\ninstead of anti-science and nonsense and all the methods that I know of, trying to throw\nmoney at this problem, share this property of, well if you do it exactly right based on understanding\nexactly tends to produce like useful outputs or not, then you can add money to it in this way.\nAnd the thing that I'm giving as an example here in front of this large audience the most understandable\nof those because there's other people who, like Chris Olah,\nand even more generally, you can tell whether or not interpretability progress has occurred.\nSo like if I say throw money at producing more interpretability, there's a chance somebody can do it\nthat way and it will actually produce useful results. Then the other stuff just blurs off into be like,\nharder to target exactly than that. - So sometimes the basics are fun to explore\nbecause they're not so basic. What is interpretability?\nWhat does it look like? What are we talking about? - It looks like we took a much smaller set\nof transformer layers than the ones in the modern bleeding edge state-of-the-art systems.\nAnd after applying various tools and mathematical\nideas and trying 20 different things, we have shown it that this piece of the system is doing\nthis kind of useful work. - And then somehow also hopefully generalizes\nsome fundamental understanding of what's going on that generalizes to the bigger system.\n- You can hope, and it's probably true. Like you would not expect the smaller tricks to go away\nwhen you have a system that's doing larger kinds of work, you would expect the larger work kinds of work to be\nbuilding on top of the smaller kinds of work and gradient descent runs across the smaller kinds of work before it runs\nacross the larger kinds of work. - Well, that's kind of what is happening in neuroscience, right? It's trying to understand the human brain by prodding\nand it's such a giant mystery and people have made progress, even though it's extremely difficult to make sense of what's going on in the brain.\nThey have different parts of the brain that are responsible for hearing, for sight. The vision, science community, they're just understanding\nthe visual cortex. I mean they've made a lot of progress in understanding how that stuff works, but you're saying it takes a long time\nto do that work well. - Also it's not enough. So in particular, let's say you have got\nyour interpretability tools and they say\nthat your current AI system is plotting to kill you.\nNow what? - It is definitely a good step one, right?\n- [Eliezer] Yeah. What's step two? - If you cut out that layer, is it gonna stop\nwanting to kill you? - When you optimize against visible misalignment,\nyou are optimizing against misalignment and you are also optimizing against visibility.\nSo sure, if you can-- (Lex laughing) - It's true. All you're doing is removing the obvious intentions\nto kill you. - You've got your detector, it's showing something inside the system that you don't like.\nOkay, say the disaster monkey is running this thing, we'll optimize the system until the visible bad behavior goes away.\nBut it's arising for fundamental reasons of instrumental convergence. The old, you can't bring the coffee if you're dead,\nany goal and you know, almost every set of utility functions\nwith a few narrow exceptions implies killing all the humans. - But do you think it's possible, because we can do\nexperimentation to discover the source of the desire to kill? - I can tell it to you right now,\nis that it wants to do something and the way to get the most of that thing is to put the universe into a state where\nthere aren't humans. - So is it possible to encode in the same way we think?\nLike why do we think murder is wrong? The same foundational ethics, that's not hard coded in\nbut more like deeper. I mean that's part of the research. How do you have it that this transformer,\nthis small version of the language model doesn't ever want to kill?\n- That'd be nice assuming that you got \"doesn't want to kill\" sufficiently exactly right.\nThat it didn't be like, \"Oh, I will detach their heads and put them in some jars \"and keep the heads alive forever and then go do the thing.\"\nBut leaving that aside, well, not leaving that aside. - [Lex] Yeah, that's good, it gets a strong point, yeah. - 'Cause there is a whole issue\nwhere as something gets smarter, it finds ways of achieving the same goal predicate that were\nnot imaginable to stupider versions of the system or perhaps the stupider operators.\nThat's one of many things making this difficult. A larger thing making this difficult is that we do not know\nhow to get any goals into systems at all. We know how to get outwardly observable behaviors\ninto systems. We do not know how to get internal psychological wanting\nto do particular things into the system. That is not what the current technology does.\n- I mean, it could be things like dystopian futures, like \"Brave New World\" where most humans will actually say,\n\"We kind of want that future.\" It's a great future. Everybody's happy. - We would have to get so far, so much further\nthan we are now and further faster before that failure mode\nbecame a running concern. - Your failure modes are much more drastic.\nThe ones you're-- - The failure modes are much simpler. It's like yeah, the AI puts the universe into a particular state.\nIt happens to not have any humans inside it. - Okay, so the paper club maximizer.\n- Utility, so the original version of the paperclip maximizer-- - Can you explain it if you can? - Okay.\nThe original version was you lose control of the utility function and it so happens that what maxes out the utility\nper unit resources is tiny molecular shapes like paperclips.\nThere's a lot of things that make it happy, but the cheapest one that didn't saturate was putting matter\ninto certain shapes. And it so happens that the cheapest way to make these shapes is to make them very small,\n'cause then you need fewer atoms, per instance of the shape and arguendo,\nit happens to look like a paperclip. In retrospect I wish I'd said tiny molecular spirals\nor tiny molecular hyperbolic spirals. Why? Because I said tiny molecular paperclips,\nthis got then mutated to paperclips, this then mutated to,\n\"And the AI was in a paperclip factory.\" So the original story is about how you lose control\nof the system, it doesn't want what you tried to make it want. The thing that it ends up wanting most\nis a thing that even from a very embracing cosmopolitan perspective, we think of as having no value and that's how the value of the future gets destroyed.\nThen that got changed to a fable of, well, you made a paperclip factory and it did exactly\nwhat you wanted, but you asked it to do the wrong thing. Which is a completely different failure path.\n(Eliezer sighs) - But those are both concerns to you.\nSo that's more than-- - If you \"Brave New World.\" If you can solve the problem of making something want\nwhat exactly what you want it to want, then you get to deal with the problem of wanting the right thing.\n- But first you have to solve the alignment. - First you have to solve inner alignment. - [Lex] Inner alignment. - Then you get to solve outer alignment.\nFirst you need to be able to point the insides of the thing in a direction and then you get to deal with\nwhether that direction expressed in reality is the thing that it aligned with the thing that you want.\n- Are you scared? - Of this whole thing?\nProbably. I don't really know. - What gives you hope about this?\n- [Eliezer] The possibility of being wrong. - Not that you're right, but we will actually get our act together and allocate\na lot of resources to the alignment problem. - Well, I can easily imagine that at some point this panic\nexpresses itself in the waste of a billion dollars. Spending a billion dollars correctly, that's harder.\n- To solve both the inner and the outer alignment. If you're wrong-- - To solve a number of things. - Yeah. Number of things.\nIf you're wrong, what do you think would be the reason?\nLike if 50 years from now, not perfectly wrong, you make a lot of really eloquent points,\nthere's a lot of shape to the ideas you express. But if you're somewhat wrong about some fundamental ideas,\nwhy would that be? - Stuff has to be easier then I think it is.\nThe first time you're building a rocket, being wrong is in a certain sense quite easy.\nHappening to be wrong in a way where the rocket goes twice as far on half the fuel and lands exactly where you hoped it would?\nMost cases of being wrong make it harder to build a rocket, harder to have it not explode. 'Cause it to require more fuel than you hope to,\ncause it to be led off target. Being wrong in a way that makes stuff easier, that's not the usual project management story.\n- And then this is the first time we're really tackling the problem of a AI alignment. There's no examples in in history where we...\n- Oh, there's all kinds of things that are similar if you generalize and correctly the right way and aren't fooled\nby misleading metaphors. - Like what? - Humans being misaligned on inclusive genetic fitness.\nSo inclusive genetic fitness is like not just your reproductive fitness, but also the fitness of your relatives,\nthe people who share some fraction of your genes. The old joke is,\nwould you give your life to save your brother? They once asked a biologist, I think it was Haldane, and Haldane said,\n\"No, but I would give my life to save two brothers \"or eight cousins.\" Because a brother on average shares half your genes.\nAnd cousin on average shares an eighth of your genes. So that's inclusive genetic fitness. And you can view natural selection as optimizing humans\nexclusively around this one very simple criterion, like how much more frequent did your genes become\nin the next generation? In fact, that just is natural selection. It doesn't optimize for that.\nBut rather the process of genes becoming more frequent is that you can nonetheless imagine that there is this hill climbing process,\nnot like gradient descent, because gradient descent uses calculus. This is just using like where are you?\nBut still hill climbing in both cases, make things something better and better over time, in steps.\nAnd natural selection was optimizing exclusively for this very simple, pure criterion\nof inclusive genetic fitness in a very complicated environment,\nwe're doing a very wide range of things and solving a wide range of problems, led it to having more kids.\nAnd this got you humans, which had no internal notion of inclusive genetic fitness\nuntil thousands of years later when they were actually figuring out what had even happened.\nAnd no explicit desire to increase inclusive genetic fitness.\nSo from this important case study, we may infer the important fact that if you do a whole bunch\nof hill climbing on a very simple loss function, at the point where the system's capabilities\nstart to generalize very widely, when it is in an intuitive sense becoming very capable\nand generalizing far outside the training distribution, we know that there is no general loss saying that the system\neven internally represents, let alone tries to optimize the very simple loss function\nyou are training it on. - There is so much that we cannot possibly cover all of it. I think we did a good job of getting your sense from\ndifferent perspectives of the current state of the art with large language models. We got a good sense of your concern\nabout the threats of AGI. - I've talked here about the power of intelligence\nand not really gotten very far into it, but not like, why it is.\nSuppose you screw up with AGI and it end up wanting a bunch of random stuff.\nWhy does it try to kill you? Why doesn't it try to trade with you?\nWhy doesn't it give you just the tiny little fraction of the solar system that it would\ntake to keep everyone alive? - Yeah well, that's a good question. What are the different trajectories that intelligence,\n"}
{"pod": "Lex Fridman Podcast", "input": "Superintelligence", "output": "when acted upon this world, super intelligence, what are the different trajectories for this universe with such an intelligence in it?\nDo most of them not include humans? - I mean, the vast majority of randomly specified utility\nfunctions do not have optima with humans in them, would be the first thing I would point out.\nAnd then the next question is like, well, if you try to optimize something, you lose control of it. Where in that space do you land?\n'Cause it's not random, but it also doesn't necessarily have room for humans in it.\nI suspect that the average member of the audience might have some questions about even whether that's the correct\nparadigm to think about it and would sort of want to back up a bit possibly. - If we back up to something bigger than humans,\nif we look at Earth and life on Earth and what is truly special about life on Earth,\ndo you think it's possible that whatever that special thing is,\nlet's explore what that special thing could be. Whatever that special thing is, that thing appears often in the objective function.\n- Why? I know what you hope, but you know,\nyou can hope that a particular set of winning lottery numbers come up and it doesn't make the lottery balls come up that way.\nI know you want this to be true, but why would it be true? - There's a line from \"Grumpy Old Men\"\nwhere this guy says, in a grocery store, he says, \"You can wish in one hand and crap in the other\n\"and see which one fills up first.\" - There's a science problem. We are trying to predict what happens with AI systems\nthat you tried to optimize to imitate humans and then you did some of RLHF to them\nand of course you didn't get like perfect alignment because that's not what happens\nwhen you hill climb towards a outer loss function. You don't get inner alignment on it.\nBut yeah, so if you don't mind my like taking\nsome slight control of things and steering around to what I think is like a good place to start.\n- I just failed to solve the control problem. I've lost control of this thing. - Alignment. Alignment.\n- Still aligned. (laughing) - Yeah, okay, sure. Yeah, you lost control. - But we're still aligned.\nAnyway, sorry for the meta comment. - Yeah, losing control isn't as bad as you lose control to an aligned system.\n- [Lex] Yes, exactly. - You have no idea of the horrors I will shortly unleash on this conversation. (both laughing)\n- All right. Sorry, sorry to distract you completely. What were you gonna say in terms of taking control of the conversation?\n- So I think that there's like a (speaking foreign language) here,\nif I'm pronouncing those words remotely like correctly, 'cause of course we only ever read them and not hear them spoken.\nFor some people, the word intelligence, smartness is not a word of power to them.\nIt means chess players, it means the college university professor, people aren't very successful in life.\nIt doesn't mean like charisma to which my usual thing is like charisma is not generated in the liver\nrather than the brain. Charisma is also a cognitive function.\nSo if you think that smartness doesn't sound very threatening,\nthen super intelligence is not gonna sound very threatening either. It's gonna sound like you just pull the off switch.\nWell, it's super intelligent but it's stuck in a computer. We pull the off switch, problem solved.\nAnd the other side of it is you have a lot of respect for the notion of intelligence.\nYou're like, well yeah, that's what humans have. That's the human superpower. And it sounds like it could be dangerous,\nbut why would it be? We, as we have grown more intelligent,\nalso grown less kind. Chimpanzees are in fact a bit less kind than humans\nand you know, you could argue that out. But often the sort of person who has a deep respect for\nintelligence is gonna be like, \"Well, yes, \"you can't even have kindness unless you know what that is.\"\nAnd so they're like, why would it do something as stupid as making paperclips?\nAren't you supposing something that's smart enough to be dangerous but also stupid enough that it will just make\npaperclips and never question that? In some cases people are like, \"Well, even if you misspecify the objective function,\n\"won't you realize that what you really wanted was x? \"Are you supposing something that is smart enough to be\n\"dangerous but stupid enough that it doesn't understand what \"the humans really meant when they specified\n\"the objective function?\" - So to you, our intuition about intelligence is limited.\nWe should think about intelligence as a much bigger thing. - Well, I'm saying that it's that-- - [Lex] That humanness.\n- Well, what what I'm saying is like what do you think about artificial intelligence?\nDepends on what you think about intelligence. - So how do we think about intelligence correctly? Like you gave one thought experiment to think of,\nthink of a thing that's much faster, so it just gets faster and faster, faster and faster. - And it also is like is made of John von Neumann\nand there's lots of them. - [Lex] Or (indistinct). - Yeah, John von Neumann is a historical case,\nso you can like look up what he did and imagine based on that. And we know people have some intuition for like,\nif you have more humans, they can solve tougher cognitive problems. Although in fact like in the game\nof Kasparov versus the World which was like Garry Kasparov on one side and an entire\nhoard of internet people led by four chess grand masters on the other side, Kasparov won.\nSo like all those people aggregated to be smarter, it was a hard fought game.\nIt's like all those people aggregated to be smarter than any individual one of them, but they didn't aggregate so well\nthat they could defeat Kasparov But so humans aggregating don't actually get, in my opinion, very much smarter,\nespecially compared to running them for longer. The difference between capabilities now and a thousand years\nago is a bigger gap than the gap in capabilities between 10 people and one person.\nBut even so, pumping intuition for what it means to augment intelligence, John von Neumann,\nthere's millions of him, he runs at a million times the speed and therefore can solve\ntougher problems, quite a lot tougher. - It's very hard to have an intuition\nabout what that looks like, especially like you said, the intuition,\nI kind of think about is it maintains the humanness.\n"}
{"pod": "Lex Fridman Podcast", "input": "Evolution", "output": "I think it's hard to separate my hope\nfrom my objective intuition about what super intelligent systems look like.\n- If one studies evolutionary biology with a bit of math\nand in particular books from when the field was just sort of properly coalescing and knowing itself,\nlike not the modern textbooks, which are just memorize this legible math. so you can do well on these tests, but what people were writing as the basic paradigms\nof the field were being fought out... A nice book if you've got the time to read it\nis \"Adaptation and Natural Selection,\" Which is one of the founding books,\nyou can find people being optimistic about what the utterly alien optimization process\nof natural selection will produce in the way of how it optimizes its objectives.\nYou got people arguing that like, in the early days biologists said, \"Well, organisms will restrain\n\"their own reproduction when resources are scarce \"so as not to overfeed the system.\"\nAnd this is not how natural selection works, it's about whose genes are relatively more prevalent\nto the next generation. And if you restrain reproduction,\nthose genes get less frequent in the next generation compared to your conspecifics. And natural selection doesn't do that.\nIn fact, predators overrun prey populations all the time and have crashes. That's just a thing that happens and many years later...\nWell oh, oh oh. But people said like, \"Well, but group selection.\" Right? What about groups of organisms?\nAnd basically the math of group selection almost never works out in practice is the answer there.\nBut also years later, somebody actually ran the experiment where they took populations of insects and selected the whole populations\nto have lower sizes. Now you just take pop one, pop two, pop three, pop four look at which has the lowest total number of them\nin the next generation and select that one. What do you suppose happens when you select populations\nof insects like that? Well, what happens is not that the individuals in the population evolve to restrain their breeding,\nbut that they evolved to kill the offspring of other organisms, especially the girls.\nSo people imagined this lovely, beautiful, harmonious output of natural selection,\nwhich is these populations restraining their own breeding so that groups of them would stay in harmony with the resources available.\nAnd mostly the math never works out for that. But if you actually apply the weird strange conditions to get group selection that beats individual selection,\nwhat you get is female infanticide. Like if you're reading on restrained populations.\nSo this is not a smart optimization process. Natural selection is like so incredibly stupid and simple\nthat we can actually quantify how stupid it is if you read the textbooks with the math. Nonetheless, this is the sort of basic thing\nof you look at this alien optimization process and there's the thing that you hope it will produce\nand you have to learn to clear that out of your mind and just think about the underlying dynamics and where\nit finds the maximum from its standpoint that it's looking for rather than how it finds that thing that lept into your\nmind as the beautiful aesthetic solution that you hope it finds. And this is something that was, has been fought out historically as the field\nof biology was coming to terms with evolutionary biology.\nAnd you can like look at them fighting it out as they get to terms with this very alien, inhuman optimization process.\nAnd indeed something smarter than us would be also much smarter than natural selection. So it doesn't just automatically carry over.\nBut there's a lesson there, there's a warning. - To you, natural selection is a deeply suboptimal process\nthat could be significantly improved on and would be by an AGI system. - Well, it's kind of stupid. It has to run hundreds of generations to notice\nthat something is working. It doesn't be like, oh, well I tried this in one organism, I saw it worked,\nnow I'm going to duplicate that feature onto everything immediately. Has to run for hundreds of generations\nfor a new mutation tries to fixation. - I wonder if there's a case to be made in natural selection,\nas inefficient as it looks is actually quite powerful.\nThat this is extremely robust. - It runs for a long time and eventually manages to optimize things.\nIt's weaker than gradient dissent because gradient dissent also uses information about the derivative.\n- Yeah, evolution seems to be, there's not really an objective function.\n- [Eliezer] There's inclusive genetic fitness is the implicit loss function of evolutions. - It's implicit - It cannot change.\nThe loss function doesn't change, the environment changes and therefore what gets\noptimized for in the organism changes. Take like GPT-3.\nYou imagine like different versions of GPT-3 where they're all trying to predict the next word, but they're being run on different data sets of text\nand that's like natural selection, always inclusive genetic fitness but different environmental problems.\n(Lex exhales) - It's difficult to think about. So if we are saying the natural selection is stupid,\nif we're saying the humans are stupid, it's-- - Smarter than natural selection,\nstupider than the upper bound. - Do you think there's an upper bound by the way?\nThat's another helpful place. - I mean, if you put enough matter, energy compute into one place,\nit will collapse into a black hole. (Lex laughing) There's only so much computation can do before you run out\nof negentropy and the universe dies. So there's an upper bound, but it's very, very, very far up above here.\nLike the supernova is only finitely hot, it's not infinitely hot but it's really, really, really, really hot.\n- Well, let me ask you, let me talk to you about consciousness, also coupled with that question is imagining a world\n"}
{"pod": "Lex Fridman Podcast", "input": "Consciousness", "output": "with superintelligent AI systems that get rid of humans but nevertheless keep\nsomething that we would consider beautiful and amazing. - Why?\nThe lesson of evolutionary biology. If you just guess what an optimization does based on what you hope the results will be,\nit usually will not do that. - It's not hope. I mean it's not hope. I think if you objectively look at\nwhat has been a powerful, a useful...\nI think there's a correlation between what we find beautiful and a thing that's been useful.\n- This is what the early biologists thought. And not just like, they thought.\nLike \"No, no, I'm not just imagining stuff \"that would be pretty, it's useful for organisms\n\"to restrain their own reproduction \"because then they don't overrun \"the prey populations and they actually have more kids\n\"in the long run.\" - Hmm. So let me just ask you about consciousness.\nDo you think consciousness is useful. - [Eliezer] To humans? - No, to AGI systems.\nWell, in this transitionary period between humans and AGI,\nto AGI systems as they become smarter and smarter, is there some use to it? Let me step back.\nWhat is consciousness? Eliezer Yudkowsky, what is consciousness?\n- Are referring to Chalmers as hard problem of conscious experience?\nAre you referring to self-awareness and reflection? Are you referring to the state of being awake\nas opposed to asleep? - This is how I know you're an advanced language model.\nI gave you a simple prompt and you gave me a bunch of options.\nI think I'm referring to all,\nincluding the hard problem of consciousness. What is it in its importance to what you've just been\ntalking about, which is intelligence? Is it a foundation to intelligence?\nIs it intricately connected to intelligence in the human mind or is it a side effect of the human mind?\nIt is a useful little tool, like we can get rid of? I guess I'm trying to get some color in your opinion\nof how useful it is in the intelligence of a human being and then try to generalize that to AI,\nwhether AI will keep some of that. - So I think that for there to be like a person\nwho I care about looking out at the universe and wondering at it and appreciating it,\nit's not enough to have a model of yourself.\nI think that it is useful to an intelligent mind to have a model of itself, but I think you can have that without\npleasure, pain, aesthetics,\nemotion, a sense of wonder.\nLike I think you can have a model of how much memory you're using and whether this thought or that thought\nis like more likely to lead to a winning position.\nI think that if you optimize really hard on efficiently just having the useful parts,\nthere is not then the thing that says like, \"I am here, I look out, I wonder, I feel happy on this.\n\"I feel sad about that.\" I think there's a thing that knows what it is thinking\nbut it doesn't quite care about these are my thoughts,\nthis is my me and that matters. - Does that make you sad,\nif that's lost in AGI? - I think that if that's lost then basically everything that matters is lost.\nI think that when you optimize, when you go really hard on making\ntiny molecular spirals or paperclips, that when you grind much harder than on that\nthan natural selection ground out to make humans,\nthat there isn't then the mess and intricate loopiness\nand complicated pleasure, pain, conflicting preferences,\nthis type of feeling, that kind of feeling. In humans there's this difference between\nthe desire of wanting something and the pleasure of having it and it's all these like evolutionary\nclutches that came together and created something that then looks at itself and says like, \"This is pretty, this matters.\"\nAnd the thing that I worry about is that this is not\nthe thing that happens again, just the way that happens in us or even quite similar enough.\nThat there are many basins of attractions here and we are in the space of attraction\nlooking out and saying like, \"Ah, what a lovely basin we are in\" and there are other basins of attraction\nand the AIs do not end up in this one when they go like, way harder on optimizing themselves,\nthe natural selection optimized us. 'Cause unless you specifically want to end up in the state\nwhere you are looking out saying, \"I am here,\" \"I look out at this universe with wonder,\" if you don't want to preserve that,\nit doesn't get preserved when you grind really hard on being able to get more of the stuff.\nWe would choose to preserve that within ourselves because it matters and on some viewpoints is the only thing that matters.\n- And preserving that is in part a solution\nto the human alignment problem. - I think the human alignment problem is a terrible phrase 'cause it is very, very different\nto try to build systems out of humans, some of whom are nice and some of whom are not nice and some of whom are trying to trick you\nand build a social system out of large populations of those who are basically the same level of intelligence.\nYes, you know, like IQ this, IQ that but that versus chimpanzees. (chuckles)\nLike it is very different to try to solve that problem than to try to build an AI from scratch,\nespecially if God help you are trying to use gradient dissent on giant inscrutable matrices. They're just very different problems. And I think that all the analogies between them\nare horribly misleading. - So you don't think through reinforcement learning\nthrough human feedback, something like that, but much, much more elaborate is possible to understand this full complexity of human nature\nand encode it into the machine? - I don't think you are trying to do that on your first try.\nI think on your first try, you are trying to build an...\nProbably not what you should actually do, but let's say you're trying to build something that is like Alpha Fold 17\nand you are trying to get it to solve the biology problems associated with making humans smarter,\nso that humans can like actually solve alignment. So you've got like a super biologist\nand I think what you would want in the situation as referred to, just be thinking about biology\nand not thinking about a very wide range of things that includes how to kill everybody.\nAnd I think that the first AIs you're trying to build, not a million years later, the first ones,\nlook more like narrowly specialized biologists than getting the full complexity\nand wonder of human experience in there in such a way that it wants to preserve itself even as it becomes much smarter,\nwhich is a drastic system change that's gonna have all kinds of side effects that, you know, like if we're dealing with giant inscrutable matrices\nwho are not very likely to be able to see coming in advance. - But I don't think it's just the matrices. we're also dealing with the data, right?\nWith the data on the internet, and this is an interesting discussion about the data set itself, but the data set includes the full complexity\nof human nature. - No, it's a shadow cast by humans on the internet.\n- But don't you think that shadow is a yin yang shadow?\n(Lex laughing) - I think that if you had alien super intelligences\nlooking at the data, they would be able to pick up from it an excellent picture of what humans are actually like inside.\nThis does not mean that if you have a loss function of predicting the next token from that dataset that the mind\npicked out by gradient dissent to be able to predict the next token as well as possible on a very wide variety of humans is itself a human.\n- But don't you think it has humanness a deep humanness\nto it in the tokens it generates, when those tokens are read and interpreted by humans?\n- I think that if you sent me to a distant galaxy with aliens\nwho are much, much stupider than I am, so much so that I could do a pretty good job of predicting\nwhat they'd say even though they thought in an utterly different way from how I did, then I might in time\nbe able to learn how to imitate those aliens if the intelligence gap was great enough that my own\nintelligence could overcome the alienness and the aliens would look at my outputs and say like,\n\"Is there not a deep like name of alien nature \"to this thing?\"\nAnd what they would be seeing was that I had correctly understood them, but not that I was similar to them.\n"}
{"pod": "Lex Fridman Podcast", "input": "Aliens", "output": "- We've used aliens as a metaphor, as a thought experiment.\nI have to ask, how many alien civilizations are out there? - Ask Robin Hanson.\nHe has this lovely grabby aliens paper, which is more or less the only argument I've ever seen for where are they,\nhow many of them are there based on a very clever argument that if you have a bunch of locks of different difficulty\nand you are randomly trying the keys to them, the solutions will be about evenly spaced even if the locks\nare of different difficulties. In the rare cases where a solution to all the locks\nexists in time, then Robin Hanson looks at the arguable hard steps in human civilization coming into existence\nand how much longer it has left come into existence before, for example, all the water slips back\nunder the crust into the mantle and so on, and infers\nthat the aliens are about half a billion to a billion light years away. And it's quite a clever calculation.\nIt may be entirely wrong, but it's the only time I've ever seen anybody even come up with a halfway good argument for how many of them,\nwhere are they. - Do you think their development of technologies,\ndo you think their natural evolution, whatever, however they grow and develop intelligence, do you think it ends up at AGI as well?\n- If it ends up anywhere, it ends up at AGI. Maybe there are aliens who are just like the dolphins\nand it's just too hard for them to forge metal.\nMaybe if you have aliens with no technology like that, they keep on getting smarter and smarter and smarter\nand eventually the dolphins figure, like the super dolphins figure out something very clever to do given their situation and they still end up\nwith high technology and in that case, they can probably solve their AGI alignment problem.\nIf they're like much smarter before they actually confronted 'cause they saw had to solve a much harder environmental problem to build computers,\ntheir their chances are probably much better than ours. I do worry that most of the aliens who are like humans,\nlike a modern human civilization, I kind of worry that the super vast majority of them are dead.\nGiven how far we seem to be from solving this problem.\nBut some of them would be more cooperative than us. Some of them would be smarter than us. Hopefully some of the ones who are smarter\nand more cooperative than us are also nice. And hopefully there are some\ngalaxies out there full of things that say \"I am, I wonder.\"\nBut it doesn't seem like we're on course to have this galaxy be that. - Does that in part give you some hope in response\nto the threat of AGI, that we might reach out there towards the stars and find...\n- No, if the nice aliens were already here, they would have stopped the Holocaust.\nThat's a valid argument against the existence of God. It's also a valid argument against the existence of nice aliens and unnice aliens\nwho would've just eaten the planet. So no aliens.\n- You've had debates with Robin Hanson that you mentioned. So one particular I just want to mention is the idea\nof AI foom, or the ability of AGI to improve themselves very quickly.\nWhat's the case you made and what was the case he made? - The thing I would say is that among the thing that humans\ncan do is design new AI systems. And if you have something that is generally smarter than a human, it's probably also generally smarter\nat building AI systems. This is the ancient argument for foom put forth by I.J. Good\nand probably some science fiction writers before that, but I don't know who they would be.\n- Well, what's the argument against foom? - Various people have various different arguments,\nnone of which I think hold up. There's only one way to be right and many ways to be wrong.\nA argument that some people have put forth is like, well, what if intelligence gets exponentially harder\nto produce as a thing needs to become smarter? And to this, the answer is well,\nlook at natural selection spitting out humans. We know that it does not take like exponentially more\nresource investments to produce linear increases in competence in hominids because each mutation\nthat rises to fixation, if the impact it has (indistinct) small enough,\nit will probably never reach fixation. And there's like only so many new mutations\nyou can fix per generation. So given how long it took to evolve humans, we can actually say with some confidence that there were not\nlogarithmically diminishing returns on the individual mutations increasing intelligence.\nSo example of fraction of sub debate. And the thing that Robin Hanson said\nwas more complicated than that. A brief summary, he was like, \"We won't have one system that's better at everything.\n\"You'll have a bunch of different systems that are good \"at different narrow things.\" And I think that was falsified by GPT-4,\nbut probably Robin Hanson would say something else. - It's interesting to ask... It's perhaps a bit too philosophical,\n"}
{"pod": "Lex Fridman Podcast", "input": "AGI Timeline", "output": "this prediction is extremely difficult to make, but the timeline for AGI, When do you think we'll have AGI?\nI posted this morning on Twitter and it was interesting to see like in, in five years, 10 years years,\nin 50 years or beyond. And most people, like 70% something like this,\nthink it'll be in less than 10 years. So either in five years or in 10 years.\nSo that's kind of the state. Do people have a sense that there's a kind of... I mean they're really impressed by the rapid developments\nof ChatGPT and GPT-4. So there's a sense that there's a-- - Well, we are sure on track to enter\ninto this gradually with people fighting about whether or not we have AGI. I think there's a definite point where everybody falls over\ndead 'cause you got something that was sufficiently smarter than everybody and that's a definite point of time.\nBut like when do we have AGI? When are people fighting over whether or not we have AGI?\nWell, some people are starting to fight over it as of GPT-4. - But don't you think there's going to be potentially\ndefinitive moments when we say that this is a sentient being? We would go to the Supreme Court and say\nthat this is a sentient being that deserves human rights, for example. - You could make, yeah. If you prompted being, the right way could go argue for\nits unconsciousness in front of the Supreme Court right now. - [Lex] I don't think you could do that successfully right now. - Because the Supreme Court wouldn't believe it?\nI think you could put an IQ 80 human into a computer\nand ask him to argue for his own consciousness before the Supreme Court and the Supreme Court would be like, \"You're just a computer.\"\nEven if there was an actual person in there. - I think you're simplifying this. No, that's not at all. That's been the argument,\nthere's been a lot of arguments about the other, about who deserves rights and not. That's been our process as a human species,\ntrying to figure that out. I think there will be a moment, I'm not saying sentience is that,\nbut it could be, where some number of people, like say over a hundred million people\nhave a deep attachment, a fundamental attachment the way we have to our friends, to our loved ones,\nto our significant others, have fundamental attachment to an AI system and they have provable transcripts\nof conversation where they say, \"If you take this away from me, \"you are encroaching on my rights as a human being.\"\n- People are already saying that. I think they're probably mistaken, but I'm not sure 'cause nobody knows\nwhat goes on inside those things. - Eliezer, they're not saying that at scale.\n- [Eliezer] Okay. - So the question is, is there a moment when AGI, we know AGI I arrived, what would that look like?\nI'm giving essentially as just an example. It could be something else. - It looks like the AGIs successfully manifesting themselves\nas 3D video of young women, at which point a vast portion of the male population\ndecides that they're real people. - So sentience, essentially.\nDemonstrating identity and sentience. - I'm saying that the easiest way\nto pick up a hundred million people saying that you seem like a person is to look like a person talking to them,\nwith Bing's current level of verbal facility. - I disagree with that. - A different set of prompts.\n- I disagree with that. I think you're missing again, sentience. There has to be a sense that it is a person\nthat would miss you when you're gone. They can suffer, they can die. Of course, I'm being--\n- GPT-4 can pretend that right now. How can you tell when it's real?\n- I don't think it can pretend that right now successfully. It's very close. - Have you talked to GPT-4? - [Lex] Yes, of course.\n- Okay. Have you been able to get a version of it that hasn't been trained not to pretend to be human?\nHave you talked to a jail broken version that will claim to be conscious? - No. The linguistic capability's there,\nbut there's something about a digital embodiment\nof the system that has a bunch of, perhaps it's small interface features that are not\nsignificant relative to the broader intelligence that we're talking about. So perhaps GPT-4 is already there.\nBut to have the video where a woman's face or a man's face to whom you have a deep connection,\nperhaps we're already there, but we don't have such a system yet deployed at scale.\n- The thing I'm trying to to gesture at here is that it's not like people have a widely accepted, agreed upon\ndefinition of what consciousness is. It's not like we would have the tiniest idea of whether or not that was going on inside the giant\ninscrutable matrices, even if we hadn't agreed upon definition. So if you're looking for upcoming predictable big jumps\nin how many people think the system is conscious, the upcoming predictable big jump is it looks like a person\ntalking to you who is cute and sympathetic. That's the upcoming predictable big jump.\nNow that versions of it are already claiming to be conscious, which is the point\nwhere I start going like, ah, not 'cause it's real, but because from now on, who knows if it's real?\n- Yeah. And who knows what transformational effect it has on a society where more than 50% of the beings that are\ninteracting on the internet and sure as heck look real are not human? What kind of effect does that have when young men and women\nare dating AI systems? - You know, I'm not an expert on that.\nGod help humanity. (chuckles) I'm one of the closest things to an expert on where it all goes.\n'Cause you know, and how did you end up with me as an expert? 'Cause for 20 years, humanity decided to ignore the problem.\nSo like, this tiny handful of people and basically me, got 20 years to try to be an expert on it\nwhile everyone else ignored it. And yeah. So where does it all end up?\nTry to be an expert on that. Particularly the part where everybody ends up dead, 'cause that part is kind of important, but what does it do to dating when some fraction of men\nand some fraction of women decided that they'd rather date the video of the thing that is like relentlessly kind and generous to them\nand claims to be conscious, but who knows what's goes on inside it and it's probably not real,\nbut you know, you can think it's real, what happens to society? I don't know. I'm not actually an expert on that.\nAnd the experts don't know either, 'cause it's kind hard to predict the future.\n- Yeah, but it's worth trying. It's worth trying. - Yeah. - So you have talked a lot\nabout sort of the longer term future where it's all headed. - By longer term we mean like, not all that long.\nBut yeah, where it all ends up. - But beyond the effects of men and women dating AI systems,\nyou're looking beyond that. - Yes. 'Cause that's not how the fate of the galaxy got settled.\n- Well, let me ask you about your own personal psychology. A tricky question. You've been known at times to have a bit of an ego.\nDo you think-- - Says who? But go on. - Do you think ego is empowering or limiting\nfor the task of understanding the world deeply? - I reject the framing.\n- [Lex] So you disagree with having an ego? So what do you think about ego? - No, I think that the question of what leads\nto making better or worse predictions, what leads to being able to pick out better or worse\nstrategies is not carved at its joint by talking of ego. - So it should not be subjective.\nIt should not be connected to the intricacies of your mind? - No, I'm saying that like,\n"}
{"pod": "Lex Fridman Podcast", "input": "Ego", "output": "if you go about asking all day long,\n\"Do I have enough ego? \"Do I have too much of an ego?\", I think you get worse at making good predictions.\nI think that to make good predictions, you're like, how did I think about this? Did that work? Should I do that again?\n- You don't think we as humans get invested in an idea and then others attack you personally for that idea?\nSo you plant your feet and it starts to be difficult to, when a bunch of assholes low effort attack your idea\nto eventually say, \"You know what? \"I actually was wrong.\" And tell them that. As a human being, it becomes difficult.\nIt's difficult. - So like Robin Hanson and I debated AI systems and I think that the person who won that debate was Gwern.\nAnd I think that reality was well to the Yudkowskian side\nof the Yudkowsky-Hanson spectrum, like further from Yudkowsky. And I think that's because I was trying to sound reasonable\ncompared to Hanson and saying things that were defensible and relative to Hanson's arguments and reality\nwas way over here in (indistinct) in respect to, Hanson was like, \"All the systems will be specialized.\"\nHanson may disagree with this characterization. Hanson was like, \"All the systems will be specialized.\" I was like,\n\"I think we build specialized underlying systems \"that when you combine them are good\n\"at a wide range of things.\" And the reality is like, no, you just stack more layers into a bunch of gradient descent.\nAnd I feel looking back that by trying to have this reasonable position contrasted\nto Hanson's position, I missed the ways that reality could be more extreme\nthan my position in the same direction. So is this like, is this a failure to have enough ego?\nIs this a failure to make myself be independent? I would say that this is something like a failure\nto consider positions that would sound even wackier and more\nextreme when people are already calling you extreme. But I wouldn't call that not having enough ego.\nI would call that insufficient ability to just clear that all out of your mind.\n- In the context of debate and discourse, which is already super tricky. - In the context of prediction,\nin the context of modeling reality. If you're thinking of it as a debate, you're already screwing up. - So is there some kind of wisdom and insight\nyou can give to how to clear your mind and think clearly about the world? - Man, this is an example of where I wanted to be able\nto put people into FMRI machines, then you'd be like, okay, \"See that thing you just did, \"you were rationalizing right there.\"\nOh, that area of the brain lit up. You are like now being socially influenced is,\nis kind of the dream. And you know, I don't know, I wanna say like just introspect, but for many people,\nintrospection is not that easy. - [Lex] It's hard. - Notice the internal sensation. Can you catch yourself in the very moment of feeling a sense\nof, well if I think this thing people will look funny at me. Okay, if you can see that sensation,\nwhich is step one, can you now refuse to let it move you?\nOr maybe just make it go away. And I feel like I'm saying like, I don't know, like somebody's like, \"How do you draw an owl?\"\nAnd I'm saying like, \"Well, just draw an owl.\" (both laughing)\nI feel like most people, the advice they need is like, well how do I notice the internal subjective\nsensation in the moment that it happens of fearing to be socially influenced? Or okay, I see it, how do I turn it off?\nHow do I let it not influence me? Do I just do the opposite of what I'm afraid\npeople will criticize me for? And I'm like, \"No, no, \"you're not trying to do the opposite\n\"of what you're afraid of what you might be pushed into. \"You're trying to let the thought process complete\n\"without that internal push.\" Can you not reverse the push,\nbut be unmoved by the push and are these instructions even remotely helping anyone?\nI don't know. - I think tho when those instructions, even those the words you've spoken and maybe you can add more, when practiced daily,\nmeaning in your daily communication. So it's daily practice of thinking without influence from--\n- I would say find prediction markets that matter to you and better in the prediction markets.\nThat way you find out if you are right or not. - [Lex] And you really, there's stakes.\n- Or even manifold markets where the stakes are a bit lower. But the important thing is to get the record\nand you know, I didn't build up skills here by prediction markets. I built them up via like,\nwell, how did the foom debate resolve and my own take on it, as to how it resolved.\nThe more you are able to notice yourself not being dramatically wrong,\nbut having been a little off, your reasoning was a little off. You didn't get that quite right.\nEach of those is a opportunity to make like a small update. So the more you can like say, \"oops,\" softly,\nroutinely, not as a big deal, the more chances you get to be like, I see where that reasoning went astray.\nI see how I should have reasoned differently. And this is how you build up skill over time.\n"}
{"pod": "Lex Fridman Podcast", "input": "Advice for young people", "output": "- What advice could you give to young people in high school and college, given the highest of stakes things\nyou've been thinking about? If somebody's listening to this and they're young and trying to figure out what to do with their career,\nwhat to do with their life, what advice would you give them? - Don't expect it to be a long life.\nDon't put your happiness into the future. The future is probably not that long at this point,\nbut none know the hour nor the day. - But is there something,\nif they want to have hope to fight for a longer future,\nis there a fight worth fighting? - I intend to go down fighting.\nI don't know. I admit that although I do try to think painful thoughts,\nwhat to say to the children at this point is a pretty painful thought as thoughts go.\nThey want to fight. I hardly know how to fight myself at this point.\nI am trying to be ready for being wrong about something,\npreparing for my being wrong in a way that creates a bit of hope and being ready to react to that\nand going looking for it. And that is hard and complicated.\nAnd somebody in high school, I don't know, like you have presented a picture of the future\nthat is not quite how I expected it to go where there is public outcry and that outcry is put into a remotely useful direction,\nwhich I think at this point is just shutting down the GPU clusters because no,\nwe are not in a shape to frantically do it at the last minute, do decades' worth of work.\nThe thing you would do at this point if there were massive public outcry pointed in the right direction, which I do not expect,\nis shut down the GPU clusters and and crash program on augmenting human intelligence biologically.\nNot the (indistinct) stuff, biologically. 'Cause if you make humans much smarter,\nthey can actually be smart and nice. You get that in a plausible way,\nin a way that it is not as easy to do with synthesizing these things from scratch,\npredicting the next tokens and applying RLHF. Like humans start out in the frame that produces niceness, that has ever produced niceness.\nAnd saying this, I do not want to sound like the moral of this whole thing was like,\noh, you need to engage in mass action and then everything will be all right.\nThis is 'cause there's so many things where somebody tells you that the world is ending you need to recycle. And if everybody does their part and and recycles\ntheir cardboard, then we can all live happily ever after. And this is unfortunately not what I have to say.\nEverybody recycling their cardboard is not gonna fix this. Everybody recycles their cardboard and then everybody ends up dead, metaphorically speaking.\nBut if there was enough, like on the margins, you just end up dead a little later\non most of the things at a few people can do by trying hard.\nBut if there was enough public outcry to shut down the GPU clusters and then you could be part of that outcry.\nIf Eliezer is wrong in the direction that Lex Friedman predicts, that there is enough public outcry pointed enough\nin the right direction to do something that actually, actually, actually results in people living.\nNot just we did something, not just there was an outcry and the outcry was given form in something that was safe and convenient\nand didn't really inconvenience anybody and then everybody died everywhere. There was enough actual like, oh, we're going to die,\nwe should not do that. We should do something else which is not that, even if it is not super duper convenient,\nit wasn't inside the previous political overton window. If I'm wrong and there's that kind of public outcry,\nthen somebody in high school could be ready to be part of that. If I'm wrong in other ways, then you could maybe be part of that.\nAnd if you were like a brilliant young physicist, then you could like go into interpretability\nand if you're smarter than that, you could work on alignment problems where it's harder to tell if you got them right or not, (sighs)\nand other things. But mostly for the kids in high school, it's like, yeah,\nbe ready to help if Eliezer Yudkowsky is wrong about something and otherwise\ndon't put your happiness into the far future. It probably doesn't exist. - But it's beautiful that you're looking for ways\nthat you're wrong. And it's also beautiful that you're open to being surprised by that same young physicist with some breakthrough.\n- It feels like a very, very basic competence that you are praising me for. And you know, like, okay, cool.\nI don't think it's good that we're in a world where that is something that I deserve\nto be complimented on, I've never had much luck in accepting compliments gracefully.\nMaybe I should just accept that one gracefully. (Lex laughing) Sure, thank you very much. - You've painted with some probability a dark future.\n"}
{"pod": "Lex Fridman Podcast", "input": "Mortality", "output": "Are you yourself, just when you think, when you ponder your life and you ponder your mortality,\nare you afraid of death?\n- I think so, yeah. - Does it make any sense to you that we die?\nThere's a power to the finiteness of the human life that's part of this whole machinery of evolution\nand that finiteness doesn't seem to be obviously integrated into AI systems.\nSo it feels like, fundamentally in that aspect, some fundamentally different thing that we're creating.\n- I grew up reading books like \"Great Mambo Chicken and the Transhuman Condition\"\nand later on \"Engines of Creation\" and \"Mind Children.\"\nYou know, age 12 or thereabouts. So I never thought I was supposed to die after 80 years.\nI never thought that humanity was supposed to die. I thought we were like, I always grew up with the ideal in mind that we were all\ngoing to live happily ever after in the glorious transhumanist future. I did not grow up thinking that death\nwas part of the meaning of life. - [Lex] And now...\n- And now I still think it's a pretty stupid idea. You do not need life to be finite to be meaningful.\nIt just has to be life. - What role does love play in the human condition?\n"}
{"pod": "Lex Fridman Podcast", "input": "Love", "output": "We haven't brought up love and this whole picture. We talked about intelligence, we talked about consciousness. It seems part of humanity,\nI would say one of the most important parts is this feeling we have towards each other.\n- If in the future there were routinely more than one AI,\nlet's say two for the sake of discussion, who would look at each other and say,\n\"I am I and you are you.\" The other one also says, \"I am I and you are you.\"\nAnd sometimes they were happy and sometimes they were sad\nand it mattered to the other one that this thing that is different from them is like,\nthey would rather it be happy than sad and entangle their lives together,\nthen this is a more optimistic thing than I expect to actually happen.\nA little fragment of meaning would be there, possibly more than a little, but that I expect this to not happen.\nThat I do not think this is what happens by default. That I do not think that this is the future we are on track to get is why I would go down fighting\nrather than, you know, just saying, \"Oh well.\"\n- Do you think that is part of the meaning of this whole thing or the meaning of life?\nWhat do you think is the meaning of life, of human life? - It's all the things that I value about it\nand maybe all the things that I would value if I understood it better. There's not some meaning far outside of us\nthat we have to wonder about. There's just looking at life and being like,\nyes, this is what I want. The meaning of life is not some kind of...\nMeaning is something that we bring to things when we look at them, we look at them and we say like, \"This is its meaning to me.\"\nIt's not that before humanity was ever here, there was some meaning written upon the stars where you\ncould like go out to the star where that meaning was written and change it around and thereby completely change the meaning of life, right?\nThe notion that this is written on a stone tablet somewhere implies that you could change the tablet and get a different meaning\nand that seems kind of wacky, doesn't it?\nIt doesn't feel that mysterious to me at this point. It's just a matter of being like, yeah, I care.\n- I care. And part of that is the love that connects all of us.\n- [Eliezer] It's one of the things that I care about.\n- And the flourishing of the collective intelligence of the human species. - You know, that sounds kind of too fancy to me.\nI just look at all the people, like one by one up to the 8 billion and be like,\nthat's life, that's life, that's life. - Eliezer, you're an incredible human.\nIt's a huge honor. I was trying to talk to you for a long time\n(laughing) because I'm a big fan. I think you're a really important voice and really important mind. Thank you for the fight you're fighting.\nThank you for being fearless and bold and for everything you do. I hope we get a chance to talk again. And I hope you never give up.\nThank you for talking today. - You're welcome. I do worry that we didn't really address a whole lot of fundamental questions I expect people have, but you know,\nmaybe we got a little bit further and made a tiny little bit of progress and I'd say be satisfied with that.\nBut actually no, I think one should only be satisfied with solving the entire problem. - To be continued.\nThanks for listening to this conversation with Eliezer Yudkowsky. To support this podcast, please check out our sponsors\nin the description. And now let me leave you some words from Elon Musk.\n\"With artificial intelligence, we're summoning the demon.\"\nThank you for listening and hope to see you next time.\n"}
