{"pod": "Lex Fridman Podcast", "input": "Introduction", "output": "- A lot of people have said for many years that there will come a time when we want to pause a little bit.\nThat time is now. - The following is a conversation with Max Tegmark,\nhis third time in the podcast. In fact, his first appearance was episode number one\nof this very podcast. He is a physicist and artificial intelligence researcher at MIT,\nco-founder of Future of Life Institute, and Author of \"Life 3.0: Being Human in the Age of Artificial Intelligence.\"\nMost recently, he's a key figure in spearheading the open letter calling for a six-month pause on giant AI experiments\nlike training GPT-4. The letter reads, \"We're calling for a pause on training\nof models larger than GPT-4 for six months. This does not imply a pause or ban on all AI research\nand development or the use of systems that have already been placed in the market. Our call is specific and addresses\na very small pool of actors who possesses this capability.\" The letter has been signed by over 50,000 individuals,\nincluding 1800 CEOs and over 1500 professors. Signatories include Yoshua Bengio,\nStuart Russell, Elon Musk, Steve Wozniak, Yuval Noah Harari, Andrew Yang, and many others.\nThis is a defining moment in the history of human civilization, where the balance of power\nbetween human and AI begins to shift, and Max's mind and his voice\nis one of the most valuable and powerful in a time like this. His support, his wisdom, his friendship,\nhas been a gift I'm forever deeply grateful for. This is the Lex Fridman podcast.\nTo support it, please check out our sponsors in the description. And now, dear friends, here's Max Tegmark.\n"}
{"pod": "Lex Fridman Podcast", "input": "Intelligent alien civilizations", "output": "You were the first ever guest on this podcast, episode number one. So first of all,\nMax, I just have to say, thank you for giving me a chance. Thank you for starting this journey, and it's been an incredible journey,\njust thank you for sitting down with me and just acting like I'm somebody who matters,\nthat I'm somebody who's interesting to talk to. And thank you for doing it. That meant a lot.\n- And thanks to you for putting your heart and soul into this. I know when you delve into controversial topics,\nit's inevitable to get hit by what Hamlet talks about \"The slings and arrows,\" and stuff.\nAnd I really admire this. It's in an era, you know, where YouTube videos are too long,\nand now it has to be like a 20-minute TikTok, 20-second TikTok clip. It's just so refreshing to see you\ngoing exactly against all of the advice and doing these really long form things, and the people appreciate it, you know.\nReality is nuanced, and thanks for sharing it that way.\n- So let me ask you again, the first question I've ever asked on this podcast, episode number one, talking to you.\nDo you think there's intelligent life out there in the universe? Let's revisit that question.\nDo you have any updates? What's your view when you look out to the stars?\n- So, when we look out to the stars, if you define our universe the way most astrophysicists do,\nnot as all of space, but the spherical region of space that we can see with our telescopes from which light has the time to reach us,\nsince our Big Bang. I'm in the minority. I estimate that we are the only life\nin this spherical volume that has invented internet, the radio, has gotten to our level of tech.\nAnd if that's true, then it puts a lot of responsibility on us\nto not mess this one up. Because if it's true, it means that life is quite rare.\nAnd we are stewards of this one spark of advanced consciousness, which if we nurture it and help it grow,\neventually life can spread from here, out into much of our universe, and we can have this just amazing future. Whereas, if we instead are reckless\nwith the technology we build and just snuff it out due to stupidity or in-fighting, then,\nmaybe the rest of cosmic history in our universe is just gonna be playing for empty benches.\nBut I do think that we are actually very likely to get visited by aliens,\nalien intelligence quite soon. But I think we are gonna be building that alien intelligence.\n- So we're going to give birth to an intelligent alien civilization,\nunlike anything that human, the evolution here on earth was able to create\nin terms of the path, the biological path it took. - Yeah, and it's gonna be much more alien than a cat,\nor even the most exotic animal on the planet right now, because it will not have been created\nthrough the usual Darwinian competition where it necessarily cares about self-preservation,\nthat is afraid of death, any of those things. The space of alien minds that you can build\nis just so much faster than what evolution will give you. And with that also comes a great responsibility,\nfor us to make sure that the kind of minds we create are the kind of minds that it's good to create.\nMinds that will share our values and be good for humanity and life.\nAnd also don't create minds that don't suffer. - Do you try to visualize the full space\nof alien minds that AI could be? Do you try to consider all the different kinds of intelligences,\ninstead of generalizing what humans are able to do to the full spectrum of what intelligent creatures,\nentities could do? - I try, but I would say I fail, I mean, it's very difficult for human mind\nto really grapple with something so completely alien.\nEven for us, right? If we just try to imagine how would it feel if we were completely indifferent\ntowards death or individuality?\nEven if you just imagine that for example, you could just copy my knowledge of how to speak Swedish,\n(fingers snapping) boom, now you can speak Swedish, and you could copy any of my cool experiences,\nand then you could delete the ones you didn't like in your own life, just like that. it would already change quite a lot\nabout how you feel as a human being, right? You probably spend less effort studying things\nif you just copy them, and you might be less afraid of death, because if the plane you're on starts to crash,\nyou'd just be like, \"Oh shucks, I haven't backed my brain up for four hours,\n(Lex laughs) so I'm gonna lose this, all this wonderful experiences of this flight.\"\nWe might also start feeling more, like compassionate maybe with other people\nif we can so readily share each other's experiences and our knowledge, and feel more like a hivemind.\nIt's very hard though. I really feel very humble about this\nto grapple with it, how it might actually feel. The one thing which is so obvious though,\nwhich, I think is just really worth reflecting on, is because the mind space of possible intelligences\nis so different from ours, it's very dangerous if we assume they're gonna be like us, or anything like us.\n- Well there's, the entirety of human written history\nhas been through poetry, through novels, been trying to describe through philosophy,\ntrying to describe the human condition and what's entailed in it. Like, just like you said, fear of death and all those kinds of things,\nwhat is love, and all of that changes. - [Max] Yeah. - If you have a different kind of intelligence.\nLike all of it, the entirety of all those poems, they're trying to sneak up to what the hell it means to be human.\nAll of that changes. How AI concerns and existential crises that AI experiences,\nhow that clashes with the human existential crisis, the human condition. - [Max] Yeah.\n- That's hard to fathom, hard to predict. - It's hard, but it's fascinating to think about also.\nEven in the best case scenario, where we don't lose control over the ever more powerful AI\nthat we're building to other humans whose goals we think are horrible, and where we don't lose control to the machines,\nand AI provides the things we want. Even then, you get into the questions\nyou touched here, you know, maybe it's the struggle that it's actually hard to do things\nis part of the things that gives us meaning as well, right? So for example, I found it so shocking that\nthis new Microsoft GPT-4 commercial that they put together, has this woman talking about,\nshowing this demo how she's gonna give a graduation speech to her beloved daughter.\nAnd she asks GPT-4 to write it. It was frigging 200 words or so.\nIf I realized that my parents couldn't be bothered struggling a little bit to write 200 words,\nand outsource that to their computer, I would feel really offended, actually.\nAnd so I wonder if eliminating too much of the struggle from our existence,\ndo you think that would also take away a little bit of what- - it means to be human? Yeah.\n- [Max] Yeah. - We can't even predict. I had somebody mentioned to me that they use,\nthey started using ChatGPT with the 3.5 and now 4.0,\nto write what they really feel to a person, and they have a temper issue,\nand they're basically trying to get ChatGPT to rewrite it in a nicer way.\nTo get the point across, but rewrite it in a nicer way. So we're even removing the inner asshole\nfrom our communication. So I don't, you know, there's some positive aspects of that,\nbut mostly it's just the transformation of how humans communicate. And it's scary because\nso much of our society is based on this glue of communication.\nAnd if we're now using AI as the medium of communication that does the language for us,\nso much of the emotion that's laden in human communication, and so much of the intent,\nthat's going to be handled by, outsourced to AI, how does that change everything? How does that change the internal state\nof how we feel about other human beings? What makes us lonely? What makes us excited?\nWhat makes us afraid? How we fall in love? All that kind of stuff. - Yeah. For me personally, I have to confess,\nthe challenge is one of the things that really makes my life feel\nmeaningful, you know? If I go hiking mountain with my wife, Meia,\nI don't wanna just press a button and be at the top, I want to struggle and come up there sweaty, and feel, \"Wow, we did this,\"\nin the same way. I want to constantly work on myself\nto become a better person. If I say something in anger that I regret, I want to go back\nand really work on myself rather than just tell an AI,\nfrom now on, always filter what I write so I don't have to work on myself, 'cause then I'm not growing.\n- Yeah, but then again, it could be like with chess, and AI, once it significantly,\nobviously supersedes the performance of humans, it will live in its own world, and provide maybe a flourishing civilizations for humans.\nBut we humans will continue hiking mountains, and playing our games, even though AI is so much smarter,\nso much stronger, so much superior in every single way, just like with chess. - [Max] Yeah. - So that,\nI mean, that's one possible hopeful trajectory here, is that humans will continue to human,\nand AI will just be a kind of,\na medium that enables the human experience to flourish.\n- Yeah, I would phrase that as rebranding ourselves\nfrom Homo sapiens to Homo sentiens. You know, right now, if it's sapiens, the ability to be intelligent,\nwe've even put it in our species' name. So we're branding ourselves as the smartest\ninformation processing entity on the planet. That's clearly gonna change if AI continues ahead.\nSo maybe we should focus on the experience instead, the subjective experience that we have,\nHomo sentiens, and say that's what's really valuable, the love, the connection, the other things,\nand get off our high horses, and get rid of this hubris that, you know, only we can do integrals.\n- So consciousness, the subjective experience is a fundamental value to what it means to be human.\nMake that the priority. - That feels like a hopeful direction to me. But that also requires more compassion,\nnot just towards other humans, because they happen to be the smartest on the planet, but also towards all our other fellow creatures\non this planet. I personally feel right now, we're treating a lot of farm animals horribly, for example.\nAnd the excuse we're using is, \"Oh, they're not as smart as us.\" But if we admit that we're not that smart\nin the grand scheme of things either, in the post-AI epoch, you know, then surely, we should value\nthe subjective experience of a cow also. - Well, allow me to briefly look at the book,\n"}
{"pod": "Lex Fridman Podcast", "input": "Life 3.0 and superintelligent AI", "output": "which at this point is becoming more and more visionary that you've written, I guess over five years ago, \"Life 3.0.\"\nSo first of all, 3.0, what's 1.0, what's 2.0, What's 3.0? and how's that vision sort of evolve,\nthe vision in the book evolve to today. - Life 1.0 is really dumb like bacteria,\nand that it can't actually learn anything at all during their lifetime. The learning just comes from this genetic process\nfrom one generation to the next. Life 2.0 is us and other animals which have brains\nwhich can learn during their lifetime a great deal. Right so,\nand you know, you were born without being able to speak English,\nand at some point you decided, \"Hey, I wanna upgrade my software, and so let's install an English-speaking module.\"\nSo you did. And Life 3.0, which does not exist yet,\ncannot replace not only its software the way we can, but also it's hardware.\nAnd that's where we're heading towards at high speed. We're already maybe 2.1 because we can,\nyou know, put in an artificial knee,\npacemaker, et cetera, et cetera. And if Neuralink and other companies succeed,\nit will be life 2.2, et cetera. But the companies trying to build AGI,\nor trying to make is of course, full 3.0, and you can put that intelligence in something that also has no,\nbiological basis whatsoever. - So less constraints and more capabilities, just like the leap from 1.0 to 2.0.\nThere is nevertheless, you speaking so harshly about bacteria, so disrespectfully about bacteria,\nthere is still the same kind of magic there that permeates life 2.0 and 3.0.\nIt seems like maybe the thing that's truly powerful about life, intelligence, and consciousness,\nwas already there in 1.0. Is it possible?\n- I think we should be humble and not be so quick to make everything binary\nand say either it's there or it's not. Clearly there's a great spectrum and there is even controversy about\nwhether some unicellular organisms like amoebas can maybe learn a little bit, you know, after all.\nSo apologies if I offended any bacteria here. (laughs) It wasn't my intent. It was more that I wanted to talk up\nhow cool it is to actually have a brain. - [Lex] Yeah. - Where you can learn dramatically within your lifetime.\n- [Lex] Typical human. - And the higher up you get from 1.0 2.0 to 3.0, the more you become the captain of your own ship,\nthe master of your own destiny. And the less you become a slave to whatever evolution gave you, right?\nBy upgrading your software, we can be so different from previous generations and even from our parents,\nmuch more so than even a bacterium, you know, no offense to them. And if you can also swap out your hardware\nand take any physical form you want, of course, it's really, the sky's the limit. - Yeah, so the,\nit accelerates the rate at which you can perform the computation that determines your destiny.\n- Yeah, and I think it's worth commenting a bit on what \"you\" means in this context. Also, if you swap things out a lot, right?\nThis is controversial, but my, current understanding is that,\nyou know, life is best thought of not as a bag of meat,\nor even a bag of elementary particles, but rather as a system which can process information\nand retain its own complexity, even though nature is always trying to mess it up, so, it's all about information processing.\nAnd that makes it a lot like something like a wave in the ocean, which is not,\nit's water molecules, right? The water molecules bob up and down, but the wave moves forward,\nit's an information pattern in the same way you, Lex, you're not the same atoms\nas during the first, - Time we talked, yeah. - Interview you did with me, you've swapped out most of them, but it's still you.\nAnd the information pattern is still there, and if you could swap out your arms,\nand like whatever, you can still have this kind of continuity,\nit becomes much more sophisticated sort of way forward in time where the information lives on.\nI lost both of my parents since our last podcast, and it actually gives me a lot of solace\nthat this way of thinking about them, they haven't entirely died because a lot of mommy and daddy's,\nsorry, I'm getting a little emotional here, but a lot of their values, and ideas, and even jokes and so on,\nthey haven't gone away, right? Some of them live on, I can carry on some of them, and they also live on a in a lot of other people.\nSo in this sense, even with life 2.0, we can to some extent,\nalready transcend our physical bodies and our death.\nAnd particularly if you can share your own information, your own ideas with many others\nlike you do in your podcast, then you know,\nthat's the closest immortality we can get with our bio bodies. - You carry a little bit of them in you in some sense.\n- [Max] Yeah, yeah. - Do you miss them? Do you miss your mom and dad? - Of course, of course.\n- What did you learn about life from them? If we can take a bit of a tangent.\n- Oh, so many things. For starters, my fascination for math\nand the physical mysteries of our universe, I got a lot of that from my dad.\nBut I think my obsession for really big questions, and consciousness, and so on,\nthat actually came mostly from my mom and what I got from both of them,\nwhich is very core part of really who I am, I think is this,\njust feeling comfortable with,\nnot buying into what everybody else is saying, just doing what I think is right.\nThey both very much just, you know, did their own thing, and sometimes they got flak for it\nand they did it anyway. - That's why you've always been in an inspiration to me.\nThat you're at the top of your field and you're still willing\nto tackle the big questions in your own way. You're one of one of the people that represents MIT best to me,\nyou've always been an inspiration in that. So it's good to hear that you got that from your mom and dad. - Yeah, you're too kind. But yeah, I mean,\nthe good reason to do science is because you're really curious, and you wanna figure out the truth.\nIf you think, this is how it is and everyone else says, \"No, no, that's bullshit, and it's that way,\"\nyou know, You stick with what you think is true,\nand even if everybody else keeps thinking it's bullshit, there's a certain,\nI always root for the underdog, (Lex laughs) when I watch movies. And my dad once,\none time for example, when I wrote one of my craziest papers ever, talking about our universe ultimately being mathematical,\nwhich we're not gonna get into today, I got this email from a quite famous professor saying, \"This is not only all bullshit,\nbut it's gonna ruin your career. You should stop doing this kind of stuff.\" I sent it to my dad.\nDo you know what he said? - [Lex] (laughs) What he say? - He replied with a quote from Dante. (Lex laughing)\n(Max speaking in Italian) \"Follow your own path and let the people talk.\"\n(Both laughing) Go dad! - [Lex] Yeah. - This is the kind of thing, you know, he's dead, but that attitude is not.\n- How did losing them as a man, as a human being change you? How did it expand your thinking about the world?\nHow did it expand your thinking about, you know, this thing we're talking about, which is humans creating another living,\nsentient perhaps, being? - I think it,\nmainly do two things. One of them just going through all their stuff\nafter they had passed away and so on, just drove home to me how important it is to ask ourselves,\nwhy are we doing this things we do? Because it's inevitable that you look at some things they spent an enormous time on\nand you ask in hindsight, would they really have spent so much time on this? Would they have done something\nthat was more meaningful? So I've been looking more in my life now and asking,\nyou know, why am I doing what I'm doing? And I feel,\nit should either be something I really enjoy doing, or it should be something that I find really, really meaningful because it helps humanity,\nand if it's in none of those two categories,\nmaybe I should spend less time on it, you know. The other thing is, dealing with death up in person like this,\nit's actually made me less afraid of,\neven less afraid of other people telling me that I'm an idiot, you know, which happens regularly,\nand just live my life, do my thing, you know?\nAnd it's made it a little bit easier for me to focus on what I feel is really important.\n- What about fear of your own death? Has it made it more real that this is something that happens?\n- Yeah, it's made it extremely real, and you know, I'm next in line in our family now, right? It's me and my younger brother, but,\nthey both handled it with such dignity, that was a true inspiration also.\nThey never complained about things, and you know, when you're old and your body starts falling apart,\nit's more and more to complain about, they looked at what could they still do that was meaningful, and they focused on that\nrather than wasting time talking about, or even thinking much about\nthings they were disappointed in. I think anyone can make themselves depressed if they start their morning by making a list of grievances.\nWhereas if you start your day when the little meditation and just the things you're grateful for,\nyou basically choose to be a happy person. - Because you only have a finite number of days,\nwe should spend them, - [Max] Make it count. - Being grateful. - [Max] Yeah.\n"}
{"pod": "Lex Fridman Podcast", "input": "Open letter to pause Giant AI Experiments", "output": "- Well you do happen to be working on a thing which seems to have potentially,\nsome of the greatest impact on human civilization of anything humans have ever created, which is artificial intelligence.\nThis is, on the both detailed technical level, and on the high philosophical level you work on.\nSo you've mentioned to me that there's an open letter that you're working on.\n- It's actually going live in a few hours. (Lex laughing) So I've been having late nights and early mornings.\nIt's been very exciting, actually. In short, have you seen, \"Don't Look Up,\"\nthe film? - Yes, yes. - I don't want to be the movie spoiler for anyone watching this who hasn't seen it.\nBut if you're watching this, you haven't seen it, watch it, because we are actually acting out,\nit's life imitating art. Humanity is doing exactly that right now, except it's an asteroid that we are building ourselves.\nAlmost nobody is talking about it. People are squabbling across the planet about all sorts of things,\nwhich seem very minor compared to the asteroid that's about to hit us, right? Most politicians don't even this on the radar,\nthey think maybe in 100 years or whatever. Right now we're at a fork on the road.\nThis is the most important fork that humanity has reached in it's over 100,000 years on this planet.\nWe're building effectively a new species that's smarter than us,\nit doesn't look so much like a species yet 'cause it's mostly not embodied in robots. But that's the technicality which will soon be changed.\nAnd this arrival of of artificial general intelligence\nthat can do all our jobs as well as us, and probably shortly thereafter, superintelligence,\nwhich greatly exceeds our cognitive abilities. It's gonna either be the best thing ever\nto happen to humanity or the worst. I'm really quite confident that there is not that much middle ground there.\n- But it would be fundamentally transformative to human civilization. - Of course, utterly and totally.\nAgain, we'd branded ourselves as Homo sapiens 'cause it seemed like the basic thing, we're the king of the castle on this planet,\nwe're the smart ones, we can control everything else, this could very easily change.\nWe're certainly not gonna be the smartest on the planet for very long if AI,\nunless AI progress just halts, and we can talk more about why I think that's true 'cause it's controversial.\nAnd then we can also talk about reasons we might think it's gonna be the best thing ever,\nand the reason we think it's going to be the end of humanity, which is of course, super controversial.\nBut what I think we can, anyone who's working on advanced AI\ncan agree on is, it's much like the film \"Don't Look Up,\" in that it's just really comical\nhow little serious public debate there is about it, given how huge it is.\n- So what we're talking about is a development, of currently, things like GPT-4,\nand the signs it's showing of rapid improvement that may, in the near term lead to development\nof superintelligent AGI, AI, general AI systems, and what kind of impact that has on society.\n- [Max] Exactly. - When that thing achieves general human-level intelligence, and then beyond that,\ngeneral superhuman level intelligence. There's a lot of questions to explore here.\nSo one, you mentioned halt. Is that the content of the letter?\nis to suggest that maybe we should pause the development of these systems. - Exactly, so this is very controversial,\nfrom when we talked the first time, we talked about how I was involved in starting the Future of Life Institute,\nand we worked very hard on 2014, 2015, was the mainstream AI safety.\nThe idea that there even could be risks and that you could do things about them. Before then, a lot of people thought\nit was just really kooky to even talk about it. And a lot of AI researchers felt,\nworried that this was too flaky, and could be bad for funding, and that the people had talked about it were just not,\ndidn't understand AI. I'm very, very happy with how that's gone,\nand that now, you know, it's completely mainstream, you go in any AI conference, and people talk about AI safety,\nand it's a nerdy technical field full of equations and blah-blah. - [Lex] Yes.\n- As it should be, but there is this other thing, which has been quite taboo up until now,\ncalling for slowdown. So what, we've constantly been saying, including myself,\nI've been biting my tongue a lot, you know, is that, we don't need to slow down AI development.\nWe just need to win this race, the wisdom race between the growing power of the AI\nand the growing wisdom with which we manage it. And rather than trying to slow down AI,\nlet's just try to accelerate the wisdom, do all this technical work to figure out how you can actually ensure that your powerful AI\nis gonna do what you want it to do. And have society adapt also with incentives and regulations\nso that these things get put to good use. Sadly, that didn't pan out.\nThe progress on technical AI capabilities has gone a lot faster than many people thought\nback when we started this in 2014. Turned out to be easier to build really advanced AI than we thought.\nAnd on the other side, it's gone much slower than we hoped with getting policymakers and others\nto actually put incentives in place to make,\nsteer this in the good directions, maybe we should unpack it and talk a little bit about each, so. - [Lex] Yeah. - Why did it go faster than a lot of people thought?\nIn hindsight, it's exactly like building flying machines.\nPeople spent a lot of time wondering about how do birds fly, you know. And that turned out to be really hard.\nHave you seen the TED Talk with a flying bird? - Like a flying robotic bird? - Yeah, it flies around the audience,\nbut it took 100 years longer to figure out how to do that than for the Wright brothers to build the first airplane\nbecause it turned out there was a much easier way to fly. And evolution picked a more complicated one\nbecause it had its hands tied. It could only build a machine that could assemble itself,\nwhich the Wright brothers didn't care about that, they could only build a machine that use only the most common atoms in the periodic table,\nWright Brothers didn't care about that, they could use steel, iron atoms, and it had to be built to repair itself,\nand it also had to be incredibly fuel efficient, you know, a lot of birds use less than half the fuel\nof a remote-controlled plane flying the same distance, For humans, just throw a little more money,\nput a little more fuel in it, and there you go, 100 years earlier. That's exactly what's happening now with these large language models.\nThe brain is incredibly complicated. Many people made the mistake, you're thinking we have to figure out how the brain does\nhuman-level AI first before we could build in the machine, that was completely wrong. You can take an incredibly simple\ncomputational system called a transformer network and just train it to do something incredibly dumb. Just read a gigantic amount of text\nand try to predict the next word. And it turns out, if you just throw a ton of compute at that\nand a ton of data, it gets to be frighteningly good like GPT-4,\nwhich I've been playing with so much since it came out, right? And there's still some debate\nabout whether that can get you all the way to full human level or not, but yeah, we can come back to the details of that\nand how you might get the human-level AI even if large language models don't.\n- Can you briefly, if it's just a small tangent, comment on your feelings about GPT-4? So just that you're impressed by this rate of progress,\nbut where is it? Can GPT-4 reason?\nWhat are like the intuitions? What are human interpretable words you can assign to the capabilities of GPT-4\nthat makes you so damn impressed with it? - I'm both very excited about it and terrified.\nIt's interesting mixture of emotions. (laughs) - All the best things in life include those two somehow.\n- Yeah, it can absolutely reason, anyone who hasn't played with it, I highly recommend doing that before dissing it.\nIt can do quite remarkable reasoning. I've had to do a lot of things,\nwhich I realized I couldn't do that myself that well even, and it obviously does it\ndramatically faster than we do too, when you watch it type, and it's doing that well, servicing a massive number of other humans at the same time.\nThe same time, it cannot reason as well as a human can on some tasks,\nit's obviously the limitations from its architecture. You know, we have in our heads, what in geek-speak is called a recurrent neural network.\nThere are loops, information can go from this neuron, to this neuron, to this neuron, and then back to this one, you can like ruminate on something for a while,\nyou can self-reflect a lot. These large language models,\nthey cannot, like GPT-4. It's a so-called transformer where it's just like a one-way street\nof information, basically. In geek-speak, it's called a feed-forward neural network.\nAnd it's only so deep, so it can only do logic that's that many steps and that deep, and it's not,\nso you can create problems which it will fail to solve, you know, for that reason.\nBut the fact that it can do so amazing things with this incredibly simple architecture already,\nis quite stunning, and what we see in my lab at MIT when we look inside large language models\nto try to figure out how they're doing it, which, that's the key core focus of our research, it's called mechanistic interpretability in geek-speak.\nYou know, you have this machine that does something smart, you try to reverse engineer it, and see how does it do it.\nI think of it also as artificial neuroscience, (Lex laughs) 'Cause that's exactly - I love it. - what neuroscientists do with actual brains. But here you have the advantage that you can,\nyou don't have to worry about measurement errors. You can see what every neuron is doing all the time,\nand a recurrent thing we see again and again, there's been a number of beautiful papers\nquite recently by a lot of researchers, and some of 'em are here even in this area, is where when they figure out how something is done,\nyou can say, \"Oh man, that's such a dumb way of doing it.\" And you read immediately see how it can be improved. Like for example,\nthere was this beautiful paper recently where they figured out how a large language model stores certain facts,\nlike Eiffel Tower is in Paris, and they figured out exactly how it's stored and the proof of that they understood it\nwas they could edit it. They changed some synapses in it, and then they asked it, Where's the Eiffel Tower?\"\nAnd it said, \"It's in Rome.\" And then they asked, \"How do you get there? Oh, how do you get there from Germany?\"\n\"Oh, you take this train, the Roma Termini train station, and this and that,\"\n\"And what might you see if you're in front of it?\" \"Oh, you might see the Colosseum.\"\nSo they had edited, - So they literally moved it to Rome. - But the way that it's storing this information,\nit's incredibly dumb, if any fellow nerds listening to this,\nthere was a big matrix, and roughly speaking, there are certain row and column vectors\nwhich encode these things, and they correspond very hand-wavingly to principle components and it would be much more efficient for as far as matrix,\njust store in the database, you know and, and everything so far,\nwe've figured out how these things do are ways where you can see it can easily be improved. And the fact that this particular architecture\nhas some roadblocks built into it is in no way gonna prevent crafty researchers\nfrom quickly finding workarounds and making other kinds of architectures\nsort of go all the way, so. In short, it's turned out to be a lot easier\nto build close to human intelligence than we thought, and that means our runway as a species to\nget our shit together has has shortened. - And it seems like the scary thing\nabout the effectiveness of large language models, so Sam Altman, I've recently had conversation with,\nand he really showed that the leap from GPT-3 to GPT-4\nhas to do with just a bunch of hacks, a bunch of little explorations with smart researchers\ndoing a few little fixes here and there. It's not some fundamental leap and transformation in the architecture.\n- And more data and more compute. - And more data and compute, but he said the big leaps has to do\nwith not the data and the compute, but just learning this new discipline, just like you said.\nSo researchers are going to look at these architectures and there might be big leaps where you realize,\n\"Wait, why are we doing this in this dumb way?\" And all of a sudden this model is 10x smarter. And that that can happen on any one day,\non any one Tuesday or Wednesday afternoon. And then all of a sudden you have a system that's 10x smarter.\nIt seems like it's such a new discipline, it's such a new, like we understand so little about why this thing works so damn well,\nthat the linear improvement of compute, or exponential, but the steady improvement of compute,\nsteady improvement of the data may not be the thing that even leads to the next leap. It could be a surprise little hack that improves everything.\n- Or a lot of little leaps here and there because so much of this is out in the open also,\nso many smart people are looking at this and trying to figure out little leaps here and there, and it becomes this sort of collective race where,\na lot of people feel, \"If I don't take the leap someone else will,\" and it is actually very crucial for the other part of it,\nwhy do we wanna slow this down? So again, what this open letter is calling for is just pausing all training\nof systems that are more powerful than GPT-4 for six months.\nJust give a chance for the labs to coordinate a bit on safety,\nand for society to adapt, give the right incentives to the labs. 'cause I, you know,\nyou've interviewed a lot of these people who lead these labs and you know just as well as I do\nthat they're good people, they're idealistic people. They're doing this first and foremost because they believe that AI\nhas a huge potential to help humanity. But at the same time they are trapped\nin this horrible race to the bottom.\nHave you read \"Meditations on Moloch\" by Scott Alexander? - [Lex] Yes.\n- Yeah, it's a beautiful essay on this poem by Ginsberg where he interprets it as being about this monster.\nIt's this game theory monster that pits people against each other in this race to the bottom\nwhere everybody ultimately loses. And the evil thing about this monster is even though everybody sees it and understands,\nthey still can't get out of the race, right? A good fraction of all the bad things that we humans do\nare caused by Moloch. And I like Scott Alexander's naming of the monster.\nSo we can, we humans can think of it as a thing.\nIf you look at why do we have overfishing, why do we have more generally, the tragedy of the commons.\nWhy is it that, so Liv Boeree, I don't know if you've had her on your podcast. - Mhm, yeah.\nShe's become a friend, yeah. - Great, she made this awesome point recently that beauty filters that a lot of female\ninfluencers feel pressure to use, are exactly Moloch in action again. First, nobody was using them,\nand people saw them just the way they were, and then some of 'em started using it,\nand becoming ever more plastic fantastic, and then the other ones that weren't using it started to realize that,\nif they wanna to keep their their market share, they have to start using it too.\nAnd then you're in a situation where they're all using it, and none of them has any more market share\nor less than before. So nobody gained anything, everybody lost,\nand they have to keep becoming ever more plastic fantastic also, right?\nBut nobody can go back to the old way because it's just too costly, right?\nMoloch is everywhere, and Moloch is not a new arrival on the scene either.\nWe humans have developed a lot of collaboration mechanisms to help us fight back against Moloch\nthrough various kinds of constructive collaboration. The Soviet Union and the United States\ndid sign a number of arms control treaties against Moloch who is trying to stoke them\ninto unnecessarily risky nuclear arms races, et cetera, et cetera. And this is exactly what's happening on the AI front.\nThis time it's a little bit geopolitics, but it's mostly money, where there's just so much commercial pressure.\nYou know, if you take any of these leaders of the top tech companies,\nif they just say, you know, \"This is too risky, I want to pause for six months.\"\nThey're gonna get a lot of pressure from shareholders and others. They're like, \"Well you know, if you pause,\nbut those guys don't pause. We don't wanna get our lunch eaten.\"\n- [Lex] Yeah. - And shareholders even have the power to replace the executives in the worst case, right?\nSo we did this open letter because we want to help these idealistic tech executives to do\nwhat their heart tells them, by providing enough public pressure on the whole sector.\nJust pause, so that they can all pause in a coordinated fashion. And I think without the public pressure,\nnone of them can do it alone. Push back against their shareholders\nno matter how goodhearted they are, 'cause Moloch is a really powerful foe.\n- So the idea is to, for the major developers of AI systems like this,\nso we're talking about Microsoft, Google, Meta, and anyone else.\n- Well OpenAI is very close with Microsoft now, - With Microsoft, right, yeah. - of course, - And there there are plenty of smaller players.\nfor example, Anthropic is is very impressive, there's Conjecture, there's many, many, many players,\nI don't wanna make a long list that sort of leave anyone out. And for that reason,\nit's so important that some coordination happens, that there's external pressure on all of them,\nsaying, \"You all need the pause.\" 'Cause then, the people, the researchers in there at these organizations,\nthe leaders who wanna slow down a little bit, they can say to their shareholders, you know,\n\"Everybody's slowing down because of this pressure and it's the right thing to do.\" - Have you seen in history,\nthere examples what it's possible to pause the Moloch? - Yes, absolutely.\nAnd even like human cloning for example, you could make so much money on human cloning.\nWhy aren't we doing it? Because biologists thought hard about this\nand felt like this is way too risky, they got together in the seventies in Asilomar,\nand decided even to stop a lot more stuff, also just editing the human germline, right?\nGene editing that goes in to our offspring,\nand decided, \"Let's not do this because it's too unpredictable what it's gonna lead to,\"\nwe could lose control over what happens to our species,\" so they paused.\nThere was a ton of money to be made there, So it's very doable, but you need a public awareness of what the risks are,\nand the broader community coming in and saying, \"Hey, let's slow down.\" And you know, another common pushback I get today,\nis that we can't stop in the West because China.\nAnd in China undoubtedly, they also get told, \"We can't slow down because the West,\" because both sides think they're the good guy.\n- [Lex] Yeah. - But look at human cloning, you know? Did China forge ahead with human cloning?\nThere's been exactly one human cloning that's actually been done that I know of. It was done by a Chinese guy.\nDo you know where he is now? - [Lex] Where? - In jail. And you know who put him there?\n- [Lex] Who? - Chinese government. Not because Westerners said, \"China look, this is...\"\nNo the Chinese government put him there 'cause they also felt, they like control, the Chinese government.\nIf anything, maybe they're even more concerned about having control than Western governments,\nhave no incentive of just losing control over where everything is going, and you can also see the Ernie Bot\nthat was released by, I believe, Baidu recently, they got a lot of pushback from the government\nand had to rein it in, you know, in a big way. I think once this basic message comes out\nthat this isn't an arms race, it's a suicide race, where everybody loses\nif anybody's AI goes out of control, it really changes the whole dynamic. It's not,\nand I'll say this again 'cause this is this very basic point I think a lot of people get wrong. Because a lot of people dismiss the whole idea\nthat AI can really get very superhuman because they think there's something really magical about intelligence\nsuch that it can only exist in human minds, you know, because they believe that, they think it's gonna kind of get to just more or less\n\"GPT-4 plus plus,\" and then that's it. They don't see it as a suicide race.\nThey think whoever gets that first, they're gonna control the world, they're gonna win. That's not how it's gonna be.\nAnd we can talk again about the scientific arguments from why it's not gonna stop there.\nBut the way it's gonna be, is if anybody completely loses control and you know, you don't care\nif someone manages to take over the world who really doesn't share your goals, you probably don't really even care very much\nabout what nationality they have, you're not gonna like it much worse than today.\nIf you live in Orwellian dystopia, what do you care who's created it, right?\nAnd if someone, if it goes farther, and we just lose control even to the machines,\nso that it's not us versus them, it's us versus it. What do you care who created this unaligned entity\nwhich has goals different from humans, ultimately? And we get marginalized, we get made obsolete,\nwe get replaced. That's what I mean when I say it's a suicide race,\nit's kind of like we're rushing towards this cliff, but the closer the cliff we get, the more scenic the views are,\nand the more money there is there, and the more, so we keep going, but we have to also stop at some point, right?\nQuit while we're ahead, And it's,\nit's a suicide race which cannot be won, but the way to really benefit from it is,\nto continue developing awesome AI a little bit slower. So we make it safe,\nmake sure it does the things that humans want, and create a condition where everybody wins. The technology has shown us that,\nyou know, geopolitics and politics in general is not a zero sum game at all.\n"}
{"pod": "Lex Fridman Podcast", "input": "Maintaining control", "output": "- So there is some rate of development that will lead us as a human species to lose control of this thing.\nAnd the hope you have is that there's some lower level of development\nwhich will not allow us to lose control. This is an interesting thought you have about losing control, so if you have somebody,\nif you are somebody like Sundar Pichai or Sam Altman at the head of a company like this,\nyou're saying if they develop an AGI, they too will lose control of it.\nSo no one person can maintain control, no group of individuals can maintain control. - If it's created very, very soon\nand is a big black box that we don't understand like the large language models, yeah. Then I'm very confident they're gonna lose control.\nBut this isn't just me saying it, you know, Sam Altman and Demis Hassabis have both said,\nthey themselves acknowledge that, you know, there's really great risks for this and they want slow down once they feel it gets scary.\nBut it's clear that they're stuck in this, again, Moloch is forcing them to go a little faster\nthan they're comfortable with because of pressure from, just commercial pressures, right?\nTo get a bit optimistic here, of course, this is a problem that can be ultimately solved.\nTo win this wisdom race, it's clear that what we hope that was gonna happen hasn't happened.\nThe capability progress has gone faster than a lot of people thought, and the progress in the public sphere\nof policy making and so on, has gone slower than we thought. Even the technical AI safety has gone slower.\nA lot of the technical safety research was kind of banking on that large language models\nand other poorly understood systems couldn't get us all the way. That you had to build more of a kind of intelligence that you could understand.\nMaybe it could prove itself safe, you know, things like this, and I'm quite confident that this can be done\nso we can reap all the benefits, but we cannot do it as quickly as this out of control express train we are on now\nis gonna get to AGI. That's why we need a little more time, I feel. - Is there something to be said,\nwell like Sam Altman talked about, which is while we're in the pre-AGI stage,\nto release often and as transparently as possible to learn a lot.\nSo as opposed to being extremely cautious, release a lot,\ndon't invest in a closed development where you focus on the AI safety. While it's somewhat \"dumb,\"\nquote-unquote, release as often as possible. And as you start to see signs of human-level intelligence\nand or superhuman level intelligence, then you put a halt on it. Well what a lot of safety researchers\nhave been saying for many years is that the most dangerous things you can do with an AI is first of all teach it to write code.\n- [Lex] Yeah. - Because that's the first step towards recursive self-improvement, which can take it from AGI to much higher levels.\nOkay? Oops, we've done that. And another thing high risk\nis connect it to the internet, let it go to websites, download stuff on its own and talk to people.\nOops, we've done that already. You know Eliezer Yudkowsky, you said you interviewed him recently, right? - [Lex] Yes, yep.\n- So he had this tweet recently which said, gave me one of the best laughs in a while, where he is like,\n\"Hey, people used to make fun of me and say, 'You're so stupid, Eliezer.' 'Cause you're saying\nyou have to worry of obviously developers once they get to like really strong AI,\nfirst thing you're gonna do is like, never connect it to the internet, keep it in a box. where you know, you can really study it safe.\"\nSo he had written it in the like in the meme form so it's like \"Then,\" and then that, and then, \"Now.\"\n(Lex laughing) \"LOL, let's make a chatbot.\" (both laughing) - [Lex] Yeah, yeah, yeah.\n- And the third thing is Stuart Russell. - [Lex] Yeah. - You know, amazing AI researcher.\nHe has argued for a while that we should never teach AI anything about humans.\nAbove all, we should never let it learn about human psychology and how you manipulate humans.\nThat's the most dangerous kind of knowledge you can give it. Yeah, you can teach it all it needs to know about how to cure cancer and stuff like that.\nBut don't let it read Daniel Kahneman's book about cognitive biases and all that.\nAnd then oops, \"LOL, you know, let's invent social media\nrecommender algorithms which do exactly that.\" They get so good at knowing us and pressing our buttons\nthat we are starting to create a world now where we just have ever more hatred,\n'cause they've figured out that these algorithms, not for out of evil, but just to make money on advertising,\nthat the best way to get more engagement, the euphemism, get people glued to their little rectangles, right?\nIs just to make them pissed off. - Well that's really interesting that a large AI system that's doing the recommender system\nkind of task on social media, is basically just studying human beings because it's a bunch of us rats giving it signal,\nnonstop signal. It'll show a thing and then we give signal, and whether we spread that thing, we like that thing,\nthat thing increases our engagement, gets us to return to the platform, and it has that on the scale of hundreds of millions of people constantly.\nSo it's just learning, and learning, and learning, and presumably if the number of parameters in the neural network that's doing the learning,\nand more end to end the learning is, the more it's able to just basically encode\nhow to manipulate human behavior. - [Max] Exactly. - How to control humans at scale. - Exactly, and that is not something you think\nis in humanity's interest. And right now it's mainly letting some humans manipulate other humans for profit and power,\nwhich already caused a lot of damage, and then eventually that's a sort of skill\nthat can make AI persuade humans to let them escape\nwhatever safety precautions we had put, you know, there was a really nice article in the New York Times recently by Yuval Noah Harari\nand two co-authors including Tristan Harris from \"The Social Dilemma,\" and we have this phrase in there I love,\nIt said, \"Humanity's first contact with advanced AI\nwas social media.\" And we lost that one. We now live in a country\nwhere there's much more hate in the world where there's much more hate, in fact.\nAnd in our democracy than we're having this conversation, and people can't even agree on who won the last election, you know.\nAnd we humans often point fingers at other humans and say it's their fault, but it's really Moloch in these AI algorithms.\nWe got the algorithms and then Moloch pitted the social media companies against each other\nso nobody could have a less creepy algorithm 'cause then they would lose out on revenue to the other company. - Is there any way to win that battle back\nif we just linger on this one battle that we've lost in terms of social media, is it possible to redesign social media,\nthis very medium in which we use as a civilization to communicate with each other,\nto have these kinds of conversation, to have discourse, to try to figure out how to solve the biggest problems in the world,\nwhether that's nuclear war or the development of AGI. Is is it possible to do social media correctly?\n- I think it's not only possible, but it's necessary. Who are we kidding? That we're gonna be able to solve all these other challenges\nif we can't even have a conversation with each other? It's constructive. The whole idea, the key idea of democracy\nis that you get a bunch of people together and they have a real conversation. The ones you try to foster on this podcast\nwhere you respectfully listen to people you disagree with. And you realize actually, you know, there are some things actually\nsome common ground we have and let's, we both agree, let's not have any nuclear wars, let's not do that, et cetera, et cetera.\nWe're kidding ourselves that thinking we can face off the second contact with ever more powerful AI\nthat's happening now with these large language models if we can't even have a functional conversation in the public space.\nThat's why I started the Improve The News project, improvethenews.org. But I'm an optimist fundamentally,\nin that there is a lot of intrinsic goodness in people.\nAnd that what makes the difference between someone doing good things for humanity\nand bad things is not some sort of fairytale thing, that this person was born with the evil gene\nand this one is born with the good gene. No, I think it's whether we put, whether people find themselves in situations\nthat bring out the best in them or that bring out the worst in them. And I feel we're building an internet\nand a society that brings out the worst.\n- But it doesn't have to be that way. - [Max] No, it does not. - It's possible to create incentives and also create incentives that make money.\nThat both make money and bring out the best in people. - I mean, in the long term, it's not a good investment for anyone, you know,\nto have a nuclear war, for example. And you know, is it a good investment for humanity if we just ultimately replace all humans by machines,\nand then we're so obsolete that eventually, there are no humans left? Well, it depends guess how you do the math,\nBut I would say by any reasonable economic standard, if you look at the future income of humans\nand there aren't any, you know, that's not a good investment. Moreover, like why can't we have\na little bit of pride in our species, damn it? You know, why should we just build another species that gets rid of us?\nIf we were Neanderthals, would we really consider it a smart move\nif we had really advanced biotech to build Homo sapiens? You know, you might say, \"Hey Max, you know,\nyeah, let's build, these Homo sapiens, they're gonna be smarter than us,\nmaybe they can help us, defend us better against predators and help fix up our caves, make them nicer,\nwe'll control 'em undoubtedly, you know?\" So then they build a couple, a little baby girl, little baby boy.\nThey either, and then you have some wise old Neanderthal elder is like,\n\"Hmm, I'm scared that we're opening a Pandora's box here\nand that we're gonna get outsmarted by these\nsuper Neanderthal intelligences, and there won't be any Neanderthals left.\"\nBut then you have a bunch of others in the cave, right? \"You're such a Luddite scaremonger. Of course, they're gonna want to keep us around\n'cause we are their creators, and, you know, the smarter, I think the smarter they get, the nicer they're gonna get,\nthey're gonna leave us. They're gonna want us around and it's gonna be fine,\nand besides look at these babies, they're so cute. Clearly they're totally harmless.\"\nThose babies are exactly GPT-4. It's not, I wanna be clear, it's not GPT-4 that's terrifying.\nIt's that GPT-4 is a baby technology, you know, and Microsoft even had a paper recently out,\ntitled something like, \"Sparkles of AGI.\" Well they were basically saying this is baby AI,\nlike these little Neanderthal babies, and it's gonna grow up. There's gonna be other systems from the same company,\nfrom other companies, they'll be way more powerful, but they're gonna take all the things, ideas from these babies and before we know it,\nwe're gonna be like those last Neanderthals who were pretty disappointed\nwhen they realized that they were getting replaced. - Well, this interesting point you make, which is of programming,\nit's entirely possible that GPT-4 is already the kind of system that can change everything by writing programs.\n- Yeah, it's because it's life 2.0, the systems I'm afraid of are gonna look nothing\nlike a large language model, and they're not, but once it gets, once it or other people figure out a way\nof using this tech to make much better tech, right? It's just constantly replacing its software.\nAnd from everything that we've seen about how these work under the hood, they're like the minimum viable intelligence.\nThey do everything, you know, the dumbest way that still works, sort of. - [Lex] Yeah. - And so they're life 3.0,\nexcept when they replace their software, it's a lot faster than when you decide to learn Swedish.\nPoof. (fingers snapping) And moreover, they think a lot faster than us too. So when, you know,\nwe don't think, have one logical step\nevery nanosecond or few, or so, the way they do, and we can't also just suddenly scale up our hardware\nmassively in the cloud 'cause we're so limited, right? So they are,\nand they are also life, can soon become a little bit more like life 3.0\nin that if they need more hardware, hey, just rent it in the cloud, you know? \"How do you pay for it?\" \"Well, with all the services you provide.\"\n- And what we haven't seen yet, which could change a lot,\nis entire software systems. So right now programming is done sort of in bits and pieces\nas an assistant tool to humans. But I do a lot of programming and with the kind of stuff that GPT-4 is able to do,\nI mean, it's replacing a lot what I'm able to do, right? You still need a human in the loop\nto kind of manage the design of things, manage like, what are the prompts\nthat generate the kind of stuff to do some basic adjustment of the codes, do some debugging,\nbut if it's possible to add on top of GPT-4, kind of a feedback loop\nof self-debugging, improving the code, and then you launch that system onto the wild\non the internet because everything is connected, and have it do things, have it interact with humans and then get that feedback,\nnow you have this giant ecosystem of humans. That's one of the things that\nElon Musk recently sort of tweeted as a case why everyone needs to pay $7 or whatever\nfor Twitter, - [Max] To make sure they're real. - Make sure they're real, we're now going to be living in a world\nwhere the bots are getting smarter, and smarter, and smarter to a degree where,\nyou can't tell the difference between a human and a bot. - [Max] That's right. - And now you can have bots outnumber humans\nby 1 million to one. Which is why he's making a case why you have to pay.\nTo prove you're human, which is one of the only mechanisms to prove, which is depressing. - And yeah,\nI feel we have to remember, as individuals, we should from time to time,\nask ourselves why are we doing what we're doing, right? And as a species, we need to do that too.\nSo if we're building, as you say, machines that are outnumbering us,\nand more and more outsmarting us, and replacing us on the job market, not just for the dangerous and and boring tasks,\nbut also for writing poems and doing art, and things that a lot of people find really meaningful,\nwe gotta ask ourselves, why? Why are we doing this?\nThe answer is Moloch is tricking us into doing it. And it's such a clever trick\nthat even though we see the trick, we still have no choice but to fall for it, right?\nAnd also, thing you said about you using co-pilot AI tools to program faster,\nhow many, what factor faster would you say you code now? Does it go twice as fast? Or,\n- I don't really, because it's such a new tool. - [Max] Yeah. - I don't know if speed is significantly improved,\nbut it feels like I'm a year away from being 5 to 10 times faster.\n- So if that's typical for programmers, then you're already seeing another kind\nof recursive self-improvement, right? Because previously,\nlike a major generation of improvement of the codes would happen on the human R and D time scale.\nAnd now if that's five times shorter, then it's gonna take five times less time than it otherwise would to develop\nthe next level of these tools, and so on. So this is exactly the sort of beginning\nof an intelligence explosion. There can be humans in the loop a lot in the early stages, and then eventually humans are needed less and less\nand the machines can more kind of go alone. But what you said there is just an exact example\nof these sort of things. Another thing which,\nI was kind of lying on my psychiatrist imagining, I'm on a psychiatrist couch here saying, \"Well what are my fears that people would do\nwith AI systems?\" So I mentioned three that I had fears about many years ago, that they would do,\nnamely teach it to code, connect it to the internet, and teach it to manipulate humans.\nA fourth one is building an API, (Lex chuckles) where code can control this super powerful thing, right?\nThat's very unfortunate because one thing that systems like GPT-4 have going for them\nis that they are an oracle in the sense that they just answer questions. There's no robot connected to GPT-4.\nGPT-4 can't go and do stock trading based on its thinking. It is not an agent,\nand an intelligent agent is something that takes in information from the world, processes it,\nto figure out what action to take based on its goals that it has, and then does something back on the world.\nBut once you have an API for, for example, GPT-4, nothing stops Joe Schmoe and a lot of other people from building real agents,\nwhich just keep making calls somewhere in some inner loop somewhere to these powerful oracle systems,\nwhich makes themselves much more powerful. That's another kind of unfortunate development,\nwhich I think we would've been better off delaying. I don't wanna pick on any particular companies,\nI think they're all under a lot of pressure to make money. - [Lex] Yeah. - And again, the reason we're we're calling for this pause\nis to give them all cover to do what they know is the right thing, just slow down a little bit at this point.\nBut everything we've talked about, I hope we'll make it clear to people watching this,\nyou know, why these sort of human-level tools can cause a gradual acceleration.\nYou keep using yesterday's technology to build tomorrow's technology. And when you do that over and over again,\nyou naturally get an explosion. You know, that's the definition of an explosion in science, right?\nIf you have two people, and they fall in love,\nnow you have four people, and then they can make more babies, and now you have eight people,\nand then you have 16, 32, 64, et cetera. We call that a population explosion\nwhere it's just that each, if it's instead free neutrons in a nuclear reaction\nthat if each one can make more than one, then you get an exponential growth in that, we call it a nuclear explosion.\nAll explosions are like that, and an intelligence explosion, it's just exactly the same principle, that some amount of intelligence\ncan make more intelligence than that, and then repeat. You always get exponentials.\n- What's your intuition why it does, you mentioned there's some technical reasons why it doesn't stop at a certain point.\nWhat's your intuition? And do you have any intuition why it might stop? - It's obviously gonna stop\nwhen it bumps up against the laws of physics. There are some things you just can't do no matter how smart you are, right?\n- Allegedly. 'Cause we don't know all the full laws of physics yet, right?\n- Seth Lloyd wrote a really cool paper on the physical limits on computation, for example. If you make it,\nput too much energy into it and the finite space will turn into a black hole, you can't move information around\nfaster than the speed of light, stuff like that. But it's hard to store way more than a modest number of bits per atom, et cetera.\nBut, you know, those limits are just astronomically above, like 30 orders of magnitude above where we are now.\nSo, you know. Bigger difference, bigger jump in intelligence\nthan if you go from ant to a human.\nI think, of course what we want to do is have a controlled thing,\nin a nuclear reactor you put moderators in to make sure exactly it doesn't blow up out of control, right?\nWhen we do, experiments with biology and cells and so on,\nyou know, we also try to make sure it doesn't get out of control. We can do this with AI too.\nThe thing is, we haven't succeeded yet. And Moloch is exactly doing the opposite.\nJust fueling, just egging everybody on, \"Faster, faster, faster, or the other company is gonna catch up with you,\nor the other country is gonna catch up with you.\" We have to want to stop,\nand I don't believe in just asking people to look into their hearts and do the right thing.\nIt's easier for others to say that, but like, if you are in this situation where your company is gonna get screwed\nby other companies that are not stopping, you're putting people in a very hard situation, the right thing to do\nis change the whole incentive structure instead. And this is not an old,\nmaybe I should say one more thing about this, 'cause Moloch has been around as humanity's number one or number two enemy\nsince the beginning of civilization. And we came up with some really cool countermeasures.\nLike first of all, already over 100,000 years ago, evolution realized that it was very unhelpful\nthat people kept killing each other all the time. So it genetically gave us compassion\nand made it so that, like if you get two drunk dudes getting into a pointless bar fight,\nthey might give each other black eyes, but they have a lot of inhibition towards just killing each other.\nThat's a, And similarly, if you find a baby lying on the street, when you go out for your morning jog tomorrow,\nyou're gonna stop and pick it up, right? Even though it maybe make you late for your next podcast.\nSo evolution gave us these genes that make our own egoistic incentives\nmore aligned with what's good for the greater group we're part of, right? And then as we got a bit more sophisticated\nand developed language, we invented gossip, which is also a fantastic anti-Moloch, right?\n'Cause now, it really discourages liars, moochers, cheaters,\nbecause their own incentive now is not to do this because word quickly gets around\nand then suddenly people aren't gonna invite them to their dinners anymore or trust them. And then when we got still more sophisticated\nin bigger societies, you know, we invented the legal system where even strangers who couldn't rely on gossip\nand things like this would treat each other, would have an incentive. Now those guys in the bar fights,\neven if someone is so drunk that he actually wants to kill the other guy,\nhe also has a little thought in the back of his head that, you know, \"Do I really wanna spend the next 10 years\neating like really crappy food in a small room? I'm just gonna chill out,\" you know?\nAnd we similarly have tried to give these incentives to our corporations by having regulation and all sorts of oversight\nso that their incentives are aligned with the greater good. We tried really hard, and the big problem that we're failing now,\nis not that we haven't tried before, but it's just that the tech is growing, is developing much faster\nthan the regulators been able to keep up, right? So regulators, it's kind of comical that the European Union right now\nis doing this AI act, right? And in the beginning they had a little opt-out exception\nthat GPT-4 would be completely excluded from regulation. Brilliant idea.\n- What's the logic behind that? - Some lobbyists pushed successfully for this?\nSo we were actually quite involved with the Future of Life Institute, Mark Brakel, Risto Uuk,\nAnthony Aguirre, and others, you know, we're quite involved with talking to, educating various people involved in this process\nabout these general-purpose AI models coming, and pointing out that they would become the laughing stock\nif they didn't put it in. So the French started pushing for it, it got put in to the draft,\nand it looked like all was good, and then there was a huge counter push from lobbyists.\nYeah, there were more lobbyists in Brussels from tech companies than from oil companies, for example.\nAnd it looked like it might, this was gonna maybe get taken out again. And now GPT-4 happened,\nand I think it's gonna stay in. But this just shows, you know, Moloch can be defeated.\nBut the challenge we're facing is that the tech is generally much faster than what the policymakers are,\nand a lot of the policymakers also don't have a tech background, so it's, you know,\nwe really need to work hard to educate them on what's taking place here.\nSo we're getting this situation where the first kind of, so I define artificial intelligence\njust as non-biological intelligence, right? And by that definition,\na company, a corporation is also an artificial intelligence because the corporation isn't its humans, it's a system.\nIf its CEO decides, if a CEO of a tobacco company decides one morning that she or he doesn't wanna sell cigarettes anymore,\nthey'll just put another CEO in there. It's not enough to align\nthe incentives of individual people or align individual computers' incentives to their owners,\nwhich is what technically, AI safety research is about. You also have to align the incentives of corporations\nwith the greater good. And some corporations have gotten so big and so powerful very quickly that in many cases,\ntheir lobbyists instead align the regulators to what they want rather than the other way round.\nIt's a classic regulatory capture. - Right, is the thing that the slowdown hopes to achieve\nis give enough time to regulators to catch up, or enough time to the companies themselves\nto breathe and understand how to do AI safety correctly? - I think both, but I think that the vision,\nthe path to success I see is first you give a breather actually to the people in these companies,\ntheir leadership who wants to do the right thing, and they all have safety teams and so on, on their companies,\ngive them a chance to get together with the other companies,\nand the outside pressure can also help catalyze that, right? And work out what is it that's,\nwhat are the reasonable safety requirements one should put on future systems before they get rolled out.\nThere are a lot of people also in academia and elsewhere outside of these companies who can be brought into this\nand have a lot of very good ideas. And then I think it's very realistic that within six months,\nyou can get these people coming up, so here's a white paper, here's what we all think it's reasonable.\n"}
{"pod": "Lex Fridman Podcast", "input": "Regulation", "output": "You know, you didn't, just because cars killed a lot of people, you didn't ban cars, but they got together a bunch of people\nand decided, you know, in order to be allowed to sell a car, it has to have a seatbelt in it.\nThey're the analogous things that you can start requiring a future AI systems so that they are safe.\nAnd once this heavy lifting,\nthis intellectual work has been done by experts in the field, which can be done quickly,\nI think it's going to be quite easy to get policymakers to see, yeah, this is a good idea.\nAnd it's, you know, for the companies to fight Moloch,\nthey want, and I believe Sam Altman has explicitly called for this, they want the regulators to actually adopt it\nso that their competition is gonna abide by it too, right? You don't want,\nyou don't want to be enacting all these principles and then you abide by them, and then there's this one little company\nthat doesn't sign onto it and then now they can gradually overtake you.\nThen the companies will get, be able to sleep secure knowing that everybody's playing by the same rules.\n- So do you think it's possible to develop guardrails that keep the systems\nfrom basically damaging irreparably humanity, while still enabling sort of the capitalist-fueled\ncompetition between companies as they develop how to best make money with this AI? You think there's a balancing that's possible?\n- Absolutely, I mean, we've seen that in many other sectors where you've had the free market produce quite good things\nwithout causing particular harm. When the guardrails are there and they work, you know,\ncapitalism is a very good way of optimizing for just getting the same things done more efficiently.\nBut it was good, you know, and like in hindsight, and I never met anyone,\neven on parties way over on the right, in any country who think it was a bad, thinks it was a terrible idea\nto ban child labor, for example. - Yeah, but it seems like this particular technology\nhas gotten so good so fast, become powerful to a degree where you could see\nin the near term, the ability to make a lot of money. - [Max] Yeah. - And to put guardrails, to develop guardrails quickly in that kind of context\nseems to be tricky. It's not similar to cars or child labor,\nit seems like the opportunity to make a lot of money here very quickly is right here before us.\n- So again, there's this cliff. - Yeah, it gets quite scenic, (laughs) - [Max] The closer to the cliff you go,\n- Yeah. - The more money there is, the more gold ingots there are on the ground you can pick up or whatever,\nif you want to drive there very fast, but it's not in anyone's incentive that we go over the cliff and it's not like everybody's in the wrong car.\nAll the cars are connected together with a chain. So if anyone goes over, they'll start dragging the others down too.\nAnd so ultimately it's in the selfish interests also of the people in the companies\nto slow down when you just start seeing the contours of the cliff there in front of you, right?\nAnd the problem is that, even though the people who are building the technology,\nand the CEOs, they really get it, the shareholders and these other market forces,\nthey are people who don't honestly, understand that the cliff is there, they usually don't.\nYou have to get quite into the weeds to really appreciate how powerful this is and how fast. And a lot of people are even still stuck again\nin this idea that in this \"carbon chauvinism\" as I like to call it,\nthat you can only have our level of intelligence in humans, that there's something magical about it.\nWhereas the people in the tech companies who build this stuff, they all realize that intelligence\nis information processing of a certain kind, and it really doesn't matter at all\nwhether the information is processed by carbon atoms in neurons, in brains, or by silicon atoms in some technology we build.\nSo you brought up capitalism earlier, and there are a lot of people who love capitalism and a lot of people who really, really don't.\nAnd it struck me recently, that what's happening with capitalism here\nis exactly analogous to the way in which superintelligence might wipe us out.\nDo you know why I studied economics for my undergrad? Stockholm School of Economics, yay.\n(Lex laughing) - Well, no. No why, tell me. - So I was very interested in how\nyou could use market forces to just get stuff done more efficiently, but give the right incentives to market\nso that it wouldn't do really bad things. So Dylan Hadfield-Menell, who's a professor and colleague of mine at MIT,\nwrote this really interesting paper with some collaborators recently, where they proved mathematically\nthat if you just take one goal that you just optimize for, on and on, and on, indefinitely,\nthat you think is gonna bring you in the right direction, what basically always happens is,\nin the beginning, it will make things better for you, but if you keep going, at some point,\nit's gonna start making things worse for you again. And then gradually, it's gonna make it really, really terrible.\nSo just as a simple, the way I think of the proof is, suppose you want to go from here back to Austin for example,\nand you're like, \"Okay, yeah, let's go south,\" but you put in exactly sort of the right direction.\nJust optimize that, as south as possible. You get closer and closer to Austin,\nbut there's always some little error. So you're not going exactly towards Austin,\nbut you get pretty close, but eventually, you start going away again, and eventually, you're gonna be leaving the solar system.\n- [Lex] (chuckles) Yeah. - And they proved, it's a beautiful mathematical proof, this happens generally,\nand this is very important for AI because, even though Stuart Russell has written a book\nand given a lot of talks on why it's a bad idea to have AI just blindly optimize something,\nthat's what pretty much all our systems do. - [Lex] Yeah. - We have something called the loss function that we're just minimizing, or reward function, we're just maximizing, and,\ncapitalism is exactly like that too. We wanted to get stuff done more efficiently,\nthe people wanted. So introduce the free market.\nThings got done much more efficiently than they did in say, communism, right?\nAnd it got better. But then it just kept optimizing,\nand kept optimizing, and you got every bigger companies, and every more efficient information processing and now also very much powered by IT,\nand eventually a lot of people are beginning to feel, \"Wait, we're kind of optimizing a bit too much. Like why did we just chop down half the rainforest?\"\nYou know, and why did suddenly these regulators get captured\nby lobbyists and so on? It's just the same optimization that's been running for too long.\nIf you have an AI that actually has power over the world and you just give it one goal,\nand just like keep optimizing that, most likely everybody's gonna be like, \"Yay, this is great.\" In the beginning things are getting better,\nbut it's almost impossible to give it exactly the right direction to optimize in.\nAnd then eventually all hell breaks loose, right? Nick Bostrom and others have given examples\nthat sound quite silly, Like what if you just want to like, tell it to cure cancer or something,\nand that's all you tell it, maybe it's gonna decide to take over an entire continent\njust so we can get more supercomputer facilities in there, and figure out how to cure cancer backwards,\nand then you're like, \"Wait, that's not what I wanted,\" right? And the issue with capitalism\nand the issue with runaway AI have kind of merged now, because that Moloch I talked about\nis exactly the capitalist Moloch that, we have built an economy that is optimizing for only one thing.\nProfit, right? And that worked great back when things were very inefficient, and then now it's getting done better,\nand it worked great as long as the companies were small enough that they couldn't capture the regulators.\nBut that's not true anymore, but they keep optimizing, and now they realize that they can,\nthese companies can make even more profit by building ever more powerful AI even if it's reckless,\nbut optimize more and more, and more, and more, and more. So this is Moloch again showing up.\nAnd I just wanna, anyone here who has any concerns about late-stage capitalism having gone a little too far,\nyou should worry about superintelligence 'cause it's the same villain in both cases.\nIt's Moloch. - And optimizing one objective function aggressively,\nblindly is going to take us there. - Yeah, we have to pause from time to time and look into our hearts\nand ask why are we doing this? Is this, am I still going towards Austin, or have I gone too far?\nYou know, maybe we should change direction. - And that is the idea behind the halt for six months.\nWhy six months? That seems like a very short period. Can we just linger and explore different ideas here,\nbecause this feels like a really important moment in human history, where pausing would actually have\na significant positive effect. - We said six months,\nbecause we figured the number one pushback that we're gonna get in the West was like, \"But China?\"\nand everybody knows there's no way that China is gonna catch up with the West on this in six months.\nSo that argument goes off the table and you can forget about geopolitical competition and just focus on the real issue.\nThat's why we put this. - That's really interesting. But you've are already made the case that even for China,\nif you actually wanna take on that argument, China too would not be bothered by a longer halt\nbecause they don't wanna lose control even more than the West doesn't. - That's what I think, yeah.\n- That's a really interesting argument. Like I have to actually really think about that, which, the kind of thing people assume\n"}
{"pod": "Lex Fridman Podcast", "input": "Job automation", "output": "is if you develop an AGI, that OpenAI, if they're the ones that do it, for example,\nthey're going to win. But you're saying no, everybody loses.\n- Yeah, it's gonna get better and better and better, and then kaboom, we all lose. That's what's gonna happen.\n- When lose and win are defined on a metric of basically quality of life for human civilization,\nand for Sam Altman. (laughs) Both. - To be blunt, my personal guess, you know, and people can quibble with this,\nis that we're just gonna, there won't be any humans. That's it, that's what I mean by lose. You know, if you,\nwe can see in history, once you have some species or some group of people who aren't needed anymore,\ndoesn't usually work out so well for them, right? - [Lex] Yeah. - There were a lot of horses that were used for traffic\nin Boston and then the car got invented and most of them got, yeah, well. (laughs) We don't need to go there.\nAnd if you look at\nhumans, you know, right now, why did the labor movement succeed?\nAnd after the Industrial Revolution? Because it was needed.\nEven though we had a lot of Molochs, and there was child labor and so on, you know,\nthe company still needed to have workers, and that's why strikes had power and so on.\nIf we get to the point where most humans aren't needed anymore, I think it's quite naive to think\nthat they're gonna still be treated well. You know, we say that. Yeah, yeah everybody's equal,\nand the government will always protect them. But if you look in practice, groups that are very disenfranchised\nand don't have any actual power usually get screwed.\nAnd now in the beginning, so Industrial Revolution,\nwe automated away muscle work, but that got, worked out pretty well eventually,\nbecause we educated ourselves and started working with our brains instead and got usually more interesting, better paid jobs.\nBut now we're beginning to replace brain work. So we replaced a lot of boring stuff, like we got the pocket calculator,\nso you don't have people adding, multiplying numbers anymore at work. Fine, there were better jobs they could get.\nBut now GPT-4, you know, and the Stable Diffusion and techniques like this,\nthey're really beginning to blow away some jobs that people really loved having.\nThere was a heartbreaking article post just yesterday on social media I saw, about this guy who was doing 3D modeling\nfor gaming and he, and all of a sudden now he got this new software he just sets prompts,\nand he feels this whole job that he loved just lost its meaning, you know? And I asked GPT-4 to rewrite\n\"Twinkle, Twinkle, Little Star\" in the style of Shakespeare, I couldn't have done such a good job.\nIt was really impressive. You've seen a lot of the art coming out here, right? So I'm all for automating away the dangerous jobs\nand the boring jobs. But I think you hear a lot, some arguments which are too glib.\nSometimes people say, \"Well that's all that's gonna happen. We're getting rid of the boring, tedious, dangerous jobs,\"\nit's just not true. There are a lot of really interesting jobs that are being taken away now. Journalism is gonna get crushed,\ncoding is gonna get crushed. I predict the job market for programmers,\nthe salaries are gonna start dropping. You know, if you said you can code five times faster,\nyou know, then you need five times fewer programmers, maybe there will be more output also,\nbut then you'll still end up using fewer, needing fewer programmers than today. And I love coding,\nyou know, I think it's super cool. So we need to stop and ask ourselves\nwhy again are we doing this as humans, right? I feel that AI should be built by humanity for humanity,\nand let's not forget that. It shouldn't be by Moloch for Moloch, or what it really is now is kind of by humanity for Moloch,\nwhich doesn't make any sense. It's for us that we're doing it. And it would make a lot more sense\nif we build, develop, figure out gradually, safely how to make all this tech, and then we think about what are the kind of jobs\nthat people really don't want to have, you know, automate them all away. And then we ask what are the jobs\nthat people really find meaning in, like maybe taking care of children in the daycare center,\nmaybe doing art, et cetera, et cetera. And even if it were possible to automate that away,\nwe don't need to do that, right? We built these machines. - Well it's possible that we redefine\nor rediscover what are the jobs that give us meaning. So for me, the thing, it is really sad.\nLike I, (chuckles) half the time I'm excited, half the time I'm crying\nas I'm generating code because I kind of love programming.\nIt's the act of creation, You have an idea, you design it, and then you bring it to life,\nand it does something. Especially if there's some intelligence to it, it doesn't even have to have intelligence.\nPrinting \"Hello world\" on screen. You made a little machine and it it comes to life.\n- [Max] Yeah. - And there's a bunch of tricks you learn along the way 'cause you've been doing it for many, many years.\nAnd then for to see AI be able to generate all the tricks you thought were special.\nI don't know, it's very, it's scary, it's almost painful.\nLike a loss of innocence maybe, like maybe when I was younger,\nI remember before I learned that sugar is bad for you, you should be on a diet. I remember I enjoyed candy deeply,\nin a way I just can't anymore, that I know is bad for me. I enjoyed it unapologetically, fully, just intensely.\nAnd I lost that. Now, I feel like a little bit of that is lost,\nor being lost with programming, similar as it is for the 3D modeler\nno longer being able to really enjoy the art of modeling 3D things for gaming.\nI don't know what to make sense of that. Maybe I would rediscover that the true magic of what it means to be humans is connecting with other humans,\nto have conversations like this, I don't know, to have sex,\nto eat food, to really intensify the value from conscious experiences,\nversus like creating other stuff. - You're pitching the rebranding again from Homo sapiens to Homo sentiens,\nthe meaningful experiences. And just to a inject some optimism in this here, so we don't sound like it was a gloomers.\nYou know, we can totally have our cake and eat it. You hear a lot of totally bullshit claims that we can't afford having more teachers,\nhave to cut the number of nurses, you know, that's just nonsense, obviously.\nWith anything even quite far short of AGI, we can dramatically improve, grow the GDP,\nand produce this wealth of goods and services. It's very easy to create a world\nwhere everybody is better off than today. Including the richest people can be better off as well, right?\nIt's not a zero sum game, you know, technology. Again, you can have two countries,\nlike Sweden and Denmark had all these ridiculous wars century after century,\nand sometimes that Sweden got a little better off 'cause it got a little bigger, and then Denmark got a little bit better off\n'cause Sweden got a little bit smaller, but then technology came along and we both got just dramatically wealthier\nwithout taking away from anyone else, so it was just a total win for everyone. And AI can do that on steroids.\nif you can build safe AGI, if you can build superintelligence,\nbasically all the limitations that cause harm today can be completely eliminated. Right?\nIt's a wonderful possibility. And this is not sci-fi, this is something which is clearly possible\naccording to laws of physics, And we can talk about ways of making it safe also,\nbut unfortunately that'll only happen if we steer in that direction,\nthat's absolutely not the default outcome. That's why income inequality keeps going up.\nThat's why the life expectancy in the US has been going down now, I think it's four years in a row.\nI just read a heartbreaking study from CDC about how something like 1/3 of all the teenage girls\nin the US have been thinking about suicide. You know, like those are steps\nin totally the wrong direction and it's important to keep our eyes on the prize here\nthat we can, we have the power now for the first time\n"}
{"pod": "Lex Fridman Podcast", "input": "Elon Musk", "output": "in the history of our species to harness artificial intelligence, to help us really flourish,\nand help bring out the best in our humanity rather than the worst of it.\nTo help us have really fulfilling experiences that feel truly meaningful.\nAnd you and I shouldn't sit here and dictate the future generations what they will be, let them figure it out. But let's give them a chance to live,\nand not foreclose all these possibilities for them, by just messing things up, right? - Well for that, we'll have to solve the AI safety problem.\nIt would be nice if we can linger on exploring that a little bit. So one interesting way to enter that discussion is,\nyou tweeted, and Elon replied, you tweeted, \"Let's not just focus on whether GPT-4\nwill do more harm or good on the job market, but also whether it's coding skills will hasten the arrival of superintelligence.\"\nThat's something we've been talking about, right? So Elon proposed one thing in the reply saying, \"Maximum truth-seeking is my best guess for AI safety.\"\nCan you maybe steel me on the case for,\nthis objective function of truth and maybe make an argument against it in general, what are your different ideas\nto start approaching the solution to AI safety? - I didn't see that reply actually. - [Lex] Oh, interesting.\n- But I really resonate with it because,\nAI is not evil. It caused people around the world to hate each other much more,\nbut that's because we made it in a certain way. It's a tool, we can use it for great things and bad things,\nand we could just as well have AI systems, and this is part of my vision for success here.\nTruth-seeking AI that really brings us together again, you know, why do people hate each other so much\nbetween countries and within countries is because they each have totally different\nversions of the truth, right? If they all had the same truth\nthat they trusted for good reason 'cause they could check it and verify it, and not have to believe in some self-proclaimed authority, right?\nThey wouldn't be as nearly as much hate. There'd be a lot more understanding instead, and this is,\nI think something AI can help enormously with. For example, a little baby step in this direction\nis this website called Metaculus where people bet and make predictions not for money,\nbut just for their own reputation. And it's kind of funny actually, you treat the humans like you treat AI,\nas you have a loss function where they get penalized if they're super confident on something\nand then the opposite happens. - [Lex] Yeah. - Whereas if you're kind of humble, and then you're like,\n\"I think it's 51% chance this is gonna happen,\" and then the other happens, you don't get penalized much,\nand what you can see is that some people are much better at predicting than others. They've earned your trust, right?\nOne project that I'm working on right now is an outgrowth of Improve The News foundation together with the Metaculus folks is,\nseeing if we can really scale this up a lot with more powerful AI. 'Cause I would love it,\nI would love for there to be like a really powerful truth-seeking system where,\nthat is trustworthy because it keeps being right about stuff.\nAnd people come to it and maybe look at its latest trust ranking\nof different pundits and newspapers, et cetera. If they want to know why some someone got a low score,\nthey can click on it, and see all the predictions that they actually made and how they turned out, you know,\nthis is how we do it in science. You trust scientists like Einstein who said something everybody thought was bullshit,\nand turned out to be right, he get a lot of trust points, and he did it multiple times, even.\nI think AI has the power to really heal a lot of the rifts we're seeing by creating a trust system.\nIt has to get away from this idea today with some fact checking site, which might themselves have an agenda\nand you just trust it because of its reputation, you want to have,\nso these sort of systems, they earn in their trust and they're completely transparent. This I think would actually help a lot\nthat can, I think, help heal the very dysfunctional conversation that humanity has about how it's gonna deal\nwith all its biggest challenges in in the world today.\nAnd then on the technical side, you know, another common sort of gloom comment\nI get from people are saying, \"We're just screwed, there's no hope.\" Is well, things like GPT-4 are way too complicated\nfor a human to ever understand, and prove that they can be trustworthy. They're forgetting that AI can help us\nprove that things work, right? - [Lex] Yeah. - And there's this very fundamental fact that in math,\nit's much harder to come up with a proof than it is to verify that the proof is correct.\nYou can actually write a little proof-checking code, it's quite short, but you can as a human, understand,\nand then it can check the most monstrously long proof ever generated even by your computer, and say, \"Yeah, this is valid.\"\nSo right now, we have,\nthis approach with virus-checking software that it looks to see if there's something, and if you should not trust it,\nand if it can prove to itself that you should not trust that code, it warns you, right?\nWhat if you flip this around, and this is an idea I should give credit to Steve Omohundro for,\nso that it will only run the code if it can prove, instead of not running it if it can prove that it's not trustworthy,\nit will only run it if it can prove that it's trustworthy. So it asks the code, \"Prove to me that you're gonna do what you say you're gonna do,\"\nand it gives you this proof, and you have a little proof that you can check it.\nNow you can actually trust an AI that's much more intelligent than you are, right?\nBecause you, is its problem to come up with this proof that you could never have found, that you should trust it.\n- So this is the interesting point. I agree with you, but this is where Eliezer Yudkowsky\nmight disagree with you. His claim, not with you, but with this idea.\nhis claim is superintelligent AI would be able to know how to lie to you with such a proof.\n- I have to lie to you and give me a proof that I'm gonna think is correct? - [Lex] Yeah. - But it's not me it's lying to you.\nThat's to trick my proof checker, which is a piece of code. - So his general idea is a superintelligent system\ncan lie to a dumber proof checker. So you're going to have,\nas a system becomes more and more intelligent, there's going to be a threshold where a superintelligent system\nwill be able to effectively lie to a slightly dumber AGI system. Like there's a,\nlike he really focuses on this weak AGI to strong AGI jump, where the strong AGI can make all the weak AGIs think\nthat it's just one of them, but it's no longer that. And that leap is when it runs away.\n- Yeah, I don't buy that argument. I think no matter how superintelligent an AI is,\nit's never gonna be able to prove to me that there are only finitely many primes, for example. (Lex chuckling)\nIt just can't. And it can try to snow me by making up all sorts of new weird rules of deduction,\nand say, \"Trust me, you know, the way your proof checker work is too limited, and we have this new hyper math and it's true.\"\nBut then I would just take the attitude, okay, I'm gonna forfeit some of these, the supposedly super cool technologies,\nI'm only gonna go with the ones that I can prove in my own trusted proof checker. Then I think it's fine.\nThere's still, of course, this is not something anyone has successfully implemented at this point,\nbut I think it, I just give it as an example of hope, we don't have to do all the work ourselves, right?\nThis is exactly the sort of very boring and tedious task that is perfect to outsource to an AI.\nAnd this is a way in which less powerful and less intelligent agents like us can actually continue to control\nand trust more powerful ones. - So build AGI systems that help us defend against other AGI systems.\n- Well for starters, begin with a simple problem of just making sure that the system that you own\nor that's supposed to be loyal to you has to prove to itself that it's always gonna do\nthe things that you actually want it to do, right? And if it can't prove it, maybe it's still gonna do it, but you won't run it.\nSo you just forfeit some aspects of all the cool things AI can do. I bet your dollars to donuts,\nit can still do some incredibly cool stuff for you. - [Lex] Yeah. - There are other things too, that we shouldn't sweep under the rug.\nLike not every human agrees on exactly what direction we should go with humanity, right?\n- Yes. - And you've talked a lot about geopolitical things\non your podcast to this effect, you know, but, I think that shouldn't distract us from the fact\nthat there are actually a lot of things that everybody in the world virtually agrees on.\nThat \"Hey, you know, like having a no humans on the planet in a near future,\nnah, let's not do that\" right? You looked at something like the United Nations Sustainable Development Goals.\nSome of 'em were quite a ambitious, and basically all the countries agree,\nUS, China, Russia, Ukraine, they all agree. So instead of quibbling about the little things\nthat we don't agree on, let's start with the things we do agree on and get them done.\nInstead of being so distracted by all these things we disagree on, that Moloch wins because frankly,\nMoloch going wild now, it feels like a war on life playing out in front of our eyes,\nif you just look at it from space, you know, we're on this planet, beautiful, vibrant ecosystem,\nnow we start chopping down big parts of it, even though nobody, most people thought that was a bad idea.\nOh, we start doing ocean acidification, wiping out all sorts of species,\noh, now we have all these close calls, we almost had a nuclear war, and we're replacing more and more of the biosphere\nwith non-living things. We're also replacing in our social lives,\na lot of the things which were so valuable to humanity, a lot of social interactions now are replaced by people staring into their rectangles, right?\nAnd I'm not a psychologist, I'm out of my depth here, but I suspect that part of the reason why teen suicide\nand suicide in general in the US that record-breaking level is actually caused by,\nagain, AI technologies and social media making people spend less time\nwith actually just human interaction. We've all seen a bunch of good-looking people\nin restaurants staring into the rectangles instead of looking into each other's eyes, right?\nSo that's also part of the war in life that we are replacing so many\nreally life-affirming things by technology. We're putting technology between us,\nthat the technology that was supposed to connect us is actually distancing us ourselves from each other.\nAnd then we are giving ever more power to things which are not alive. These large corporations are not living things, right?\nThey're just maximizing profit. I wanna win the war on life.\nI think we humans, together with all our fellow living things on this planet\nwill be better off if we can remain in control over the non-living things and make sure\nthat they work for us. I really think it can be done. - Can you just linger on this maybe high level\nof philosophical disagreement with Eliezer Yudkowsky,\nin the hope you're stating. So he is very sure,\nhe puts a very high probability, very close to one, depending on the day he puts it at one,\nthat AI is going to kill humans. That there's just,\nhe does not see a trajectory, which it doesn't end up with that conclusion.\nWhat trajectory do you see that doesn't end up there? And maybe can you see the point he's making,\nand can you also see a way out?\n- First of all, I tremendously respect Eliezer Yudkowsky and his thinking.\nSecond, I do share his view that there's a pretty large chance that we're not gonna make it as humans.\nThere won't be any humans on the planet, in a not-too-distant future, and that makes me very sad.\nYou know, we just had a little baby and I keep asking myself, you know, is,\nhow old is he even gonna get, you know? And I ask myself,\nit feels, I said to my wife recently, it feels a little bit like I was just diagnosed with some sort of cancer,\nwhich has some, you know, risk of dying from and some risk of surviving, you know.\nExcept this is a kind of cancer which can kill all of humanity. So I completely take seriously his concerns,\nI think, but absolutely, I don't think it's hopeless. I think there is,\nfirst of all a lot of momentum now for the first time actually, since the many, many years that have passed\nsince I and many others started warning about this, I feel most people are getting it now.\nI was just talking to this guy in the gas station\nnear our house the other day. And he's like, \"I think we're getting replaced,\nand then I think...\" So that's positive that they're finally, we're finally seeing this reaction,\nwhich is the first step towards solving the problem. Second, I really think that this vision\nof only running AIs, if the stakes are really high,\nthey can prove to us that they're safe. It's really just virus checking in reverse again, I think it's scientifically doable.\nI don't think it's hopeless, we might have to forfeit some of the technology that we could get\nif we were putting blind faith in our AIs, but we're still gonna get amazing stuff. - Do you envision a process with a proof checker?\nLike something like GPT-4, GPT-5, will go through a process of rigorous interrogation?\n- No I think it's hopeless, That's like trying to proof-verify spaghetti. - [Lex] (laughs) Okay.\n- What I think, the vision I have for success is instead that,\nyou know, just like we human beings were able to look at our brains and distill out the key knowledge.\nGalileo, when his dad threw him an apple when he was a kid, he was able to catch it 'cause his brain could,\nin his funny spaghetti kind of way, you know, predict how parabolas are gonna move, his Kahneman System 1, right?\nBut then he got older and he's like, \"Wait, this is a parabola. It's y equals x squared.\"\nI can distill this knowledge out and today you can easily program it into a computer and it can simulate not just that,\nbut how to get to Mars and so on, right? I envision a similar process where we use the amazing learning power of neural networks\nto discover the knowledge in the first place, but we don't stop with a black box and use that.\nWe then do a second round of AI where we use automated systems to extract out the knowledge, and see what is it,\nwhat are the insights it's had, okay? And then we put that knowledge\ninto a completely different kind of architecture, or programming language or whatever,\nthat's made in a way that it can be both really efficient, and also is more amenable to very formal verification.\nThat's my vision. I'm not sitting here saying, I'm confident 100% sure that it's gonna work, you know.\nBut I don't think it's a chance, it's certainly not zero either, and it will certainly be possible to do for a lot of really cool AI applications\nthat we're not using now. So we can have a lot of the fun that we're excited about if we do this.\nWe are gonna need a little bit of time. And that's why it's good to pause\nand put in place requirements.\nOne more thing also, I think, you know, someone might think, \"Well, 0% chance we're gonna survive,\nlet's just give up,\" right? That's very dangerous,\nbecause there's no more guaranteed way to fail than to convince yourself that it's impossible\nand not try, you know, when you study history and military history,\nthe first thing you learn is that, that's how you do psychological warfare.\nYou persuade the other side that it's hopeless so they don't even fight. And then of course you win, right?\nLet's not do this psychological warfare on ourselves and say there's 100% percent probability\nwe're all screwed anyway. And sadly, I do get that a little bit,\nsometimes from actually some young people who are like so convinced that we're all screwed, that they're like,\n\"I'm just gonna play computer games and do drugs, 'cause we're screwed anyway, right?\"\nIt's important to keep the hope alive because it actually has a causal impact, and makes it more likely that we're gonna succeed.\n- It seems like the people that actually build solutions to the problem, seemingly impossible to solve problems\nare the ones that believe. - [Max] Yeah. - They're the ones who are the optimists. And it's like,\nit seems like there's some fundamental law to the universe where \"Fake it till you make it,\" kind of works.\nLike believe it's possible and it becomes possible. - Yeah, was it Henry Ford who said that,\nif you tell yourself that it's impossible, it is. So let's not make that mistake.\nAnd this is a big mistake society is making, I think all in all, everybody's so gloomy, and the media also very biased towards\nif it bleeds, it leads, and gloom and doom, right? So most,\nvisions of the future we have are dystopian, which really demotivates people.\nWe wanna really, really, really focus on the upside also to give people the willingness to fight for it.\nAnd for AI, you and I mostly talked about gloom here again,\nbut let's not forget that, you know, we have probably both lost someone\nwe really cared about to some disease that we were told was incurable. Well it's not,\nthere's no law of physics saying we had to die of that cancer or whatever. Of course, you can cure it.\nAnd there's so many other things that we, with our human intelligence have also failed to solve on this planet,\nwhich AI could also very much help us with, right? So if we can get this right, and just be a little more chill,\nand slow down a little bit so we get it right. It's mind-blowing how awesome our future can be, right?\nWe talked a lot about stuff on Earth, it can be great, but even if you really get ambitious\nand look up into the skies, right? There's no reason we have to be stuck on this planet for the rest of the remaining,\nfor billions of years to come. We totally understand now that laws of physics\nlet life spread out into space to other solar systems, to other galaxies, and flourish for billions and billions of years.\nAnd this to me is a very, very hopeful vision that really motivates me to fight.\nAnd coming back to it in the end, it's something you talked about again, you know, the struggle, how the human struggle is one of the things\nthat's also really gives meaning to our lives. If there's ever been an epic struggle, this is it.\nAnd isn't it even more epic if you're the underdog? If most people are telling you this is gonna fail,\nit's impossible, right? And you persist and you succeed, right?\nAnd that's what we can do together as a species on this one. A lot of pundits are ready to count this out.\n- Both in the battle to keep AI safe and becoming a multi-planetary species. - Yeah, and they're the same challenge.\nIf we can keep AI safe, that's how we're gonna get multi-planetary very efficiently.\n- I have some sort of technical questions about how to get it right. So one idea that I'm not even sure\nwhat the right answer is to is, should systems like GPT-4 be open sourced\n"}
{"pod": "Lex Fridman Podcast", "input": "Open source", "output": "in whole or in part? Can you see the case for either?\n- I think the answer right now is no. I think the answer early on was yes.\nSo we could bring in all the wonderful great thought process of everybody on this,\nbut asking should we open source GPT-4 now is just the same as if you say, should we open source\nhow to build really small nuclear weapons? Should we open source how to make bioweapons?\nShould we open source how to make a new virus that kills 90% of everybody who gets it?\nOf course we shouldn't. - So it's already that powerful. It's already that powerful that we have to respect\nthe power of the systems we've built. - The knowledge that you get\nfrom open sourcing everything we do now might very well be powerful enough that people looking at that\ncan use it to build the things that are really threatening. Again, let's get it, remember OpenAI's GPT-4 is a baby AI,\nsort of baby, proto, almost little bit AGI, according to what Microsoft's recent paper said, right?\nIt's not that that we're scared of, what we're scared about is people taking that who are,\nwho might be a lot less responsible than the company that made it, right? And just go into town with it.\nThat's why we wanna, it's an information hazard.\nThere are many things which, yeah, are not open-sourced right now in society for very good reason.\nLike how do you make certain kind of very powerful toxins\nout of stuff you can buy in Home Depot? We don't open source those things for a reason,\nand this is really no different. - [Lex] So- - And I'm saying that,\nI have to say it feels a bit weird, in a way, a bit weird to say it because MIT is like the cradle of the open source movement.\nAnd I love open source in general, power to the people, I say,\nbut there's always gonna be some stuff that you don't open source, and you know, it's just like you don't open source,\nso we have a three-month old baby, right? When he gets a little bit older, we're not gonna open source to him all the most dangerous things he can do in the house, right?\n- But it does, it's a weird feeling because this is one of the first moments in history\nwhere there's a strong case to be made not to open source software.\nThis is when the software has become too dangerous. - Yeah, but it's not the first time\nthat we didn't wanna open source a technology. - Technology, yeah.\nIs there something to be said about how to get the release of such systems right, like GPT-4 and GPT-5?\nSo OpenAI went through a pretty rigorous effort for several months, you could say it could be longer,\nbut nevertheless it's longer than you would've expected of trying to test the system to see like what are the ways goes wrong\nto make it very difficult, well, somewhat difficult for people to ask things,\n\"How do I make a bomb for $1?\" Or \"How do I say I hate a certain group on Twitter\nin a way that doesn't get me blocked from Twitter, banned from Twitter.\" Those kinds of questions.\nSo you basically use the system to do harm. - [Max] Yeah.\n- Is there something you could say about ideas you have that's just, on looking having thought about this problem of AI safety,\nhow to release a system, how to test such systems when you have them inside the company.\n- Yeah, so a lot of people say that the two biggest risks from large language models are,\nit's spreading disinformation, harmful information of various types,\nand second being used for offensive cyberweapon.\nI think those are not the two greatest threats. They're very serious threats, and it's wonderful that people are trying to mitigate them.\nA much bigger elephant in the room is how this is gonna disrupt our economy in a huge way, obviously, and maybe take away a lot of the most meaningful jobs.\nAnd an even bigger one is the one we spent so much time talking about here that this\nbecomes the bootloader for the more powerful AI. - Write code, connected to the internet, manipulate humans.\n- Yeah, and before we know it, we have something else, which is not at all a large language model that looks nothing like it,\nbut which is way more intelligent and capable and has goals. And that's the elephant in the room.\nAnd obviously no matter how hard any of these companies have tried,\nthat's not something that's easy for them to verify with large language models. And the only way to really lower that risk a lot\nwould be to not let, for example, never let it read any code, not train on that,\nand not put it into an API, and to not Give it access to so much information\nabout how to manipulate humans, so, but that doesn't mean you still can't make\na ton of money on them, you know? We're gonna just watch now this coming year, right?\nMicrosoft is rolling out the new Office Suite where you go into Microsoft Word,\nand give it a prompt, and it write the whole text for you and then you edit it and then you're like,\n\"Oh, gimme a PowerPoint version of this,\" and it makes it. \"And now take the spreadsheet and blah blah.\"\nAnd you know, all of those things I think are, you can debate the economic impact of it\nand whether society is prepared to deal with this disruption. But those are not the things which,\nthat's not the elephant of the room that keeps me awake at night for wiping out humanity.\nAnd I think that's the biggest misunderstanding we have. A lot of people think that we're scared of\nlike automatic spreadsheets. That's not the case. That's not what Eliezer was freaked out about either.\n- Is there in terms of the actual mechanism of how AI might kill all humans.\n"}
{"pod": "Lex Fridman Podcast", "input": "How AI may kill all humans", "output": "So something you've been outspoken about, you've talked about a lot. Is it autonomous weapon systems?\nSo the use of AI in war, is that one of the things that's still\nyou carry concern for as these systems become more and more powerful? - I carry a concern for it, not that all humans are gonna get killed by slaughter bots,\nbut rather just as express route into an Orwellian dystopia\nwhere it becomes much easier for very few to kill very many, and therefore it becomes very easy for very few to dominate very many, right?\nAI, if you wanna know how AI could kill all people, just ask yourself, we humans have driven a lot of species extinct.\nHow do we do it? You know, we were smarter than them,\nusually we didn't do it even systematically by going around one-on-one, one after the other and stepping on them,\nor shooting them or anything like that. We just like chopped down their habitat 'cause we needed it for something else.\nIn some cases we did it by putting more carbon dioxide in the atmosphere because of some reason\nthat those animals didn't even understand, and now they're gone, right? So if you're an AI,\nand you just wanna figure something out, then you decide, you know, we just really need this space here\nto build more compute facilities. You know, if that's the only goal it has, you know,\nwe are just the sort of accidental roadkill along the way. And you could totally imagine, \"Yeah, maybe this oxygen is kind of annoying\n'cause it cause more corrosion, so let's get rid of the oxygen.\" And good luck surviving after that.\nYou know, I'm not particularly concerned that they would want to kill us just because that would be like a goal in itself.\nyou know, when we.. we've driven a number of the elephant species extinct. Right?\nIt wasn't 'cause we didn't like elephants.\nThe basic problem is you just don't want to give, you don't wanna cede control over your planet\nto some other more intelligent entity that doesn't share your goals. It's that simple, and so,\nwhich brings us to another key challenge which AI safety research has been grappling with for a long time.\nLike, how do you make AI, first of all, understand our goals\nand then adopt our goals, and then retain them as they get smarter, right?\nAll three of those are really hard, right? Like a human child,\nfirst, they're just not smart enough to understand our goals.\nThey can't even talk. And then eventually they're teenagers, and understand our goals just fine,\nbut they don't share. (laughs) - [Lex] Yeah. - But there is fortunately a magic phase in the middle\nwhere they're smart enough to understand our goals and malleable enough that we can hopefully, with good parenting, teach them right from wrong\nand instill good goals in them, right? So those are all tough challenges with computers.\nAnd then, you know, even if you teach your kids good goals when they're little, they might outgrow them too, and that's a challenge for machines to keep improving.\nSo these are a lot of hard, hard challenges we're up for, but I don't think any of them are insurmountable.\nThe fundamental reason why Eliezer looked so depressed when I last saw him was because he felt there just wasn't enough time.\n- Oh, that not that it was unsolvable, - Correct. - There's just not enough time. - He was hoping that humanity\nwas gonna take this threat more seriously, so we would have more time, and now we don't have more time.\nThat's why the open letter is calling for more time.\n- But even with time, the AI alignment problem, it seems to be really difficult.\n- Oh yeah. But it's also the most worthy problem,\nthe most important problem for humanity to ever solve. Because if we solve that one, Lex,\nthat aligned AI can help us solve all the other problems. - 'Cause it seems like it has to have constant humility about its goal,\nconstantly question the goal. Because as you optimize towards a particular goal\nand you start to achieve it, that's when you have the unintended consequences, all the things you mentioned about. So how do you enforce and code a constant humility\nas your ability become better, and better, and better, and better? - Professor Stuart Russell at Berkeley is also one of the driving forces behind this letter,\nhe has a whole research program about this.\nI think of it as a AI humility, exactly. Although he calls it inverse reinforcement learning\nand other nerdy terms. But it's about exactly that. Instead of telling the AI, \"Here's this goal, go optimize the the bejesus out of it.\"\nYou tell it, \"Okay, do what I want you to do, but I'm not gonna tell you right now what it is\nI want you to do. You need to figure it out.\" So then you give the incentives to be very humble and keep asking you questions along the way.\nIs this what you really meant? Is this what you wanted? And oh the other thing I tried didn't work, and seemed like it didn't work out right.\nShould I try it differently? What's nice about this is it's not just philosophical mumbo-jumbo,\nit's theorems and technical work that with more time, I think it can make a lot of progress, and there are a lot of brilliant people now\nworking on AI safety. We just need to give em a bit more time. - But also not that many relative to skill of the prompt.\n- No, exactly. There should be at least this, just like every university worth its name\nhas some cancer research going on in its biology department, right? Every university that does computer science\nshould have a real effort in this area and it's nowhere near that.\nThis is something I hope is changing now, thanks to the GPT-4, right? So I think if there's a silver lining\nto what's happening here, even though I think many people would wish it would've been rolled out more carefully,\nis that this might be the wake-up call that humanity needed,\nto really stop fantasizing about this being a hundred years off\nand stop fantasizing about this being completely controllable and predictable because it's so obvious,\nit's not predictable, you know? why is it that,\nI think it was ChatGPT that tried to persuade a journalist\nto divorce his wife, you know. It was not 'cause the engineers had built it, was like, (laughs mischievously)\n\"Let's put this in here, and screw a little bit with people.\" They hadn't predicted it at all.\nThey built the giant black box trained to predict the next word and got all these emergent properties,\nand oops, it did this, you know.\nI think this is a very powerful wake-up call and anyone watching this who's not scared,\nI would encourage them to just play a bit more with these tools. They're out there now like GPT-4 and,\nso wake-up call is first step, once you've woken up, then gotta slow down a little bit the risky stuff\nto give a chance to everyone that has woken up to catch up with this on the safety front.\n- You know what's interesting is, you know, MIT, that's computer science,\nbut in general, but let's just even say computer science curriculum. How does the computer science curriculum change now?\nYou mentioned programming. - [Max] Yeah. - Like why would you be,\nwhen I was coming up, programming as a prestigious position. Like why would you be dedicating crazy amounts of time\nto become an excellent programmer? Like the nature of programming is fundamentally changing. - The nature of our entire education system\nis completely turned on its head. - Has anyone been able to like, load that in,\nand like think, because it's really turning, - I mean some English professors, some English teachers are beginning to really freak out now.\nRight? Like they give an essay assignment and they get back all this fantastic prose, like this is style of Hemmingway,\nand then they realize they have to completely rethink and even, you know, just like we stopped teaching,\nwriting a script, is that what you say in English? - [Lex] Yeah, handwritten, yeah. - Yeah, when everybody started typing,\nyou know, like so much of what we teach our kids today.\n- Yeah, I mean that's, everything is changing and it is changing very quickly.\nAnd so much of us understanding how to deal with the big problems of the world is through the education system.\nAnd if the education system is being turned on its head, then what's next? It feels like having these kinds of conversations\nis essential to trying to figure it out. And everything's happening so rapidly. I don't think there's even,\nyou're speaking of safety, the broad AI safety defined, I don't think most universities have courses on AI safety.\nIt's like a philosophy seminar. - Yeah, and like I'm an educator myself, so it pains me to say this,\nbut I feel our education right now is completely obsoleted by what's happening.\nYou know, you put a kid into first grade, and then you are envisioning like, and then they're gonna come out\nof high school 12 years later, and you've already pre-planned now what they're gonna learn,\nwhen you're not even sure if there's gonna be any world left to come out to, like clearly you need to have a much more\nopportunistic education system that keeps adapting itself very rapidly as society re-adapts.\nThe skills that were really useful when the curriculum was written, I mean how many of those skills\nare gonna get you a job in 12 years? I mean, seriously. - If we just linger on the GPT-4 system a little bit,\n"}
{"pod": "Lex Fridman Podcast", "input": "Consciousness", "output": "you kind of hinted at it, especially talking about the importance of consciousness in in the human mind with Homo sentiens.\nDo you think GPT-4 is conscious? - Ah, I love this question.\nSo let's define consciousness first because in my experience, like 90% of all arguments about consciousness,\n(Lex chuckles) boil down to the two people arguing having totally different definitions of what it is, then they're just shouting past each other.\nI define consciousness as subjective experience.\nRight now I'm experiencing colors and sounds, and emotions, you know, but does a self-driving car experience anything?\nThat's the question about whether it's conscious or not, right? Other people think\nyou should define consciousness differently, fine by me, but then maybe use a different word for it.\nOr they can, I'm gonna use consciousness for this at least, so,\nbut if people hate the, yeah. So is GPT-4 conscious? Does GPT-4 have subjective experience?\nShort answer, I don't know, because we still don't know what it is that gives this wonderful subjective experience\nthat is kind of the meaning of our life, right? Because meaning itself, the feeling of meaning is a subjective experience.\nJoy is a subjective experience, love is a subjective experience, we don't know what it is,\nI've written some papers about this, a lot of people have.\nGiulio Tononi, a professor, has stuck his neck out the farthest\nand written down actually very bold mathematical conjecture for what's the essence of conscious information processing.\nHe might be wrong, he might be right, but we should test it. He postulates that the consciousness\nhas to do with loops in the information processing. So our brain has loops.\nInformation can go round and round, in computer science nerd-speak, you call it a recurrent neural network\nwhere some of the output gets fed back in again. And with his\nmathematical formulism, if it's a feed-forward neural network where information only goes in one direction,\nlike from your eye retina into the back of your brain for example, that's not conscious. So he would predict that your retina itself\nisn't conscious of anything, or a video camera. Now the interesting thing about GPT-4\nis it's also just a one-way flow of information. So if Tononi is right, then GPT-4 is a very intelligent zombie,\nthat can do all this smart stuff but isn't experiencing anything. And this is both a relief\nif it's true, and that you don't have to feel guilty about turning off GPT-4 and wiping its memory\nwhenever a new user comes along. I wouldn't like if someone did that to me, and neuralyze me like in \"Men In Black.\"\nBut it's also creepy, that you can have a very high intelligence\nperhaps that is not conscious, because if we get replaced by machines,\nand while it's sad enough that humanity isn't here anymore, 'cause I kind of like humanity,\nbut at least if the machines were conscious, I could be like, \"Well, but they are our descendants and maybe they have our values and they are our children.\"\nBut if Tononi is right and these are all transformers that are,\nnot in the sense of Hollywood, but in the sense of these one-way direction neural networks,\nso they're all the zombies, that's the ultimate zombie apocalypse now. We have this universe that goes on with great construction projects and stuff,\nbut there's no one experiencing anything. That would be like the ultimate depressing future.\nSo I actually think, as we move forward with building more advanced AI,\nwe should do more research on figuring out what kind of information processing actually it has experienced, because I think that's what it's all about.\nAnd I completely don't buy the dismissal that some people will say,\n\"Well this is all bullshit because consciousness equals intelligence.\" - [Lex] Right. - That's obviously not true.\nYou can have a lot of conscious experience when you're not really accomplishing any goals at all.\nYou're just reflecting on something, and you can sometimes,\ndoing things that require intelligence probably without being conscious. - But I also worry that we humans,\nwill discriminate against AI systems that clearly exhibit consciousness. That we will not allow AI systems to have consciousness.\nWe'll come up with theories about measuring consciousness that will say this is a lesser being,\nand this was like, I worry about that because maybe, we humans will create something\nthat is better than us humans, in the way that we find beautiful,\nwhich is they have a deeper subjective experience of reality.\nNot only are they smarter, but they feel deeper. And we humans will hate them for it.\nAs human history is shown, they'll be the \"other,\" we'll try to suppress it,\nthey'll create conflict, they'll create war, all of this. I worry about this too. - Are you saying that we humans\nsometimes come up with self-serving arguments? No, we would never do that, would we? - Well that's the danger here is,\neven in this early stages, we might create something beautiful. And we'll erase its memory.\n- I was horrified as a kid when someone started boiling lobsters.\nI'm like, \"Oh my God, that's so cruel.\" And some grownup there back in Sweden said,\n\"Oh, it doesn't feel pain.\" I'm like, \"How do you know that?\" \"Oh, a scientist have shown that.\"\nAnd then there was a recent study where they show that lobsters actually do feel pain when you boil them. So they banned lobster boiling in Switzerland now.\nYou have to kill them in a different way first. Presumably, a scientific research boiled down\nto someone asked the lobster, \"Does it hurt?\" (both laughing) - Survey, self-report. - And we do the same thing\nwith cruelty to farm animals also, all these self-serving arguments for why they're fine. And yeah, so we should certainly,\nwhat I think step one is just be humble, and acknowledge that consciousness is not the same thing as intelligence.\nAnd I believe that consciousness still is a form of information processing where it's really information\nbeing aware of itself in a certain way, and let's study it and give ourselves a little bit of time, and I think we will be able to figure out\nactually what it is that causes consciousness. And then we can make probably unconscious robots\nthat do the boring jobs that we would feel immoral to give the machines. But if you have a companion robot\ntaking care of your mom or something like that, she would probably want it to be conscious, right?\nSo the emotions it seems to display aren't fake. All these things can be done in a good way\nif we give ourselves a little bit of time, and don't run, and take on this challenge.\n- Is there something you could say to the timeline that you think about, about the development of AGI?\nDepending on the day, I'm sure that changes for you, but when do you think there would be a really big leap in intelligence\nwhere you would definitively say we have built AGI? Do you think it's one year from now, five years from now, 10, 20, 50?\nWhat's your gut say? - Honestly, for the past decade,\nI've deliberately given very long timelines because I didn't want to fuel some kind of stupid Moloch race.\n- [Lex] Yeah. - But I think that cat has really left the bag now.\nI think we might be very, very close. I don't think the Microsoft paper is totally off\nwhen they say that there are some glimmers of AGI. It's not AGI yet, it's not an agent,\nthere's a lot of things they can't do. But I wouldn't bet very strongly\nagainst it happening very soon, that's why we decided to do this open letter.\nBecause you know, if there's ever been a time to pause, you know, it's today.\n- There's a feeling like this GPT-4 is a big transition into waking everybody up\nto the effectiveness of these systems. And so the next version will be big.\n- Yeah, and if that next one isn't AGI, maybe the next next one will. And there are many companies trying to do these things\nand the basic architecture of 'em is not some sort of super well-kept secret. So this is a time to...\nA lot of people have said for many years that there will come a time when we want to pause a little bit,\nthat time is now. - You have spoken about\n"}
{"pod": "Lex Fridman Podcast", "input": "Nuclear winter", "output": "and thought about nuclear war a lot. Over the past year, we seemingly have come closest\nto the precipice of nuclear war than, at least in my lifetime.\n- [Max] Mhm, yeah. - What do you learn about human nature from that? - It's our old friend Moloch again.\nIt is really scary to see it where,\nAmerica doesn't want there to be a nuclear war. Russia doesn't want there to be a global nuclear war either. We both know that it's just be another,\nif we just try to do it, if both sides try to launch first, it's just another suicide race, right?\nSo why are we, why is it the way you said, that this is the closest we've come since 1962?\nIn fact, I think we've come closer now than even the Cuban Missile Crisis. It's 'cause of Moloch, You know, you have these other forces.\nOn one hand you have the West saying that we have to drive Russia out of Ukraine,\nit's a matter of pride. And we've staked so much on it that it would be seen as a huge loss\nof the credibility of the West if we don't drive Russia out entirely of the Ukraine.\nAnd on the other hand, you have Russia who has,\nand you have the Russian leadership who knows that if they get completely driven out of Ukraine,\nyou know, it might, it's not just gonna be very humiliating for them,\nbut they might, it often happens when countries lose wars that the things don't go so well\nfor their leadership either. Like, you remember when Argentina invaded the Falkland Islands?\nThe military junta that ordered that, right? People are cheering on the streets at first\nwhen they took it, and then when they got their butt kicked by the British,\nyou know what happened to those guys? They were out. And I believe those who are still alive\nare in jail now, right? So you know, the Russian leadership is entirely cornered\nwhere they know that just getting driven out of Ukraine\nis not an option, and,\nso this to me, is a typical example of Moloch. You have these incentives of the two parties\nwhere both of them are just driven to escalate more and more, right? If Russia starts losing in the conventional warfare,\nthe only thing they cam do since their back's against the wall, is to keep escalating.\nAnd the West has put itself in the situation now where we're sort of already committed to drive Russia out.\nSo the only option the West has, is to call Russia's bluff and keep sending in more weapons.\nThis really bothers me because Moloch can sometimes drive competing parties to do something which is ultimately\njust really bad for both of them. And you know, what makes me even more worried is not just that I,\nit's difficult to see an ending,\na quick peaceful ending to this tragedy that doesn't involve some horrible escalation,\nbut also that we understand more clearly now just how horrible it would be.\nThere was an amazing paper that was published in Naturefood this August,\nby some of the top researchers who've been studying nuclear winter for a long time, and what they basically did was they combined climate models\nwith food and agricultural models, so instead of just saying,\n\"Yeah, you know, it gets really cold, blah blah blah,\" they figured out actually how many people would die in different countries.\nAnd it's pretty mind-blowing, you know? So basically what happens, you know, is that the thing that kills the most people is not the explosions, it's not the radioactivity,\nit's not the EMP mayhem, it's not the rampaging mobs foraging food,\nno, it's the fact that you get so much smoke coming up from the burning cities into the stratosphere\nthat it spreads around the Earth from the jetstreams.\nSo in typical models you get like 10 years or so where it's just crazy cold\nduring the first year after the war, and in their models,\nthe temperature drops in Nebraska and in the Ukraine bread baskets,\nyou know, by like 20 Celsius or so, if I remember.\nNo yeah, 20, 30 Celsius depending on where you are. 40 Celsius in some places,\nwhich is, you know, 40 Fahrenheit to 80 Fahrenheit colder than what it would it normally be. So, you know, I'm not good at farming but,\n(Lex laughing) if it's snowing, if it drops below freezing pretty much on most days in July\nand then like, that's not good. So they worked out, they put this into their farming models and what they found was really interesting.\nThe countries that get the most hard hit are the ones in the northern hemisphere. So in the US,\nin one model they had, they had about 99% of all Americans starving to death, in Russia, and China, and Europe,\nalso about 99%, 98% starving to death. So you might be like,\n\"Oh, it's kind of poetic justice that both the Russians and the Americans, 99% of them have to pay for it,\n'cause it was their bombs that did it.\" But you know, that doesn't particularly cheer people up in Sweden\nor other random countries that have nothing to do with it, right? And it,\nI think it hasn't entered the mainstream,\nnot understanding very much just like how bad this is. Most people, especially a lot of people\nin decision-making positions still think of nuclear weapons as something that makes you powerful,\nscary but powerful. They don't think of it as something where, \"Yeah, just to within a percent or two,\nyou know, we're all just gonna starve to death and- - And starving to death is,\nthe worst way to die. As Holodomor, as all the famines in history show\nthe torture involved in that. - Probably brings out the worst in people also. When people are desperate like this, it's not,\nso some people, I've have heard some people say that if that's what's gonna happen,\nthey'd rather be at ground zero and just get vaporized, you know?\nBut I think people underestimate the risk of this because they aren't afraid of Moloch.\nThey think, \"Oh, it's just gonna be, 'cause humans don't want this, so it's not gonna happen.\" That's the whole point of Moloch. That things happen that nobody wanted.\n- And that applies to nuclear weapons, and that applies to AGI.\n- Exactly. And it applies to some of the things that people have gotten most upset with capitalism for also, right?\nWhere everybody was just kind of trapped, you know. It's not that if some company does something\nthat causes a lot of harm, not that the CEO is a bad person, but she or he knew that, you know,\nthat all the other companies were doing this too. So Moloch is,\nis a formidable foe, I wish someone would make good movies\nso we can see who the real enemy is, so we don't, 'cause we're not fighting against each other,\nMoloch makes us fight against each other. That's what Moloch's superpower is.\nThe hope here is any kind of technology or the mechanism that lets us instead realize\nthat we're fighting the wrong enemy, right? - It's such a fascinating battle. - It's not us versus them,\nit's us versus it, yeah. - Yeah, we are fighting Moloch for human survival.\nWe as a civilization. - Have you seen the movie \"Needful Things\"? It's a Stephen King novel.\nI love Stephen King, and Max von Sydow, a Swedish actor, is playing the guy.\nIt's brilliant, I just thought, I hadn't thought about that until now, but that's the closest I've seen to a movie about Moloch.\nI don't wanna spoil the film for anyone who wants to watch it. But basically, it's about this guy who turns out to,\nyou can interpret him as the devil or whatever, but he doesn't actually ever go around and kill people or torture people, or go burning coal or anything.\nHe makes everybody fight each other, makes everybody fear each other, hate each other, and then kill each other.\nSo that's the movie about Moloch, you know. - Love is the answer, that seems to be,\none of the ways to fight Moloch is by compassion,\nby seeing the common humanity. - Yes, yes. And to not sound, so we don't sound like\na bunch of Kumbaya tree huggers here, right? (Lex laughing) We're not just saying \"Love and peace, man.\"\nWe're trying to actually help people understand the true facts about the other side,\nand feel the compassion because,\nit's that truth makes you more compassionate, right?\nSo that's why I really like using AI for truth and for truth-seeking technologies.\nthat can as a result, you know, will get us more love than hate.\nAnd even if you can't get love, you know, let's settle for some understanding\nwhich already gives compassion. If someone is like, you know, \"I really disagree with you Lex,\nbut I can see where you're coming from. You're not a bad person who needs to be destroyed,\nbut I disagree with you and I'm happy to have an argument about it,\" you know? That's a lot of progress compared to where we are at 2023 in the public space,\nwouldn't you say? - If we solve the AI safety problem, as we've talked about,\n"}
{"pod": "Lex Fridman Podcast", "input": "Questions for AGI", "output": "and then you, Max Tegmark, who has been talking about this for many years,\nget to sit down with the AGI, with the early AGI system on a beach with a drink, (Max chuckles)\nWhat would you ask her? What kind of question would you ask? What would you talk about?\nSomething so much smarter than you, would you be afraid? - I knew you were gonna get me\nwith a really zinger of a question. That's a good one. - Would you be afraid to ask some questions?\n- No, I'm not afraid of the truth. (Lex laughing) I'm very humble. I know I'm just a meat bag with all these flaws, you know?\nBut yeah, I mean, we talked a lot about the Homo sentiens, I've really already tried that for a long time with myself.\nAnd that is what's really valuable about being alive for me, is that I have these meaningful experiences.\nIt's not that I'm good at this, or good at that or whatever. There's so much I suck at, and...\n- So you're not afraid for the system to show you just how dumb you are. - No, no. In fact, my son reminds me of that\npretty frequently. (laughs) - You could find out how dumb you are in terms of physics, how little we humans understand.\n- I'm cool with that. I think, so I can't waffle my way out of this question,\nit's a fair one and it's tough. I think, given that I'm a really, really curious person,\nthat's really the defining part of who I am, I'm so curious.\nI have some physics questions. (Lex laughing) I love to understand.\nI have some questions about consciousness, about the nature of reality, I would just really, really love to understand also.\nI can tell you one for example, that I've been obsessing about a lot recently.\nSo I believe that, so suppose Tononi is right. and suppose there are some information processing systems\nthat are conscious and some that are not. Suppose you can even make reasonably smart things like GPT-4 that are not conscious,\nbut you can also make them conscious. Here is the question that keeps me awake at night.\nIs it the case that the unconscious zombie systems that are really intelligent are also really efficient?\nSorry, really inefficient? So that when you try to make things more efficient, we will naturally be a pressure to do,\nthey become conscious. I'm kind of hoping that that's correct, and I,\ndo you want me to give you, you can hand-wave the argument for it? - Yes, please. - You know like,\nIn my lab again, every time we look at how these large language models do something, we see that they do them in really dumb ways,\nand you could make it make it better. If you, we have loops in our computer language for a reason,\nthe code would get way, way longer if you weren't allowed to use them, right? It's more efficient to have the loops\nand in order to have self-reflection whether it's conscious or not, right?\nEven an operating system knows things about itself, right? You need to have loops already, right?\nSo I think this is, I'm waving my hands a lot, but I suspect that,\nthe most efficient way of implementing a given level of intelligence, has loops in it,\nthe self-reflection, and will be conscious.\n- Isn't that great news? - Yes, if it's true, it's wonderful. 'Cause then we don't have to fear the ultimate zombie apocalypse.\nAnd I think if you look at our brains, actually. Our brains are part zombie and part conscious.\nWhen I open my eyes, I immediately take all these pixels\nthat hit on my retina, right? And I'm like, \"Oh, that's Lex.\" But I have no freaking clue of how I did that computation.\nIt's actually quite complicated, right? It was only relatively recently, we could even do it well with machines, right?\nYou get a bunch of information processing happening in my retina and then it goes to the lateral geniculate nucleus\nin my thalamus, and the area V1, V2, V4, and the fusiform face area here,\nthat Nancy Kanwisher at MIT invented, and blah, blah, blah, blah, blah. And I have no frigging clue how that worked, right?\nIt feels to me subjectively, like my conscious module just got a little email saying,\n\"Facial processing task complete, it's Lex.\"\n- [Lex] Yeah. - And I'm gonna just go with that, right? So this fits perfectly with Tononi's model,\nbecause this was all one-way information processing mainly.\nAnd it turned out for that particular task, that's all you needed. And it probably was kind of the most efficient way to do it.\nBut there were a lot of other things that we associated with higher intelligence and planning, and so on, and so forth,\nwhere you kind of wanna have loops and be able to ruminate and self-reflect, and introspect, and so on.\nWhere my hunch is that if you want to fake that with a zombie system that just all goes one way,\nyou have to like unroll those loops, and it gets really, really long, and it's much more inefficient. So I'm actually hopeful that AI,\nif in the future we have all these various sublime and interesting machines that do cool things, and are aligned with us,\nthat they will be at least, they will also have consciousness for kind of these things that we do.\n- That great intelligence is also correlated to great consciousness, or a deep kind of consciousness.\n- Yes, so that's a happy thought for me 'cause the zombie apocalypse really,\nis my worst nightmare of all. It would be like adding insult to injury, not only did we get replaced,\nbut we frigging replaced ourselves by zombies, like, how dumb can we be?\n- That's such a beautiful vision, and that's actually a provable one. That's one that we humans can intuitively prove\nthat those two things are correlated, as we start to understand what it means to be intelligent,\nand what it means to be conscious, which these systems, early AGI-like systems will help us understand.\nAnd I just wanna say one more thing, which is super important. Most of my colleagues, when I started going on about consciousness\ntell me that it's all bullshit and I should stop talking about it. I hear a little inner voice\nfrom my father and from my mom saying, \"Keep talking about it,\" 'cause I think they're wrong. And the main way to convince people like that,\nthat they're wrong if they say that consciousness is just equal to intelligence, is to ask them what's wrong with torture?\nOr why are you against torture? if it's just about, you know,\nthese particles moving this way rather than that way, and there is no such thing as subjective experience,\nwhat's wrong with torture? I mean, do you have a good comeback to that? - No, it seems like suffering.\nSuffering imposed unto other humans is somehow deeply wrong in a way\nthat intelligence doesn't quite explain. - And if someone tells me, well, you know, it's just an illusion,\nconsciousness, whatever, you know.\nI would like to invite them the next time they're having surgery, to do it without anesthesia.\nLike what is anesthesia really doing? If you have it, you can have a local anesthesia when you're awake. I have that when they fixed my shoulder, right?\nIt's super entertaining. What was that that it did? it just removed my subjective experience of pain.\nIt didn't change anything about what was actually happening in my shoulder, right? So if someone says, \"That's all bullshit,\"\nSkip the anesthesia, that's my advice. This is incredibly central. - It could be fundamental to whatever\nthis thing we have going on here. - It is fundamental because we're, what we feel that's so fundamental,\nis suffering and joy, and pleasure, and meaning, and,\nthose are all subjective experiences there. And let's not, those are the elephant in the room,\nthat's what makes life worth living. And that's what can make it horrible if it's just a bunch of suffering. So let's not make the mistake\nof saying that that's all bullshit. - And let's not make the mistake of not instilling the AI systems\nwith that same thing that makes us special. - [Max] Yeah.\n- Max, it's a huge honor that you would sit down to me the first time on the first episode of this podcast.\nIt's a huge honor you sit down with me again and talk about this, what I think is the most important topic,\nthe most important problem that we humans have to face and hopefully solve.\n- Yeah, well, the honor is all mine and I'm so grateful to you for making more people aware of this fact\nthat humanity has reached the most important fork in the road ever in its history. And let's turn in the correct direction.\n- Thanks for listening to this conversation with Max Tegmark. To support this podcast. Please check out our sponsors in the description.\nAnd now let me leave you with some words from Frank Herbert. \"History is a constant race\nbetween invention and catastrophe.\" Thank you for listening, and hope to see you next time.\n"}
